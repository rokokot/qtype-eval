SLURM_JOB_ID: 64466523
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 19:53:59 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Running experiment: probe_layer2_complexity_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:55:00,252][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/id
experiment_name: probe_layer2_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 19:55:00,252][__main__][INFO] - Normalized task: complexity
[2025-05-07 19:55:00,252][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:55:00,252][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:55:00,257][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-07 19:55:00,257][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:55:04,620][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:55:07,018][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:55:07,019][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:55:07,450][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:07,625][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:07,960][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 19:55:07,967][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:55:07,967][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 19:55:07,970][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:55:08,069][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:08,218][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:08,252][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 19:55:08,254][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:55:08,254][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 19:55:08,257][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:55:08,384][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:08,506][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:55:08,548][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 19:55:08,550][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:55:08,550][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 19:55:08,552][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 19:55:08,552][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:55:08,552][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:55:08,553][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:55:08,553][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:55:08,553][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:55:08,554][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:55:08,554][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:55:08,554][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:55:08,554][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:55:08,555][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-07 19:55:08,555][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 19:55:08,555][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-07 19:55:08,555][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 19:55:08,555][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:55:08,555][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:55:08,555][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 19:55:08,556][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:55:19,141][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:55:19,142][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:55:19,142][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:55:19,142][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:55:19,145][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:55:19,145][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:55:19,145][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:55:19,145][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:55:19,146][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 19:55:19,146][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:55:19,147][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3902Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4457Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5132Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4859Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4707Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4544Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4883Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4878Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4987Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4960Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4766Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4718Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4606Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4597Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4481Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4413Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4402Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4457Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4314Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4250Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4183Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4202Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4142Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4053Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4022Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4002Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3922Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3873Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3867Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3873Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3826Epoch 1/15: [================              ] 32/60 batches, loss: 0.3785Epoch 1/15: [================              ] 33/60 batches, loss: 0.3744Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3723Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3675Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3658Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3588Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3542Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3499Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3458Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3428Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3417Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3390Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3368Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3352Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3311Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3290Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3258Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3236Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3205Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3191Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3170Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3136Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3142Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3126Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3120Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3135Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3119Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3108Epoch 1/15: [==============================] 60/60 batches, loss: 0.3081
[2025-05-07 19:55:27,760][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3081
[2025-05-07 19:55:28,061][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1082, Metrics: {'mse': 0.10482309758663177, 'rmse': 0.32376395350105264, 'r2': -1.5071804523468018}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3154Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2127Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1877Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1883Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1654Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1823Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1854Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1913Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1955Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1865Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1879Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1853Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1908Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1944Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1879Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1876Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1899Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1883Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1916Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1927Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1888Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1925Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1916Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1870Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1888Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1856Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1839Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1819Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1798Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1776Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1764Epoch 2/15: [================              ] 32/60 batches, loss: 0.1749Epoch 2/15: [================              ] 33/60 batches, loss: 0.1732Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1722Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1738Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1726Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1730Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1722Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1728Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1730Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1739Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1727Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1720Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1736Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1719Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1712Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1690Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1680Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1679Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1669Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1652Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1645Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1630Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1623Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1613Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1602Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1603Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1604Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1597Epoch 2/15: [==============================] 60/60 batches, loss: 0.1580
[2025-05-07 19:55:30,385][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1580
[2025-05-07 19:55:30,731][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1101, Metrics: {'mse': 0.10646968334913254, 'rmse': 0.3262969251297544, 'r2': -1.5465636253356934}
[2025-05-07 19:55:30,732][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1117Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1291Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1129Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1298Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1293Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1250Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1308Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1352Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1291Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1244Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1263Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1297Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1294Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1269Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1227Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1219Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1195Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1175Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1152Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1171Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1148Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1148Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1128Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1106Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1106Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1117Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1109Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1087Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1084Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1069Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1082Epoch 3/15: [================              ] 32/60 batches, loss: 0.1087Epoch 3/15: [================              ] 33/60 batches, loss: 0.1121Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1123Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1120Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1131Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1128Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1122Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1127Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1117Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1121Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1116Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1114Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1108Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1096Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1094Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1094Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1101Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1111Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1105Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1096Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1104Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1096Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1105Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1108Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1099Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1099Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1090Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1083Epoch 3/15: [==============================] 60/60 batches, loss: 0.1109
[2025-05-07 19:55:32,701][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1109
[2025-05-07 19:55:33,059][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0964, Metrics: {'mse': 0.09329430013895035, 'rmse': 0.3054411565898583, 'r2': -1.2314321994781494}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0608Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0875Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1025Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1201Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1113Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1170Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1166Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1087Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1035Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0967Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1011Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0984Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0961Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0944Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0943Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0953Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0995Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0994Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1004Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1017Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1039Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1044Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1041Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1088Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1071Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1091Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1073Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1066Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1069Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1060Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1055Epoch 4/15: [================              ] 32/60 batches, loss: 0.1059Epoch 4/15: [================              ] 33/60 batches, loss: 0.1046Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1047Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1040Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1025Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1015Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1015Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1011Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1004Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1012Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1021Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1022Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1013Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1003Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0995Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0982Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0977Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0970Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0971Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0974Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0973Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0967Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0960Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0960Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0971Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0963Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0961Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0962Epoch 4/15: [==============================] 60/60 batches, loss: 0.0954
[2025-05-07 19:55:35,413][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0954
[2025-05-07 19:55:35,712][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0786, Metrics: {'mse': 0.0765729770064354, 'rmse': 0.27671822673332414, 'r2': -0.8314880132675171}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1062Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1032Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0984Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0997Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1017Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1033Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1004Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0969Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0946Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0934Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0933Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0957Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0989Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0992Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0973Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0985Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0977Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0963Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0949Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0981Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0967Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0958Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0964Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0958Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0959Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0964Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0955Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0958Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0982Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0994Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1002Epoch 5/15: [================              ] 32/60 batches, loss: 0.0990Epoch 5/15: [================              ] 33/60 batches, loss: 0.0985Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0980Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0995Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0990Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0984Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0971Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0957Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0949Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0943Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0937Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0941Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0932Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0934Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0922Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0915Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0916Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0909Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0903Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0894Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0886Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0884Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0877Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0870Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0863Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0859Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0864Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0856Epoch 5/15: [==============================] 60/60 batches, loss: 0.0849
[2025-05-07 19:55:37,964][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0849
[2025-05-07 19:55:38,290][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0676, Metrics: {'mse': 0.06589873135089874, 'rmse': 0.2567074820703494, 'r2': -0.5761792659759521}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0506Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0508Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0648Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0578Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0622Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0587Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0658Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0737Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0769Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0752Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0720Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0722Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0744Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0753Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0755Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0754Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0742Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0727Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0728Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0732Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0724Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0736Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0729Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0718Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0718Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0716Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0711Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0703Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0704Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0712Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0704Epoch 6/15: [================              ] 32/60 batches, loss: 0.0706Epoch 6/15: [================              ] 33/60 batches, loss: 0.0718Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0725Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0720Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0708Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0710Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0711Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0712Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0712Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0708Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0711Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0706Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0708Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0707Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0700Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0701Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0696Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0693Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0693Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0712Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0712Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0713Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0710Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0715Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0714Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0709Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0704Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0697Epoch 6/15: [==============================] 60/60 batches, loss: 0.0693
[2025-05-07 19:55:40,624][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0693
[2025-05-07 19:55:40,933][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0603, Metrics: {'mse': 0.05895182117819786, 'rmse': 0.24279996124010783, 'r2': -0.4100217819213867}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0307Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0335Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0542Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0520Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0556Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0635Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0669Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0667Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0653Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0605Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0578Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0589Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0586Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0611Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0619Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0611Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0598Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0592Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0588Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0584Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0588Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0587Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0615Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0628Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0616Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0609Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0607Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0612Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0628Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0627Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0633Epoch 7/15: [================              ] 32/60 batches, loss: 0.0629Epoch 7/15: [================              ] 33/60 batches, loss: 0.0640Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0634Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0630Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0624Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0623Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0615Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0619Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0622Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0622Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0635Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0627Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0621Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0625Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0626Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0622Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0625Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0629Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0625Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0628Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0623Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0623Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0617Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0615Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0607Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0608Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0604Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0606Epoch 7/15: [==============================] 60/60 batches, loss: 0.0613
[2025-05-07 19:55:43,271][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0613
[2025-05-07 19:55:43,574][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0637, Metrics: {'mse': 0.062178418040275574, 'rmse': 0.24935600662561866, 'r2': -0.4871962070465088}
[2025-05-07 19:55:43,575][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0453Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0568Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0608Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0548Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0554Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0548Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0542Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0579Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0567Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0591Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0557Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0551Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0531Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0546Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0556Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0552Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0562Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0559Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0559Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0559Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0564Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0582Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0587Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0598Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0583Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0574Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0581Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0576Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0575Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0568Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0566Epoch 8/15: [================              ] 32/60 batches, loss: 0.0559Epoch 8/15: [================              ] 33/60 batches, loss: 0.0554Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0555Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0558Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0552Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0566Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0562Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0569Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0566Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0574Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0575Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0569Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0572Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0573Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0567Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0567Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0571Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0577Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0580Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0583Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0586Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0583Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0579Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0587Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0594Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0597Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0605Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0600Epoch 8/15: [==============================] 60/60 batches, loss: 0.0595
[2025-05-07 19:55:45,471][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0595
[2025-05-07 19:55:45,802][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0590, Metrics: {'mse': 0.057764094322919846, 'rmse': 0.24034162003889348, 'r2': -0.3816133737564087}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0440Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0385Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0341Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0330Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0420Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0421Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0423Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0437Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0447Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0446Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0502Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0475Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0481Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0502Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0491Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0500Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0497Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0500Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0494Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0496Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0490Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0499Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0502Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0493Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0505Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0509Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0525Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0532Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0536Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0552Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0553Epoch 9/15: [================              ] 32/60 batches, loss: 0.0544Epoch 9/15: [================              ] 33/60 batches, loss: 0.0542Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0538Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0551Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0556Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0560Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0558Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0559Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0561Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0562Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0571Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0565Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0564Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0561Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0562Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0558Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0555Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0559Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0554Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0551Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0551Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0549Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0545Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0543Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0541Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0539Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0543Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0543Epoch 9/15: [==============================] 60/60 batches, loss: 0.0537
[2025-05-07 19:55:48,135][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0537
[2025-05-07 19:55:48,546][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0488, Metrics: {'mse': 0.047724973410367966, 'rmse': 0.21846046189269117, 'r2': -0.1414957046508789}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0431Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0457Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0530Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0528Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0503Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0456Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0448Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0463Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0477Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0535Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0513Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0499Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0528Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0527Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0533Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0525Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0512Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0515Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0516Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0531Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0521Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0524Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0526Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0528Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0534Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0524Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0525Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0519Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0523Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0514Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0510Epoch 10/15: [================              ] 32/60 batches, loss: 0.0526Epoch 10/15: [================              ] 33/60 batches, loss: 0.0528Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0531Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0527Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0524Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0522Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0521Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0525Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0522Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0529Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0529Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0523Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0522Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0523Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0521Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0516Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0515Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0516Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0515Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0511Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0506Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0503Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0500Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0496Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0494Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0500Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0504Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0503Epoch 10/15: [==============================] 60/60 batches, loss: 0.0508
[2025-05-07 19:55:50,912][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0508
[2025-05-07 19:55:51,250][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0455, Metrics: {'mse': 0.04461163282394409, 'rmse': 0.21121466053270094, 'r2': -0.06703019142150879}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0657Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0574Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0468Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0465Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0452Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0450Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0489Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0484Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0499Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0484Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0477Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0522Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0503Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0496Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0489Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0499Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0497Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0530Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0537Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0530Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0519Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0509Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0520Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0514Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0507Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0508Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0504Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0506Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0506Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0503Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0496Epoch 11/15: [================              ] 32/60 batches, loss: 0.0494Epoch 11/15: [================              ] 33/60 batches, loss: 0.0496Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0495Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0498Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0494Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0491Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0483Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0485Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0490Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0495Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0495Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0489Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0489Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0488Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0489Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0487Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0489Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0491Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0484Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0482Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0484Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0486Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0488Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0486Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0490Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0485Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0486Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0491Epoch 11/15: [==============================] 60/60 batches, loss: 0.0492
[2025-05-07 19:55:53,470][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0492
[2025-05-07 19:55:53,751][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0594, Metrics: {'mse': 0.05826668068766594, 'rmse': 0.24138492224591399, 'r2': -0.3936343193054199}
[2025-05-07 19:55:53,752][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0345Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0447Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0404Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0407Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0399Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0436Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0411Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0440Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0444Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0448Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0440Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0453Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0439Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0433Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0443Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0459Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0454Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0465Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0485Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0486Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0474Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0474Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0464Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0482Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0493Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0495Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0494Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0494Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0523Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0519Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0514Epoch 12/15: [================              ] 32/60 batches, loss: 0.0509Epoch 12/15: [================              ] 33/60 batches, loss: 0.0503Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0507Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0502Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0497Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0494Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0495Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0494Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0489Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0500Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0500Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0502Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0508Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0504Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0504Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0504Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0498Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0497Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0491Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0498Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0492Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0493Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0491Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0492Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0492Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0496Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0493Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0491Epoch 12/15: [==============================] 60/60 batches, loss: 0.0486
[2025-05-07 19:55:55,683][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0486
[2025-05-07 19:55:56,077][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0624, Metrics: {'mse': 0.06129618361592293, 'rmse': 0.24758066082778543, 'r2': -0.4660947322845459}
[2025-05-07 19:55:56,077][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0308Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0456Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0460Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0462Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0437Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0409Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0422Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0422Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0422Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0420Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0407Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0412Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0422Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0443Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0461Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0458Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0463Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0460Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0448Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0466Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0466Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0469Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0466Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0467Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0469Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0458Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0465Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0462Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0454Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0451Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0441Epoch 13/15: [================              ] 32/60 batches, loss: 0.0437Epoch 13/15: [================              ] 33/60 batches, loss: 0.0440Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0440Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0439Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0436Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0441Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0439Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0439Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0434Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0447Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0441Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0439Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0443Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0445Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0448Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0447Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0446Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0448Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0449Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0447Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0450Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0447Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0446Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0449Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0449Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0450Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0454Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0451Epoch 13/15: [==============================] 60/60 batches, loss: 0.0449
[2025-05-07 19:55:58,046][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0449
[2025-05-07 19:55:58,410][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0417, Metrics: {'mse': 0.04109001159667969, 'rmse': 0.20270671325015283, 'r2': 0.017200708389282227}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0261Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0330Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0401Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0372Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0368Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0375Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0369Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0392Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0381Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0409Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0392Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0411Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0417Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0432Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0424Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0419Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0421Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0423Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0419Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0422Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0438Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0439Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0429Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0428Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0431Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0419Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0422Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0418Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0418Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0422Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0428Epoch 14/15: [================              ] 32/60 batches, loss: 0.0423Epoch 14/15: [================              ] 33/60 batches, loss: 0.0419Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0418Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0418Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0425Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0429Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0428Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0429Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0429Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0429Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0431Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0429Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0436Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0432Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0436Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0438Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0438Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0451Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0459Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0456Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0453Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0449Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0449Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0448Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0444Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0445Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0446Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0440Epoch 14/15: [==============================] 60/60 batches, loss: 0.0440
[2025-05-07 19:56:00,789][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0440
[2025-05-07 19:56:01,310][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0504, Metrics: {'mse': 0.049834176898002625, 'rmse': 0.2232356980816523, 'r2': -0.19194412231445312}
[2025-05-07 19:56:01,311][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0372Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0328Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0393Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0335Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0370Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0375Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0418Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0423Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0425Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0417Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0423Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0429Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0413Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0418Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0407Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0409Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0409Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0406Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0409Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0401Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0394Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0386Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0386Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0393Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0399Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0416Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0412Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0406Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0400Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0400Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0400Epoch 15/15: [================              ] 32/60 batches, loss: 0.0399Epoch 15/15: [================              ] 33/60 batches, loss: 0.0396Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0392Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0390Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0389Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0387Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0384Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0387Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0392Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0387Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0385Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0384Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0385Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0386Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0391Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0388Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0382Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0380Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0386Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0389Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0387Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0387Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0384Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0390Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0393Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0393Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0394Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0398Epoch 15/15: [==============================] 60/60 batches, loss: 0.0394
[2025-05-07 19:56:03,314][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0394
[2025-05-07 19:56:03,801][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0376, Metrics: {'mse': 0.03725246712565422, 'rmse': 0.19300898198180888, 'r2': 0.10898792743682861}
[2025-05-07 19:56:04,226][src.training.lm_trainer][INFO] - Training completed in 39.08 seconds
[2025-05-07 19:56:04,226][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:56:07,158][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021579589694738388, 'rmse': 0.14689993088745273, 'r2': 0.40530359745025635}
[2025-05-07 19:56:07,158][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03725246712565422, 'rmse': 0.19300898198180888, 'r2': 0.10898792743682861}
[2025-05-07 19:56:07,158][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.025125887244939804, 'rmse': 0.15851147354352557, 'r2': 0.38377833366394043}
[2025-05-07 19:56:09,484][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/id/id/model.pt
[2025-05-07 19:56:09,485][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▃▃▂▂▁▁
wandb:     best_val_mse █▇▅▄▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇▇██
wandb:    best_val_rmse █▇▅▄▄▄▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▄▅▆▆▆▇▇▆▆▇▇
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▇▅▄▃▄▃▂▂▃▃▁▂▁
wandb:          val_mse ██▇▅▄▃▄▃▂▂▃▃▁▂▁
wandb:           val_r2 ▁▁▂▄▅▆▅▆▇▇▆▆█▇█
wandb:         val_rmse ██▇▅▄▄▄▃▂▂▄▄▂▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03763
wandb:     best_val_mse 0.03725
wandb:      best_val_r2 0.10899
wandb:    best_val_rmse 0.19301
wandb:            epoch 15
wandb:   final_test_mse 0.02513
wandb:    final_test_r2 0.38378
wandb:  final_test_rmse 0.15851
wandb:  final_train_mse 0.02158
wandb:   final_train_r2 0.4053
wandb: final_train_rmse 0.1469
wandb:    final_val_mse 0.03725
wandb:     final_val_r2 0.10899
wandb:   final_val_rmse 0.19301
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03943
wandb:       train_time 39.08294
wandb:         val_loss 0.03763
wandb:          val_mse 0.03725
wandb:           val_r2 0.10899
wandb:         val_rmse 0.19301
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195500-4ocxs7xa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195500-4ocxs7xa/logs
Experiment probe_layer2_complexity_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/id/id/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_complexity_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:56:52,939][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/id
experiment_name: probe_layer2_complexity_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 19:56:52,939][__main__][INFO] - Normalized task: complexity
[2025-05-07 19:56:52,939][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:56:52,939][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:56:52,943][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-07 19:56:52,944][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:56:56,589][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:56:59,089][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:56:59,090][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:56:59,356][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 19:56:59,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 19:56:59,900][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 19:56:59,907][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:56:59,908][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 19:56:59,915][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:57:00,022][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:57:00,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:57:00,160][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 19:57:00,162][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:57:00,162][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 19:57:00,166][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:57:00,291][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:57:00,410][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:57:00,441][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 19:57:00,442][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:57:00,442][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 19:57:00,446][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 19:57:00,447][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:57:00,447][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:57:00,447][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:57:00,447][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:57:00,447][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:57:00,447][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Sample label: 0.41827768087387085
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:57:00,448][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:57:00,448][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 19:57:00,448][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:57:00,449][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:57:00,449][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 19:57:00,449][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:57:00,450][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:57:00,450][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 19:57:00,450][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:57:09,417][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:57:09,418][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:57:09,418][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:57:09,418][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:57:09,421][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:57:09,422][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:57:09,422][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:57:09,422][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:57:09,422][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 19:57:09,423][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:57:09,423][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3976Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4263Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4731Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4509Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4138Epoch 1/15: [===                           ] 6/60 batches, loss: 0.3904Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4314Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4323Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4469Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4572Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4418Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4411Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4307Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4219Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4165Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4156Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4073Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4182Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4053Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.3962Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3898Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.3916Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3839Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3796Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3779Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3752Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3714Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3698Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3752Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3805Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3743Epoch 1/15: [================              ] 32/60 batches, loss: 0.3693Epoch 1/15: [================              ] 33/60 batches, loss: 0.3646Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3636Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3606Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3587Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3533Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3482Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3441Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3396Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3391Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3376Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3343Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3337Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3330Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3288Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3278Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3241Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3223Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3189Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3176Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3154Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3129Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3129Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3110Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3122Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3142Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3126Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3097Epoch 1/15: [==============================] 60/60 batches, loss: 0.3064
[2025-05-07 19:57:16,785][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3064
[2025-05-07 19:57:17,102][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1529, Metrics: {'mse': 0.14695309102535248, 'rmse': 0.38334461131644004, 'r2': -2.5148541927337646}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3603Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2744Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2523Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2409Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2133Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2311Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2263Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2361Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2375Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2378Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2308Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2294Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2315Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2300Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2205Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2177Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2155Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2106Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2104Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2103Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2051Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2060Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2067Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2023Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2063Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2039Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2031Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2028Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1995Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1977Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1960Epoch 2/15: [================              ] 32/60 batches, loss: 0.1918Epoch 2/15: [================              ] 33/60 batches, loss: 0.1892Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1905Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1907Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1911Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1902Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1892Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1884Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1899Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1894Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1877Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1869Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1879Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1867Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1848Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1835Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1827Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1830Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1830Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1806Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1795Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1795Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1785Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1779Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1768Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1757Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1751Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1737Epoch 2/15: [==============================] 60/60 batches, loss: 0.1717
[2025-05-07 19:57:19,400][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1717
[2025-05-07 19:57:19,770][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1458, Metrics: {'mse': 0.1396961510181427, 'rmse': 0.3737594828471148, 'r2': -2.341280937194824}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0608Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0921Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0872Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1095Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1030Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1090Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1135Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1153Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1128Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1127Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1117Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1100Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1188Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1181Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1162Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1186Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1189Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1174Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1168Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1147Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1145Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1113Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1097Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1097Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1122Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1147Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1147Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1141Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1157Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1148Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1149Epoch 3/15: [================              ] 32/60 batches, loss: 0.1144Epoch 3/15: [================              ] 33/60 batches, loss: 0.1168Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1149Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1138Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1150Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1160Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1155Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1164Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1159Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1162Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1158Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1177Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1177Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1164Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1152Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1156Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1153Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1159Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1167Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1171Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1175Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1164Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1187Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1180Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1170Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1164Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1157Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1155Epoch 3/15: [==============================] 60/60 batches, loss: 0.1172
[2025-05-07 19:57:22,073][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1172
[2025-05-07 19:57:22,406][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1646, Metrics: {'mse': 0.15760816633701324, 'rmse': 0.3969989500452278, 'r2': -2.7697043418884277}
[2025-05-07 19:57:22,407][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0591Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0776Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0818Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1183Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1219Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1257Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1301Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1282Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1234Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1164Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1162Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1163Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1138Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1147Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1105Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1138Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1186Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1156Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1134Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1161Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1217Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1252Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1241Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1268Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1245Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1256Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1246Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1236Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1218Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1200Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1184Epoch 4/15: [================              ] 32/60 batches, loss: 0.1166Epoch 4/15: [================              ] 33/60 batches, loss: 0.1159Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1149Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1146Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1134Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1123Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1127Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1115Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1100Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1103Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1109Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1099Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1095Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1094Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1083Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1070Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1074Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1074Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1067Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1063Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1064Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1060Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1055Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1052Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1048Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1046Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1043Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1035Epoch 4/15: [==============================] 60/60 batches, loss: 0.1027
[2025-05-07 19:57:24,302][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1027
[2025-05-07 19:57:24,564][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1353, Metrics: {'mse': 0.12985944747924805, 'rmse': 0.3603601635575831, 'r2': -2.1060049533843994}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0675Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0752Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0874Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0815Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1085Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1135Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1134Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1193Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1137Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1134Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1170Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1139Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1107Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1115Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1086Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1085Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1063Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1058Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1042Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1073Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1065Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1041Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1049Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1043Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1029Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1038Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1012Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1000Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1007Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1017Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1024Epoch 5/15: [================              ] 32/60 batches, loss: 0.1022Epoch 5/15: [================              ] 33/60 batches, loss: 0.1015Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1027Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1040Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1028Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1028Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1023Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1026Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1014Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1010Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1010Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1015Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1002Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1002Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1011Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1006Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1011Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1005Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1007Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1001Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1003Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1002Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0992Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0985Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0975Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0968Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0968Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0961Epoch 5/15: [==============================] 60/60 batches, loss: 0.0950
[2025-05-07 19:57:26,801][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0950
[2025-05-07 19:57:27,169][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1271, Metrics: {'mse': 0.12173889577388763, 'rmse': 0.3489110141194853, 'r2': -1.911775827407837}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1217Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1074Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0978Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0879Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0845Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0835Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0820Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0924Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0919Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0871Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0846Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0830Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0841Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0882Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0860Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0878Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0858Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0852Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0836Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0818Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0809Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0806Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0813Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0807Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0826Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0837Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0833Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0820Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0815Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0813Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0808Epoch 6/15: [================              ] 32/60 batches, loss: 0.0807Epoch 6/15: [================              ] 33/60 batches, loss: 0.0818Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0811Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0802Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0813Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0808Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0812Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0805Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0803Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0791Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0786Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0776Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0776Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0774Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0770Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0776Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0780Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0776Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0774Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0799Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0796Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0797Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0800Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0799Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0796Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0799Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0793Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0787Epoch 6/15: [==============================] 60/60 batches, loss: 0.0785
[2025-05-07 19:57:29,464][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0785
[2025-05-07 19:57:29,779][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1042, Metrics: {'mse': 0.09990521520376205, 'rmse': 0.31607786256516296, 'r2': -1.3895533084869385}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0461Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0916Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0926Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0890Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0837Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0748Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0713Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0698Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0716Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0685Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0709Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0747Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0794Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0796Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0791Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0801Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0790Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0803Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0801Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0813Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0796Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0824Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0825Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0826Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0830Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0818Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0804Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0793Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0789Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0798Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0789Epoch 7/15: [================              ] 32/60 batches, loss: 0.0782Epoch 7/15: [================              ] 33/60 batches, loss: 0.0777Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0765Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0764Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0768Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0764Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0763Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0757Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0759Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0751Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0759Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0754Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0760Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0752Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0752Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0765Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0766Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0772Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0763Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0764Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0777Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0772Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0778Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0784Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0785Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0783Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0783Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0786Epoch 7/15: [==============================] 60/60 batches, loss: 0.0792
[2025-05-07 19:57:32,073][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0792
[2025-05-07 19:57:32,449][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1068, Metrics: {'mse': 0.1027384102344513, 'rmse': 0.32052832984691276, 'r2': -1.4573183059692383}
[2025-05-07 19:57:32,450][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0790Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0733Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0670Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0660Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0684Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0742Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0679Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0678Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0668Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0704Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0681Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0690Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0679Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0677Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0662Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0655Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0683Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0680Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0692Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0692Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0686Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0678Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0678Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0680Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0687Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0673Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0685Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0698Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0686Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0683Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0682Epoch 8/15: [================              ] 32/60 batches, loss: 0.0684Epoch 8/15: [================              ] 33/60 batches, loss: 0.0675Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0662Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0663Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0669Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0672Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0684Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0686Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0682Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0687Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0679Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0677Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0679Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0677Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0675Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0673Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0676Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0679Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0677Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0679Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0680Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0680Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0675Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0676Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0676Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0674Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0682Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0682Epoch 8/15: [==============================] 60/60 batches, loss: 0.0683
[2025-05-07 19:57:34,386][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0683
[2025-05-07 19:57:34,759][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1118, Metrics: {'mse': 0.10759822279214859, 'rmse': 0.3280216803690704, 'r2': -1.5735564231872559}
[2025-05-07 19:57:34,760][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0966Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0811Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0714Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0645Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0709Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0654Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0604Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0650Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0631Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0598Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0635Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0616Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0627Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0654Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0653Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0641Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0647Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0654Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0655Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0662Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0662Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0679Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0680Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0672Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0680Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0681Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0681Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0705Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0703Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0708Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0704Epoch 9/15: [================              ] 32/60 batches, loss: 0.0694Epoch 9/15: [================              ] 33/60 batches, loss: 0.0687Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0682Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0681Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0679Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0681Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0677Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0671Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0676Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0676Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0676Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0674Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0674Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0674Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0668Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0667Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0665Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0666Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0672Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0675Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0677Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0668Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0662Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0658Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0656Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0655Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0657Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0650Epoch 9/15: [==============================] 60/60 batches, loss: 0.0644
[2025-05-07 19:57:36,751][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0644
[2025-05-07 19:57:37,059][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0874, Metrics: {'mse': 0.0842680037021637, 'rmse': 0.2902895170380145, 'r2': -1.0155391693115234}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0959Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0927Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0795Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0787Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0794Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0758Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0750Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0723Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0712Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0686Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0645Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0613Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0641Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0639Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0632Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0619Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0624Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0636Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0629Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0622Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0605Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0599Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0589Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0595Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0587Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0589Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0591Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0606Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0599Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0610Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0610Epoch 10/15: [================              ] 32/60 batches, loss: 0.0617Epoch 10/15: [================              ] 33/60 batches, loss: 0.0627Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0625Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0626Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0625Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0623Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0617Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0618Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0619Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0613Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0613Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0612Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0606Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0604Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0600Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0612Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0612Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0611Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0612Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0606Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0604Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0600Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0599Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0598Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0597Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0602Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0600Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0599Epoch 10/15: [==============================] 60/60 batches, loss: 0.0594
[2025-05-07 19:57:39,393][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0594
[2025-05-07 19:57:39,734][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0838, Metrics: {'mse': 0.08088572323322296, 'rmse': 0.2844041547397347, 'r2': -0.9346412420272827}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0534Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0600Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0620Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0570Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0539Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0536Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0606Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0583Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0571Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0580Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0582Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0567Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0550Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0568Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0583Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0567Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0552Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0563Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0561Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0557Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0558Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0553Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0547Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0542Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0545Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0535Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0535Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0526Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0528Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0543Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0541Epoch 11/15: [================              ] 32/60 batches, loss: 0.0532Epoch 11/15: [================              ] 33/60 batches, loss: 0.0533Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0535Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0532Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0536Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0538Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0535Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0539Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0540Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0550Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0549Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0551Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0549Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0550Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0550Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0552Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0553Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0553Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0551Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0551Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0551Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0545Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0549Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0548Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0555Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0557Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0553Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0558Epoch 11/15: [==============================] 60/60 batches, loss: 0.0555
[2025-05-07 19:57:42,000][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0555
[2025-05-07 19:57:42,302][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1025, Metrics: {'mse': 0.098978191614151, 'rmse': 0.3146079967422173, 'r2': -1.3673806190490723}
[2025-05-07 19:57:42,303][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0343Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0667Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0603Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0723Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0701Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0636Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0601Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0574Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0582Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0571Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0575Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0577Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0574Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0585Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0587Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0616Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0606Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0605Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0592Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0607Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0595Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0590Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0586Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0578Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0602Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0599Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0589Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0593Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0594Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0597Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0596Epoch 12/15: [================              ] 32/60 batches, loss: 0.0596Epoch 12/15: [================              ] 33/60 batches, loss: 0.0590Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0596Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0592Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0584Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0577Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0571Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0576Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0579Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0587Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0584Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0584Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0587Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0587Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0590Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0588Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0588Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0587Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0584Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0583Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0585Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0587Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0598Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0595Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0593Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0592Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0586Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0581Epoch 12/15: [==============================] 60/60 batches, loss: 0.0582
[2025-05-07 19:57:44,270][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0582
[2025-05-07 19:57:44,657][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0823, Metrics: {'mse': 0.0796254351735115, 'rmse': 0.28217979228412426, 'r2': -0.9044973850250244}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0608Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0609Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0518Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0561Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0584Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0530Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0503Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0484Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0493Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0521Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0502Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0487Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0513Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0527Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0509Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0512Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0507Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0507Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0505Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0506Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0519Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0518Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0525Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0536Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0541Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0535Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0534Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0529Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0531Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0533Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0536Epoch 13/15: [================              ] 32/60 batches, loss: 0.0535Epoch 13/15: [================              ] 33/60 batches, loss: 0.0526Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0531Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0534Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0540Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0541Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0537Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0532Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0529Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0539Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0548Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0549Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0548Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0553Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0555Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0548Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0551Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0550Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0552Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0556Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0553Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0550Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0549Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0545Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0544Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0542Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0540Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0538Epoch 13/15: [==============================] 60/60 batches, loss: 0.0533
[2025-05-07 19:57:46,956][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0533
[2025-05-07 19:57:47,410][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0954, Metrics: {'mse': 0.09222739189863205, 'rmse': 0.30368963087111167, 'r2': -1.2059135437011719}
[2025-05-07 19:57:47,411][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0417Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0654Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0572Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0558Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0562Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0569Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0568Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0557Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0579Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0566Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0587Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0571Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0559Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0581Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0560Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0542Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0537Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0550Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0543Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0559Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0561Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0560Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0556Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0549Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0540Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0532Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0539Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0535Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0531Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0537Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0533Epoch 14/15: [================              ] 32/60 batches, loss: 0.0533Epoch 14/15: [================              ] 33/60 batches, loss: 0.0532Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0529Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0523Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0521Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0527Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0523Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0535Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0531Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0529Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0525Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0524Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0529Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0525Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0531Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0528Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0529Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0540Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0548Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0543Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0542Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0536Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0533Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0532Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0530Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0525Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0524Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0525Epoch 14/15: [==============================] 60/60 batches, loss: 0.0520
[2025-05-07 19:57:49,317][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0520
[2025-05-07 19:57:49,748][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0708, Metrics: {'mse': 0.0686541423201561, 'rmse': 0.2620193548579114, 'r2': -0.6420837640762329}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0415Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0462Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0447Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0436Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0499Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0507Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0513Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0517Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0505Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0530Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0514Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0513Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0510Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0496Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0491Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0477Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0474Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0476Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0478Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0473Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0489Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0486Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0475Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0494Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0491Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0510Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0516Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0509Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0512Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0513Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0510Epoch 15/15: [================              ] 32/60 batches, loss: 0.0501Epoch 15/15: [================              ] 33/60 batches, loss: 0.0499Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0497Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0493Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0493Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0493Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0489Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0492Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0496Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0500Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0499Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0499Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0492Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0487Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0498Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0501Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0501Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0500Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0496Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0498Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0496Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0495Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0490Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0486Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0488Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0492Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0489Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0489Epoch 15/15: [==============================] 60/60 batches, loss: 0.0489
[2025-05-07 19:57:52,154][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0489
[2025-05-07 19:57:52,563][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0951, Metrics: {'mse': 0.09214990586042404, 'rmse': 0.30356202967503043, 'r2': -1.2040603160858154}
[2025-05-07 19:57:52,563][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 19:57:52,564][src.training.lm_trainer][INFO] - Training completed in 38.46 seconds
[2025-05-07 19:57:52,564][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:57:55,523][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03676700219511986, 'rmse': 0.19174723516942782, 'r2': -0.013235330581665039}
[2025-05-07 19:57:55,523][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0686541423201561, 'rmse': 0.2620193548579114, 'r2': -0.6420837640762329}
[2025-05-07 19:57:55,523][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04702572524547577, 'rmse': 0.21685415662485183, 'r2': -0.15332317352294922}
[2025-05-07 19:57:58,175][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/id/id/model.pt
[2025-05-07 19:57:58,176][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▆▄▂▂▂▁
wandb:     best_val_mse █▇▆▆▄▂▂▂▁
wandb:      best_val_r2 ▁▂▃▃▅▇▇▇█
wandb:    best_val_rmse █▇▇▆▄▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▃▁▃▄▅▅▅▆▆▅▆▆▇
wandb:       train_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇▇█▆▅▃▄▄▂▂▃▂▃▁▃
wandb:          val_mse ▇▇█▆▅▃▄▄▂▂▃▂▃▁▃
wandb:           val_r2 ▂▂▁▃▄▆▅▅▇▇▆▇▆█▆
wandb:         val_rmse ▇▇█▆▆▄▄▄▂▂▄▂▃▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07083
wandb:     best_val_mse 0.06865
wandb:      best_val_r2 -0.64208
wandb:    best_val_rmse 0.26202
wandb:            epoch 15
wandb:   final_test_mse 0.04703
wandb:    final_test_r2 -0.15332
wandb:  final_test_rmse 0.21685
wandb:  final_train_mse 0.03677
wandb:   final_train_r2 -0.01324
wandb: final_train_rmse 0.19175
wandb:    final_val_mse 0.06865
wandb:     final_val_r2 -0.64208
wandb:   final_val_rmse 0.26202
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0489
wandb:       train_time 38.46287
wandb:         val_loss 0.09514
wandb:          val_mse 0.09215
wandb:           val_r2 -1.20406
wandb:         val_rmse 0.30356
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195653-k4cgvn7j
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195653-k4cgvn7j/logs
Experiment probe_layer2_complexity_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_complexity_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 19:58:45,397][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/id
experiment_name: probe_layer2_complexity_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 19:58:45,397][__main__][INFO] - Normalized task: complexity
[2025-05-07 19:58:45,397][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 19:58:45,397][__main__][INFO] - Determined Task Type: regression
[2025-05-07 19:58:45,402][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-07 19:58:45,402][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 19:58:49,323][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 19:58:51,653][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 19:58:51,654][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:58:51,903][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 19:58:51,989][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 19:58:52,246][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 19:58:52,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:58:52,254][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 19:58:52,257][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:58:52,349][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:58:52,436][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:58:52,481][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 19:58:52,482][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:58:52,482][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 19:58:52,484][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 19:58:52,533][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:58:52,597][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 19:58:52,631][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 19:58:52,633][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 19:58:52,633][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 19:58:52,656][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 19:58:52,657][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:58:52,657][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:58:52,657][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:58:52,657][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:58:52,658][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:58:52,658][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:58:52,658][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:58:52,658][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:58:52,659][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 19:58:52,659][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 19:58:52,659][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-07 19:58:52,659][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 19:58:52,660][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-07 19:58:52,660][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 19:58:52,660][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 19:58:52,660][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 19:58:52,660][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 19:58:52,660][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 19:59:02,116][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 19:59:02,117][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 19:59:02,117][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 19:59:02,117][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 19:59:02,120][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 19:59:02,120][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 19:59:02,120][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 19:59:02,121][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 19:59:02,121][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 19:59:02,122][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 19:59:02,122][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3773Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4632Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4842Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4873Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4627Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4284Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4628Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4709Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4862Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4815Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4664Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4655Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4499Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4479Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4428Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4389Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4349Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4425Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4266Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4152Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4126Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4152Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4064Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4030Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3996Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3976Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3907Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3840Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3851Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3894Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3825Epoch 1/15: [================              ] 32/60 batches, loss: 0.3769Epoch 1/15: [================              ] 33/60 batches, loss: 0.3721Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3702Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3682Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3667Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3591Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3544Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3493Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3468Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3450Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3413Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3386Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3386Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3358Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3324Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3307Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3260Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3243Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3219Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3216Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3216Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3184Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3197Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3188Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3199Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3212Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3190Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3166Epoch 1/15: [==============================] 60/60 batches, loss: 0.3138
[2025-05-07 19:59:09,226][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3138
[2025-05-07 19:59:09,513][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1497, Metrics: {'mse': 0.1435922086238861, 'rmse': 0.3789356259628885, 'r2': -2.4344677925109863}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2323Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1875Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1844Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1885Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1664Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1842Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1819Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1927Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1951Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1881Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1876Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1845Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1874Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1917Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1866Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1883Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1917Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1903Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1869Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1903Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1892Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1902Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1869Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1842Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1863Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1831Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1785Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1779Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1752Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1739Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1747Epoch 2/15: [================              ] 32/60 batches, loss: 0.1747Epoch 2/15: [================              ] 33/60 batches, loss: 0.1730Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1735Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1770Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1755Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1746Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1742Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1754Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1787Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1819Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1816Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1791Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1809Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1783Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1779Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1777Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1778Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1790Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1780Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1773Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1761Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1750Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1732Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1719Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1708Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1700Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1695Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1687Epoch 2/15: [==============================] 60/60 batches, loss: 0.1665
[2025-05-07 19:59:11,818][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1665
[2025-05-07 19:59:12,125][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1639, Metrics: {'mse': 0.1570577174425125, 'rmse': 0.39630508127263836, 'r2': -2.7565386295318604}
[2025-05-07 19:59:12,126][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1571Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1733Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1559Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1626Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1467Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1399Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1354Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1403Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1369Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1296Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1296Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1313Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1336Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1322Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1291Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1273Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1256Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1233Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1217Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1209Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1190Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1189Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1164Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1171Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1186Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1190Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1175Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1143Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1145Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1134Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1117Epoch 3/15: [================              ] 32/60 batches, loss: 0.1119Epoch 3/15: [================              ] 33/60 batches, loss: 0.1144Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1125Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1110Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1128Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1121Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1120Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1117Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1102Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1091Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1101Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1110Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1103Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1091Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1087Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1085Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1080Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1081Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1084Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1084Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1101Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1097Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1117Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1111Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1102Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1099Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1092Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1090Epoch 3/15: [==============================] 60/60 batches, loss: 0.1107
[2025-05-07 19:59:14,061][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1107
[2025-05-07 19:59:14,440][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1394, Metrics: {'mse': 0.13341215252876282, 'rmse': 0.36525628335288474, 'r2': -2.19097900390625}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0652Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1027Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0858Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1214Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1140Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1055Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1148Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1137Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1105Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1024Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1003Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1029Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1051Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1036Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1010Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1029Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1055Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1054Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1039Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1054Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1079Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1084Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1079Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1096Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1090Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1086Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1069Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1092Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1097Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1097Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1086Epoch 4/15: [================              ] 32/60 batches, loss: 0.1068Epoch 4/15: [================              ] 33/60 batches, loss: 0.1059Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1071Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1058Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1035Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1036Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1043Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1048Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1041Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1039Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1047Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1049Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1037Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1031Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1016Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0999Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0997Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0997Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0986Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0994Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1002Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1003Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1000Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1003Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1000Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0991Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0992Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0982Epoch 4/15: [==============================] 60/60 batches, loss: 0.0978
[2025-05-07 19:59:16,719][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0978
[2025-05-07 19:59:17,016][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1129, Metrics: {'mse': 0.1081126257777214, 'rmse': 0.32880484451680664, 'r2': -1.585860013961792}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0796Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0817Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0878Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1104Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1207Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1187Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1089Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1062Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1022Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1046Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1074Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1070Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1069Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1079Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1076Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1112Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1106Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1098Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1104Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1128Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1117Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1110Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1138Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1128Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1119Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1119Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1115Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1093Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1101Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1097Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1084Epoch 5/15: [================              ] 32/60 batches, loss: 0.1072Epoch 5/15: [================              ] 33/60 batches, loss: 0.1053Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1041Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1044Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1035Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1031Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1017Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1017Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1007Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1009Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1007Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1008Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1001Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0996Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0994Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0985Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0991Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0987Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0987Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0978Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0981Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0977Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0975Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0978Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0969Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0970Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0978Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0974Epoch 5/15: [==============================] 60/60 batches, loss: 0.0964
[2025-05-07 19:59:19,285][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0964
[2025-05-07 19:59:19,677][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1168, Metrics: {'mse': 0.11206077039241791, 'rmse': 0.3347547914405676, 'r2': -1.6802923679351807}
[2025-05-07 19:59:19,678][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1001Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0769Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0861Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0784Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0717Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0728Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0743Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0817Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0798Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0812Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0803Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0783Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0785Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0775Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0764Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0812Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0796Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0802Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0795Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0783Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0782Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0786Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0772Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0772Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0788Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0776Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0773Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0771Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0775Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0778Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0766Epoch 6/15: [================              ] 32/60 batches, loss: 0.0778Epoch 6/15: [================              ] 33/60 batches, loss: 0.0781Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0788Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0785Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0785Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0796Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0796Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0790Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0793Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0785Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0793Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0783Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0783Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0790Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0781Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0782Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0780Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0770Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0769Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0781Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0782Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0782Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0776Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0777Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0774Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0775Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0771Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0783Epoch 6/15: [==============================] 60/60 batches, loss: 0.0779
[2025-05-07 19:59:21,599][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0779
[2025-05-07 19:59:21,943][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0851, Metrics: {'mse': 0.0820336788892746, 'rmse': 0.28641522112009793, 'r2': -0.9620983600616455}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0678Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1000Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0847Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0872Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0892Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0932Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0921Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0901Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0915Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0894Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0842Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0850Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0858Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0842Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0912Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0892Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0871Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0862Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0857Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0847Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0818Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0828Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0846Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0872Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0856Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0841Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0823Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0813Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0808Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0803Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0804Epoch 7/15: [================              ] 32/60 batches, loss: 0.0806Epoch 7/15: [================              ] 33/60 batches, loss: 0.0804Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0798Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0794Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0782Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0773Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0761Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0760Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0764Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0754Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0758Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0750Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0751Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0746Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0750Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0750Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0754Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0757Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0758Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0757Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0752Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0753Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0756Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0754Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0751Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0747Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0747Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0748Epoch 7/15: [==============================] 60/60 batches, loss: 0.0757
[2025-05-07 19:59:24,317][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0757
[2025-05-07 19:59:24,643][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1185, Metrics: {'mse': 0.11413612961769104, 'rmse': 0.337840390743456, 'r2': -1.729931116104126}
[2025-05-07 19:59:24,644][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0746Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0588Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0571Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0557Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0587Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0613Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0624Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0629Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0602Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0619Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0623Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0612Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0608Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0607Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0601Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0610Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0604Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0585Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0583Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0590Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0576Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0588Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0613Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0616Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0602Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0595Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0604Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0607Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0606Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0607Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0615Epoch 8/15: [================              ] 32/60 batches, loss: 0.0612Epoch 8/15: [================              ] 33/60 batches, loss: 0.0605Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0605Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0604Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0605Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0607Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0616Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0615Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0615Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0618Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0630Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0639Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0637Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0638Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0634Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0634Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0642Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0642Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0643Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0644Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0639Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0641Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0644Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0645Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0643Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0641Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0641Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0639Epoch 8/15: [==============================] 60/60 batches, loss: 0.0639
[2025-05-07 19:59:26,618][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0639
[2025-05-07 19:59:27,012][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1167, Metrics: {'mse': 0.11194635927677155, 'rmse': 0.3345838598569446, 'r2': -1.677555799484253}
[2025-05-07 19:59:27,012][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1381Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1196Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0938Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0993Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0940Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0862Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0835Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0815Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0755Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0751Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0749Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0759Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0731Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0709Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0728Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0719Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0711Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0697Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0684Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0680Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0699Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0686Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0691Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0677Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0691Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0690Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0689Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0702Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0703Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0707Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0709Epoch 9/15: [================              ] 32/60 batches, loss: 0.0696Epoch 9/15: [================              ] 33/60 batches, loss: 0.0690Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0698Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0709Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0705Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0701Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0696Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0692Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0709Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0711Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0716Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0717Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0712Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0709Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0706Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0703Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0693Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0699Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0697Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0703Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0711Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0705Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0696Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0696Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0693Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0684Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0684Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0684Epoch 9/15: [==============================] 60/60 batches, loss: 0.0690
[2025-05-07 19:59:28,994][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0690
[2025-05-07 19:59:29,374][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0968, Metrics: {'mse': 0.09290602803230286, 'rmse': 0.30480490158838136, 'r2': -1.2221455574035645}
[2025-05-07 19:59:29,374][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0333Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0628Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0694Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0630Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0584Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0547Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0573Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0573Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0596Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0607Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0585Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0598Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0597Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0580Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0586Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0598Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0612Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0594Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0597Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0609Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0605Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0623Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0622Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0627Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0622Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0637Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0635Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0640Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0634Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0631Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0634Epoch 10/15: [================              ] 32/60 batches, loss: 0.0640Epoch 10/15: [================              ] 33/60 batches, loss: 0.0634Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0630Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0648Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0650Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0643Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0650Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0657Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0665Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0664Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0660Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0653Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0654Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0649Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0658Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0659Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0657Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0657Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0657Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0653Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0650Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0645Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0638Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0638Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0636Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0635Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0634Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0634Epoch 10/15: [==============================] 60/60 batches, loss: 0.0629
[2025-05-07 19:59:31,319][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0629
[2025-05-07 19:59:31,724][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0981, Metrics: {'mse': 0.09437084943056107, 'rmse': 0.30719838774082303, 'r2': -1.257181167602539}
[2025-05-07 19:59:31,725][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 19:59:31,725][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 19:59:31,725][src.training.lm_trainer][INFO] - Training completed in 25.05 seconds
[2025-05-07 19:59:31,725][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 19:59:34,509][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03777269646525383, 'rmse': 0.19435199115330368, 'r2': -0.04095053672790527}
[2025-05-07 19:59:34,510][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0820336788892746, 'rmse': 0.28641522112009793, 'r2': -0.9620983600616455}
[2025-05-07 19:59:34,510][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.058384835720062256, 'rmse': 0.24162954231646067, 'r2': -0.43190956115722656}
[2025-05-07 19:59:37,110][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/id/id/model.pt
[2025-05-07 19:59:37,111][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▁
wandb:     best_val_mse █▇▄▁
wandb:      best_val_r2 ▁▂▅█
wandb:    best_val_rmse █▇▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▅▄▆▄▄▆
wandb:       train_loss █▄▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▆▃▄▁▄▄▂▂
wandb:          val_mse ▇█▆▃▄▁▄▄▂▂
wandb:           val_r2 ▂▁▃▆▅█▅▅▇▇
wandb:         val_rmse ▇█▆▄▄▁▄▄▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08508
wandb:     best_val_mse 0.08203
wandb:      best_val_r2 -0.9621
wandb:    best_val_rmse 0.28642
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.05838
wandb:    final_test_r2 -0.43191
wandb:  final_test_rmse 0.24163
wandb:  final_train_mse 0.03777
wandb:   final_train_r2 -0.04095
wandb: final_train_rmse 0.19435
wandb:    final_val_mse 0.08203
wandb:     final_val_r2 -0.9621
wandb:   final_val_rmse 0.28642
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06288
wandb:       train_time 25.05381
wandb:         val_loss 0.09806
wandb:          val_mse 0.09437
wandb:           val_r2 -1.25718
wandb:         val_rmse 0.3072
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195845-o0xpoakl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_195845-o0xpoakl/logs
Experiment probe_layer2_complexity_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_complexity_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:00:17,035][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/id
experiment_name: probe_layer2_complexity_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 20:00:17,035][__main__][INFO] - Normalized task: complexity
[2025-05-07 20:00:17,035][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:00:17,035][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:00:17,039][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-07 20:00:17,039][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:00:22,322][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:00:24,712][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:00:24,713][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:00:25,023][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 20:00:25,192][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 20:00:25,657][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:00:25,664][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:00:25,664][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:00:25,669][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:00:25,826][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:00:26,063][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:00:26,138][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:00:26,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:00:26,139][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:00:26,157][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:00:26,271][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:00:26,421][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:00:26,483][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:00:26,485][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:00:26,485][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:00:26,489][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 20:00:26,490][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:00:26,490][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:00:26,490][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 20:00:26,491][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:00:26,491][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:00:26,491][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 20:00:26,492][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:00:26,492][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:00:26,492][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:00:26,493][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 20:00:26,493][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:00:36,860][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:00:36,861][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:00:36,861][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:00:36,861][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:00:36,864][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:00:36,865][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:00:36,865][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:00:36,865][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:00:36,865][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:00:36,866][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:00:36,866][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4187Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4819Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4738Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5166Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4891Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4650Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4973Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4948Epoch 1/15: [====                          ] 9/60 batches, loss: 0.5049Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5051Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4906Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4805Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4618Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4492Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4458Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4457Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4464Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4585Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4411Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4320Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4244Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4193Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4114Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4048Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4013Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4004Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3946Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3903Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3908Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3960Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3898Epoch 1/15: [================              ] 32/60 batches, loss: 0.3853Epoch 1/15: [================              ] 33/60 batches, loss: 0.3801Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3775Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3750Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3735Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3675Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3635Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3620Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3596Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3593Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3593Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3558Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3544Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3506Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3464Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3451Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3410Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3413Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3372Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3370Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3357Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3329Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3330Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3305Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3294Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3309Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3295Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3263Epoch 1/15: [==============================] 60/60 batches, loss: 0.3233
[2025-05-07 20:00:44,345][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3233
[2025-05-07 20:00:44,764][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1661, Metrics: {'mse': 0.15986007452011108, 'rmse': 0.3998250548929007, 'r2': -2.823566198348999}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2889Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2523Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2292Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2254Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1990Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1991Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2058Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2124Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2063Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1993Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2019Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1988Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2065Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2128Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2038Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2026Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2026Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2019Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2019Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2008Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1948Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1933Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1907Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1872Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1902Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1904Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1893Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1852Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1836Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1831Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1816Epoch 2/15: [================              ] 32/60 batches, loss: 0.1801Epoch 2/15: [================              ] 33/60 batches, loss: 0.1795Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1810Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1810Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1797Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1794Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1797Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1801Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1835Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1832Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1822Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1820Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1823Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1813Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1795Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1774Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1769Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1768Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1762Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1759Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1757Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1753Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1743Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1738Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1722Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1720Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1715Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1700Epoch 2/15: [==============================] 60/60 batches, loss: 0.1687
[2025-05-07 20:00:47,137][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1687
[2025-05-07 20:00:47,433][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1465, Metrics: {'mse': 0.14068694412708282, 'rmse': 0.3750825830761578, 'r2': -2.3649792671203613}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0630Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0939Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0852Epoch 3/15: [==                            ] 4/60 batches, loss: 0.0923Epoch 3/15: [==                            ] 5/60 batches, loss: 0.0956Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1095Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1063Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1146Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1172Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1169Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1191Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1217Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1235Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1265Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1286Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1287Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1306Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1274Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1249Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1256Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1238Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1230Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1213Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1203Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1200Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1216Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1201Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1193Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1213Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1203Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1206Epoch 3/15: [================              ] 32/60 batches, loss: 0.1203Epoch 3/15: [================              ] 33/60 batches, loss: 0.1235Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1220Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1200Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1206Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1186Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1174Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1189Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1182Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1181Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1182Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1191Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1180Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1171Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1169Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1171Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1170Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1192Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1190Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1185Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1187Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1182Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1210Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1207Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1199Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1210Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1197Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1193Epoch 3/15: [==============================] 60/60 batches, loss: 0.1213
[2025-05-07 20:00:49,712][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1213
[2025-05-07 20:00:50,026][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1304, Metrics: {'mse': 0.12542615830898285, 'rmse': 0.35415555665411047, 'r2': -1.9999685287475586}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0965Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1011Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0929Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1255Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1173Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1093Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1084Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1060Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1039Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0984Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1008Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1012Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1027Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1029Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1045Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1082Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1086Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1084Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1060Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1089Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1096Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1105Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1125Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1149Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1139Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1159Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1137Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1149Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1137Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1124Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1122Epoch 4/15: [================              ] 32/60 batches, loss: 0.1101Epoch 4/15: [================              ] 33/60 batches, loss: 0.1111Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1104Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1096Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1092Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1091Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1089Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1074Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1061Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1064Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1066Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1067Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1058Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1060Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1044Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1031Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1024Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1020Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1022Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1026Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1045Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1039Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1055Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1054Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1054Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1046Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1041Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1031Epoch 4/15: [==============================] 60/60 batches, loss: 0.1029
[2025-05-07 20:00:52,310][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1029
[2025-05-07 20:00:52,613][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1449, Metrics: {'mse': 0.13979607820510864, 'rmse': 0.3738931374137651, 'r2': -2.3436710834503174}
[2025-05-07 20:00:52,613][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0726Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0799Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0971Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0938Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1052Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0995Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1011Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0975Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0958Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1002Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1034Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1065Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1075Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1141Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1114Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1138Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1140Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1146Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1140Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1140Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1151Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1131Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1144Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1122Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1110Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1094Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1069Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1054Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1081Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1076Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1073Epoch 5/15: [================              ] 32/60 batches, loss: 0.1058Epoch 5/15: [================              ] 33/60 batches, loss: 0.1047Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1032Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1018Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1014Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1008Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1008Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1007Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0994Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0994Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1002Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1005Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1001Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1000Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0995Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0983Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0979Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0981Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0984Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0971Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0967Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0975Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0967Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0969Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0961Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0961Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0968Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0962Epoch 5/15: [==============================] 60/60 batches, loss: 0.0961
[2025-05-07 20:00:54,544][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0961
[2025-05-07 20:00:54,845][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1160, Metrics: {'mse': 0.11187265813350677, 'rmse': 0.33447370320177156, 'r2': -1.675793170928955}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1194Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1186Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1096Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0916Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0896Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0901Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0871Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0932Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0947Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0908Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0910Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0887Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0865Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0878Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0884Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0902Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0882Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0882Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0865Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0845Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0831Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0837Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0844Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0844Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0852Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0847Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0847Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0840Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0844Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0835Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0828Epoch 6/15: [================              ] 32/60 batches, loss: 0.0821Epoch 6/15: [================              ] 33/60 batches, loss: 0.0855Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0845Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0841Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0835Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0835Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0839Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0839Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0835Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0822Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0822Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0810Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0810Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0812Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0801Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0808Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0808Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0806Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0799Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0810Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0804Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0807Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0799Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0804Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0797Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0795Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0797Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0800Epoch 6/15: [==============================] 60/60 batches, loss: 0.0795
[2025-05-07 20:00:57,173][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0795
[2025-05-07 20:00:57,512][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1048, Metrics: {'mse': 0.10103455930948257, 'rmse': 0.31785933887410417, 'r2': -1.4165654182434082}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0392Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1066Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1079Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1101Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1133Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1008Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0989Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0959Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0941Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0881Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0844Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0816Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0797Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0768Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0767Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0778Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0770Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0775Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0777Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0767Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0760Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0750Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0762Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0763Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0748Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0741Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0736Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0717Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0721Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0718Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0719Epoch 7/15: [================              ] 32/60 batches, loss: 0.0717Epoch 7/15: [================              ] 33/60 batches, loss: 0.0725Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0717Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0718Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0728Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0717Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0720Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0715Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0719Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0721Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0721Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0714Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0711Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0704Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0711Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0714Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0717Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0718Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0725Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0726Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0724Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0721Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0725Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0722Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0722Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0725Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0719Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0719Epoch 7/15: [==============================] 60/60 batches, loss: 0.0732
[2025-05-07 20:00:59,823][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0732
[2025-05-07 20:01:00,132][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1035, Metrics: {'mse': 0.09954705834388733, 'rmse': 0.3155107895839496, 'r2': -1.3809869289398193}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0501Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0553Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0688Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0648Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0660Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0618Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0611Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0590Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0602Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0595Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0595Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0585Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0565Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0579Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0587Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0586Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0596Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0600Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0617Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0614Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0602Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0609Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0639Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0632Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0635Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0624Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0630Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0622Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0621Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0612Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0628Epoch 8/15: [================              ] 32/60 batches, loss: 0.0627Epoch 8/15: [================              ] 33/60 batches, loss: 0.0631Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0627Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0624Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0627Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0632Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0645Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0645Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0641Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0651Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0644Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0642Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0648Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0646Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0639Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0637Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0637Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0637Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0632Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0636Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0635Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0634Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0635Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0635Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0631Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0625Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0630Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0630Epoch 8/15: [==============================] 60/60 batches, loss: 0.0633
[2025-05-07 20:01:02,469][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0633
[2025-05-07 20:01:02,866][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0967, Metrics: {'mse': 0.09334240853786469, 'rmse': 0.3055198987592538, 'r2': -1.2325828075408936}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1322Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1012Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0820Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0768Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0718Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0699Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0721Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0687Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0658Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0642Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0642Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0643Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0632Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0632Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0650Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0638Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0643Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0669Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0664Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0664Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0666Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0667Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0663Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0669Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0672Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0675Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0678Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0671Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0676Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0689Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0685Epoch 9/15: [================              ] 32/60 batches, loss: 0.0690Epoch 9/15: [================              ] 33/60 batches, loss: 0.0687Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0692Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0691Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0694Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0699Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0705Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0720Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0744Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0738Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0736Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0725Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0725Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0720Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0713Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0709Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0705Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0708Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0705Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0700Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0698Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0691Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0687Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0683Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0696Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0694Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0699Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0700Epoch 9/15: [==============================] 60/60 batches, loss: 0.0699
[2025-05-07 20:01:05,250][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0699
[2025-05-07 20:01:05,522][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0954, Metrics: {'mse': 0.09190836548805237, 'rmse': 0.30316392510991863, 'r2': -1.1982829570770264}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0549Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0529Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0506Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0534Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0577Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0560Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0554Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0561Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0585Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0591Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0588Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0600Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0598Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0606Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0633Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0621Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0604Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0583Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0592Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0601Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0600Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0625Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0620Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0627Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0622Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0625Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0640Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0640Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0644Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0642Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0641Epoch 10/15: [================              ] 32/60 batches, loss: 0.0641Epoch 10/15: [================              ] 33/60 batches, loss: 0.0636Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0634Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0638Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0625Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0614Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0606Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0612Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0611Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0618Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0612Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0608Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0622Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0617Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0616Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0614Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0612Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0609Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0616Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0624Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0628Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0623Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0618Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0618Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0617Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0617Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0609Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0607Epoch 10/15: [==============================] 60/60 batches, loss: 0.0606
[2025-05-07 20:01:07,804][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0606
[2025-05-07 20:01:08,115][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0869, Metrics: {'mse': 0.08382979780435562, 'rmse': 0.28953375935174747, 'r2': -1.0050582885742188}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0235Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0499Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0470Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0542Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0611Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0651Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0646Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0683Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0689Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0671Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0656Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0683Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0655Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0650Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0646Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0632Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0625Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0640Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0647Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0640Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0648Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0643Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0629Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0618Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0634Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0634Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0633Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0621Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0611Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0609Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0608Epoch 11/15: [================              ] 32/60 batches, loss: 0.0614Epoch 11/15: [================              ] 33/60 batches, loss: 0.0611Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0615Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0608Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0605Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0606Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0603Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0601Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0613Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0608Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0599Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0604Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0601Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0596Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0596Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0591Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0588Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0586Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0588Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0585Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0587Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0582Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0588Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0589Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0594Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0592Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0588Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0583Epoch 11/15: [==============================] 60/60 batches, loss: 0.0587
[2025-05-07 20:01:10,383][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0587
[2025-05-07 20:01:10,688][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1011, Metrics: {'mse': 0.09760715812444687, 'rmse': 0.31242144312522285, 'r2': -1.334587812423706}
[2025-05-07 20:01:10,689][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0494Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0652Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0550Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0553Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0627Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0615Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0569Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0539Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0517Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0501Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0512Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0525Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0528Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0526Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0525Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0539Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0527Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0526Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0537Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0543Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0536Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0549Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0561Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0567Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0569Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0566Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0574Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0572Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0581Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0585Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0586Epoch 12/15: [================              ] 32/60 batches, loss: 0.0594Epoch 12/15: [================              ] 33/60 batches, loss: 0.0599Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0593Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0592Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0585Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0577Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0582Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0581Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0576Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0578Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0577Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0576Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0573Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0571Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0571Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0566Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0566Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0559Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0554Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0555Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0550Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0550Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0554Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0551Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0549Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0549Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0549Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0550Epoch 12/15: [==============================] 60/60 batches, loss: 0.0548
[2025-05-07 20:01:12,559][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0548
[2025-05-07 20:01:12,872][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0867, Metrics: {'mse': 0.08380524069070816, 'rmse': 0.2894913482139115, 'r2': -1.0044710636138916}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0626Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0617Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0644Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0581Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0625Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0550Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0560Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0574Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0594Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0574Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0545Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0543Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0559Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0584Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0562Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0565Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0563Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0559Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0575Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0582Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0576Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0572Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0573Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0572Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0584Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0572Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0572Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0565Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0570Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0576Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0570Epoch 13/15: [================              ] 32/60 batches, loss: 0.0568Epoch 13/15: [================              ] 33/60 batches, loss: 0.0570Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0570Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0577Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0583Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0581Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0575Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0576Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0576Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0574Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0576Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0580Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0574Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0569Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0567Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0569Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0566Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0566Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0563Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0565Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0562Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0561Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0557Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0558Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0559Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0560Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0558Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0560Epoch 13/15: [==============================] 60/60 batches, loss: 0.0559
[2025-05-07 20:01:15,188][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0559
[2025-05-07 20:01:15,482][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0917, Metrics: {'mse': 0.08881334960460663, 'rmse': 0.2980156868431704, 'r2': -1.124255895614624}
[2025-05-07 20:01:15,483][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0364Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0380Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0621Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0601Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0626Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0647Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0683Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0673Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0681Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0667Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0641Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0613Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0617Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0624Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0617Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0599Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0586Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0577Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0581Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0577Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0575Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0577Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0565Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0553Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0556Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0555Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0552Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0545Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0549Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0556Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0551Epoch 14/15: [================              ] 32/60 batches, loss: 0.0556Epoch 14/15: [================              ] 33/60 batches, loss: 0.0551Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0546Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0539Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0538Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0543Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0541Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0538Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0533Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0535Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0532Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0533Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0530Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0525Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0522Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0520Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0519Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0521Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0523Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0523Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0529Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0525Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0528Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0524Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0519Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0517Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0514Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0516Epoch 14/15: [==============================] 60/60 batches, loss: 0.0515
[2025-05-07 20:01:17,361][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0515
[2025-05-07 20:01:17,671][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0874, Metrics: {'mse': 0.08481491357088089, 'rmse': 0.2912300011518059, 'r2': -1.0286204814910889}
[2025-05-07 20:01:17,672][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0399Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0566Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0636Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0694Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0626Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0590Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0571Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0537Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0574Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0591Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0608Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0634Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0608Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0590Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0574Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0565Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0557Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0555Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0554Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0540Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0533Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0537Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0538Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0532Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0527Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0516Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0519Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0521Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0522Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0528Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0519Epoch 15/15: [================              ] 32/60 batches, loss: 0.0515Epoch 15/15: [================              ] 33/60 batches, loss: 0.0515Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0508Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0504Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0497Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0496Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0495Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0493Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0491Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0492Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0494Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0491Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0489Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0488Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0485Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0482Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0483Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0486Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0481Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0486Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0485Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0485Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0484Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0485Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0485Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0489Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0490Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0493Epoch 15/15: [==============================] 60/60 batches, loss: 0.0490
[2025-05-07 20:01:19,552][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0490
[2025-05-07 20:01:19,910][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0838, Metrics: {'mse': 0.08121788501739502, 'rmse': 0.28498751730101274, 'r2': -0.9425859451293945}
[2025-05-07 20:01:20,369][src.training.lm_trainer][INFO] - Training completed in 38.70 seconds
[2025-05-07 20:01:20,369][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:01:22,973][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03922109678387642, 'rmse': 0.1980431689906936, 'r2': -0.08086597919464111}
[2025-05-07 20:01:22,973][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08121788501739502, 'rmse': 0.28498751730101274, 'r2': -0.9425859451293945}
[2025-05-07 20:01:22,973][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05170547217130661, 'rmse': 0.22738837299058764, 'r2': -0.2680957317352295}
[2025-05-07 20:01:25,778][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/id/id/model.pt
[2025-05-07 20:01:25,779][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▃▂▂▁▁▁
wandb:     best_val_mse █▆▅▄▃▃▂▂▁▁▁
wandb:      best_val_r2 ▁▃▄▅▆▆▇▇███
wandb:    best_val_rmse █▆▅▄▃▃▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▃▅▅▅▆▆▆▅▆▆▆
wandb:       train_loss █▄▃▂▂▂▂▁▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▆▄▃▃▂▂▁▂▁▂▁▁
wandb:          val_mse █▆▅▆▄▃▃▂▂▁▂▁▂▁▁
wandb:           val_r2 ▁▃▄▃▅▆▆▇▇█▇█▇██
wandb:         val_rmse █▆▅▆▄▃▃▂▂▁▃▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08382
wandb:     best_val_mse 0.08122
wandb:      best_val_r2 -0.94259
wandb:    best_val_rmse 0.28499
wandb:            epoch 15
wandb:   final_test_mse 0.05171
wandb:    final_test_r2 -0.2681
wandb:  final_test_rmse 0.22739
wandb:  final_train_mse 0.03922
wandb:   final_train_r2 -0.08087
wandb: final_train_rmse 0.19804
wandb:    final_val_mse 0.08122
wandb:     final_val_r2 -0.94259
wandb:   final_val_rmse 0.28499
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04904
wandb:       train_time 38.69752
wandb:         val_loss 0.08382
wandb:          val_mse 0.08122
wandb:           val_r2 -0.94259
wandb:         val_rmse 0.28499
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200017-gcej0150
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200017-gcej0150/logs
Experiment probe_layer2_complexity_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/id/id/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/id"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:02:00,250][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/id
experiment_name: probe_layer2_avg_links_len_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:02:00,250][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:02:00,251][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:02:00,251][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:02:00,251][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:02:00,256][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:02:00,256][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:02:00,256][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:02:03,320][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:02:05,540][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:02:05,540][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:02:05,717][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:05,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:06,044][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:02:06,051][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:02:06,051][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:02:06,054][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:02:06,113][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:06,205][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:06,243][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:02:06,245][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:02:06,245][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:02:06,257][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:02:06,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:06,441][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:02:06,464][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:02:06,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:02:06,466][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:02:06,469][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:02:06,469][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:02:06,470][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:02:06,470][src.data.datasets][INFO] -   Mean: 0.1820, Std: 0.1352
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Sample label: 0.15700000524520874
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:02:06,470][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:02:06,471][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9170
[2025-05-07 20:02:06,471][src.data.datasets][INFO] -   Mean: 0.2802, Std: 0.1921
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Sample label: 0.1289999932050705
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:02:06,471][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:02:06,472][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7260
[2025-05-07 20:02:06,472][src.data.datasets][INFO] -   Mean: 0.2993, Std: 0.1490
[2025-05-07 20:02:06,472][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:02:06,472][src.data.datasets][INFO] - Sample label: 0.22599999606609344
[2025-05-07 20:02:06,472][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:02:06,472][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:02:06,472][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:02:06,472][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:02:06,473][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:02:14,117][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:02:14,118][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:02:14,118][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:02:14,118][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:02:14,122][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:02:14,122][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:02:14,122][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:02:14,122][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:02:14,123][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:02:14,123][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:02:14,123][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3258Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4080Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4549Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4493Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4317Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4081Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4415Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4430Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4577Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4553Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4416Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4383Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4249Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4248Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4154Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4124Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4098Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4177Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4062Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.3982Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3931Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.3905Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.3853Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3781Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3753Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3722Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3647Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3613Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3617Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3625Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3583Epoch 1/15: [================              ] 32/60 batches, loss: 0.3532Epoch 1/15: [================              ] 33/60 batches, loss: 0.3479Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3456Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3420Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3393Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3325Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3271Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3243Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3218Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3206Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3205Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3167Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3158Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3134Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3103Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3091Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3054Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3032Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.2993Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.2982Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.2969Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.2941Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2947Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2931Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2941Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2951Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2934Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2917Epoch 1/15: [==============================] 60/60 batches, loss: 0.2885
[2025-05-07 20:02:20,800][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2885
[2025-05-07 20:02:21,111][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1477, Metrics: {'mse': 0.1371656060218811, 'rmse': 0.37035875313252836, 'r2': -2.7170560359954834}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3138Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2139Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1831Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1858Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1638Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1835Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1792Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1869Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1866Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1767Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1772Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1738Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1771Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1778Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1711Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1698Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1719Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1678Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1676Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1668Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1641Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1664Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1659Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1623Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1650Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1623Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1593Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1576Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1553Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1541Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1529Epoch 2/15: [================              ] 32/60 batches, loss: 0.1516Epoch 2/15: [================              ] 33/60 batches, loss: 0.1515Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1529Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1542Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1528Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1546Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1543Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1543Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1549Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1565Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1559Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1549Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1547Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1539Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1529Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1512Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1508Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1509Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1513Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1496Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1496Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1486Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1481Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1470Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1456Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1452Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1448Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1439Epoch 2/15: [==============================] 60/60 batches, loss: 0.1427
[2025-05-07 20:02:23,426][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1427
[2025-05-07 20:02:23,707][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1330, Metrics: {'mse': 0.12266263365745544, 'rmse': 0.3502322567346638, 'r2': -2.3240396976470947}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0830Epoch 3/15: [=                             ] 2/60 batches, loss: 0.0956Epoch 3/15: [=                             ] 3/60 batches, loss: 0.0848Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1011Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1056Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1055Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1103Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1219Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1160Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1105Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1088Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1161Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1172Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1159Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1124Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1125Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1111Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1078Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1048Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1056Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1034Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1026Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1006Epoch 3/15: [============                  ] 24/60 batches, loss: 0.0994Epoch 3/15: [============                  ] 25/60 batches, loss: 0.0982Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.0991Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.0980Epoch 3/15: [==============                ] 28/60 batches, loss: 0.0961Epoch 3/15: [==============                ] 29/60 batches, loss: 0.0969Epoch 3/15: [===============               ] 30/60 batches, loss: 0.0952Epoch 3/15: [===============               ] 31/60 batches, loss: 0.0968Epoch 3/15: [================              ] 32/60 batches, loss: 0.0965Epoch 3/15: [================              ] 33/60 batches, loss: 0.0997Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1004Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1006Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1015Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1003Epoch 3/15: [===================           ] 38/60 batches, loss: 0.0989Epoch 3/15: [===================           ] 39/60 batches, loss: 0.0991Epoch 3/15: [====================          ] 40/60 batches, loss: 0.0986Epoch 3/15: [====================          ] 41/60 batches, loss: 0.0983Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.0986Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.0983Epoch 3/15: [======================        ] 44/60 batches, loss: 0.0983Epoch 3/15: [======================        ] 45/60 batches, loss: 0.0976Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.0968Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.0967Epoch 3/15: [========================      ] 48/60 batches, loss: 0.0964Epoch 3/15: [========================      ] 49/60 batches, loss: 0.0961Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.0960Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.0956Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.0955Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.0949Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.0965Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.0967Epoch 3/15: [============================  ] 56/60 batches, loss: 0.0961Epoch 3/15: [============================  ] 57/60 batches, loss: 0.0958Epoch 3/15: [============================= ] 58/60 batches, loss: 0.0949Epoch 3/15: [============================= ] 59/60 batches, loss: 0.0944Epoch 3/15: [==============================] 60/60 batches, loss: 0.0955
[2025-05-07 20:02:26,016][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0955
[2025-05-07 20:02:26,331][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1273, Metrics: {'mse': 0.11723797768354416, 'rmse': 0.34240031787885966, 'r2': -2.177036762237549}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0421Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0727Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0780Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1014Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0935Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1010Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1006Epoch 4/15: [====                          ] 8/60 batches, loss: 0.0965Epoch 4/15: [====                          ] 9/60 batches, loss: 0.0890Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0816Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0821Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0802Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0795Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0791Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0792Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0817Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0841Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0821Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0822Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0846Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.0859Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.0859Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.0860Epoch 4/15: [============                  ] 24/60 batches, loss: 0.0891Epoch 4/15: [============                  ] 25/60 batches, loss: 0.0874Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.0874Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.0866Epoch 4/15: [==============                ] 28/60 batches, loss: 0.0864Epoch 4/15: [==============                ] 29/60 batches, loss: 0.0864Epoch 4/15: [===============               ] 30/60 batches, loss: 0.0859Epoch 4/15: [===============               ] 31/60 batches, loss: 0.0854Epoch 4/15: [================              ] 32/60 batches, loss: 0.0864Epoch 4/15: [================              ] 33/60 batches, loss: 0.0856Epoch 4/15: [=================             ] 34/60 batches, loss: 0.0851Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0847Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0841Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0829Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0830Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0826Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0823Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0827Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0832Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0838Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0834Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0827Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0822Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0809Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0804Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0806Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0801Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0804Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0809Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0811Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0804Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0799Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0808Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0802Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0797Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0795Epoch 4/15: [==============================] 60/60 batches, loss: 0.0789
[2025-05-07 20:02:28,591][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0789
[2025-05-07 20:02:28,916][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1015, Metrics: {'mse': 0.09306622296571732, 'rmse': 0.30506757114730715, 'r2': -1.522005319595337}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0780Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0795Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0747Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0748Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0809Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0770Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0747Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0736Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0768Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0759Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0780Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0813Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0800Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0820Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0796Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0785Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0780Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0769Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0765Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0795Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0782Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0776Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0772Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0781Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0786Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0787Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0787Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0778Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0792Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0792Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0794Epoch 5/15: [================              ] 32/60 batches, loss: 0.0780Epoch 5/15: [================              ] 33/60 batches, loss: 0.0777Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0770Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0780Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0771Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0774Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0768Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0758Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0754Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0747Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0750Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0757Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0756Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0756Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0749Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0746Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0747Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0740Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0741Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0739Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0735Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0735Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0726Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0722Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0717Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0713Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0714Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0707Epoch 5/15: [==============================] 60/60 batches, loss: 0.0701
[2025-05-07 20:02:31,201][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0701
[2025-05-07 20:02:31,468][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0898, Metrics: {'mse': 0.08188291639089584, 'rmse': 0.2861519113878079, 'r2': -1.2189483642578125}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0517Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0501Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0643Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0555Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0562Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0554Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0612Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0687Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0706Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0674Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0646Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0632Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0670Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0671Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0654Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0662Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0647Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0629Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0616Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0635Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0628Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0639Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0627Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0618Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0618Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0621Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0614Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0605Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0599Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0596Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0588Epoch 6/15: [================              ] 32/60 batches, loss: 0.0580Epoch 6/15: [================              ] 33/60 batches, loss: 0.0580Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0586Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0580Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0573Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0581Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0580Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0582Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0584Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0579Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0581Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0573Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0574Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0579Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0573Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0570Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0565Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0560Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0557Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0576Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0571Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0569Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0572Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0579Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0575Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0572Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0568Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0566Epoch 6/15: [==============================] 60/60 batches, loss: 0.0560
[2025-05-07 20:02:33,746][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0560
[2025-05-07 20:02:34,131][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0858, Metrics: {'mse': 0.07839681953191757, 'rmse': 0.27999432053510936, 'r2': -1.1244785785675049}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0135Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0334Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0446Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0464Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0465Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0462Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0485Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0519Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0557Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0523Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0514Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0515Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0511Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0517Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0528Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0528Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0517Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0505Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0510Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0504Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0494Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0493Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0519Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0532Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0529Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0520Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0526Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0517Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0530Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0525Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0524Epoch 7/15: [================              ] 32/60 batches, loss: 0.0518Epoch 7/15: [================              ] 33/60 batches, loss: 0.0528Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0522Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0515Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0513Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0513Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0509Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0507Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0507Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0502Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0511Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0507Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0504Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0513Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0517Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0515Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0516Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0524Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0522Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0523Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0518Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0515Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0510Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0511Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0508Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0508Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0504Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0510Epoch 7/15: [==============================] 60/60 batches, loss: 0.0511
[2025-05-07 20:02:36,450][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0511
[2025-05-07 20:02:36,810][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0845, Metrics: {'mse': 0.07714085280895233, 'rmse': 0.2777424216949084, 'r2': -1.0904431343078613}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0487Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0584Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0562Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0491Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0483Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0463Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0433Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0418Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0412Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0424Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0403Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0397Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0380Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0383Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0387Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0387Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0384Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0376Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0379Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0384Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0403Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0407Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0418Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0431Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0425Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0432Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0435Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0439Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0440Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0432Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0423Epoch 8/15: [================              ] 32/60 batches, loss: 0.0419Epoch 8/15: [================              ] 33/60 batches, loss: 0.0414Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0422Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0425Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0419Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0425Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0424Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0428Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0423Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0424Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0427Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0428Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0429Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0433Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0430Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0433Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0440Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0438Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0439Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0442Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0445Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0444Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0443Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0445Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0457Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0455Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0462Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0457Epoch 8/15: [==============================] 60/60 batches, loss: 0.0455
[2025-05-07 20:02:39,166][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0455
[2025-05-07 20:02:39,479][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0787, Metrics: {'mse': 0.07183114439249039, 'rmse': 0.26801332875901973, 'r2': -0.9465551376342773}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0468Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0395Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0365Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0346Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0380Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0406Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0456Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0462Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0447Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0459Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0515Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0492Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0481Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0520Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0514Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0516Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0502Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0497Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0477Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0469Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0471Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0467Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0470Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0461Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0462Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0464Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0472Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0476Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0471Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0471Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0472Epoch 9/15: [================              ] 32/60 batches, loss: 0.0465Epoch 9/15: [================              ] 33/60 batches, loss: 0.0460Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0452Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0457Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0462Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0463Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0459Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0460Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0461Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0457Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0456Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0451Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0450Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0444Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0444Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0442Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0441Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0448Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0444Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0443Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0446Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0446Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0442Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0439Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0439Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0434Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0440Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0439Epoch 9/15: [==============================] 60/60 batches, loss: 0.0434
[2025-05-07 20:02:41,816][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0434
[2025-05-07 20:02:42,121][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0665, Metrics: {'mse': 0.06039077788591385, 'rmse': 0.24574535170764442, 'r2': -0.6365320682525635}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0514Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0521Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0530Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0482Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0439Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0391Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0372Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0376Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0388Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0480Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0463Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0444Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0449Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0436Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0446Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0430Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0430Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0430Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0428Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0442Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0433Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0423Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0411Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0412Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0411Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0406Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0408Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0407Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0410Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0404Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0408Epoch 10/15: [================              ] 32/60 batches, loss: 0.0424Epoch 10/15: [================              ] 33/60 batches, loss: 0.0426Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0431Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0425Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0425Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0417Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0414Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0420Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0415Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0421Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0415Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0413Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0414Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0414Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0414Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0411Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0410Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0412Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0413Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0410Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0408Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0407Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0404Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0402Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0400Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0409Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0410Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0413Epoch 10/15: [==============================] 60/60 batches, loss: 0.0414
[2025-05-07 20:02:44,390][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0414
[2025-05-07 20:02:44,722][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0679, Metrics: {'mse': 0.061789803206920624, 'rmse': 0.24857554828848435, 'r2': -0.6744444370269775}
[2025-05-07 20:02:44,723][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0340Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0384Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0307Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0346Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0359Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0375Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0379Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0384Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0375Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0370Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0362Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0407Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0386Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0382Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0376Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0382Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0379Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0397Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0400Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0387Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0381Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0375Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0382Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0376Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0375Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0373Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0370Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0367Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0363Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0370Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0368Epoch 11/15: [================              ] 32/60 batches, loss: 0.0364Epoch 11/15: [================              ] 33/60 batches, loss: 0.0371Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0378Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0382Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0380Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0376Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0368Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0368Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0370Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0370Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0369Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0365Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0360Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0358Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0364Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0364Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0363Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0363Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0359Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0359Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0361Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0361Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0364Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0365Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0368Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0365Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0363Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0364Epoch 11/15: [==============================] 60/60 batches, loss: 0.0362
[2025-05-07 20:02:46,642][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0362
[2025-05-07 20:02:46,894][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0782, Metrics: {'mse': 0.07162180542945862, 'rmse': 0.26762250546144023, 'r2': -0.9408820867538452}
[2025-05-07 20:02:46,895][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0339Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0431Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0361Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0379Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0360Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0365Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0337Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0348Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0332Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0323Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0320Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0340Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0333Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0342Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0351Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0357Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0347Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0358Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0371Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0390Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0378Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0369Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0364Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0372Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0382Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0380Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0379Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0384Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0404Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0399Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0393Epoch 12/15: [================              ] 32/60 batches, loss: 0.0394Epoch 12/15: [================              ] 33/60 batches, loss: 0.0396Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0394Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0386Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0382Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0388Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0384Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0383Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0382Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0387Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0386Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0388Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0385Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0382Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0380Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0379Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0376Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0374Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0373Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0376Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0372Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0370Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0368Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0366Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0367Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0366Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0367Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0364Epoch 12/15: [==============================] 60/60 batches, loss: 0.0361
[2025-05-07 20:02:48,770][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0361
[2025-05-07 20:02:49,045][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0764, Metrics: {'mse': 0.06989620625972748, 'rmse': 0.26437890660891894, 'r2': -0.8941200971603394}
[2025-05-07 20:02:49,046][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0359Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0401Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0379Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0334Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0322Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0287Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0289Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0295Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0305Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0290Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0281Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0284Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0292Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0315Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0305Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0304Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0309Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0303Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0296Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0321Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0322Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0327Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0323Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0323Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0324Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0313Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0311Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0313Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0311Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0307Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0300Epoch 13/15: [================              ] 32/60 batches, loss: 0.0298Epoch 13/15: [================              ] 33/60 batches, loss: 0.0298Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0299Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0303Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0301Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0301Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0299Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0298Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0294Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0304Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0301Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0301Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0305Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0308Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0312Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0313Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0314Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0314Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0315Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0316Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0314Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0314Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0312Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0314Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0312Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0314Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0313Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0313Epoch 13/15: [==============================] 60/60 batches, loss: 0.0313
[2025-05-07 20:02:50,935][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0313
[2025-05-07 20:02:51,249][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0640, Metrics: {'mse': 0.05824416130781174, 'rmse': 0.24133827153564297, 'r2': -0.578360915184021}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0283Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0297Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0342Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0340Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0350Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0316Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0320Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0353Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0335Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0335Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0327Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0324Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0312Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0324Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0313Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0303Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0302Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0307Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0304Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0307Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0304Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0296Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0292Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0287Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0290Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0285Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0284Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0284Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0288Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0293Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0304Epoch 14/15: [================              ] 32/60 batches, loss: 0.0301Epoch 14/15: [================              ] 33/60 batches, loss: 0.0297Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0297Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0296Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0299Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0299Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0299Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0304Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0302Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0301Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0301Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0298Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0307Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0305Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0311Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0308Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0309Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0314Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0317Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0315Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0313Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0312Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0312Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0311Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0308Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0309Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0310Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0307Epoch 14/15: [==============================] 60/60 batches, loss: 0.0304
[2025-05-07 20:02:53,610][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0304
[2025-05-07 20:02:53,975][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0644, Metrics: {'mse': 0.05876276269555092, 'rmse': 0.24241031887184777, 'r2': -0.5924144983291626}
[2025-05-07 20:02:53,976][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0318Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0209Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0331Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0274Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0268Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0271Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0296Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0330Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0331Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0329Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0347Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0339Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0339Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0324Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0325Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0324Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0330Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0330Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0333Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0336Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0328Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0326Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0322Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0320Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0318Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0321Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0315Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0309Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0306Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0305Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0301Epoch 15/15: [================              ] 32/60 batches, loss: 0.0303Epoch 15/15: [================              ] 33/60 batches, loss: 0.0301Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0297Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0294Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0292Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0291Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0289Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0291Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0297Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0292Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0289Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0291Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0290Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0287Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0292Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0289Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0286Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0286Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0294Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0293Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0293Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0294Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0291Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0291Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0294Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0292Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0294Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0294Epoch 15/15: [==============================] 60/60 batches, loss: 0.0291
[2025-05-07 20:02:55,880][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0291
[2025-05-07 20:02:56,135][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0585, Metrics: {'mse': 0.05309596285223961, 'rmse': 0.23042561240504408, 'r2': -0.43884968757629395}
[2025-05-07 20:02:56,586][src.training.lm_trainer][INFO] - Training completed in 38.56 seconds
[2025-05-07 20:02:56,586][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:02:59,279][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01604108139872551, 'rmse': 0.12665339079047788, 'r2': 0.12185442447662354}
[2025-05-07 20:02:59,279][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05309596285223961, 'rmse': 0.23042561240504408, 'r2': -0.43884968757629395}
[2025-05-07 20:02:59,280][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04375019669532776, 'rmse': 0.20916547682475653, 'r2': -0.9706580638885498}
[2025-05-07 20:03:02,195][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/id/id/model.pt
[2025-05-07 20:03:02,196][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▄▃▃▃▃▂▁▁
wandb:     best_val_mse █▇▆▄▃▃▃▃▂▁▁
wandb:      best_val_r2 ▁▂▃▅▆▆▆▆▇██
wandb:    best_val_rmse █▇▇▅▄▃▃▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▅▅▆▆▆▇▇▆▆▇▇
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▄▃▃▃▃▂▂▃▂▁▁▁
wandb:          val_mse █▇▆▄▃▃▃▃▂▂▃▂▁▁▁
wandb:           val_r2 ▁▂▃▅▆▆▆▆▇▇▆▇███
wandb:         val_rmse █▇▇▅▄▃▃▃▂▂▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05846
wandb:     best_val_mse 0.0531
wandb:      best_val_r2 -0.43885
wandb:    best_val_rmse 0.23043
wandb:            epoch 15
wandb:   final_test_mse 0.04375
wandb:    final_test_r2 -0.97066
wandb:  final_test_rmse 0.20917
wandb:  final_train_mse 0.01604
wandb:   final_train_r2 0.12185
wandb: final_train_rmse 0.12665
wandb:    final_val_mse 0.0531
wandb:     final_val_r2 -0.43885
wandb:   final_val_rmse 0.23043
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02911
wandb:       train_time 38.55971
wandb:         val_loss 0.05846
wandb:          val_mse 0.0531
wandb:           val_r2 -0.43885
wandb:         val_rmse 0.23043
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200200-zg7vuewb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200200-zg7vuewb/logs
Experiment probe_layer2_avg_links_len_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/id"         "wandb.mode=offline" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:03:34,672][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/id
experiment_name: probe_layer2_avg_max_depth_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:03:34,672][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:03:34,672][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:03:34,672][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:03:34,672][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:03:34,676][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:03:34,676][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:03:34,677][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:03:38,646][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:03:41,079][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:03:41,079][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:03:41,368][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:41,507][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:41,853][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:03:41,860][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:03:41,861][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:03:41,865][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:03:41,982][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:42,124][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:42,166][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:03:42,167][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:03:42,168][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:03:42,171][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:03:42,283][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:42,399][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:03:42,443][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:03:42,444][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:03:42,444][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:03:42,449][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:03:42,449][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:03:42,450][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:03:42,450][src.data.datasets][INFO] -   Mean: 0.2764, Std: 0.1695
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 20:03:42,450][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:03:42,451][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:03:42,451][src.data.datasets][INFO] -   Mean: 0.3215, Std: 0.2018
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:03:42,451][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:03:42,452][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:03:42,452][src.data.datasets][INFO] -   Mean: 0.2675, Std: 0.1795
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Sample label: 0.4440000057220459
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:03:42,452][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:03:42,452][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:03:42,453][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:03:51,248][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:03:51,249][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:03:51,249][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:03:51,249][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:03:51,252][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:03:51,252][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:03:51,252][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:03:51,252][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:03:51,253][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:03:51,253][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:03:51,253][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3930Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4615Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5052Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4752Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4500Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4287Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4637Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4539Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4755Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4712Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4632Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4627Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4500Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4510Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4428Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4426Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4377Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4427Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4283Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4196Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4102Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4134Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4055Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3975Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3936Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3906Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3820Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3783Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3790Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3789Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3745Epoch 1/15: [================              ] 32/60 batches, loss: 0.3693Epoch 1/15: [================              ] 33/60 batches, loss: 0.3653Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3616Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3579Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3575Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3498Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3444Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3412Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3386Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3354Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3350Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3327Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3320Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3300Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3257Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3237Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3207Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3186Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3151Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3129Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3112Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3073Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3066Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3044Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3053Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3078Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3068Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3059Epoch 1/15: [==============================] 60/60 batches, loss: 0.3032
[2025-05-07 20:03:58,161][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3032
[2025-05-07 20:03:58,459][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0844, Metrics: {'mse': 0.08135878294706345, 'rmse': 0.2852346103597238, 'r2': -0.9970847368240356}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2635Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1936Epoch 2/15: [=                             ] 3/60 batches, loss: 0.1773Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1718Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1532Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1707Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1796Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1834Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1850Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1751Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1751Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1742Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1828Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1904Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1845Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1883Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1929Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1900Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1907Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1910Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1867Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1912Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1896Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1847Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1880Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1844Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1822Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1794Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1769Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1749Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1727Epoch 2/15: [================              ] 32/60 batches, loss: 0.1706Epoch 2/15: [================              ] 33/60 batches, loss: 0.1680Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1676Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1694Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1691Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1690Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1681Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1677Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1686Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1697Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1683Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1670Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1691Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1680Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1678Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1662Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1660Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1659Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1648Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1627Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1616Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1606Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1594Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1585Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1573Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1571Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1576Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1567Epoch 2/15: [==============================] 60/60 batches, loss: 0.1548
[2025-05-07 20:04:00,809][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1548
[2025-05-07 20:04:01,091][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0717, Metrics: {'mse': 0.06869341433048248, 'rmse': 0.26209428519233774, 'r2': -0.6861923933029175}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1021Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1118Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1020Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1218Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1154Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1177Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1164Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1216Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1172Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1103Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1112Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1126Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1140Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1137Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1098Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1079Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1069Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1060Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1036Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1053Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1030Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1031Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1020Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1006Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1002Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1015Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1004Epoch 3/15: [==============                ] 28/60 batches, loss: 0.0984Epoch 3/15: [==============                ] 29/60 batches, loss: 0.0981Epoch 3/15: [===============               ] 30/60 batches, loss: 0.0971Epoch 3/15: [===============               ] 31/60 batches, loss: 0.0984Epoch 3/15: [================              ] 32/60 batches, loss: 0.0993Epoch 3/15: [================              ] 33/60 batches, loss: 0.1037Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1040Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1038Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1042Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1026Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1016Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1024Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1021Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1024Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1026Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1026Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1027Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1021Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1015Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1013Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1022Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1031Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1033Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1039Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1043Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1038Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1042Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1045Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1036Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1030Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1030Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1024Epoch 3/15: [==============================] 60/60 batches, loss: 0.1044
[2025-05-07 20:04:03,446][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1044
[2025-05-07 20:04:03,710][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0721, Metrics: {'mse': 0.06953230500221252, 'rmse': 0.26368978934007387, 'r2': -0.7067843675613403}
[2025-05-07 20:04:03,710][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0797Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0941Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0991Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1179Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1143Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1167Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1225Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1141Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1117Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1051Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1056Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1030Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1002Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1009Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1014Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1000Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1021Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1034Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1018Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1038Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1031Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1039Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1028Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1052Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1029Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1070Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1052Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1044Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1041Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1026Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1013Epoch 4/15: [================              ] 32/60 batches, loss: 0.1005Epoch 4/15: [================              ] 33/60 batches, loss: 0.0992Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1007Epoch 4/15: [=================             ] 35/60 batches, loss: 0.0992Epoch 4/15: [==================            ] 36/60 batches, loss: 0.0977Epoch 4/15: [==================            ] 37/60 batches, loss: 0.0973Epoch 4/15: [===================           ] 38/60 batches, loss: 0.0968Epoch 4/15: [===================           ] 39/60 batches, loss: 0.0964Epoch 4/15: [====================          ] 40/60 batches, loss: 0.0959Epoch 4/15: [====================          ] 41/60 batches, loss: 0.0952Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.0959Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.0963Epoch 4/15: [======================        ] 44/60 batches, loss: 0.0951Epoch 4/15: [======================        ] 45/60 batches, loss: 0.0946Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.0939Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.0931Epoch 4/15: [========================      ] 48/60 batches, loss: 0.0920Epoch 4/15: [========================      ] 49/60 batches, loss: 0.0917Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.0915Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.0917Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.0911Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0905Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0904Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.0905Epoch 4/15: [============================  ] 56/60 batches, loss: 0.0918Epoch 4/15: [============================  ] 57/60 batches, loss: 0.0912Epoch 4/15: [============================= ] 58/60 batches, loss: 0.0902Epoch 4/15: [============================= ] 59/60 batches, loss: 0.0901Epoch 4/15: [==============================] 60/60 batches, loss: 0.0898
[2025-05-07 20:04:05,611][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0898
[2025-05-07 20:04:05,915][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0663, Metrics: {'mse': 0.06429440528154373, 'rmse': 0.2535634147142362, 'r2': -0.5782115459442139}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1191Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1001Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0862Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0817Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0839Epoch 5/15: [===                           ] 6/60 batches, loss: 0.0847Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0802Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0783Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0818Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0837Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0839Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0898Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0908Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.0921Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.0901Epoch 5/15: [========                      ] 16/60 batches, loss: 0.0918Epoch 5/15: [========                      ] 17/60 batches, loss: 0.0935Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.0931Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.0929Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.0955Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.0931Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.0910Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.0925Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0915Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0903Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0911Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0905Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0904Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0923Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0931Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0939Epoch 5/15: [================              ] 32/60 batches, loss: 0.0924Epoch 5/15: [================              ] 33/60 batches, loss: 0.0908Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0902Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0907Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0897Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0898Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0885Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0881Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0876Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0870Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0868Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0863Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0859Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0851Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0844Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0837Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0835Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0827Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0823Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0815Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0808Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0807Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0808Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0800Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0794Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0793Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0793Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0788Epoch 5/15: [==============================] 60/60 batches, loss: 0.0784
[2025-05-07 20:04:08,233][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0784
[2025-05-07 20:04:08,489][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0552, Metrics: {'mse': 0.053463056683540344, 'rmse': 0.2312207963906801, 'r2': -0.31233835220336914}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0589Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0556Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0712Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0635Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0629Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0638Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0687Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0747Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0763Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0735Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0731Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0719Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0719Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0725Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0727Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0725Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0711Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0698Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0677Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0662Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0652Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0680Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0671Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0664Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0662Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0668Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0660Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0656Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0658Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0687Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0683Epoch 6/15: [================              ] 32/60 batches, loss: 0.0691Epoch 6/15: [================              ] 33/60 batches, loss: 0.0699Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0693Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0691Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0683Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0686Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0688Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0682Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0684Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0672Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0669Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0664Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0662Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0655Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0649Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0650Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0646Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0642Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0638Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0654Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0653Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0649Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0648Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0655Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0657Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0651Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0646Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0648Epoch 6/15: [==============================] 60/60 batches, loss: 0.0649
[2025-05-07 20:04:10,738][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0649
[2025-05-07 20:04:11,095][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0469, Metrics: {'mse': 0.04530562087893486, 'rmse': 0.2128511707248397, 'r2': -0.11210072040557861}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0289Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0440Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0589Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0544Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0584Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0620Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0661Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0627Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0604Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0567Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0561Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0588Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0582Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0612Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0612Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0621Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0604Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0604Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0601Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0588Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0591Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0588Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0615Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0622Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0609Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0597Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0594Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0602Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0602Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0604Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0603Epoch 7/15: [================              ] 32/60 batches, loss: 0.0594Epoch 7/15: [================              ] 33/60 batches, loss: 0.0599Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0596Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0591Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0588Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0586Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0576Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0581Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0578Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0576Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0591Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0580Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0576Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0575Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0574Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0573Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0576Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0586Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0584Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0586Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0588Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0589Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0580Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0577Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0571Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0568Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0565Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0564Epoch 7/15: [==============================] 60/60 batches, loss: 0.0579
[2025-05-07 20:04:13,327][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0579
[2025-05-07 20:04:13,582][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0493, Metrics: {'mse': 0.04782259091734886, 'rmse': 0.21868376921332974, 'r2': -0.17388391494750977}
[2025-05-07 20:04:13,583][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0489Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0466Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0522Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0557Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0543Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0523Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0522Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0583Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0562Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0564Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0553Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0542Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0532Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0531Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0530Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0521Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0522Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0515Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0528Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0519Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0524Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0535Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0536Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0536Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0524Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0513Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0534Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0532Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0527Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0532Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0528Epoch 8/15: [================              ] 32/60 batches, loss: 0.0518Epoch 8/15: [================              ] 33/60 batches, loss: 0.0520Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0526Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0525Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0521Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0528Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0522Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0525Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0519Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0534Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0533Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0537Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0536Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0540Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0538Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0549Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0552Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0549Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0546Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0551Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0554Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0554Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0550Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0552Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0554Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0556Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0556Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0552Epoch 8/15: [==============================] 60/60 batches, loss: 0.0550
[2025-05-07 20:04:15,493][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0550
[2025-05-07 20:04:15,831][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0455, Metrics: {'mse': 0.044086601585149765, 'rmse': 0.2099680965888622, 'r2': -0.08217787742614746}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0392Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0325Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0292Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0268Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0367Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0379Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0383Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0370Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0359Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0388Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0445Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0444Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0425Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0437Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0439Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0436Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0424Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0468Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0477Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0479Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0484Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0477Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0475Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0477Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0503Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0495Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0509Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0518Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0518Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0518Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0520Epoch 9/15: [================              ] 32/60 batches, loss: 0.0517Epoch 9/15: [================              ] 33/60 batches, loss: 0.0522Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0520Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0526Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0526Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0520Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0516Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0516Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0515Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0520Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0526Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0520Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0519Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0517Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0526Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0524Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0518Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0516Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0511Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0509Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0511Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0510Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0506Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0504Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0506Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0504Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0504Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0505Epoch 9/15: [==============================] 60/60 batches, loss: 0.0502
[2025-05-07 20:04:18,215][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0502
[2025-05-07 20:04:18,551][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0423, Metrics: {'mse': 0.04097498953342438, 'rmse': 0.20242279894672038, 'r2': -0.005798220634460449}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0307Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0402Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0431Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0427Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0460Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0415Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0393Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0395Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0416Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0434Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0418Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0452Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0479Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0487Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0480Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0489Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0480Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0477Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0484Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0504Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0495Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0504Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0512Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0515Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0513Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0511Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0512Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0505Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0504Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0504Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0502Epoch 10/15: [================              ] 32/60 batches, loss: 0.0514Epoch 10/15: [================              ] 33/60 batches, loss: 0.0511Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0509Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0507Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0500Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0503Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0497Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0503Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0500Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0503Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0502Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0498Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0498Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0495Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0492Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0487Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0483Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0480Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0476Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0473Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0470Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0466Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0460Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0456Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0453Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0455Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0465Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0470Epoch 10/15: [==============================] 60/60 batches, loss: 0.0474
[2025-05-07 20:04:20,844][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0474
[2025-05-07 20:04:21,139][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0447, Metrics: {'mse': 0.043388355523347855, 'rmse': 0.20829871704681202, 'r2': -0.06503832340240479}
[2025-05-07 20:04:21,140][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1065Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0793Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0627Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0661Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0589Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0602Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0579Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0584Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0574Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0541Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0537Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0569Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0535Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0525Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0511Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0499Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0493Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0513Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0528Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0539Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0527Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0521Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0518Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0511Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0503Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0498Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0499Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0491Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0486Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0488Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0484Epoch 11/15: [================              ] 32/60 batches, loss: 0.0480Epoch 11/15: [================              ] 33/60 batches, loss: 0.0482Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0480Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0480Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0479Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0476Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0471Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0479Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0481Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0482Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0478Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0470Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0467Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0465Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0465Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0461Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0461Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0459Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0458Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0458Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0462Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0461Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0460Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0457Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0458Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0454Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0456Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0461Epoch 11/15: [==============================] 60/60 batches, loss: 0.0466
[2025-05-07 20:04:23,045][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0466
[2025-05-07 20:04:23,348][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0411, Metrics: {'mse': 0.03993254899978638, 'rmse': 0.19983130135138083, 'r2': 0.01979011297225952}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0331Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0423Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0491Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0476Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0427Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0417Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0393Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0400Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0416Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0398Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0391Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0415Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0404Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0398Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0420Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0421Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0415Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0431Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0443Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0443Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0433Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0429Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0416Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0420Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0444Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0448Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0441Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0438Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0465Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0457Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0459Epoch 12/15: [================              ] 32/60 batches, loss: 0.0459Epoch 12/15: [================              ] 33/60 batches, loss: 0.0452Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0467Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0466Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0458Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0452Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0463Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0460Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0453Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0456Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0456Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0459Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0468Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0466Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0461Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0458Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0452Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0449Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0448Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0451Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0453Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0450Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0446Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0448Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0446Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0448Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0453Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0454Epoch 12/15: [==============================] 60/60 batches, loss: 0.0450
[2025-05-07 20:04:25,647][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0450
[2025-05-07 20:04:25,950][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0426, Metrics: {'mse': 0.04150024428963661, 'rmse': 0.20371608745908265, 'r2': -0.018691539764404297}
[2025-05-07 20:04:25,951][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0312Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0458Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0379Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0384Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0356Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0333Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0320Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0320Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0346Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0341Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0344Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0342Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0356Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0356Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0367Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0359Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0362Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0367Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0370Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0406Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0400Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0404Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0408Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0405Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0411Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0401Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0417Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0414Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0410Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0404Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0398Epoch 13/15: [================              ] 32/60 batches, loss: 0.0391Epoch 13/15: [================              ] 33/60 batches, loss: 0.0398Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0395Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0393Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0397Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0409Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0406Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0410Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0409Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0412Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0407Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0407Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0410Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0411Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0412Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0411Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0414Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0415Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0412Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0410Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0415Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0412Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0420Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0421Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0417Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0422Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0428Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0427Epoch 13/15: [==============================] 60/60 batches, loss: 0.0423
[2025-05-07 20:04:27,865][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0423
[2025-05-07 20:04:28,156][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0352, Metrics: {'mse': 0.034245338290929794, 'rmse': 0.18505496019001974, 'r2': 0.15939205884933472}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0267Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0298Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0333Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0402Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0392Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0400Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0427Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0464Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0456Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0483Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0484Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0498Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0508Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0502Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0479Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0476Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0472Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0461Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0457Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0472Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0486Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0478Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0465Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0462Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0462Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0454Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0458Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0454Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0450Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0447Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0443Epoch 14/15: [================              ] 32/60 batches, loss: 0.0440Epoch 14/15: [================              ] 33/60 batches, loss: 0.0435Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0438Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0440Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0438Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0454Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0448Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0452Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0449Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0447Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0444Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0439Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0441Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0438Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0440Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0439Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0433Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0439Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0442Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0439Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0440Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0436Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0440Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0438Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0434Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0430Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0426Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0420Epoch 14/15: [==============================] 60/60 batches, loss: 0.0416
[2025-05-07 20:04:30,483][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0416
[2025-05-07 20:04:30,772][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0422, Metrics: {'mse': 0.041168637573719025, 'rmse': 0.20290056080188398, 'r2': -0.010551691055297852}
[2025-05-07 20:04:30,773][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0286Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0276Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0323Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0265Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0325Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0361Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0353Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0351Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0376Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0363Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0372Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0381Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0364Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0370Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0366Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0369Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0370Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0373Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0369Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0357Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0362Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0356Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0358Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0364Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0361Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0370Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0369Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0369Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0368Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0368Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0369Epoch 15/15: [================              ] 32/60 batches, loss: 0.0369Epoch 15/15: [================              ] 33/60 batches, loss: 0.0369Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0367Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0366Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0361Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0359Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0358Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0363Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0366Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0363Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0370Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0367Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0365Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0370Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0379Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0374Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0375Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0373Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0373Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0374Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0377Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0376Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0372Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0380Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0380Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0380Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0381Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0383Epoch 15/15: [==============================] 60/60 batches, loss: 0.0379
[2025-05-07 20:04:32,728][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0379
[2025-05-07 20:04:32,995][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0371, Metrics: {'mse': 0.0361943356692791, 'rmse': 0.1902480897914066, 'r2': 0.11155074834823608}
[2025-05-07 20:04:32,996][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:04:32,996][src.training.lm_trainer][INFO] - Training completed in 37.72 seconds
[2025-05-07 20:04:32,996][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:04:35,696][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.023497337475419044, 'rmse': 0.15328841272392066, 'r2': 0.18251711130142212}
[2025-05-07 20:04:35,697][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.034245338290929794, 'rmse': 0.18505496019001974, 'r2': 0.15939205884933472}
[2025-05-07 20:04:35,697][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.026289580389857292, 'rmse': 0.16214061918549988, 'r2': 0.18396371603012085}
[2025-05-07 20:04:38,610][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/id/id/model.pt
[2025-05-07 20:04:38,612][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▂▂▂▁
wandb:     best_val_mse █▆▅▄▃▂▂▂▁
wandb:      best_val_r2 ▁▃▄▅▆▇▇▇█
wandb:    best_val_rmse █▆▆▄▃▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▄▅▆▆▆▆▆▆▆▇▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▆▅▄▃▃▂▂▂▂▂▁▂▁
wandb:          val_mse █▆▆▅▄▃▃▂▂▂▂▂▁▂▁
wandb:           val_r2 ▁▃▃▄▅▆▆▇▇▇▇▇█▇█
wandb:         val_rmse █▆▆▆▄▃▃▃▂▃▂▂▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0352
wandb:     best_val_mse 0.03425
wandb:      best_val_r2 0.15939
wandb:    best_val_rmse 0.18505
wandb:            epoch 15
wandb:   final_test_mse 0.02629
wandb:    final_test_r2 0.18396
wandb:  final_test_rmse 0.16214
wandb:  final_train_mse 0.0235
wandb:   final_train_r2 0.18252
wandb: final_train_rmse 0.15329
wandb:    final_val_mse 0.03425
wandb:     final_val_r2 0.15939
wandb:   final_val_rmse 0.18505
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03791
wandb:       train_time 37.71665
wandb:         val_loss 0.03713
wandb:          val_mse 0.03619
wandb:           val_r2 0.11155
wandb:         val_rmse 0.19025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200334-eqho75rq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200334-eqho75rq/logs
Experiment probe_layer2_avg_max_depth_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/id/id/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id"         "wandb.mode=offline" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:05:15,771][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id
experiment_name: probe_layer2_avg_subordinate_chain_len_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:05:15,771][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:05:15,771][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:05:15,771][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:05:15,771][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:05:15,776][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:05:15,776][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:05:15,776][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:05:20,216][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:05:22,615][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:05:22,616][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:05:22,799][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:22,901][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:23,165][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:05:23,172][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:05:23,172][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:05:23,175][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:05:23,309][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:23,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:23,426][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:05:23,427][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:05:23,428][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:05:23,428][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:05:23,498][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:23,612][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:05:23,634][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:05:23,636][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:05:23,636][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:05:23,638][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:05:23,639][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:05:23,639][src.data.datasets][INFO] -   Mean: 0.0833, Std: 0.1932
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:05:23,639][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:05:23,640][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:05:23,640][src.data.datasets][INFO] -   Mean: 0.1851, Std: 0.2231
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:05:23,640][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:05:23,641][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:05:23,641][src.data.datasets][INFO] -   Mean: 0.2145, Std: 0.2290
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:05:23,641][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:05:23,642][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:05:23,642][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:05:31,365][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:05:31,366][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:05:31,367][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:05:31,367][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:05:31,370][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:05:31,370][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:05:31,370][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:05:31,370][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:05:31,371][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:05:31,371][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:05:31,372][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.3738Epoch 1/15: [=                             ] 2/60 batches, loss: 0.4140Epoch 1/15: [=                             ] 3/60 batches, loss: 0.4734Epoch 1/15: [==                            ] 4/60 batches, loss: 0.4416Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4247Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4167Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4674Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4811Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4792Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4800Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.4623Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4584Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4461Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4469Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4400Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4371Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4287Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4402Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4265Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4197Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4108Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4136Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4042Epoch 1/15: [============                  ] 24/60 batches, loss: 0.3990Epoch 1/15: [============                  ] 25/60 batches, loss: 0.3961Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.3934Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.3854Epoch 1/15: [==============                ] 28/60 batches, loss: 0.3810Epoch 1/15: [==============                ] 29/60 batches, loss: 0.3829Epoch 1/15: [===============               ] 30/60 batches, loss: 0.3814Epoch 1/15: [===============               ] 31/60 batches, loss: 0.3764Epoch 1/15: [================              ] 32/60 batches, loss: 0.3714Epoch 1/15: [================              ] 33/60 batches, loss: 0.3688Epoch 1/15: [=================             ] 34/60 batches, loss: 0.3661Epoch 1/15: [=================             ] 35/60 batches, loss: 0.3599Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3591Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3534Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3490Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3455Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3406Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3423Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3407Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3372Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3367Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3345Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3300Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3291Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3263Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3250Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3233Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3208Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3192Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3160Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3169Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3150Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3152Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3140Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3134Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3121Epoch 1/15: [==============================] 60/60 batches, loss: 0.3113
[2025-05-07 20:05:37,519][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3113
[2025-05-07 20:05:37,806][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1199, Metrics: {'mse': 0.11576800793409348, 'rmse': 0.34024698078615406, 'r2': -1.3252465724945068}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3021Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2393Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2131Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2058Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1739Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1834Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1978Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1976Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1980Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1927Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1951Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1938Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1987Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1999Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1973Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1976Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1965Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1976Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1983Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1984Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1993Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1993Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1970Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1923Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1943Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1919Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1894Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1872Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1845Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1830Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1818Epoch 2/15: [================              ] 32/60 batches, loss: 0.1785Epoch 2/15: [================              ] 33/60 batches, loss: 0.1772Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1766Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1789Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1791Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1798Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1819Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1833Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1855Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1854Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1837Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1815Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1828Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1817Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1821Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1802Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1794Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1789Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1776Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1778Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1776Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1769Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1762Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1757Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1739Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1735Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1732Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1720Epoch 2/15: [==============================] 60/60 batches, loss: 0.1707
[2025-05-07 20:05:40,153][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1707
[2025-05-07 20:05:40,490][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1312, Metrics: {'mse': 0.12505947053432465, 'rmse': 0.35363748462843225, 'r2': -1.511869192123413}
[2025-05-07 20:05:40,491][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1207Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1266Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1187Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1179Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1261Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1301Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1454Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1490Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1455Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1421Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1382Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1385Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1393Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1361Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1375Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1397Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1413Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1372Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1334Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1349Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1309Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1285Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1264Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1243Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1248Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1246Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1236Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1218Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1222Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1199Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1217Epoch 3/15: [================              ] 32/60 batches, loss: 0.1215Epoch 3/15: [================              ] 33/60 batches, loss: 0.1227Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1217Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1209Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1226Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1236Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1226Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1231Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1228Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1229Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1233Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1218Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1233Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1223Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1230Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1238Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1240Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1250Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1243Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1240Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1244Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1238Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1241Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1252Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1239Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1225Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1213Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1204Epoch 3/15: [==============================] 60/60 batches, loss: 0.1222
[2025-05-07 20:05:42,394][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1222
[2025-05-07 20:05:42,708][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1182, Metrics: {'mse': 0.11259280145168304, 'rmse': 0.33554850834370137, 'r2': -1.2614712715148926}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0973Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0903Epoch 4/15: [=                             ] 3/60 batches, loss: 0.0883Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1041Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1134Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1091Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1076Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1057Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1032Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.0956Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.0931Epoch 4/15: [======                        ] 12/60 batches, loss: 0.0970Epoch 4/15: [======                        ] 13/60 batches, loss: 0.0964Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.0946Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.0916Epoch 4/15: [========                      ] 16/60 batches, loss: 0.0953Epoch 4/15: [========                      ] 17/60 batches, loss: 0.0952Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.0962Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.0958Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.0953Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1019Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1020Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1003Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1045Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1043Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1055Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1052Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1044Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1051Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1061Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1072Epoch 4/15: [================              ] 32/60 batches, loss: 0.1075Epoch 4/15: [================              ] 33/60 batches, loss: 0.1079Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1069Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1069Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1053Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1053Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1044Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1039Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1028Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1056Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1070Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1057Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1046Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1039Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1025Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1013Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1006Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1003Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1013Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1005Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1001Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.0996Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.0994Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1007Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1005Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1007Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1006Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1004Epoch 4/15: [==============================] 60/60 batches, loss: 0.0994
[2025-05-07 20:05:45,095][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0994
[2025-05-07 20:05:45,419][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0889, Metrics: {'mse': 0.08486878871917725, 'rmse': 0.29132248234418373, 'r2': -0.7046234607696533}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0585Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0713Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0702Epoch 5/15: [==                            ] 4/60 batches, loss: 0.0735Epoch 5/15: [==                            ] 5/60 batches, loss: 0.0844Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1027Epoch 5/15: [===                           ] 7/60 batches, loss: 0.0986Epoch 5/15: [====                          ] 8/60 batches, loss: 0.0984Epoch 5/15: [====                          ] 9/60 batches, loss: 0.0933Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.0929Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.0919Epoch 5/15: [======                        ] 12/60 batches, loss: 0.0926Epoch 5/15: [======                        ] 13/60 batches, loss: 0.0996Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1025Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1051Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1074Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1062Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1034Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1021Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1018Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1032Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1001Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1002Epoch 5/15: [============                  ] 24/60 batches, loss: 0.0973Epoch 5/15: [============                  ] 25/60 batches, loss: 0.0979Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.0977Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.0970Epoch 5/15: [==============                ] 28/60 batches, loss: 0.0964Epoch 5/15: [==============                ] 29/60 batches, loss: 0.0979Epoch 5/15: [===============               ] 30/60 batches, loss: 0.0986Epoch 5/15: [===============               ] 31/60 batches, loss: 0.0982Epoch 5/15: [================              ] 32/60 batches, loss: 0.0964Epoch 5/15: [================              ] 33/60 batches, loss: 0.0959Epoch 5/15: [=================             ] 34/60 batches, loss: 0.0952Epoch 5/15: [=================             ] 35/60 batches, loss: 0.0942Epoch 5/15: [==================            ] 36/60 batches, loss: 0.0943Epoch 5/15: [==================            ] 37/60 batches, loss: 0.0946Epoch 5/15: [===================           ] 38/60 batches, loss: 0.0945Epoch 5/15: [===================           ] 39/60 batches, loss: 0.0941Epoch 5/15: [====================          ] 40/60 batches, loss: 0.0936Epoch 5/15: [====================          ] 41/60 batches, loss: 0.0941Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.0952Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.0949Epoch 5/15: [======================        ] 44/60 batches, loss: 0.0936Epoch 5/15: [======================        ] 45/60 batches, loss: 0.0929Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.0925Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0912Epoch 5/15: [========================      ] 48/60 batches, loss: 0.0907Epoch 5/15: [========================      ] 49/60 batches, loss: 0.0901Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.0901Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0891Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0884Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.0876Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.0879Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.0876Epoch 5/15: [============================  ] 56/60 batches, loss: 0.0866Epoch 5/15: [============================  ] 57/60 batches, loss: 0.0866Epoch 5/15: [============================= ] 58/60 batches, loss: 0.0867Epoch 5/15: [============================= ] 59/60 batches, loss: 0.0865Epoch 5/15: [==============================] 60/60 batches, loss: 0.0858
[2025-05-07 20:05:47,657][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0858
[2025-05-07 20:05:48,037][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0876, Metrics: {'mse': 0.08378951251506805, 'rmse': 0.2894641817480499, 'r2': -0.682945728302002}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1327Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0987Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0880Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0815Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0830Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0801Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0814Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0888Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0882Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0839Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0832Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0837Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0869Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0876Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0917Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0915Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0898Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0870Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0872Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0854Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0852Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0848Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0831Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0852Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0842Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0853Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0847Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0845Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0848Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0849Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0849Epoch 6/15: [================              ] 32/60 batches, loss: 0.0842Epoch 6/15: [================              ] 33/60 batches, loss: 0.0849Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0849Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0856Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0845Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0842Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0846Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0842Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0837Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0832Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0824Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0816Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0818Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0810Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0803Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0802Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0799Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0787Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0778Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0795Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0788Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0790Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0790Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0799Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0795Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0794Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0790Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0789Epoch 6/15: [==============================] 60/60 batches, loss: 0.0794
[2025-05-07 20:05:50,372][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0794
[2025-05-07 20:05:50,664][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0758, Metrics: {'mse': 0.07248133420944214, 'rmse': 0.2692235766225576, 'r2': -0.45581650733947754}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0330Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0484Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0642Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0655Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0769Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0773Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0790Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0731Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0683Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0647Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0619Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0600Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0620Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0655Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0702Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0698Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0690Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0678Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0671Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0665Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0646Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0634Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0625Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0642Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0630Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0625Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0625Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0616Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0623Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0626Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0624Epoch 7/15: [================              ] 32/60 batches, loss: 0.0641Epoch 7/15: [================              ] 33/60 batches, loss: 0.0660Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0663Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0658Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0673Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0663Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0661Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0663Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0668Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0664Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0677Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0682Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0678Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0672Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0669Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0670Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0671Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0676Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0678Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0689Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0687Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0698Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0696Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0693Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0688Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0690Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0684Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0683Epoch 7/15: [==============================] 60/60 batches, loss: 0.0688
[2025-05-07 20:05:52,944][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0688
[2025-05-07 20:05:53,184][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0796, Metrics: {'mse': 0.07616865634918213, 'rmse': 0.2759866959641028, 'r2': -0.529877781867981}
[2025-05-07 20:05:53,185][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0470Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0625Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0692Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0665Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0671Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0677Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0694Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0724Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0694Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0708Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0697Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0711Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0703Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0725Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0723Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0708Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0696Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0679Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0681Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0681Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0677Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0670Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0671Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0682Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0673Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0656Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0662Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0665Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0663Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0653Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0660Epoch 8/15: [================              ] 32/60 batches, loss: 0.0660Epoch 8/15: [================              ] 33/60 batches, loss: 0.0649Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0664Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0673Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0665Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0672Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0671Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0670Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0668Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0669Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0668Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0659Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0663Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0662Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0659Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0663Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0664Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0661Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0655Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0653Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0651Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0649Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0647Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0642Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0648Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0646Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0649Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0649Epoch 8/15: [==============================] 60/60 batches, loss: 0.0644
[2025-05-07 20:05:55,047][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0644
[2025-05-07 20:05:55,325][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0739, Metrics: {'mse': 0.07066754251718521, 'rmse': 0.26583367453576157, 'r2': -0.41938579082489014}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0604Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0535Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0501Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0515Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0522Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0522Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0534Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0517Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0488Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0493Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0502Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0498Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0486Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0494Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0507Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0495Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0502Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0509Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0491Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0496Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0503Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0506Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0501Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0500Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0531Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0563Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0583Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0577Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0574Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0602Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0598Epoch 9/15: [================              ] 32/60 batches, loss: 0.0591Epoch 9/15: [================              ] 33/60 batches, loss: 0.0584Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0573Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0581Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0578Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0583Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0589Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0587Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0597Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0598Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0600Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0596Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0608Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0612Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0613Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0606Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0598Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0596Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0590Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0590Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0594Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0589Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0590Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0589Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0606Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0603Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0608Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0608Epoch 9/15: [==============================] 60/60 batches, loss: 0.0608
[2025-05-07 20:05:57,631][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0608
[2025-05-07 20:05:57,914][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0583, Metrics: {'mse': 0.05606737360358238, 'rmse': 0.23678550125289002, 'r2': -0.12613558769226074}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0353Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0534Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0548Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0571Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0581Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0518Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0507Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0496Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0478Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0553Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0575Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0566Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0587Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0564Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0545Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0551Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0563Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0543Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0542Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0562Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0558Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0572Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0567Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0570Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0567Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0585Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0592Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0589Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0594Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0587Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0584Epoch 10/15: [================              ] 32/60 batches, loss: 0.0591Epoch 10/15: [================              ] 33/60 batches, loss: 0.0586Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0585Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0583Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0583Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0578Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0575Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0570Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0564Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0565Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0569Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0566Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0562Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0561Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0555Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0557Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0558Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0562Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0561Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0565Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0562Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0560Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0557Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0556Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0557Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0559Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0554Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0552Epoch 10/15: [==============================] 60/60 batches, loss: 0.0549
[2025-05-07 20:06:00,194][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0549
[2025-05-07 20:06:00,484][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0621, Metrics: {'mse': 0.05967383831739426, 'rmse': 0.2442822922714503, 'r2': -0.19857287406921387}
[2025-05-07 20:06:00,485][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0576Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0551Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0530Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0573Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0552Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0537Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0530Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0549Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0525Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0494Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0471Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0566Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0545Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0535Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0523Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0524Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0575Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0575Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0571Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0582Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0582Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0572Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0566Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0561Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0560Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0560Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0555Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0547Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0546Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0553Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0544Epoch 11/15: [================              ] 32/60 batches, loss: 0.0540Epoch 11/15: [================              ] 33/60 batches, loss: 0.0537Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0539Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0529Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0526Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0525Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0520Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0520Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0527Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0527Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0519Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0515Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0515Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0523Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0515Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0507Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0516Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0519Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0519Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0517Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0515Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0529Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0535Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0532Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0533Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0532Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0531Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0527Epoch 11/15: [==============================] 60/60 batches, loss: 0.0529
[2025-05-07 20:06:02,354][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0529
[2025-05-07 20:06:02,639][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0657, Metrics: {'mse': 0.06300702691078186, 'rmse': 0.2510120055112541, 'r2': -0.26552116870880127}
[2025-05-07 20:06:02,640][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0584Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0485Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0497Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0481Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0476Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0474Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0455Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0455Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0439Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0458Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0462Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0456Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0477Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0478Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0501Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0492Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0477Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0485Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0487Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0491Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0490Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0479Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0467Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0474Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0523Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0530Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0526Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0527Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0539Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0540Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0534Epoch 12/15: [================              ] 32/60 batches, loss: 0.0539Epoch 12/15: [================              ] 33/60 batches, loss: 0.0528Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0538Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0542Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0541Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0535Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0542Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0538Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0536Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0540Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0544Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0548Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0554Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0553Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0549Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0553Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0550Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0550Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0547Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0555Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0557Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0551Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0548Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0549Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0550Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0547Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0544Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0544Epoch 12/15: [==============================] 60/60 batches, loss: 0.0538
[2025-05-07 20:06:04,577][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0538
[2025-05-07 20:06:04,923][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0713, Metrics: {'mse': 0.06839045882225037, 'rmse': 0.2615156951738277, 'r2': -0.3736497163772583}
[2025-05-07 20:06:04,924][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0415Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0414Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0515Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0515Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0524Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0493Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0604Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0589Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0548Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0558Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0544Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0536Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0533Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0523Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0554Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0543Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0539Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0521Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0518Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0514Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0516Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0512Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0509Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0533Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0536Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0523Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0538Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0529Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0529Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0530Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0522Epoch 13/15: [================              ] 32/60 batches, loss: 0.0533Epoch 13/15: [================              ] 33/60 batches, loss: 0.0526Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0519Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0516Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0518Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0517Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0510Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0520Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0524Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0530Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0530Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0531Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0531Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0524Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0528Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0525Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0522Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0520Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0517Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0516Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0520Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0516Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0516Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0520Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0522Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0530Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0533Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0529Epoch 13/15: [==============================] 60/60 batches, loss: 0.0523
[2025-05-07 20:06:06,792][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0523
[2025-05-07 20:06:07,110][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0558, Metrics: {'mse': 0.05397097393870354, 'rmse': 0.23231653823760273, 'r2': -0.08402848243713379}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0459Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0568Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0749Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0674Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0579Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0607Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0653Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0613Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0597Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0614Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0590Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0563Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0560Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0566Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0573Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0563Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0567Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0558Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0570Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0566Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0568Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0573Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0564Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0560Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0553Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0546Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0554Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0542Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0531Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0526Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0518Epoch 14/15: [================              ] 32/60 batches, loss: 0.0512Epoch 14/15: [================              ] 33/60 batches, loss: 0.0510Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0508Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0508Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0518Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0512Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0519Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0519Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0526Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0526Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0524Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0530Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0532Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0529Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0522Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0531Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0526Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0527Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0529Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0527Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0535Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0537Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0535Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0532Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0529Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0525Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0522Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0519Epoch 14/15: [==============================] 60/60 batches, loss: 0.0513
[2025-05-07 20:06:09,400][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0513
[2025-05-07 20:06:09,678][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0747, Metrics: {'mse': 0.07191649079322815, 'rmse': 0.26817250193341624, 'r2': -0.44447124004364014}
[2025-05-07 20:06:09,679][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0508Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0678Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0592Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0576Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0545Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0676Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0612Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0553Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0527Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0519Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0517Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0506Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0500Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0503Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0488Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0498Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0479Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0479Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0488Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0477Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0479Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0494Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0504Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0494Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0490Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0495Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0499Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0494Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0491Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0492Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0488Epoch 15/15: [================              ] 32/60 batches, loss: 0.0482Epoch 15/15: [================              ] 33/60 batches, loss: 0.0495Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0498Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0492Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0485Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0488Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0487Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0482Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0497Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0494Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0490Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0487Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0488Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0485Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0488Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0496Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0498Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0500Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0494Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0489Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0487Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0484Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0482Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0481Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0496Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0495Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0497Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0498Epoch 15/15: [==============================] 60/60 batches, loss: 0.0492
[2025-05-07 20:06:11,556][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0492
[2025-05-07 20:06:11,856][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0641, Metrics: {'mse': 0.06178044527769089, 'rmse': 0.24855672446685262, 'r2': -0.2408849000930786}
[2025-05-07 20:06:11,856][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:06:11,856][src.training.lm_trainer][INFO] - Training completed in 36.90 seconds
[2025-05-07 20:06:11,856][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:06:14,484][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.033984992653131485, 'rmse': 0.18435019027148164, 'r2': 0.08991754055023193}
[2025-05-07 20:06:14,485][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05397097393870354, 'rmse': 0.23231653823760273, 'r2': -0.08402848243713379}
[2025-05-07 20:06:14,485][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06518625468015671, 'rmse': 0.25531598986384835, 'r2': -0.24327707290649414}
[2025-05-07 20:06:16,633][src.training.lm_trainer][ERROR] - Error during training: [enforce fail at inline_container.cc:603] . unexpected pos 196150656 vs 196150608
[2025-05-07 20:06:16,675][src.training.lm_trainer][ERROR] - Traceback: Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/2: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/training/lm_trainer.py", line 347, in train
    torch.save(self.model.state_dict(), model_path)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 857, in save
    return
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 690, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 196150656 vs 196150608

[2025-05-07 20:06:16,733][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▅▄▃▃▁▁
wandb:     best_val_mse ██▄▄▃▃▁▁
wandb:      best_val_r2 ▁▁▅▅▆▆██
wandb:    best_val_rmse ██▅▅▃▃▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▂▅▅▆▅▆▇▆▆▆▇▆
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▇▄▄▃▃▃▁▂▂▂▁▃▂
wandb:          val_mse ▇█▇▄▄▃▃▃▁▂▂▂▁▃▂
wandb:           val_r2 ▂▁▂▅▅▆▆▆█▇▇▇█▆▇
wandb:         val_rmse ▇█▇▄▄▃▄▃▁▂▂▃▁▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05584
wandb:     best_val_mse 0.05397
wandb:      best_val_r2 -0.08403
wandb:    best_val_rmse 0.23232
wandb:            epoch 15
wandb:   final_test_mse 0.06519
wandb:    final_test_r2 -0.24328
wandb:  final_test_rmse 0.25532
wandb:  final_train_mse 0.03398
wandb:   final_train_r2 0.08992
wandb: final_train_rmse 0.18435
wandb:    final_val_mse 0.05397
wandb:     final_val_r2 -0.08403
wandb:   final_val_rmse 0.23232
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04924
wandb:       train_time 36.9025
wandb:         val_loss 0.06411
wandb:          val_mse 0.06178
wandb:           val_r2 -0.24088
wandb:         val_rmse 0.24856
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200515-s7eeho95
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200515-s7eeho95/logs
Experiment probe_layer2_avg_subordinate_chain_len_id completed successfully
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/id/id/results.json for layer 2
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
OSError: [Errno 122] Disk quota exceeded
Running experiment: probe_layer2_avg_verb_edges_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:06:49,203][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id
experiment_name: probe_layer2_avg_verb_edges_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:06:49,204][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:06:49,204][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:06:49,204][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:06:49,204][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:06:49,218][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:06:49,218][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:06:49,218][__main__][INFO] - Processing language: id
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 661, in setup_run_log_directory
    self._wl._early_logger_flush(logging.getLogger("wandb"))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 163, in _early_logger_flush
    self._logger._flush(new_logger)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 67, in _flush
    new_logger.log(level, msg, *args, **kwargs)
Message: 'Current SDK version is 0.19.9'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 661, in setup_run_log_directory
    self._wl._early_logger_flush(logging.getLogger("wandb"))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 163, in _early_logger_flush
    self._logger._flush(new_logger)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 67, in _flush
    new_logger.log(level, msg, *args, **kwargs)
Message: 'Configure stats pid to 3321763'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 661, in setup_run_log_directory
    self._wl._early_logger_flush(logging.getLogger("wandb"))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 163, in _early_logger_flush
    self._logger._flush(new_logger)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 67, in _flush
    new_logger.log(level, msg, *args, **kwargs)
Message: 'Loading settings from /user/leuven/371/vsc37132/.config/wandb/settings'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 661, in setup_run_log_directory
    self._wl._early_logger_flush(logging.getLogger("wandb"))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 163, in _early_logger_flush
    self._logger._flush(new_logger)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 67, in _flush
    new_logger.log(level, msg, *args, **kwargs)
Message: 'Loading settings from /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/settings'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 661, in setup_run_log_directory
    self._wl._early_logger_flush(logging.getLogger("wandb"))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 163, in _early_logger_flush
    self._logger._flush(new_logger)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py", line 67, in _flush
    new_logger.log(level, msg, *args, **kwargs)
Message: 'Loading settings from environment variables'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 662, in setup_run_log_directory
    self._logger.info(f"Logging user logs to {settings.log_user}")
Message: 'Logging user logs to /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200649-c7malhww/logs/debug.log'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1512, in init
    exit_stack.enter_context(wi.setup_run_log_directory(run_settings))
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 448, in enter_context
    result = _cm_type.__enter__(cm)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 663, in setup_run_log_directory
    self._logger.info(f"Logging internal logs to {settings.log_internal}")
Message: 'Logging internal logs to /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200649-c7malhww/logs/debug-internal.log'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 781, in init
    self._logger.info("calling init triggers")
Message: 'calling init triggers'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 786, in init
    self._logger.info(
Message: "wandb.init called with sweep_config: {}\nconfig: {'experiment': {'type': 'lm_probe', 'task': 'avg_verb_edges', 'language': 'id', 'languages': None, 'train_language': None, 'eval_language': None, 'use_controls': False, 'control_index': None}, 'model': {'model_type': 'lm_probe', 'lm_name': 'cis-lmu/glot500-base', 'dropout': 0.2, 'freeze_model': True, 'layer_wise': True, 'layer_index': 2, 'num_outputs': 1, 'probe_hidden_size': 128, 'probe_depth': 3, 'activation': 'silu', 'normalization': 'layer', 'weight_init': 'xavier', 'output_standardization': True, 'use_linear_probe': False, 'use_mean_pooling': True, 'use_class_weights': False}, 'training': {'task_type': 'regression', 'batch_size': 16, 'num_epochs': 15, 'lr': 0.0001, 'weight_decay': 0.01, 'patience': 4, 'scheduler_factor': 0.5, 'scheduler_patience': 2, 'random_state': 42, 'num_workers': 4, 'gradient_accumulation_steps': 2}, 'data': {'dataset_name': 'rokokot/question-type-and-complexity', 'cache_dir': '/data/leuven/371/vsc37132/qtype-eval/data/cache', 'vectors_dir': './data/features', 'languages': ['id'], 'train_language': None, 'eval_language': None}, 'seed': 42, '_wandb': {}}"
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 809, in init
    self._logger.info("starting backend")
Message: 'starting backend'
Arguments: ()
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 813, in init
    self._logger.info("sending inform_init request")
Message: 'sending inform_init request'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 821, in init
    backend = Backend(settings=settings, service=service)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/backend/backend.py", line 83, in __init__
    self._multiprocessing_setup()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/backend/backend.py", line 101, in _multiprocessing_setup
    logger.info(
Message: 'multiprocessing start_methods=fork,spawn,forkserver, using: spawn'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 823, in init
    self._logger.info("backend started and connected")
Message: 'backend started and connected'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 915, in init
    self._logger.info("updated telemetry")
Message: 'updated telemetry'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 939, in init
    self._logger.info(
Message: 'communicating run to backend with 90.0 second timeout'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1014, in init
    self._logger.info("starting run threads in backend")
Message: 'starting run threads in backend'
Arguments: ()
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1055, in init
    run._on_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2499, in _on_start
    self._console_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2454, in _console_start
    logger.info("atexit reg")
Message: 'atexit reg'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1055, in init
    run._on_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2499, in _on_start
    self._console_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2462, in _console_start
    self._redirect(self._stdout_slave_fd, self._stderr_slave_fd)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2306, in _redirect
    logger.info("redirect: %s", console)
Message: 'redirect: %s'
Arguments: ('wrap_raw',)
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1055, in init
    run._on_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2499, in _on_start
    self._console_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2462, in _console_start
    self._redirect(self._stdout_slave_fd, self._stderr_slave_fd)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2371, in _redirect
    logger.info("Wrapping output streams.")
Message: 'Wrapping output streams.'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1055, in init
    run._on_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2499, in _on_start
    self._console_start()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2462, in _console_start
    self._redirect(self._stdout_slave_fd, self._stderr_slave_fd)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2394, in _redirect
    logger.info("Redirects installed.")
Message: 'Redirects installed.'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 287, in run_lm_experiment
    wandb_run = setup_wandb(
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 81, in setup_wandb
    run = wandb.init(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1530, in init
    return wi.init(run_settings, run_config)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1056, in init
    self._logger.info("run started, returning control to user process")
Message: 'run started, returning control to user process'
Arguments: ()
[2025-05-07 20:06:52,876][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:06:55,149][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:06:55,150][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:06:55,374][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:55,464][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:55,814][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:06:55,821][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:06:55,822][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:06:55,824][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:06:55,907][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:56,066][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:56,105][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:06:56,106][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:06:56,107][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:06:56,110][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:06:56,222][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:56,330][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:06:56,359][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:06:56,360][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:06:56,360][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:06:56,363][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:06:56,363][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:06:56,363][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:06:56,363][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:06:56,364][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:06:56,364][src.data.datasets][INFO] -   Mean: 0.3916, Std: 0.3139
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Sample label: 0.75
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:06:56,364][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:06:56,365][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 20:06:56,365][src.data.datasets][INFO] -   Mean: 0.3834, Std: 0.2455
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:06:56,365][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:06:56,365][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:06:56,366][src.data.datasets][INFO] -   Mean: 0.4230, Std: 0.2527
[2025-05-07 20:06:56,366][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:06:56,366][src.data.datasets][INFO] - Sample label: 0.5830000042915344
[2025-05-07 20:06:56,366][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:06:56,366][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:06:56,366][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:06:56,366][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 20:06:56,367][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:07:05,323][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:07:05,324][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:07:05,324][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:07:05,324][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:07:05,327][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:07:05,327][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:07:05,327][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:07:05,328][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:07:05,328][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:07:05,328][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:07:05,329][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5107Epoch 1/15: [=                             ] 2/60 batches, loss: 0.5475Epoch 1/15: [=                             ] 3/60 batches, loss: 0.6137Epoch 1/15: [==                            ] 4/60 batches, loss: 0.6163Epoch 1/15: [==                            ] 5/60 batches, loss: 0.6006Epoch 1/15: [===                           ] 6/60 batches, loss: 0.5826Epoch 1/15: [===                           ] 7/60 batches, loss: 0.6002Epoch 1/15: [====                          ] 8/60 batches, loss: 0.6110Epoch 1/15: [====                          ] 9/60 batches, loss: 0.6185Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.6072Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.5721Epoch 1/15: [======                        ] 12/60 batches, loss: 0.5559Epoch 1/15: [======                        ] 13/60 batches, loss: 0.5478Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.5386Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.5235Epoch 1/15: [========                      ] 16/60 batches, loss: 0.5115Epoch 1/15: [========                      ] 17/60 batches, loss: 0.5089Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.5124Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4948Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4905Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4878Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4920Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4857Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4741Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4695Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4691Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4597Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4552Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4515Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4531Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4469Epoch 1/15: [================              ] 32/60 batches, loss: 0.4439Epoch 1/15: [================              ] 33/60 batches, loss: 0.4386Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4375Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4314Epoch 1/15: [==================            ] 36/60 batches, loss: 0.4284Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4217Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4173Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4108Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4065Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4023Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4010Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3994Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3963Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3945Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3911Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3898Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3859Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3850Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3828Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3833Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3813Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3780Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3785Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3773Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3763Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3784Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3757Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3752Epoch 1/15: [==============================] 60/60 batches, loss: 0.3720
[2025-05-07 20:07:11,692][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3720
[2025-05-07 20:07:11,950][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0844, Metrics: {'mse': 0.08935381472110748, 'rmse': 0.298921084437193, 'r2': -0.4821743965148926}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.4533Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2970Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2484Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2432Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2259Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2495Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2435Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2553Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2659Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2565Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2533Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2460Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2539Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2567Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2463Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2435Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2517Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2471Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2490Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2490Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2452Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2462Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2478Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2421Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2438Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2438Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2455Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2437Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2429Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2416Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2421Epoch 2/15: [================              ] 32/60 batches, loss: 0.2408Epoch 2/15: [================              ] 33/60 batches, loss: 0.2389Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2380Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2404Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2388Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2386Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2363Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2368Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2361Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2373Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2365Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2363Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2372Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2355Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2335Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2311Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2279Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2278Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2264Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2252Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2245Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.2225Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.2233Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.2227Epoch 2/15: [============================  ] 56/60 batches, loss: 0.2225Epoch 2/15: [============================  ] 57/60 batches, loss: 0.2236Epoch 2/15: [============================= ] 58/60 batches, loss: 0.2231Epoch 2/15: [============================= ] 59/60 batches, loss: 0.2222Epoch 2/15: [==============================] 60/60 batches, loss: 0.2207
[2025-05-07 20:07:14,218][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2207
[2025-05-07 20:07:14,536][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0804, Metrics: {'mse': 0.08465778827667236, 'rmse': 0.29096011458045645, 'r2': -0.4042781591415405}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1426Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1878Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1764Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1902Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1914Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1888Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1981Epoch 3/15: [====                          ] 8/60 batches, loss: 0.2010Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1944Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1889Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1936Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1997Epoch 3/15: [======                        ] 13/60 batches, loss: 0.2030Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1995Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1950Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1933Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1882Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1862Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1842Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1855Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1851Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1873Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1834Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1812Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1821Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1841Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1839Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1816Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1832Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1813Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1810Epoch 3/15: [================              ] 32/60 batches, loss: 0.1802Epoch 3/15: [================              ] 33/60 batches, loss: 0.1839Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1828Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1821Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1847Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1842Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1831Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1826Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1814Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1821Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1798Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1798Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1777Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1762Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1762Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1760Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1758Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1766Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1746Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1730Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1744Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1731Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1733Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1722Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1710Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1713Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1700Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1693Epoch 3/15: [==============================] 60/60 batches, loss: 0.1724
[2025-05-07 20:07:16,897][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1724
[2025-05-07 20:07:17,243][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0634, Metrics: {'mse': 0.06699129194021225, 'rmse': 0.2588267604793064, 'r2': -0.11123156547546387}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1293Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1501Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1677Epoch 4/15: [==                            ] 4/60 batches, loss: 0.2094Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1794Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1828Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1849Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1750Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1670Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1555Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1676Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1644Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1590Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1549Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1600Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1648Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1680Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1638Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1688Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1687Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1712Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1707Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1697Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1777Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1765Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1775Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1762Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1751Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1745Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1717Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1701Epoch 4/15: [================              ] 32/60 batches, loss: 0.1694Epoch 4/15: [================              ] 33/60 batches, loss: 0.1671Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1670Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1662Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1640Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1624Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1620Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1618Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1615Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1624Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1642Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1639Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1628Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1617Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1605Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1590Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1578Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1569Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1566Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1569Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1577Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1562Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1555Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1551Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1560Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1549Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1553Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1557Epoch 4/15: [==============================] 60/60 batches, loss: 0.1553
[2025-05-07 20:07:19,586][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1553
[2025-05-07 20:07:19,926][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0572, Metrics: {'mse': 0.060778241604566574, 'rmse': 0.2465324351978185, 'r2': -0.008171319961547852}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1371Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1672Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1956Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1818Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1764Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1771Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1747Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1671Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1594Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1612Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1577Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1615Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1599Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1602Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1556Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1572Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1567Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1563Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1542Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1558Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1545Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1530Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1532Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1523Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1525Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1538Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1517Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1527Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1558Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1573Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1582Epoch 5/15: [================              ] 32/60 batches, loss: 0.1574Epoch 5/15: [================              ] 33/60 batches, loss: 0.1582Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1575Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1613Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1611Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1594Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1582Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1562Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1549Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1541Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1521Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1524Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1507Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1527Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1510Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1502Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1509Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1509Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1496Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1490Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1486Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1483Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1472Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1467Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1459Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1448Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1454Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1452Epoch 5/15: [==============================] 60/60 batches, loss: 0.1448
[2025-05-07 20:07:22,225][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1448
[2025-05-07 20:07:22,544][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0509, Metrics: {'mse': 0.05408583953976631, 'rmse': 0.23256362471325198, 'r2': 0.10284024477005005}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1163Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1137Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1301Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1191Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1267Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1206Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1256Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1331Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1311Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1294Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1230Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1214Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1263Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1284Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1264Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1257Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1255Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1245Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1259Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1276Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1291Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1302Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1304Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1288Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1304Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1304Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1293Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1276Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1271Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1267Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1252Epoch 6/15: [================              ] 32/60 batches, loss: 0.1236Epoch 6/15: [================              ] 33/60 batches, loss: 0.1272Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1293Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1290Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1271Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1261Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1266Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1279Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1284Epoch 6/15: [====================          ] 41/60 batches, loss: 0.1291Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.1302Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.1300Epoch 6/15: [======================        ] 44/60 batches, loss: 0.1299Epoch 6/15: [======================        ] 45/60 batches, loss: 0.1314Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.1304Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.1312Epoch 6/15: [========================      ] 48/60 batches, loss: 0.1302Epoch 6/15: [========================      ] 49/60 batches, loss: 0.1303Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.1304Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.1326Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.1333Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.1328Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.1316Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.1320Epoch 6/15: [============================  ] 56/60 batches, loss: 0.1318Epoch 6/15: [============================  ] 57/60 batches, loss: 0.1314Epoch 6/15: [============================= ] 58/60 batches, loss: 0.1313Epoch 6/15: [============================= ] 59/60 batches, loss: 0.1298Epoch 6/15: [==============================] 60/60 batches, loss: 0.1289
[2025-05-07 20:07:24,778][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1289
[2025-05-07 20:07:25,126][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0484, Metrics: {'mse': 0.05158223584294319, 'rmse': 0.22711722929567274, 'r2': 0.1443692445755005}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.1028Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0858Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1199Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1109Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1074Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1176Epoch 7/15: [===                           ] 7/60 batches, loss: 0.1214Epoch 7/15: [====                          ] 8/60 batches, loss: 0.1274Epoch 7/15: [====                          ] 9/60 batches, loss: 0.1279Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.1240Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.1175Epoch 7/15: [======                        ] 12/60 batches, loss: 0.1189Epoch 7/15: [======                        ] 13/60 batches, loss: 0.1192Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.1177Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.1183Epoch 7/15: [========                      ] 16/60 batches, loss: 0.1168Epoch 7/15: [========                      ] 17/60 batches, loss: 0.1158Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.1158Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.1166Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.1153Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.1149Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.1156Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.1193Epoch 7/15: [============                  ] 24/60 batches, loss: 0.1199Epoch 7/15: [============                  ] 25/60 batches, loss: 0.1188Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.1178Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.1177Epoch 7/15: [==============                ] 28/60 batches, loss: 0.1181Epoch 7/15: [==============                ] 29/60 batches, loss: 0.1209Epoch 7/15: [===============               ] 30/60 batches, loss: 0.1205Epoch 7/15: [===============               ] 31/60 batches, loss: 0.1219Epoch 7/15: [================              ] 32/60 batches, loss: 0.1219Epoch 7/15: [================              ] 33/60 batches, loss: 0.1227Epoch 7/15: [=================             ] 34/60 batches, loss: 0.1216Epoch 7/15: [=================             ] 35/60 batches, loss: 0.1209Epoch 7/15: [==================            ] 36/60 batches, loss: 0.1203Epoch 7/15: [==================            ] 37/60 batches, loss: 0.1204Epoch 7/15: [===================           ] 38/60 batches, loss: 0.1198Epoch 7/15: [===================           ] 39/60 batches, loss: 0.1197Epoch 7/15: [====================          ] 40/60 batches, loss: 0.1199Epoch 7/15: [====================          ] 41/60 batches, loss: 0.1193Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.1196Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.1192Epoch 7/15: [======================        ] 44/60 batches, loss: 0.1191Epoch 7/15: [======================        ] 45/60 batches, loss: 0.1196Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.1203Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.1199Epoch 7/15: [========================      ] 48/60 batches, loss: 0.1195Epoch 7/15: [========================      ] 49/60 batches, loss: 0.1194Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.1188Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.1191Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.1181Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.1178Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.1171Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.1174Epoch 7/15: [============================  ] 56/60 batches, loss: 0.1172Epoch 7/15: [============================  ] 57/60 batches, loss: 0.1178Epoch 7/15: [============================= ] 58/60 batches, loss: 0.1172Epoch 7/15: [============================= ] 59/60 batches, loss: 0.1180Epoch 7/15: [==============================] 60/60 batches, loss: 0.1177
[2025-05-07 20:07:27,562][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1177
[2025-05-07 20:07:27,920][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0451, Metrics: {'mse': 0.04768851771950722, 'rmse': 0.2183770082208913, 'r2': 0.20895713567733765}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0869Epoch 8/15: [=                             ] 2/60 batches, loss: 0.1018Epoch 8/15: [=                             ] 3/60 batches, loss: 0.1129Epoch 8/15: [==                            ] 4/60 batches, loss: 0.1094Epoch 8/15: [==                            ] 5/60 batches, loss: 0.1102Epoch 8/15: [===                           ] 6/60 batches, loss: 0.1116Epoch 8/15: [===                           ] 7/60 batches, loss: 0.1111Epoch 8/15: [====                          ] 8/60 batches, loss: 0.1170Epoch 8/15: [====                          ] 9/60 batches, loss: 0.1133Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.1157Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.1083Epoch 8/15: [======                        ] 12/60 batches, loss: 0.1061Epoch 8/15: [======                        ] 13/60 batches, loss: 0.1060Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.1102Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.1137Epoch 8/15: [========                      ] 16/60 batches, loss: 0.1146Epoch 8/15: [========                      ] 17/60 batches, loss: 0.1159Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.1158Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.1151Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.1163Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.1165Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.1178Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.1169Epoch 8/15: [============                  ] 24/60 batches, loss: 0.1177Epoch 8/15: [============                  ] 25/60 batches, loss: 0.1159Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.1129Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.1132Epoch 8/15: [==============                ] 28/60 batches, loss: 0.1133Epoch 8/15: [==============                ] 29/60 batches, loss: 0.1140Epoch 8/15: [===============               ] 30/60 batches, loss: 0.1129Epoch 8/15: [===============               ] 31/60 batches, loss: 0.1118Epoch 8/15: [================              ] 32/60 batches, loss: 0.1111Epoch 8/15: [================              ] 33/60 batches, loss: 0.1107Epoch 8/15: [=================             ] 34/60 batches, loss: 0.1113Epoch 8/15: [=================             ] 35/60 batches, loss: 0.1114Epoch 8/15: [==================            ] 36/60 batches, loss: 0.1109Epoch 8/15: [==================            ] 37/60 batches, loss: 0.1117Epoch 8/15: [===================           ] 38/60 batches, loss: 0.1120Epoch 8/15: [===================           ] 39/60 batches, loss: 0.1130Epoch 8/15: [====================          ] 40/60 batches, loss: 0.1133Epoch 8/15: [====================          ] 41/60 batches, loss: 0.1157Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.1158Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.1152Epoch 8/15: [======================        ] 44/60 batches, loss: 0.1152Epoch 8/15: [======================        ] 45/60 batches, loss: 0.1156Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.1143Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.1134Epoch 8/15: [========================      ] 48/60 batches, loss: 0.1133Epoch 8/15: [========================      ] 49/60 batches, loss: 0.1153Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.1157Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.1153Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.1152Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.1154Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.1152Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.1162Epoch 8/15: [============================  ] 56/60 batches, loss: 0.1159Epoch 8/15: [============================  ] 57/60 batches, loss: 0.1168Epoch 8/15: [============================= ] 58/60 batches, loss: 0.1184Epoch 8/15: [============================= ] 59/60 batches, loss: 0.1178Epoch 8/15: [==============================] 60/60 batches, loss: 0.1170
[2025-05-07 20:07:30,189][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1170
[2025-05-07 20:07:30,517][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0468, Metrics: {'mse': 0.049826376140117645, 'rmse': 0.22321822537623948, 'r2': 0.17349493503570557}
[2025-05-07 20:07:30,518][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.0557Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0609Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0641Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0710Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0829Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0821Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0787Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0881Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0951Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0952Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0968Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0920Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0966Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.1017Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0999Epoch 9/15: [========                      ] 16/60 batches, loss: 0.1004Epoch 9/15: [========                      ] 17/60 batches, loss: 0.1004Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0983Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0993Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.1008Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.1003Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.1035Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.1063Epoch 9/15: [============                  ] 24/60 batches, loss: 0.1060Epoch 9/15: [============                  ] 25/60 batches, loss: 0.1063Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.1059Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.1062Epoch 9/15: [==============                ] 28/60 batches, loss: 0.1068Epoch 9/15: [==============                ] 29/60 batches, loss: 0.1069Epoch 9/15: [===============               ] 30/60 batches, loss: 0.1092Epoch 9/15: [===============               ] 31/60 batches, loss: 0.1101Epoch 9/15: [================              ] 32/60 batches, loss: 0.1098Epoch 9/15: [================              ] 33/60 batches, loss: 0.1088Epoch 9/15: [=================             ] 34/60 batches, loss: 0.1094Epoch 9/15: [=================             ] 35/60 batches, loss: 0.1091Epoch 9/15: [==================            ] 36/60 batches, loss: 0.1097Epoch 9/15: [==================            ] 37/60 batches, loss: 0.1115Epoch 9/15: [===================           ] 38/60 batches, loss: 0.1120Epoch 9/15: [===================           ] 39/60 batches, loss: 0.1124Epoch 9/15: [====================          ] 40/60 batches, loss: 0.1134Epoch 9/15: [====================          ] 41/60 batches, loss: 0.1130Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.1133Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.1131Epoch 9/15: [======================        ] 44/60 batches, loss: 0.1120Epoch 9/15: [======================        ] 45/60 batches, loss: 0.1113Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.1109Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.1103Epoch 9/15: [========================      ] 48/60 batches, loss: 0.1105Epoch 9/15: [========================      ] 49/60 batches, loss: 0.1117Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.1111Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.1102Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.1102Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.1102Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.1092Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.1089Epoch 9/15: [============================  ] 56/60 batches, loss: 0.1080Epoch 9/15: [============================  ] 57/60 batches, loss: 0.1075Epoch 9/15: [============================= ] 58/60 batches, loss: 0.1084Epoch 9/15: [============================= ] 59/60 batches, loss: 0.1079Epoch 9/15: [==============================] 60/60 batches, loss: 0.1071
[2025-05-07 20:07:32,501][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1071
[2025-05-07 20:07:32,826][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0419, Metrics: {'mse': 0.044509414583444595, 'rmse': 0.21097254462001588, 'r2': 0.261691153049469}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0789Epoch 10/15: [=                             ] 2/60 batches, loss: 0.1007Epoch 10/15: [=                             ] 3/60 batches, loss: 0.1109Epoch 10/15: [==                            ] 4/60 batches, loss: 0.1133Epoch 10/15: [==                            ] 5/60 batches, loss: 0.1042Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0984Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0954Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0963Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0993Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.1041Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.1031Epoch 10/15: [======                        ] 12/60 batches, loss: 0.1033Epoch 10/15: [======                        ] 13/60 batches, loss: 0.1038Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.1046Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.1041Epoch 10/15: [========                      ] 16/60 batches, loss: 0.1021Epoch 10/15: [========                      ] 17/60 batches, loss: 0.1013Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.1030Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.1037Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.1054Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.1049Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.1058Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.1069Epoch 10/15: [============                  ] 24/60 batches, loss: 0.1077Epoch 10/15: [============                  ] 25/60 batches, loss: 0.1085Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.1072Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.1068Epoch 10/15: [==============                ] 28/60 batches, loss: 0.1058Epoch 10/15: [==============                ] 29/60 batches, loss: 0.1075Epoch 10/15: [===============               ] 30/60 batches, loss: 0.1058Epoch 10/15: [===============               ] 31/60 batches, loss: 0.1060Epoch 10/15: [================              ] 32/60 batches, loss: 0.1089Epoch 10/15: [================              ] 33/60 batches, loss: 0.1096Epoch 10/15: [=================             ] 34/60 batches, loss: 0.1108Epoch 10/15: [=================             ] 35/60 batches, loss: 0.1093Epoch 10/15: [==================            ] 36/60 batches, loss: 0.1091Epoch 10/15: [==================            ] 37/60 batches, loss: 0.1083Epoch 10/15: [===================           ] 38/60 batches, loss: 0.1082Epoch 10/15: [===================           ] 39/60 batches, loss: 0.1078Epoch 10/15: [====================          ] 40/60 batches, loss: 0.1073Epoch 10/15: [====================          ] 41/60 batches, loss: 0.1070Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.1064Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.1057Epoch 10/15: [======================        ] 44/60 batches, loss: 0.1051Epoch 10/15: [======================        ] 45/60 batches, loss: 0.1052Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.1053Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.1044Epoch 10/15: [========================      ] 48/60 batches, loss: 0.1045Epoch 10/15: [========================      ] 49/60 batches, loss: 0.1042Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.1044Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.1036Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.1028Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.1022Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.1024Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.1021Epoch 10/15: [============================  ] 56/60 batches, loss: 0.1021Epoch 10/15: [============================  ] 57/60 batches, loss: 0.1025Epoch 10/15: [============================= ] 58/60 batches, loss: 0.1026Epoch 10/15: [============================= ] 59/60 batches, loss: 0.1026Epoch 10/15: [==============================] 60/60 batches, loss: 0.1036
[2025-05-07 20:07:35,170][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1036
[2025-05-07 20:07:35,471][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0401, Metrics: {'mse': 0.042225584387779236, 'rmse': 0.20548864783189177, 'r2': 0.299574613571167}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1377Epoch 11/15: [=                             ] 2/60 batches, loss: 0.1130Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0962Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0941Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0949Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0928Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0958Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0919Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0983Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0979Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0996Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0988Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0991Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.1006Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.1008Epoch 11/15: [========                      ] 16/60 batches, loss: 0.1037Epoch 11/15: [========                      ] 17/60 batches, loss: 0.1021Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.1081Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.1100Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.1096Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.1083Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.1070Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.1095Epoch 11/15: [============                  ] 24/60 batches, loss: 0.1087Epoch 11/15: [============                  ] 25/60 batches, loss: 0.1081Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.1073Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.1064Epoch 11/15: [==============                ] 28/60 batches, loss: 0.1079Epoch 11/15: [==============                ] 29/60 batches, loss: 0.1078Epoch 11/15: [===============               ] 30/60 batches, loss: 0.1069Epoch 11/15: [===============               ] 31/60 batches, loss: 0.1059Epoch 11/15: [================              ] 32/60 batches, loss: 0.1047Epoch 11/15: [================              ] 33/60 batches, loss: 0.1037Epoch 11/15: [=================             ] 34/60 batches, loss: 0.1034Epoch 11/15: [=================             ] 35/60 batches, loss: 0.1036Epoch 11/15: [==================            ] 36/60 batches, loss: 0.1029Epoch 11/15: [==================            ] 37/60 batches, loss: 0.1024Epoch 11/15: [===================           ] 38/60 batches, loss: 0.1011Epoch 11/15: [===================           ] 39/60 batches, loss: 0.1021Epoch 11/15: [====================          ] 40/60 batches, loss: 0.1023Epoch 11/15: [====================          ] 41/60 batches, loss: 0.1033Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.1037Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.1037Epoch 11/15: [======================        ] 44/60 batches, loss: 0.1037Epoch 11/15: [======================        ] 45/60 batches, loss: 0.1032Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.1030Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.1022Epoch 11/15: [========================      ] 48/60 batches, loss: 0.1023Epoch 11/15: [========================      ] 49/60 batches, loss: 0.1025Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.1016Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.1010Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.1012Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.1017Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.1021Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.1018Epoch 11/15: [============================  ] 56/60 batches, loss: 0.1022Epoch 11/15: [============================  ] 57/60 batches, loss: 0.1013Epoch 11/15: [============================= ] 58/60 batches, loss: 0.1017Epoch 11/15: [============================= ] 59/60 batches, loss: 0.1020Epoch 11/15: [==============================] 60/60 batches, loss: 0.1026
[2025-05-07 20:07:37,754][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1026
[2025-05-07 20:07:38,033][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0440, Metrics: {'mse': 0.04684251546859741, 'rmse': 0.21643131813256003, 'r2': 0.22299033403396606}
[2025-05-07 20:07:38,034][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0567Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0745Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0832Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0916Epoch 12/15: [==                            ] 5/60 batches, loss: 0.1009Epoch 12/15: [===                           ] 6/60 batches, loss: 0.1096Epoch 12/15: [===                           ] 7/60 batches, loss: 0.1059Epoch 12/15: [====                          ] 8/60 batches, loss: 0.1085Epoch 12/15: [====                          ] 9/60 batches, loss: 0.1071Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.1074Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.1062Epoch 12/15: [======                        ] 12/60 batches, loss: 0.1065Epoch 12/15: [======                        ] 13/60 batches, loss: 0.1049Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.1050Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.1056Epoch 12/15: [========                      ] 16/60 batches, loss: 0.1087Epoch 12/15: [========                      ] 17/60 batches, loss: 0.1060Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.1078Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.1112Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.1113Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.1109Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.1106Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.1086Epoch 12/15: [============                  ] 24/60 batches, loss: 0.1113Epoch 12/15: [============                  ] 25/60 batches, loss: 0.1127Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.1129Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.1139Epoch 12/15: [==============                ] 28/60 batches, loss: 0.1140Epoch 12/15: [==============                ] 29/60 batches, loss: 0.1152Epoch 12/15: [===============               ] 30/60 batches, loss: 0.1145Epoch 12/15: [===============               ] 31/60 batches, loss: 0.1131Epoch 12/15: [================              ] 32/60 batches, loss: 0.1125Epoch 12/15: [================              ] 33/60 batches, loss: 0.1117Epoch 12/15: [=================             ] 34/60 batches, loss: 0.1108Epoch 12/15: [=================             ] 35/60 batches, loss: 0.1093Epoch 12/15: [==================            ] 36/60 batches, loss: 0.1086Epoch 12/15: [==================            ] 37/60 batches, loss: 0.1081Epoch 12/15: [===================           ] 38/60 batches, loss: 0.1073Epoch 12/15: [===================           ] 39/60 batches, loss: 0.1075Epoch 12/15: [====================          ] 40/60 batches, loss: 0.1077Epoch 12/15: [====================          ] 41/60 batches, loss: 0.1081Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.1075Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.1075Epoch 12/15: [======================        ] 44/60 batches, loss: 0.1074Epoch 12/15: [======================        ] 45/60 batches, loss: 0.1063Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.1060Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.1055Epoch 12/15: [========================      ] 48/60 batches, loss: 0.1054Epoch 12/15: [========================      ] 49/60 batches, loss: 0.1051Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.1042Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.1045Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.1034Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.1036Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.1033Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.1032Epoch 12/15: [============================  ] 56/60 batches, loss: 0.1027Epoch 12/15: [============================  ] 57/60 batches, loss: 0.1029Epoch 12/15: [============================= ] 58/60 batches, loss: 0.1021Epoch 12/15: [============================= ] 59/60 batches, loss: 0.1013Epoch 12/15: [==============================] 60/60 batches, loss: 0.1004
[2025-05-07 20:07:39,908][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1004
[2025-05-07 20:07:40,198][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0408, Metrics: {'mse': 0.04343532398343086, 'rmse': 0.20841142958924028, 'r2': 0.2795078158378601}
[2025-05-07 20:07:40,198][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0729Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0846Epoch 13/15: [=                             ] 3/60 batches, loss: 0.1120Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0985Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0936Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0930Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0965Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0991Epoch 13/15: [====                          ] 9/60 batches, loss: 0.1013Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.1019Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0990Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0971Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0978Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0996Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.1005Epoch 13/15: [========                      ] 16/60 batches, loss: 0.1015Epoch 13/15: [========                      ] 17/60 batches, loss: 0.1027Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.1048Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.1027Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.1023Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.1035Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.1038Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.1032Epoch 13/15: [============                  ] 24/60 batches, loss: 0.1024Epoch 13/15: [============                  ] 25/60 batches, loss: 0.1012Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0999Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0999Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0996Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0995Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0990Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0977Epoch 13/15: [================              ] 32/60 batches, loss: 0.0981Epoch 13/15: [================              ] 33/60 batches, loss: 0.0993Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0991Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0989Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0992Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0985Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0979Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0971Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0958Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0964Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0956Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0952Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0958Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0957Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0952Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0951Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0947Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0949Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0959Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0958Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0954Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0953Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0947Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0956Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0960Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0955Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0954Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0946Epoch 13/15: [==============================] 60/60 batches, loss: 0.0952
[2025-05-07 20:07:42,073][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0952
[2025-05-07 20:07:42,357][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0377, Metrics: {'mse': 0.03975825011730194, 'rmse': 0.19939470935133144, 'r2': 0.3405020236968994}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0412Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0466Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0626Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0605Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0705Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0726Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0705Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0746Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0720Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0744Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0737Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0776Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0810Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0835Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0841Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0842Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0838Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0847Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0833Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0844Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0871Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0862Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0857Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0852Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0863Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0848Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0863Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0853Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0854Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0864Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0889Epoch 14/15: [================              ] 32/60 batches, loss: 0.0884Epoch 14/15: [================              ] 33/60 batches, loss: 0.0888Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0903Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0897Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0910Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0913Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0929Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0925Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0929Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0931Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0937Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0934Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0931Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0927Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0923Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0920Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0927Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0951Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0958Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0955Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0951Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0945Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0944Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0939Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0940Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0941Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0946Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0941Epoch 14/15: [==============================] 60/60 batches, loss: 0.0946
[2025-05-07 20:07:44,617][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0946
[2025-05-07 20:07:44,915][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0368, Metrics: {'mse': 0.03900423273444176, 'rmse': 0.197494892932556, 'r2': 0.3530094027519226}
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0920Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0876Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0917Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0854Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0873Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0838Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0893Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0927Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0878Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0877Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0860Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0886Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0883Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0873Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0890Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0896Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0902Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0892Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0897Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0884Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0862Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0850Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0859Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0878Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0897Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0912Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0899Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0894Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0885Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0891Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0886Epoch 15/15: [================              ] 32/60 batches, loss: 0.0895Epoch 15/15: [================              ] 33/60 batches, loss: 0.0893Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0884Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0881Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0885Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0880Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0878Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0874Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0866Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0854Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0847Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0847Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0846Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0847Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0851Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0847Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0840Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0836Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0846Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0856Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0851Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0853Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0849Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0859Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0862Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0862Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0858Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0861Epoch 15/15: [==============================] 60/60 batches, loss: 0.0860
[2025-05-07 20:07:47,275][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0860
[2025-05-07 20:07:47,653][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0363, Metrics: {'mse': 0.03755290061235428, 'rmse': 0.1937857079672138, 'r2': 0.3770837187767029}
[2025-05-07 20:07:48,028][src.training.lm_trainer][INFO] - Training completed in 39.03 seconds
[2025-05-07 20:07:48,028][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:07:50,650][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.055571313947439194, 'rmse': 0.23573568662262231, 'r2': 0.4359204173088074}
[2025-05-07 20:07:50,650][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03755290061235428, 'rmse': 0.1937857079672138, 'r2': 0.3770837187767029}
[2025-05-07 20:07:50,650][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.049730777740478516, 'rmse': 0.22300398592957596, 'r2': 0.2213761806488037}
[2025-05-07 20:07:50,681][src.training.lm_trainer][ERROR] - Error during training: [enforce fail at inline_container.cc:603] . unexpected pos 58112 vs 58064
[2025-05-07 20:07:50,700][src.training.lm_trainer][ERROR] - Traceback: Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/103: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/training/lm_trainer.py", line 347, in train
    torch.save(self.model.state_dict(), model_path)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 857, in save
    return
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 690, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 58112 vs 58064

[2025-05-07 20:07:50,705][src.training.lm_trainer][INFO] - GPU memory cleared
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2189, in _finish
    logger.info(f"finishing run {self._get_path()}")
Message: 'finishing run rokii-ku-leuven/multilingual-question-probing/c7malhww'
Arguments: ()
--- Logging error ---
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:387: UserWarning: Run (c7malhww) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  return func(self, *args, **kwargs)
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2204, in _finish
    self._atexit_cleanup(exit_code=exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2419, in _atexit_cleanup
    logger.info(f"got exitcode: {exit_code}")
Message: 'got exitcode: 0'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2204, in _finish
    self._atexit_cleanup(exit_code=exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2431, in _atexit_cleanup
    self._on_finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2676, in _on_finish
    self._console_stop()  # TODO: there's a race here with jupyter console logging
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2465, in _console_stop
    self._restore()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2401, in _restore
    logger.info("restore")
Message: 'restore'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2204, in _finish
    self._atexit_cleanup(exit_code=exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2431, in _atexit_cleanup
    self._on_finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2676, in _on_finish
    self._console_stop()  # TODO: there's a race here with jupyter console logging
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2465, in _console_stop
    self._restore()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2407, in _restore
    logger.info("restore done")
Message: 'restore done'
Arguments: ()
wandb:                                                                                
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2204, in _finish
    self._atexit_cleanup(exit_code=exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2444, in _atexit_cleanup
    Run._footer(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 3972, in _footer
    Run._footer_history_summary_info(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 4064, in _footer_history_summary_info
    logger.info("rendering history")
Message: 'rendering history'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1087, in emit
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded
Call stack:
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 541, in <module>
    main()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 184, in main
    results = run_lm_experiment(cfg, task, task_type, submetric)
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2204, in _finish
    self._atexit_cleanup(exit_code=exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2444, in _atexit_cleanup
    Run._footer(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 3972, in _footer
    Run._footer_history_summary_info(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 4096, in _footer_history_summary_info
    logger.info("rendering summary")
Message: 'rendering summary'
Arguments: ()
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▃▃▂▂▂▁▁▁
wandb:     best_val_mse █▇▅▄▃▃▂▂▂▁▁▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇▇▇███
wandb:    best_val_rmse █▇▅▅▄▃▃▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▅▆▆▆▆▆▆▆▇▇
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▄▃▃▂▃▂▂▂▂▁▁▁
wandb:          val_mse █▇▅▄▃▃▂▃▂▂▂▂▁▁▁
wandb:           val_r2 ▁▂▄▅▆▆▇▆▇▇▇▇███
wandb:         val_rmse █▇▅▅▄▃▃▃▂▂▃▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03626
wandb:     best_val_mse 0.03755
wandb:      best_val_r2 0.37708
wandb:    best_val_rmse 0.19379
wandb:            epoch 15
wandb:   final_test_mse 0.04973
wandb:    final_test_r2 0.22138
wandb:  final_test_rmse 0.223
wandb:  final_train_mse 0.05557
wandb:   final_train_r2 0.43592
wandb: final_train_rmse 0.23574
wandb:    final_val_mse 0.03755
wandb:     final_val_r2 0.37708
wandb:   final_val_rmse 0.19379
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08599
wandb:       train_time 39.02884
wandb:         val_loss 0.03626
wandb:          val_mse 0.03755
wandb:           val_r2 0.37708
wandb:         val_rmse 0.19379
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200649-c7malhww
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200649-c7malhww/logs
[2025-05-07 20:07:50,867][__main__][ERROR] - Error processing language id: [Errno 122] Disk quota exceeded
[2025-05-07 20:07:50,869][__main__][ERROR] - Traceback: Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1157, in close
    self.flush()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1067, in flush
    self.stream.flush()
OSError: [Errno 122] Disk quota exceeded

During handling of the above exception, another exception occurred:

OSError: [Errno 122] Disk quota exceeded

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 402, in run_lm_experiment
    wandb_run.finish()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2182, in finish
    return self._finish(exit_code)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 2210, in _finish
    hook.call()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 651, in dispose_handler
    handler.close()
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/logging/__init__.py", line 1162, in close
    stream.close()
OSError: [Errno 122] Disk quota exceeded

Experiment probe_layer2_avg_verb_edges_id completed successfully
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/id/id/results.json for layer 2
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
OSError: [Errno 122] Disk quota exceeded
Running experiment: probe_layer2_lexical_density_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id"         "wandb.mode=offline" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:08:24,601][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id
experiment_name: probe_layer2_lexical_density_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:08:24,601][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:08:24,601][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:08:24,601][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:08:24,602][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:08:24,610][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['id']
[2025-05-07 20:08:24,610][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:08:24,610][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:08:28,753][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:08:31,258][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:08:31,258][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:08:31,555][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:31,648][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:32,027][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-07 20:08:32,035][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:08:32,036][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-07 20:08:32,051][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:08:32,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:32,302][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:32,337][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-07 20:08:32,338][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:08:32,338][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-07 20:08:32,340][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:08:32,473][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:32,586][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:08:32,620][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-07 20:08:32,622][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:08:32,622][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-07 20:08:32,624][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-07 20:08:32,625][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:08:32,625][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:08:32,625][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:08:32,625][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:08:32,626][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:08:32,626][src.data.datasets][INFO] -   Mean: 0.6348, Std: 0.2113
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Sample label: 0.7919999957084656
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:08:32,626][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:08:32,626][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:08:32,627][src.data.datasets][INFO] -   Mean: 0.5853, Std: 0.2230
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:08:32,627][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:08:32,627][src.data.datasets][INFO] -   Mean: 0.5191, Std: 0.2023
[2025-05-07 20:08:32,627][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-07 20:08:32,628][src.data.datasets][INFO] - Sample label: 0.5289999842643738
[2025-05-07 20:08:32,628][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-07 20:08:32,628][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:08:32,628][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:08:32,628][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 20:08:32,628][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:08:41,644][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:08:41,645][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:08:41,645][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:08:41,646][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:08:41,648][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:08:41,649][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:08:41,649][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:08:41,649][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:08:41,649][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-07 20:08:41,650][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:08:41,650][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4846Epoch 1/15: [=                             ] 2/60 batches, loss: 0.5783Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5965Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5561Epoch 1/15: [==                            ] 5/60 batches, loss: 0.5084Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4673Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4899Epoch 1/15: [====                          ] 8/60 batches, loss: 0.5015Epoch 1/15: [====                          ] 9/60 batches, loss: 0.5125Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.5137Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.5027Epoch 1/15: [======                        ] 12/60 batches, loss: 0.4994Epoch 1/15: [======                        ] 13/60 batches, loss: 0.4834Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.4788Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.4790Epoch 1/15: [========                      ] 16/60 batches, loss: 0.4752Epoch 1/15: [========                      ] 17/60 batches, loss: 0.4801Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.4884Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.4750Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.4650Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.4570Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.4558Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.4433Epoch 1/15: [============                  ] 24/60 batches, loss: 0.4373Epoch 1/15: [============                  ] 25/60 batches, loss: 0.4365Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.4334Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.4285Epoch 1/15: [==============                ] 28/60 batches, loss: 0.4219Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4214Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4241Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4176Epoch 1/15: [================              ] 32/60 batches, loss: 0.4121Epoch 1/15: [================              ] 33/60 batches, loss: 0.4065Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4048Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4015Epoch 1/15: [==================            ] 36/60 batches, loss: 0.3977Epoch 1/15: [==================            ] 37/60 batches, loss: 0.3888Epoch 1/15: [===================           ] 38/60 batches, loss: 0.3835Epoch 1/15: [===================           ] 39/60 batches, loss: 0.3807Epoch 1/15: [====================          ] 40/60 batches, loss: 0.3764Epoch 1/15: [====================          ] 41/60 batches, loss: 0.3740Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.3714Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.3672Epoch 1/15: [======================        ] 44/60 batches, loss: 0.3666Epoch 1/15: [======================        ] 45/60 batches, loss: 0.3622Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.3597Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.3590Epoch 1/15: [========================      ] 48/60 batches, loss: 0.3538Epoch 1/15: [========================      ] 49/60 batches, loss: 0.3513Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.3464Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.3453Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.3441Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.3429Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.3438Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.3414Epoch 1/15: [============================  ] 56/60 batches, loss: 0.3420Epoch 1/15: [============================  ] 57/60 batches, loss: 0.3445Epoch 1/15: [============================= ] 58/60 batches, loss: 0.3434Epoch 1/15: [============================= ] 59/60 batches, loss: 0.3416Epoch 1/15: [==============================] 60/60 batches, loss: 0.3399
[2025-05-07 20:08:47,988][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3399
[2025-05-07 20:08:48,253][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0719, Metrics: {'mse': 0.07167816162109375, 'rmse': 0.2677277752141039, 'r2': -0.4418213367462158}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3436Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2387Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2288Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2398Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2068Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2196Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2248Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2328Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2366Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2302Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2290Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2241Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2268Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2246Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2200Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2176Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2146Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2139Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2167Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2173Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2127Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2157Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2132Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2121Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2101Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2081Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2049Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2029Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2008Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1990Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1980Epoch 2/15: [================              ] 32/60 batches, loss: 0.1952Epoch 2/15: [================              ] 33/60 batches, loss: 0.1939Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1948Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1941Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1911Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1914Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1915Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1918Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1934Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1945Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1942Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1926Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1944Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1929Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1907Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1890Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1888Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1888Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1875Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1861Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1858Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1846Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1829Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1827Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1814Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1816Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1805Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1794Epoch 2/15: [==============================] 60/60 batches, loss: 0.1782
[2025-05-07 20:08:50,569][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1782
[2025-05-07 20:08:50,830][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0562, Metrics: {'mse': 0.05651427060365677, 'rmse': 0.23772730302524522, 'r2': -0.1367964744567871}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1065Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1330Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1178Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1400Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1293Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1233Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1276Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1355Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1379Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1393Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1373Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1380Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1386Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1350Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1341Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1354Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1376Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1350Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1328Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1351Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1327Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1317Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1282Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1271Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1269Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1278Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1277Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1256Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1253Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1237Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1244Epoch 3/15: [================              ] 32/60 batches, loss: 0.1235Epoch 3/15: [================              ] 33/60 batches, loss: 0.1259Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1235Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1232Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1234Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1232Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1229Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1238Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1227Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1223Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1225Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1231Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1220Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1206Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1194Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1203Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1201Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1213Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1218Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1210Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1217Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1207Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1240Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1238Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1232Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1232Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1232Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1229Epoch 3/15: [==============================] 60/60 batches, loss: 0.1243
[2025-05-07 20:08:53,119][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1243
[2025-05-07 20:08:53,391][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0579, Metrics: {'mse': 0.058034539222717285, 'rmse': 0.24090358906151085, 'r2': -0.16737699508666992}
[2025-05-07 20:08:53,392][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0624Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0968Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1005Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1175Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1169Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1175Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1212Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1128Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1056Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1046Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1056Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1039Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1036Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1027Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1018Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1052Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1113Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1085Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1070Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1100Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1126Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1144Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1151Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1151Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1130Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1148Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1135Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1137Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1129Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1125Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1119Epoch 4/15: [================              ] 32/60 batches, loss: 0.1121Epoch 4/15: [================              ] 33/60 batches, loss: 0.1116Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1111Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1101Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1091Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1090Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1099Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1090Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1080Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1088Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1086Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1076Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1063Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1065Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1059Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1061Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1063Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1065Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1070Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1066Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1065Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1067Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1065Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1065Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1070Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1069Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1070Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1067Epoch 4/15: [==============================] 60/60 batches, loss: 0.1058
[2025-05-07 20:08:55,269][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1058
[2025-05-07 20:08:55,578][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0567, Metrics: {'mse': 0.057059526443481445, 'rmse': 0.23887135961324757, 'r2': -0.1477644443511963}
[2025-05-07 20:08:55,578][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1445Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1108Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0990Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1171Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1177Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1186Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1148Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1147Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1099Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1103Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1134Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1148Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1164Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1171Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1149Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1162Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1159Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1130Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1118Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1169Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1147Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1146Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1179Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1194Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1169Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1171Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1143Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1147Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1154Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1161Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1170Epoch 5/15: [================              ] 32/60 batches, loss: 0.1166Epoch 5/15: [================              ] 33/60 batches, loss: 0.1152Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1134Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1140Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1134Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1126Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1110Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1100Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1087Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1084Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1100Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1106Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1095Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1095Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1099Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1093Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1099Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1089Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1091Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1077Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1064Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1060Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1049Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1037Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1027Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1024Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1031Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1022Epoch 5/15: [==============================] 60/60 batches, loss: 0.1017
[2025-05-07 20:08:57,466][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1017
[2025-05-07 20:08:57,729][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0507, Metrics: {'mse': 0.05146118625998497, 'rmse': 0.22685058135253913, 'r2': -0.03515267372131348}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0739Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0788Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0915Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0877Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1008Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0955Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0951Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1137Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1153Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1118Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1049Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1028Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0985Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0974Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0973Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0981Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0965Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0950Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0933Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0927Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0915Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0913Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0925Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0945Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0929Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0921Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0933Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0916Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0908Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0893Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0879Epoch 6/15: [================              ] 32/60 batches, loss: 0.0899Epoch 6/15: [================              ] 33/60 batches, loss: 0.0934Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0933Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0919Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0915Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0907Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0908Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0906Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0895Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0887Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0890Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0891Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0888Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0885Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0873Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0867Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0870Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0866Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0861Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0866Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0866Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0864Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0860Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0862Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0860Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0852Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0852Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0854Epoch 6/15: [==============================] 60/60 batches, loss: 0.0845
[2025-05-07 20:08:59,935][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0845
[2025-05-07 20:09:00,200][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0430, Metrics: {'mse': 0.044310446828603745, 'rmse': 0.2105004675258555, 'r2': 0.10868597030639648}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0722Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0934Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0793Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0681Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0755Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0743Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0761Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0728Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0775Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0747Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0741Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0763Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0754Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0749Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0740Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0740Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0748Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0759Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0766Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0777Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0792Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0783Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0779Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0792Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0780Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0777Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0762Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0745Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0759Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0756Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0758Epoch 7/15: [================              ] 32/60 batches, loss: 0.0752Epoch 7/15: [================              ] 33/60 batches, loss: 0.0747Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0740Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0743Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0740Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0729Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0718Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0715Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0718Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0718Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0737Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0737Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0733Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0734Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0739Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0743Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0741Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0743Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0741Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0743Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0745Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0740Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0741Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0742Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0740Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0735Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0731Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0734Epoch 7/15: [==============================] 60/60 batches, loss: 0.0745
[2025-05-07 20:09:02,425][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0745
[2025-05-07 20:09:02,796][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0527, Metrics: {'mse': 0.053183410316705704, 'rmse': 0.2306152863899219, 'r2': -0.06979548931121826}
[2025-05-07 20:09:02,797][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1141Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0936Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0747Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0695Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0729Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0695Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0646Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0655Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0665Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0701Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0697Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0689Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0682Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0685Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0682Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0673Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0655Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0662Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0659Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0653Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0646Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0644Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0645Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0655Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0648Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0647Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0655Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0649Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0650Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0649Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0641Epoch 8/15: [================              ] 32/60 batches, loss: 0.0639Epoch 8/15: [================              ] 33/60 batches, loss: 0.0631Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0629Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0623Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0640Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0650Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0650Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0646Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0644Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0647Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0649Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0658Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0662Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0666Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0670Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0663Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0668Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0681Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0681Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0691Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0690Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0685Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0680Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0682Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0690Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0682Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0683Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0677Epoch 8/15: [==============================] 60/60 batches, loss: 0.0674
[2025-05-07 20:09:04,680][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0674
[2025-05-07 20:09:04,988][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0426, Metrics: {'mse': 0.04367769509553909, 'rmse': 0.20899209338044128, 'r2': 0.1214139461517334}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1697Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1130Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0897Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0873Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0867Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0820Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0815Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0781Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0768Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0751Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0772Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0734Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0714Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0696Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0695Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0695Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0708Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0695Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0687Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0686Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0686Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0720Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0710Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0708Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0713Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0729Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0745Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0762Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0768Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0771Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0758Epoch 9/15: [================              ] 32/60 batches, loss: 0.0756Epoch 9/15: [================              ] 33/60 batches, loss: 0.0748Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0744Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0771Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0776Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0776Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0778Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0774Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0788Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0783Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0790Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0783Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0773Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0767Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0762Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0761Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0758Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0761Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0759Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0770Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0778Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0771Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0763Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0755Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0747Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0739Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0740Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0746Epoch 9/15: [==============================] 60/60 batches, loss: 0.0742
[2025-05-07 20:09:07,324][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0742
[2025-05-07 20:09:07,606][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0456, Metrics: {'mse': 0.04624560847878456, 'rmse': 0.21504792135425202, 'r2': 0.06975984573364258}
[2025-05-07 20:09:07,607][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0388Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0387Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0472Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0474Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0580Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0526Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0553Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0554Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0532Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0566Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0527Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0512Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0557Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0551Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0552Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0553Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0545Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0564Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0566Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0591Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0587Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0578Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0568Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0559Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0550Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0542Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0567Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0571Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0572Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0562Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0556Epoch 10/15: [================              ] 32/60 batches, loss: 0.0564Epoch 10/15: [================              ] 33/60 batches, loss: 0.0558Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0562Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0571Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0570Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0571Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0571Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0578Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0578Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0582Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0579Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0575Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0580Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0579Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0578Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0579Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0580Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0583Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0576Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0575Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0573Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0570Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0570Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0573Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0577Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0577Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0575Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0572Epoch 10/15: [==============================] 60/60 batches, loss: 0.0567
[2025-05-07 20:09:09,468][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0567
[2025-05-07 20:09:09,755][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0437, Metrics: {'mse': 0.04440075531601906, 'rmse': 0.21071486733502945, 'r2': 0.10686945915222168}
[2025-05-07 20:09:09,756][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0512Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0467Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0437Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0481Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0557Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0593Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0661Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0638Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0657Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0666Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0633Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0604Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0596Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0595Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0583Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0586Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0563Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0563Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0597Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0591Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0592Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0600Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0598Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0606Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0603Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0601Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0592Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0592Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0589Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0587Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0588Epoch 11/15: [================              ] 32/60 batches, loss: 0.0585Epoch 11/15: [================              ] 33/60 batches, loss: 0.0590Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0585Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0574Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0586Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0593Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0594Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0594Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0605Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0605Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0604Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0598Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0599Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0600Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0596Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0592Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0596Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0591Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0592Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0595Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0597Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0596Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0605Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0600Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0605Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0600Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0601Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0602Epoch 11/15: [==============================] 60/60 batches, loss: 0.0598
[2025-05-07 20:09:11,734][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0598
[2025-05-07 20:09:12,001][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0526, Metrics: {'mse': 0.052806660532951355, 'rmse': 0.22979699852903074, 'r2': -0.062217116355895996}
[2025-05-07 20:09:12,002][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0189Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0648Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0604Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0696Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0629Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0679Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0651Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0616Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0614Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0607Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0610Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0643Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0644Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0628Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0627Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0640Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0643Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0640Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0648Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0657Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0636Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0640Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0619Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0639Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0639Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0637Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0629Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0623Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0620Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0625Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0614Epoch 12/15: [================              ] 32/60 batches, loss: 0.0623Epoch 12/15: [================              ] 33/60 batches, loss: 0.0621Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0622Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0614Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0610Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0606Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0610Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0608Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0605Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0606Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0608Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0602Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0615Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0612Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0610Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0614Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0613Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0621Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0618Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0617Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0616Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0619Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0621Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0614Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0613Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0615Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0614Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0613Epoch 12/15: [==============================] 60/60 batches, loss: 0.0615
[2025-05-07 20:09:13,929][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0615
[2025-05-07 20:09:14,243][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0453, Metrics: {'mse': 0.04605740308761597, 'rmse': 0.21460988581054688, 'r2': 0.07354563474655151}
[2025-05-07 20:09:14,243][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:09:14,243][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-07 20:09:14,244][src.training.lm_trainer][INFO] - Training completed in 28.96 seconds
[2025-05-07 20:09:14,244][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:09:16,911][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03938736021518707, 'rmse': 0.19846249070085528, 'r2': 0.11795777082443237}
[2025-05-07 20:09:16,911][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04367769509553909, 'rmse': 0.20899209338044128, 'r2': 0.1214139461517334}
[2025-05-07 20:09:16,912][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.037110306322574615, 'rmse': 0.1926403548651596, 'r2': 0.09308606386184692}
[2025-05-07 20:09:16,954][src.training.lm_trainer][ERROR] - Error during training: [enforce fail at inline_container.cc:603] . unexpected pos 58112 vs 58064
[2025-05-07 20:09:16,964][src.training.lm_trainer][ERROR] - Traceback: Traceback (most recent call last):
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/103: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/training/lm_trainer.py", line 347, in train
    torch.save(self.model.state_dict(), model_path)
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 857, in save
    return
  File "/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/torch/serialization.py", line 690, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 58112 vs 58064

[2025-05-07 20:09:16,967][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▁▁
wandb:     best_val_mse █▄▃▁▁
wandb:      best_val_r2 ▁▅▆██
wandb:    best_val_rmse █▄▃▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▃▄▅▅▄▆▅▅▄
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▅▄▃▁▃▁▂▁▃▂
wandb:          val_mse █▄▅▄▃▁▃▁▂▁▃▂
wandb:           val_r2 ▁▅▄▅▆█▆█▇█▆▇
wandb:         val_rmse █▄▅▅▃▁▄▁▂▁▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04263
wandb:     best_val_mse 0.04368
wandb:      best_val_r2 0.12141
wandb:    best_val_rmse 0.20899
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.03711
wandb:    final_test_r2 0.09309
wandb:  final_test_rmse 0.19264
wandb:  final_train_mse 0.03939
wandb:   final_train_r2 0.11796
wandb: final_train_rmse 0.19846
wandb:    final_val_mse 0.04368
wandb:     final_val_r2 0.12141
wandb:   final_val_rmse 0.20899
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06154
wandb:       train_time 28.95981
wandb:         val_loss 0.04528
wandb:          val_mse 0.04606
wandb:           val_r2 0.07355
wandb:         val_rmse 0.21461
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200824-yv53dj8e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_200824-yv53dj8e/logs
Experiment probe_layer2_lexical_density_id completed successfully
Warning: No test metrics found in /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id/id/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/id/id/results.json for layer 2
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
OSError: [Errno 122] Disk quota exceeded
Running experiment: probe_layer2_n_tokens_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_id"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/id"         "wandb.mode=offline" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
slurmstepd: error: *** JOB 64466523 ON k28i22 CANCELLED AT 2025-05-07T20:09:48 ***
