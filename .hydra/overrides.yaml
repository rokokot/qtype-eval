- experiment=single_submetric
- experiment.tasks=single_submetric
- model=lm_finetune
- model.lm_name=cis-lmu/glot500-base
- model.dropout=0.1
- model.head_hidden_size=768
- model.head_layers=2
- data.languages=[ar]
- data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache
- training.task_type=regression
- training.num_epochs=10
- training.batch_size=16
- training.lr=2e-5
- +training.gradient_accumulation_steps=2
- experiment_name=finetune_n_tokens_control1_ar
- output_dir=/scratch/leuven/371/vsc37132/finetune_output/single_submetric/ar/n_tokens/control1
- wandb.mode=offline
- experiment.use_controls=true
- experiment.control_index=1
- experiment.submetric=n_tokens
