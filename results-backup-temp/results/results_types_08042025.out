SLURM_JOB_ID: 64307696
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100
SLURM_NNODES: 1
SLURM_NODELIST: k28g25
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 2
Date: Tue Apr  8 18:08:18 CEST 2025
Walltime: 00-03:00:00
========================================================================
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
HYDRA_JOB_CHDIR=False
GPU information:
Tue Apr  8 18:10:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:BF:00.0 Off |                    0 |
| N/A   31C    P0             78W /  500W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running question type classification for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:11:28,347][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ar
experiment_name: question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:11:28,347][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:11:28,347][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:11:28,347][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:11:28,430][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-08 18:11:28,442][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:11:32,761][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:11:36,329][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:11:36,329][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:11:36,969][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:37,272][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:38,224][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-08 18:11:38,272][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:11:38,273][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-08 18:11:38,275][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:11:38,309][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:38,353][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:38,400][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-08 18:11:38,401][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:11:38,402][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-08 18:11:38,404][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:11:38,436][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:38,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:11:38,536][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-08 18:11:38,538][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:11:38,538][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-08 18:11:38,539][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-08 18:11:38,587][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:11:38,587][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:11:38,588][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:11:38,588][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:11:38,607][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-08 18:11:38,607][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-08 18:11:38,607][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-08 18:11:38,607][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:11:38,607][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:11:38,607][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:11:38,607][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:11:38,608][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-08 18:11:38,608][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:11:38,608][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:11:38,609][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-08 18:11:38,609][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-08 18:11:38,609][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-08 18:11:38,609][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:11:38,609][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-08 18:11:38,609][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:11:38,610][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:11:38,610][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:12:12,646][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:12:12,650][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:12:12,651][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:08<08:31,  8.25s/it]Epoch 1/10:   3%|▎         | 2/63 [00:08<03:31,  3.46s/it]Epoch 1/10:   5%|▍         | 3/63 [00:08<01:55,  1.93s/it]Epoch 1/10:   6%|▋         | 4/63 [00:08<01:11,  1.21s/it]Epoch 1/10:  10%|▉         | 6/63 [00:08<00:35,  1.62it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:08<00:21,  2.54it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:09<00:14,  3.57it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:09<00:10,  4.65it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:09<00:08,  5.70it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:09<00:07,  6.68it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:09<00:05,  7.53it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:10<00:05,  8.24it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:10<00:04,  8.80it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:10<00:04,  9.23it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:10<00:03,  9.56it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:10<00:03,  9.80it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:11<00:03,  9.96it/s]Epoch 1/10:  51%|█████     | 32/63 [00:11<00:03, 10.09it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:11<00:02, 10.18it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:11<00:02, 10.24it/s]Epoch 1/10:  60%|██████    | 38/63 [00:11<00:02, 10.29it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:12<00:02, 10.32it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:12<00:02, 10.33it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:12<00:01, 10.35it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:12<00:01, 10.36it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:12<00:01, 10.37it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:12<00:01, 10.37it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:13<00:01, 10.38it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:13<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:13<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:13<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:13<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:14<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:14<00:00,  4.32it/s]
[2025-04-08 18:12:45,171][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-08 18:12:45,503][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:12,  5.03it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.13it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.15it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.91it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.32it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.34it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.36it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-08 18:12:52,117][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-08 18:12:52,321][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.70it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.89it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.00it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.24it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.32it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.32it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.34it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.36it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.37it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-08 18:12:58,963][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-08 18:12:59,194][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.05it/s]Epoch 4/10:   3%|▎         | 2/63 [00:00<00:10,  5.94it/s]Epoch 4/10:   6%|▋         | 4/63 [00:00<00:07,  8.13it/s]Epoch 4/10:  10%|▉         | 6/63 [00:00<00:06,  9.06it/s]Epoch 4/10:  13%|█▎        | 8/63 [00:00<00:05,  9.56it/s]Epoch 4/10:  16%|█▌        | 10/63 [00:01<00:05,  9.85it/s]Epoch 4/10:  19%|█▉        | 12/63 [00:01<00:05, 10.03it/s]Epoch 4/10:  22%|██▏       | 14/63 [00:01<00:04, 10.15it/s]Epoch 4/10:  25%|██▌       | 16/63 [00:01<00:04, 10.23it/s]Epoch 4/10:  29%|██▊       | 18/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  32%|███▏      | 20/63 [00:02<00:04, 10.32it/s]Epoch 4/10:  35%|███▍      | 22/63 [00:02<00:03, 10.34it/s]Epoch 4/10:  38%|███▊      | 24/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  41%|████▏     | 26/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  44%|████▍     | 28/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 30/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  51%|█████     | 32/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  54%|█████▍    | 34/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  57%|█████▋    | 36/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  60%|██████    | 38/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 40/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  67%|██████▋   | 42/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  70%|██████▉   | 44/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  73%|███████▎  | 46/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 48/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  79%|███████▉  | 50/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  83%|████████▎ | 52/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  86%|████████▌ | 54/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 56/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 58/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 60/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.16it/s]
[2025-04-08 18:13:05,809][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-08 18:13:06,028][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.77it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.95it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.04it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.57it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-08 18:13:12,222][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-08 18:13:12,501][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  4.93it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  8.06it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.11it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-08 18:13:18,669][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-08 18:13:19,039][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-08 18:13:19,039][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.83117
wandb:        final_test_f1 0.76364
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 48.45241
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_181128-lm1x74lo
wandb: Find logs at: ./wandb/offline-run-20250408_181128-lm1x74lo/logs
Standard experiment for ar completed successfully
Running question type classification for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:13:51,852][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/en
experiment_name: question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:13:51,852][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:13:51,852][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:13:51,852][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:13:51,857][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-08 18:13:51,857][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:13:55,223][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:13:58,194][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:13:58,194][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:13:58,356][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:58,471][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:58,708][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-08 18:13:58,719][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:13:58,720][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-08 18:13:58,721][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:13:58,754][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:58,865][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:58,896][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-08 18:13:58,898][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:13:58,898][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-08 18:13:58,900][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:13:58,991][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:59,088][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:13:59,119][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-08 18:13:59,121][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:13:59,121][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-08 18:13:59,123][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-08 18:13:59,124][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:13:59,124][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:13:59,124][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:13:59,124][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:13:59,125][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-08 18:13:59,125][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:13:59,125][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-08 18:13:59,125][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:13:59,125][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:13:59,125][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:13:59,125][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:13:59,126][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:13:59,126][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:13:59,126][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:13:59,127][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:13:59,127][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:13:59,127][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-08 18:13:59,127][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:13:59,127][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-08 18:13:59,127][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:13:59,128][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:13:59,128][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:14:06,933][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:14:06,936][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:14:06,936][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:22,  1.11s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:37,  1.93it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.92it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.56it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.84it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.82it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.54it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.07it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.46it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.73it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.93it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.07it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.16it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.23it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.31it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.09it/s]
[2025-04-08 18:14:19,241][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-08 18:14:19,557][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.53it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:14:27,370][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-08 18:14:27,704][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.13it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:14:35,511][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-08 18:14:35,801][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.05it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.14it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:14:43,524][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-08 18:14:43,790][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:24,  2.96it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:11,  6.29it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  7.89it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  8.77it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:07,  9.32it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.63it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.87it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.14it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:02<00:05, 10.22it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.27it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.30it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.04it/s]
[2025-04-08 18:14:51,263][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-08 18:14:51,541][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:14:58,878][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-08 18:14:59,162][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-08 18:14:59,163][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.94545
wandb:        final_test_f1 0.94828
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.17362
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_181351-jnlifnt8
wandb: Find logs at: ./wandb/offline-run-20250408_181351-jnlifnt8/logs
Standard experiment for en completed successfully
Running question type classification for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:15:24,795][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/fi
experiment_name: question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:15:24,795][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:15:24,795][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:15:24,795][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:15:24,836][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-08 18:15:24,849][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:15:27,377][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:15:30,369][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:15:30,370][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:15:30,482][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,532][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,711][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-08 18:15:30,722][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:15:30,722][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-08 18:15:30,725][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:15:30,756][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,805][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,827][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-08 18:15:30,829][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:15:30,829][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-08 18:15:30,830][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:15:30,858][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,905][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:15:30,932][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-08 18:15:30,934][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:15:30,934][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-08 18:15:30,936][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-08 18:15:30,936][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:15:30,936][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:15:30,936][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:15:30,936][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:15:30,937][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:15:30,937][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-08 18:15:30,937][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-08 18:15:30,937][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:15:30,937][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:15:30,937][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:15:30,937][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:15:30,938][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-08 18:15:30,938][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:15:30,938][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:15:30,939][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:15:30,939][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:15:30,939][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:15:30,939][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-08 18:15:30,939][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:15:30,939][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-08 18:15:30,939][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:15:30,940][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:15:30,940][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:15:36,941][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:15:36,943][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:15:36,944][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:37,  1.32s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.41it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  3.99it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.40it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.59it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.55it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.31it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.88it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.30it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.62it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.71it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.87it/s]
[2025-04-08 18:15:48,060][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-08 18:15:48,282][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.25it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:15:56,074][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-08 18:15:56,311][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.61it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:16:04,134][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-08 18:16:04,382][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.74it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:16:12,128][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-08 18:16:12,367][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.32it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.34it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.36it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:16:20,119][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-08 18:16:20,377][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.79it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  13%|█▎        | 10/75 [00:01<00:06,  9.87it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06, 10.06it/s]Epoch 6/10:  19%|█▊        | 14/75 [00:01<00:05, 10.18it/s]Epoch 6/10:  21%|██▏       | 16/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  24%|██▍       | 18/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  27%|██▋       | 20/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  29%|██▉       | 22/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  32%|███▏      | 24/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  35%|███▍      | 26/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  37%|███▋      | 28/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  40%|████      | 30/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  45%|████▌     | 34/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  51%|█████     | 38/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  80%|████████  | 60/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:16:27,740][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-08 18:16:27,992][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.18it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:16:35,342][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-08 18:16:35,593][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.66it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 8/10:   5%|▌         | 4/75 [00:00<00:08,  8.46it/s]Epoch 8/10:   8%|▊         | 6/75 [00:00<00:07,  9.32it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:06,  9.74it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.97it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06, 10.11it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:05, 10.17it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.24it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.29it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.32it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  80%|████████  | 60/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:16:43,368][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-08 18:16:43,627][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:37,  1.98it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:14,  4.95it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:10,  6.77it/s]Epoch 9/10:   9%|▉         | 7/75 [00:01<00:08,  7.94it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:07,  8.72it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.24it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.59it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.83it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:02<00:05, 10.00it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:02<00:05, 10.12it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.20it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.26it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.30it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:03<00:04, 10.33it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:03<00:04, 10.35it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.80it/s]
[2025-04-08 18:16:51,282][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-08 18:16:51,530][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.78it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:16:58,882][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-08 18:16:59,144][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-08 18:16:59,145][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.9
wandb:        final_test_f1 0.90265
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 79.54199
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_181524-6gfb3rbz
wandb: Find logs at: ./wandb/offline-run-20250408_181524-6gfb3rbz/logs
Standard experiment for fi completed successfully
Running question type classification for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:17:25,328][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/id
experiment_name: question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:17:25,328][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:17:25,328][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:17:25,328][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:17:25,364][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-08 18:17:25,374][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:17:27,756][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:17:30,597][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:17:30,597][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:17:30,685][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:30,730][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:30,876][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-08 18:17:30,885][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:17:30,885][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-08 18:17:30,887][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:17:30,915][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:30,954][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:30,985][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-08 18:17:30,986][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:17:30,987][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-08 18:17:30,988][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:17:31,015][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:31,058][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:17:31,091][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-08 18:17:31,093][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:17:31,093][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-08 18:17:31,095][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-08 18:17:31,095][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:17:31,096][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:17:31,096][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:17:31,096][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:17:31,096][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-08 18:17:31,096][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-08 18:17:31,096][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-08 18:17:31,096][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:17:31,097][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:17:31,097][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-08 18:17:31,097][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:17:31,098][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:17:31,098][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-08 18:17:31,098][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:17:31,099][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-08 18:17:31,099][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:17:31,099][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:17:31,099][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:17:37,910][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:17:37,913][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:17:37,913][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:16,  1.30s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:34,  1.68it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:15,  3.53it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.14it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:08,  6.45it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.49it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.28it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.87it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.31it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.62it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.85it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03, 10.02it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.13it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.21it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.27it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.77it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.58it/s]
[2025-04-08 18:17:47,253][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-08 18:17:47,508][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.05it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  8.14it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.16it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.65it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.92it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.08it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:03,  8.45it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:03,  8.95it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02,  9.34it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02,  9.64it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:04<00:02,  9.85it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.01it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.12it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.20it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.26it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.30it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.33it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.35it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.36it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 60/60 [00:06<00:00,  9.94it/s]
[2025-04-08 18:17:54,006][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-08 18:17:54,277][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.74it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.92it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.03it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.56it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.86it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.25it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.30it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-08 18:18:00,655][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-08 18:18:00,939][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:12,  4.71it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.91it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.02it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.55it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.31it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.35it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-08 18:18:07,250][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-08 18:18:07,515][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:12,  4.66it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.99it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.54it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.34it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.36it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-08 18:18:13,815][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-08 18:18:14,221][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.48it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.73it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.48it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-08 18:18:20,555][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-08 18:18:20,910][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.91it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  8.05it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.61it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.06it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-08 18:18:26,818][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-08 18:18:27,100][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:12,  4.65it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.85it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-08 18:18:33,428][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-08 18:18:33,703][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:12,  4.84it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  8.00it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  9.07it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.59it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.88it/s]Epoch 9/10:  17%|█▋        | 10/60 [00:01<00:05,  9.90it/s]Epoch 9/10:  20%|██        | 12/60 [00:01<00:04, 10.08it/s]Epoch 9/10:  23%|██▎       | 14/60 [00:01<00:04, 10.19it/s]Epoch 9/10:  27%|██▋       | 16/60 [00:01<00:04, 10.25it/s]Epoch 9/10:  30%|███       | 18/60 [00:01<00:04, 10.30it/s]Epoch 9/10:  33%|███▎      | 20/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  37%|███▋      | 22/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  40%|████      | 24/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  43%|████▎     | 26/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  47%|████▋     | 28/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  50%|█████     | 30/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  53%|█████▎    | 32/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  57%|█████▋    | 34/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  60%|██████    | 36/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 38/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  67%|██████▋   | 40/60 [00:03<00:01, 10.39it/s]Epoch 9/10:  70%|███████   | 42/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  73%|███████▎  | 44/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  77%|███████▋  | 46/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  80%|████████  | 48/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  83%|████████▎ | 50/60 [00:04<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 52/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  90%|█████████ | 54/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  93%|█████████▎| 56/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 58/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.81it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-08 18:18:39,614][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-08 18:18:39,913][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:21,  2.79it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:09,  6.08it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:07,  7.73it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:06,  8.67it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.25it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:05,  9.62it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04,  9.86it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.03it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.14it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:02<00:04, 10.22it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.27it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.31it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:06<00:00,  9.90it/s]
[2025-04-08 18:18:45,977][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-08 18:18:46,247][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-08 18:18:46,248][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.74545
wandb:        final_test_f1 0.70833
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 65.98808
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_181725-p746udo0
wandb: Find logs at: ./wandb/offline-run-20250408_181725-p746udo0/logs
Standard experiment for id completed successfully
Running question type classification for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:19:09,536][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ja
experiment_name: question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:19:09,537][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:19:09,537][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:19:09,537][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:19:09,542][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-04-08 18:19:09,542][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:19:12,027][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:19:14,933][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:19:14,934][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:19:15,026][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,070][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,211][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-08 18:19:15,222][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:19:15,222][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-08 18:19:15,224][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:19:15,260][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,300][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,318][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-08 18:19:15,320][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:19:15,320][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-08 18:19:15,321][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:19:15,357][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:19:15,421][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-08 18:19:15,422][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:19:15,422][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-08 18:19:15,423][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-08 18:19:15,424][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:19:15,424][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:19:15,424][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:19:15,425][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:19:15,425][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-08 18:19:15,425][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:19:15,425][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-08 18:19:15,425][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:19:15,425][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:19:15,425][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:19:15,426][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-08 18:19:15,426][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:19:15,426][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:19:15,427][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:19:15,427][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-08 18:19:15,427][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-08 18:19:15,427][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-08 18:19:15,427][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:19:15,427][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-08 18:19:15,427][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:19:15,428][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:19:15,428][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:19:21,479][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:19:21,481][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:19:21,482][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:30,  1.23s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.56it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.18it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.60it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.77it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.71it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.44it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.98it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.36it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.65it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.93it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.97it/s]
[2025-04-08 18:19:32,202][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-08 18:19:32,392][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:19:40,186][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-08 18:19:40,384][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.91it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:19:48,197][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-08 18:19:48,412][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:19:56,146][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-08 18:19:56,362][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.61it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:20:04,116][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-08 18:20:04,342][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:20:12,137][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-08 18:20:12,355][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:20:20,114][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-08 18:20:20,343][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.83it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.32it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.34it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.36it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:20:27,686][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-08 18:20:27,907][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:20:35,254][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-08 18:20:35,482][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.88it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:20:43,272][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-08 18:20:43,508][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.8587
wandb:        final_test_f1 0.88889
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 79.67128
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_181909-hlc7c1pz
wandb: Find logs at: ./wandb/offline-run-20250408_181909-hlc7c1pz/logs
Standard experiment for ja completed successfully
Running question type classification for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:21:05,671][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ko
experiment_name: question_type_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:21:05,672][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:21:05,672][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:21:05,672][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:21:05,698][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-04-08 18:21:05,703][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:21:07,673][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:21:10,507][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:21:10,508][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:21:10,596][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,637][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,779][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-08 18:21:10,787][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:21:10,788][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-08 18:21:10,789][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:21:10,813][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,851][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,889][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-08 18:21:10,890][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:21:10,890][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-08 18:21:10,892][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:21:10,918][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,957][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:21:10,988][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-08 18:21:10,990][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:21:10,990][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-08 18:21:10,992][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-08 18:21:10,993][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:21:10,993][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:21:10,993][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:21:10,993][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:21:10,993][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-08 18:21:10,994][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:21:10,994][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:21:10,995][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:21:10,995][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:21:10,995][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:21:10,995][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:21:10,996][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:21:10,996][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-08 18:21:10,996][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:21:10,996][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-08 18:21:10,996][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:21:10,996][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:21:10,997][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:21:16,875][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:21:16,878][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:21:16,878][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:58,  1.26s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:26,  1.73it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.61it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.22it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.53it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.55it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.33it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.91it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.34it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:03,  9.65it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.87it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02, 10.03it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.14it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.21it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.27it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.30it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.26it/s]
[2025-04-08 18:21:24,834][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-08 18:21:25,080][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.34it/s]Epoch 2/10:   4%|▍         | 2/47 [00:00<00:11,  3.80it/s]Epoch 2/10:   9%|▊         | 4/47 [00:00<00:06,  6.31it/s]Epoch 2/10:  13%|█▎        | 6/47 [00:00<00:05,  7.75it/s]Epoch 2/10:  17%|█▋        | 8/47 [00:01<00:04,  8.64it/s]Epoch 2/10:  21%|██▏       | 10/47 [00:01<00:04,  9.21it/s]Epoch 2/10:  26%|██▌       | 12/47 [00:01<00:03,  9.58it/s]Epoch 2/10:  30%|██▉       | 14/47 [00:01<00:03,  9.84it/s]Epoch 2/10:  34%|███▍      | 16/47 [00:01<00:03, 10.01it/s]Epoch 2/10:  38%|███▊      | 18/47 [00:02<00:02, 10.13it/s]Epoch 2/10:  43%|████▎     | 20/47 [00:02<00:02, 10.21it/s]Epoch 2/10:  47%|████▋     | 22/47 [00:02<00:02, 10.27it/s]Epoch 2/10:  51%|█████     | 24/47 [00:02<00:02, 10.31it/s]Epoch 2/10:  55%|█████▌    | 26/47 [00:02<00:02, 10.34it/s]Epoch 2/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.35it/s]Epoch 2/10:  64%|██████▍   | 30/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  68%|██████▊   | 32/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  72%|███████▏  | 34/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  77%|███████▋  | 36/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 38/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 42/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 44/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 46/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00,  9.74it/s]
[2025-04-08 18:21:30,382][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-08 18:21:30,654][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.66it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.87it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-08 18:21:35,742][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-08 18:21:36,012][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.88it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  8.03it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.09it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.60it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-08 18:21:41,039][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-08 18:21:41,318][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.53it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.77it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.93it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.50it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-08 18:21:46,374][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-08 18:21:46,652][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  4.71it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.90it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.02it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.56it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.86it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-08 18:21:51,694][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-08 18:21:51,960][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.62it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.33it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.35it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.36it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-08 18:21:57,010][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-08 18:21:57,301][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.39it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.66it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-08 18:22:02,390][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-08 18:22:02,658][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.50it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.75it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.92it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.49it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.27it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.31it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-08 18:22:07,745][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-08 18:22:08,026][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:09,  4.64it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.53it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.16it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.23it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.28it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-08 18:22:12,669][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-08 18:22:12,959][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.86364
wandb:        final_test_f1 0.85437
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 53.81943
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182105-1etiav0r
wandb: Find logs at: ./wandb/offline-run-20250408_182105-1etiav0r/logs
Standard experiment for ko completed successfully
Running question type classification for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:22:34,097][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ru
experiment_name: question_type_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:22:34,097][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:22:34,097][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:22:34,097][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:22:34,147][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-04-08 18:22:34,157][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:22:35,925][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:22:38,759][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:22:38,760][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:22:38,823][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:38,858][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:39,027][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-08 18:22:39,039][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:22:39,040][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-08 18:22:39,041][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:22:39,065][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:39,108][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:39,139][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-08 18:22:39,141][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:22:39,141][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-08 18:22:39,143][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:22:39,175][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:39,227][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:22:39,260][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-08 18:22:39,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:22:39,262][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-08 18:22:39,263][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-08 18:22:39,263][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:22:39,264][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:22:39,264][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:22:39,264][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:22:39,264][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:22:39,264][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-08 18:22:39,264][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-08 18:22:39,264][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:22:39,265][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:22:39,265][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-08 18:22:39,265][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:22:39,266][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:22:39,266][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-08 18:22:39,266][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:22:39,267][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-08 18:22:39,267][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:22:39,267][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:22:39,267][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:22:44,873][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:22:44,876][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:22:44,877][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:37,  1.31s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.42it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.00it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.40it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.60it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.56it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.32it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.89it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.63it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.87it/s]
[2025-04-08 18:22:55,608][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-08 18:22:56,138][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.06it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:23:03,930][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-08 18:23:04,196][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.44it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.70it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:23:12,025][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-08 18:23:12,299][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:23:20,058][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-08 18:23:20,318][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.20it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.33it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.35it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.36it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:23:27,675][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-08 18:23:27,968][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.91it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:23:35,315][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-08 18:23:35,596][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-08 18:23:35,596][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.9
wandb:        final_test_f1 0.90909
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.44315
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182234-mlwcfv08
wandb: Find logs at: ./wandb/offline-run-20250408_182234-mlwcfv08/logs
Standard experiment for ru completed successfully
Running question type control=1 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:23:56,119][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ar/control1
experiment_name: question_type_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:23:56,120][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:23:56,120][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:23:56,120][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:23:56,134][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-08 18:23:56,134][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:23:57,731][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:24:00,881][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:24:00,882][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:24:00,944][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sun Apr  6 14:38:30 2025).
[2025-04-08 18:24:00,982][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sun Apr  6 14:38:30 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  94%|█████████▍| 7000/7460 [00:00<00:00, 62018.00 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 53905.70 examples/s]
[2025-04-08 18:24:01,334][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-08 18:24:01,344][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:24:01,344][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-08 18:24:01,345][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:24:01,370][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:24:01,404][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:24:01,420][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-08 18:24:01,421][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:24:01,421][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-08 18:24:01,422][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:24:01,445][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:24:01,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:24:01,493][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-08 18:24:01,495][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:24:01,495][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-08 18:24:01,497][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-08 18:24:01,498][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:24:01,498][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:24:01,498][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:24:01,498][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:24:01,498][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-08 18:24:01,498][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:24:01,499][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:24:01,499][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-08 18:24:01,499][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:24:01,500][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:24:01,500][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-08 18:24:01,500][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-08 18:24:01,501][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-08 18:24:01,501][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:24:01,501][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-08 18:24:01,501][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:24:01,501][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:24:01,502][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:24:06,672][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:24:06,675][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:24:06,675][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:07,  1.09s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:31,  1.96it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:14,  3.98it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:10,  5.62it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:01<00:07,  6.89it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:01<00:06,  7.86it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:02<00:05,  8.57it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:05,  9.09it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:02<00:04,  9.47it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:02<00:04,  9.74it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:02<00:04,  9.93it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04, 10.06it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03, 10.16it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:03<00:03, 10.23it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:03<00:03, 10.23it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:03<00:03, 10.28it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:03, 10.31it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 38/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:06<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.92it/s]
[2025-04-08 18:24:15,829][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6938
[2025-04-08 18:24:16,018][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:12,  4.86it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.01it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.07it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-08 18:24:22,643][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6948
[2025-04-08 18:24:22,852][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6910, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.71it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.90it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-08 18:24:29,480][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6931
[2025-04-08 18:24:29,696][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6913, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.02it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.12it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.15it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.63it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.90it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-08 18:24:35,865][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6930
[2025-04-08 18:24:36,082][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6914, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  8.07it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.11it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-08 18:24:42,245][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6907
[2025-04-08 18:24:42,455][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6919, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-08 18:24:42,456][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁
wandb:           train_loss ▆█▅▅▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁
wandb:             val_loss ▂▁▃▄█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69095
wandb:                epoch 5
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69067
wandb:           train_time 33.69226
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.6919
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182356-gb2syjd0
wandb: Find logs at: ./wandb/offline-run-20250408_182356-gb2syjd0/logs
Control experiment for ar (control=1) completed successfully
Running question type control=2 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:25:01,592][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ar/control2
experiment_name: question_type_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:25:01,592][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:25:01,592][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:25:01,592][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:25:01,606][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-08 18:25:01,606][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:25:03,509][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:25:06,401][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:25:06,402][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:25:06,511][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sun Apr  6 14:40:38 2025).
[2025-04-08 18:25:06,549][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sun Apr  6 14:40:38 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 75561.18 examples/s]
[2025-04-08 18:25:06,856][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-08 18:25:06,865][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:25:06,866][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-08 18:25:06,867][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:25:06,891][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:25:06,930][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:25:06,947][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-08 18:25:06,948][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:25:06,948][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-08 18:25:06,949][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:25:06,972][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:25:07,002][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:25:07,016][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-08 18:25:07,018][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:25:07,018][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-08 18:25:07,019][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-08 18:25:07,020][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:25:07,020][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:25:07,020][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:25:07,020][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:25:07,020][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-08 18:25:07,020][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:25:07,021][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:25:07,021][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-08 18:25:07,021][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:25:07,022][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:25:07,022][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-08 18:25:07,022][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-08 18:25:07,023][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-08 18:25:07,023][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:25:07,023][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-08 18:25:07,023][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:25:07,023][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:25:07,023][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:25:12,498][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:25:12,501][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:25:12,501][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:06,  1.07s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:21,  2.86it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:12,  4.56it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:09,  5.97it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:07,  7.12it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  8.00it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.66it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  9.16it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.52it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:04,  9.77it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:04,  9.96it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.09it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.18it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.24it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.28it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:03<00:03, 10.32it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:06<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.96it/s]
[2025-04-08 18:25:21,743][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6944
[2025-04-08 18:25:21,928][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6905, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:12,  4.82it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.98it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.27it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.31it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.34it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.31it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.34it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.36it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.37it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.37it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-08 18:25:28,564][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6929
[2025-04-08 18:25:28,775][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6905, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.31it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  8.31it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.27it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.71it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.96it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.11it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.20it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.25it/s]
[2025-04-08 18:25:34,923][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6949
[2025-04-08 18:25:35,129][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.61it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.50it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.38it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.79it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05, 10.01it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.14it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.22it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.28it/s]
[2025-04-08 18:25:41,260][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6947
[2025-04-08 18:25:41,458][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6912, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-08 18:25:41,458][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▆▁█▇
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▁▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69049
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69474
wandb:           train_time 26.74691
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69121
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182501-y1486oy6
wandb: Find logs at: ./wandb/offline-run-20250408_182501-y1486oy6/logs
Control experiment for ar (control=2) completed successfully
Running question type control=3 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:25:58,958][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ar/control3
experiment_name: question_type_control3_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:25:58,958][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:25:58,959][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:25:58,959][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:25:58,964][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-08 18:25:58,964][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:26:01,082][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:26:03,881][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:26:03,881][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:26:03,937][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Thu Apr  3 20:34:37 2025).
[2025-04-08 18:26:03,967][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Thu Apr  3 20:34:37 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 68858.04 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 65444.98 examples/s]
[2025-04-08 18:26:04,272][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-08 18:26:04,281][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:26:04,281][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-08 18:26:04,283][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:26:04,307][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:26:04,341][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:26:04,357][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-08 18:26:04,358][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:26:04,358][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-08 18:26:04,359][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:26:04,385][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:26:04,426][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:26:04,442][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-08 18:26:04,443][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:26:04,443][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-08 18:26:04,445][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-08 18:26:04,445][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:26:04,445][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:26:04,445][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:26:04,445][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:26:04,446][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-08 18:26:04,446][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-08 18:26:04,446][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-08 18:26:04,446][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:26:04,446][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:26:04,446][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:26:04,446][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:26:04,447][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-08 18:26:04,447][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:26:04,447][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:26:04,448][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:26:04,448][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-08 18:26:04,448][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-08 18:26:04,448][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-08 18:26:04,448][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:26:04,448][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-08 18:26:04,448][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:26:04,449][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:26:04,449][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:26:09,223][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:26:09,226][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:26:09,226][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:16,  1.23s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:23,  2.56it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:13,  4.18it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:09,  5.60it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:07,  6.78it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  7.72it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.44it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.98it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.38it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:04,  9.67it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.88it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.03it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.14it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.21it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.26it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.30it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.32it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.08it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.76it/s]
[2025-04-08 18:26:19,456][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6943
[2025-04-08 18:26:19,643][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6909, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.67it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.87it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.99it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-08 18:26:26,272][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6948
[2025-04-08 18:26:26,479][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6908, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  5.08it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  8.15it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.16it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.64it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.91it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.30it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.33it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.34it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.36it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.37it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.37it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.37it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.08it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.16it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.23it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.28it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.31it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.33it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.35it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.36it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.37it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-08 18:26:33,120][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6953
[2025-04-08 18:26:33,328][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6910, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.06it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.11it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-08 18:26:39,510][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6936
[2025-04-08 18:26:39,714][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6909, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  8.07it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.12it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.62it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-08 18:26:45,876][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6938
[2025-04-08 18:26:46,086][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6910, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-08 18:26:46,087][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▁
wandb:           train_loss ▄▆█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁
wandb:             val_loss ▂▁█▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69085
wandb:                epoch 5
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69375
wandb:           train_time 33.82439
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69098
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182559-sv7nyjkx
wandb: Find logs at: ./wandb/offline-run-20250408_182559-sv7nyjkx/logs
Control experiment for ar (control=3) completed successfully
Running question type control=1 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:27:05,603][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/en/control1
experiment_name: question_type_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:27:05,603][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:27:05,603][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:27:05,603][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:27:05,619][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-08 18:27:05,623][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:27:07,472][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:27:10,630][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:27:10,631][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:27:10,701][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:24:01 2025).
[2025-04-08 18:27:10,737][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:24:01 2025).
[2025-04-08 18:27:10,861][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-08 18:27:10,872][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:27:10,873][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-08 18:27:10,874][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:27:10,903][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:27:10,943][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:27:10,961][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-08 18:27:10,962][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:27:10,962][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-08 18:27:10,964][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:27:10,995][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:27:11,036][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:27:11,052][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-08 18:27:11,053][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:27:11,054][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-08 18:27:11,055][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-08 18:27:11,055][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:27:11,055][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:27:11,055][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:27:11,056][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:27:11,056][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-08 18:27:11,056][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:27:11,056][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-08 18:27:11,056][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:27:11,056][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:27:11,056][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:27:11,057][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:27:11,057][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:27:11,057][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:27:11,058][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:27:11,058][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-08 18:27:11,058][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:27:11,059][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:27:11,059][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:27:16,590][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:27:16,592][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:27:16,592][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:25,  1.15s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:39,  1.86it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.83it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.46it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.75it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.74it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.48it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.03it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.42it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.71it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.91it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.14it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.30it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.03it/s]
[2025-04-08 18:27:27,941][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6930
[2025-04-08 18:27:28,198][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:27:36,002][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6956
[2025-04-08 18:27:36,260][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.28it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.70it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-08 18:27:43,580][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6921
[2025-04-08 18:27:44,200][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:27:52,008][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6939
[2025-04-08 18:27:52,293][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:27:59,663][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6967
[2025-04-08 18:27:59,938][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.88it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:28:07,282][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6919
[2025-04-08 18:28:07,562][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:28:15,321][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6943
[2025-04-08 18:28:15,590][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.84it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:28:23,335][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6931
[2025-04-08 18:28:23,616][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.84it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:28:31,385][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6940
[2025-04-08 18:28:31,684][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:28:39,469][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6930
[2025-04-08 18:28:39,755][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁
wandb:        best_val_loss █▄▃▃▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ██████▁▁▁▁
wandb:           train_loss ▃▆▁▄█▁▄▃▄▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss ▅█▃▄▅▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69244
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.693
wandb:           train_time 80.52956
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69244
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182705-p5bebj6k
wandb: Find logs at: ./wandb/offline-run-20250408_182705-p5bebj6k/logs
Control experiment for en (control=1) completed successfully
Running question type control=2 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:29:00,148][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/en/control2
experiment_name: question_type_control2_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:29:00,148][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:29:00,148][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:29:00,148][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:29:00,221][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-08 18:29:00,227][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:29:01,999][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:29:04,869][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:29:04,869][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:29:04,943][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:25:06 2025).
[2025-04-08 18:29:04,973][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:25:06 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 76728.71 examples/s]
[2025-04-08 18:29:05,178][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-08 18:29:05,189][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:29:05,189][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-08 18:29:05,191][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:29:05,216][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:29:05,251][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:29:05,265][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-08 18:29:05,267][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:29:05,267][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-08 18:29:05,268][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:29:05,293][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:29:05,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:29:05,342][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-08 18:29:05,344][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:29:05,344][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-08 18:29:05,345][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-08 18:29:05,346][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:29:05,346][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:29:05,346][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:29:05,346][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:29:05,346][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-08 18:29:05,346][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:29:05,347][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:29:05,347][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:29:05,347][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:29:05,348][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:29:05,348][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:29:05,348][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-08 18:29:05,349][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:29:05,349][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-08 18:29:05,349][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:29:05,349][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:29:05,349][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:29:10,556][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:29:10,559][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:29:10,559][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:57,  1.58s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:52,  1.40it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:23,  3.06it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:15,  4.59it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  5.92it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.03it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  7.91it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.58it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:03<00:06,  9.09it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.46it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.06it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.61it/s]
[2025-04-08 18:29:21,388][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-08 18:29:21,653][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.19it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-08 18:29:29,439][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6934
[2025-04-08 18:29:29,699][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.62it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.50it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.38it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06, 10.01it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.22it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-08 18:29:37,007][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6944
[2025-04-08 18:29:37,267][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.16it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:29:44,598][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-08 18:29:44,853][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:29:44,854][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▇▁█▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69278
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69401
wandb:           train_time 32.18146
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69312
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_182900-1zp8bbui
wandb: Find logs at: ./wandb/offline-run-20250408_182900-1zp8bbui/logs
Control experiment for en (control=2) completed successfully
Running question type control=3 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:30:03,580][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/en/control3
experiment_name: question_type_control3_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:30:03,580][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:30:03,580][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:30:03,580][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:30:03,586][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-08 18:30:03,586][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:30:05,698][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:30:08,612][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:30:08,613][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:30:08,745][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:26:04 2025).
[2025-04-08 18:30:08,813][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:26:04 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 70010.33 examples/s]
[2025-04-08 18:30:09,125][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-08 18:30:09,135][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:30:09,136][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-08 18:30:09,139][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:30:09,213][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:30:09,313][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:30:09,354][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-08 18:30:09,356][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:30:09,356][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-08 18:30:09,359][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:30:09,426][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:30:09,505][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:30:09,532][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-08 18:30:09,534][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:30:09,534][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-08 18:30:09,537][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-08 18:30:09,538][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:30:09,538][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:30:09,538][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:30:09,538][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:30:09,538][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-08 18:30:09,539][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:30:09,539][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:30:09,539][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:30:09,540][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:30:09,540][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:30:09,540][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:30:09,541][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:30:09,541][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-08 18:30:09,541][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:30:09,541][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-08 18:30:09,541][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:30:09,541][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:30:09,542][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:30:16,434][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:30:16,437][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:30:16,437][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:43,  1.40s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:31,  2.29it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:18,  3.83it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:13,  5.22it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.43it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.41it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.19it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.79it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.24it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.57it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.81it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.87it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-08 18:30:27,468][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6954
[2025-04-08 18:30:27,729][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:30:35,566][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6939
[2025-04-08 18:30:35,834][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:30:43,676][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6930
[2025-04-08 18:30:44,070][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.78it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.23it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.18it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.23it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.27it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.30it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.33it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.32it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.27it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.30it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.33it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.34it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.35it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:30:51,850][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6938
[2025-04-08 18:30:52,133][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.28it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.30it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.32it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.32it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.34it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.33it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.33it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.34it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.34it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.35it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.35it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.35it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.35it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.35it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.35it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.35it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.35it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.36it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.93it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:30:59,909][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6947
[2025-04-08 18:31:00,208][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.66it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.31it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.33it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.34it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.34it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.34it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.34it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.34it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.34it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.34it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.34it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.04it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.19it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.23it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.26it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.21it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.25it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.28it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.30it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.31it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.31it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.32it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.33it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.34it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.34it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.92it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.12it/s]
[2025-04-08 18:31:07,620][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6920
[2025-04-08 18:31:07,929][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.75it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.15it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.27it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.29it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.30it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.31it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.32it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.32it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.33it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.33it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.33it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.34it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.34it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.34it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.34it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.34it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.34it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.34it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.34it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.34it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.34it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.35it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.35it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.34it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.34it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00,  8.73it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00,  9.17it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00,  9.51it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00,  9.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.46it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-08 18:31:15,440][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6944
[2025-04-08 18:31:15,734][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.47it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.72it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:31:23,533][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6947
[2025-04-08 18:31:23,816][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.4861111111111111, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:17,  4.13it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.44it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:08,  8.71it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:06,  9.72it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:31:31,630][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6948
[2025-04-08 18:31:31,931][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.96it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:31:39,292][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6942
[2025-04-08 18:31:39,571][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy █████▁
wandb:          best_val_f1 ▁▁▁▁▁▁
wandb:        best_val_loss █▆▆▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▅▃▅▇▁▆▇▇▆
wandb:           train_time ▁
wandb:         val_accuracy ███████▁██
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss ▄▃▃▂▃▃▁▁█▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.48611
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6925
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.50503
wandb:       final_train_f1 0.02961
wandb:   final_val_accuracy 0.48611
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69421
wandb:           train_time 80.64915
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69274
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183003-735o95zf
wandb: Find logs at: ./wandb/offline-run-20250408_183003-735o95zf/logs
Control experiment for en (control=3) completed successfully
Running question type control=1 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:31:58,833][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/fi/control1
experiment_name: question_type_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:31:58,833][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:31:58,833][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:31:58,833][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:31:58,858][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-08 18:31:58,863][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:32:01,425][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:32:04,402][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:32:04,403][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:32:04,486][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:24:01 2025).
[2025-04-08 18:32:04,527][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:24:01 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 71194.88 examples/s]
[2025-04-08 18:32:04,748][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-08 18:32:04,758][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:32:04,759][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-08 18:32:04,761][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:32:04,797][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:32:04,837][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:32:04,852][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-08 18:32:04,854][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:32:04,854][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-08 18:32:04,855][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:32:04,886][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:32:04,924][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:32:04,940][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-08 18:32:04,942][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:32:04,942][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-08 18:32:04,944][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-08 18:32:04,944][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:32:04,944][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:32:04,944][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:32:04,945][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:32:04,945][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:32:04,945][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-08 18:32:04,945][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-08 18:32:04,945][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:32:04,945][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:32:04,945][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:32:04,946][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-08 18:32:04,946][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:32:04,946][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:32:04,947][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:32:04,947][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-08 18:32:04,947][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:32:04,948][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:32:04,948][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:32:09,883][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:32:09,885][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:32:09,886][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:20,  1.08s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:25,  2.83it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.53it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.94it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.09it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.97it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.64it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.13it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.49it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.75it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.06it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.23it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:03<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.71it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.11it/s]
[2025-04-08 18:32:20,361][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6943
[2025-04-08 18:32:20,582][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:32:28,405][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6950
[2025-04-08 18:32:28,644][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.61it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:32:36,014][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6946
[2025-04-08 18:32:36,242][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:32:43,594][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6939
[2025-04-08 18:32:44,078][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-08 18:32:44,078][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▃█▆▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅█▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69216
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69386
wandb:           train_time 31.94962
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69228
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183158-gk2knftd
wandb: Find logs at: ./wandb/offline-run-20250408_183158-gk2knftd/logs
Control experiment for fi (control=1) completed successfully
Running question type control=2 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:33:02,838][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/fi/control2
experiment_name: question_type_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:33:02,838][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:33:02,838][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:33:02,838][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:33:02,866][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-08 18:33:02,867][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:33:04,412][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:33:07,331][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:33:07,331][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:33:07,422][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:29:05 2025).
[2025-04-08 18:33:07,452][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:29:05 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 79913.13 examples/s]
[2025-04-08 18:33:07,649][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-08 18:33:07,660][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:33:07,661][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-08 18:33:07,662][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:33:07,683][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:33:07,710][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:33:07,723][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-08 18:33:07,725][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:33:07,725][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-08 18:33:07,726][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:33:07,746][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:33:07,775][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:33:07,788][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-08 18:33:07,790][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:33:07,790][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-08 18:33:07,792][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-08 18:33:07,792][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:33:07,792][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:33:07,792][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:33:07,793][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:33:07,793][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:33:07,793][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-08 18:33:07,793][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-08 18:33:07,793][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:33:07,793][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:33:07,793][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:33:07,794][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-08 18:33:07,794][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:33:07,794][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:33:07,795][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:33:07,795][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-08 18:33:07,795][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:33:07,796][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:33:07,796][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:33:13,139][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:33:13,141][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:33:13,142][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:38,  1.33s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:30,  2.39it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  3.95it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.35it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.55it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.52it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.28it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.86it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.29it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.36it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.70it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.84it/s]
[2025-04-08 18:33:23,560][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6935
[2025-04-08 18:33:23,798][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.73it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.30it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.32it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.33it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.35it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.36it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:33:31,628][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6937
[2025-04-08 18:33:31,867][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.83it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:33:39,697][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-08 18:33:39,956][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:27,  2.74it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:11,  6.01it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:09,  7.67it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  8.62it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:07,  9.21it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.59it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.83it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.00it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:02<00:05, 10.17it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.23it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.28it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.31it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.33it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:03<00:04, 10.34it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-08 18:33:47,490][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6929
[2025-04-08 18:33:47,749][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.85it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:33:55,119][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6935
[2025-04-08 18:33:55,363][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-08 18:33:55,363][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁
wandb:           train_loss ▆██▁▇
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁
wandb:             val_loss ▃▁▆▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69217
wandb:                epoch 5
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69355
wandb:           train_time 40.28677
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69284
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183302-965v0khm
wandb: Find logs at: ./wandb/offline-run-20250408_183302-965v0khm/logs
Control experiment for fi (control=2) completed successfully
Running question type control=3 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:34:14,388][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/fi/control3
experiment_name: question_type_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:34:14,388][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:34:14,388][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:34:14,388][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:34:14,409][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-08 18:34:14,414][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:34:16,036][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:34:18,998][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:34:18,999][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:34:19,071][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:30:09 2025).
[2025-04-08 18:34:19,106][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:30:09 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 76913.74 examples/s]
[2025-04-08 18:34:19,304][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-08 18:34:19,315][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:34:19,316][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-08 18:34:19,317][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:34:19,343][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:34:19,378][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:34:19,392][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-08 18:34:19,393][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:34:19,393][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-08 18:34:19,395][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:34:19,416][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:34:19,445][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:34:19,457][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-08 18:34:19,459][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:34:19,459][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-08 18:34:19,460][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-08 18:34:19,461][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:34:19,461][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:34:19,461][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:34:19,461][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:34:19,462][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:34:19,462][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:34:19,462][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:34:19,463][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-08 18:34:19,463][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:34:19,463][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:34:19,464][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:34:19,464][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:34:19,464][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-08 18:34:19,464][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:34:19,464][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-08 18:34:19,464][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:34:19,465][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:34:19,465][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:34:24,374][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:34:24,376][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:34:24,376][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:28,  1.19s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:40,  1.81it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.75it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.37it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.67it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.66it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.42it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.98it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.38it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.67it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.13it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.29it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.97it/s]
[2025-04-08 18:34:34,695][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6950
[2025-04-08 18:34:34,915][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.72it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:34:42,756][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6929
[2025-04-08 18:34:42,988][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 3/10:   3%|▎         | 2/75 [00:00<00:11,  6.56it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:08,  8.52it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:07,  9.31it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:06,  9.70it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.94it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06, 10.09it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:05, 10.18it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.36it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.35it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:03, 10.35it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.36it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.36it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  80%|████████  | 60/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.38it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:34:50,838][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6935
[2025-04-08 18:34:51,084][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6921, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.57it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.30it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.31it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.33it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.35it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.36it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-08 18:34:58,870][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6910
[2025-04-08 18:34:59,119][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.87it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:35:06,489][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6947
[2025-04-08 18:35:06,739][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:35:14,107][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6950
[2025-04-08 18:35:14,351][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-08 18:35:14,351][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁
wandb:          best_val_f1 ▁▁▁
wandb:        best_val_loss █▇▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▁▁
wandb:           train_loss █▄▅▁▇█
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁
wandb:             val_loss ▃▂▁▄█▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69215
wandb:                epoch 6
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69495
wandb:           train_time 48.02402
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69217
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183414-rnocyu5d
wandb: Find logs at: ./wandb/offline-run-20250408_183414-rnocyu5d/logs
Control experiment for fi (control=3) completed successfully
Running question type control=1 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:35:32,769][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/id/control1
experiment_name: question_type_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:35:32,769][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:35:32,769][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:35:32,769][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:35:32,801][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-08 18:35:32,813][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:35:34,786][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:35:37,674][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:35:37,674][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:35:37,755][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:32:04 2025).
[2025-04-08 18:35:37,794][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:32:04 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  80%|████████  | 6000/7460 [00:00<00:00, 50368.92 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 30055.75 examples/s]
[2025-04-08 18:35:38,195][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-08 18:35:38,203][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:35:38,204][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-08 18:35:38,205][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:35:38,229][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:35:38,261][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:35:38,274][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-08 18:35:38,276][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:35:38,276][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-08 18:35:38,277][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:35:38,300][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:35:38,331][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:35:38,343][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-08 18:35:38,345][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:35:38,345][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-08 18:35:38,346][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-08 18:35:38,347][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:35:38,347][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:35:38,347][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:35:38,347][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:35:38,347][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-08 18:35:38,347][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:35:38,348][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:35:38,348][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:35:38,348][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:35:38,349][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:35:38,349][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:35:38,349][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:35:38,350][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-08 18:35:38,350][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:35:38,350][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-08 18:35:38,350][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:35:38,350][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:35:38,351][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:35:43,126][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:35:43,128][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:35:43,129][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:16,  1.30s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:34,  1.67it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:15,  3.52it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.12it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:08,  6.43it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.47it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.26it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.86it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.30it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.61it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.84it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03, 10.00it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.12it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.20it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.25it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.76it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.56it/s]
[2025-04-08 18:35:52,233][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6900
[2025-04-08 18:35:52,477][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6943, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.76it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.94it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.03it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.55it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-08 18:35:58,849][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6933
[2025-04-08 18:35:59,110][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.91it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  8.05it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.88it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.06it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.17it/s]
[2025-04-08 18:36:05,475][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6917
[2025-04-08 18:36:05,740][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:12,  4.62it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.83it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.96it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-08 18:36:12,061][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6939
[2025-04-08 18:36:12,352][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6942, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.00it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  8.11it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  9.14it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.62it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-08 18:36:18,248][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6929
[2025-04-08 18:36:18,529][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.90it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  8.04it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.88it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.06it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.30it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.26it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.30it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.33it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.34it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.35it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.31it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.34it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.36it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-08 18:36:24,452][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6947
[2025-04-08 18:36:24,717][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:36:24,718][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁
wandb:          best_val_f1 ▁▁▁
wandb:        best_val_loss █▄▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss ▁▆▄▇▅█
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁
wandb:             val_loss █▄▁▆▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69381
wandb:                epoch 6
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69467
wandb:           train_time 39.49204
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69387
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183532-v7n1fav1
wandb: Find logs at: ./wandb/offline-run-20250408_183532-v7n1fav1/logs
Control experiment for id (control=1) completed successfully
Running question type control=2 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:36:43,941][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/id/control2
experiment_name: question_type_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:36:43,941][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:36:43,941][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:36:43,941][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:36:43,946][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-08 18:36:43,947][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:36:45,696][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:36:48,617][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:36:48,618][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:36:48,691][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:33:07 2025).
[2025-04-08 18:36:48,722][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:33:07 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 74675.56 examples/s]
[2025-04-08 18:36:48,920][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-08 18:36:48,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:36:48,930][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-08 18:36:48,931][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:36:48,955][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:36:48,985][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:36:48,998][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-08 18:36:49,000][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:36:49,000][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-08 18:36:49,001][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:36:49,022][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:36:49,051][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:36:49,063][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-08 18:36:49,065][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:36:49,065][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-08 18:36:49,066][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-08 18:36:49,066][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:36:49,066][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:36:49,067][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:36:49,067][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:36:49,067][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-08 18:36:49,067][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-08 18:36:49,067][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-08 18:36:49,067][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:36:49,067][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:36:49,068][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:36:49,068][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:36:49,068][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:36:49,069][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:36:49,069][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-08 18:36:49,069][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:36:49,070][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:36:49,070][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:36:54,075][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:36:54,077][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:36:54,077][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:07,  1.15s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:30,  1.88it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:14,  3.86it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:09,  5.49it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.78it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.76it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.50it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  9.04it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.42it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.70it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:02<00:04,  9.90it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03, 10.04it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.14it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.20it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.26it/s]Epoch 1/10:  50%|█████     | 30/60 [00:03<00:02, 10.30it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:04<00:01, 10.33it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.34it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.35it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.37it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.35it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.36it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.36it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.71it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.76it/s]
[2025-04-08 18:37:02,918][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6931
[2025-04-08 18:37:03,182][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6941, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.79it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.96it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.03it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.56it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.35it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.36it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.37it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-08 18:37:09,561][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6919
[2025-04-08 18:37:09,857][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:13,  4.39it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.65it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.84it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.75it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.95it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.09it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.18it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.24it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.27it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.36it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.36it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.37it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-08 18:37:16,261][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6889
[2025-04-08 18:37:16,535][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6941, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:11,  4.93it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  8.03it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.09it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.88it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.36it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-08 18:37:22,441][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6931
[2025-04-08 18:37:22,717][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.47it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.24it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.28it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.36it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.36it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-08 18:37:29,036][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6929
[2025-04-08 18:37:29,314][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.56it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.79it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.94it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.36it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.37it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-08 18:37:35,248][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6923
[2025-04-08 18:37:35,551][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:13,  4.49it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.73it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.24it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.28it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.30it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.36it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.36it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.37it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.37it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.37it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-08 18:37:41,885][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6908
[2025-04-08 18:37:42,178][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.48it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.71it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.36it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.36it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.29it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.32it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.33it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.34it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.35it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.36it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.36it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.35it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.36it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.37it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-08 18:37:48,532][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6920
[2025-04-08 18:37:48,814][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:12,  4.68it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.36it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.37it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-08 18:37:54,737][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6920
[2025-04-08 18:37:55,001][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:12,  4.60it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.81it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.95it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.36it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.36it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.36it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.36it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.36it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.37it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.37it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.37it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-08 18:38:00,934][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6907
[2025-04-08 18:38:01,222][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:38:01,223][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁
wandb:        best_val_loss █▅▄▃▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate █████▁▁▁▁▁
wandb:           train_loss █▆▁██▇▄▆▆▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▅█▄▅▃▁▅▃▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69391
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69068
wandb:           train_time 65.15857
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69404
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183643-h18zcaev
wandb: Find logs at: ./wandb/offline-run-20250408_183643-h18zcaev/logs
Control experiment for id (control=2) completed successfully
Running question type control=3 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:38:23,956][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/id/control3
experiment_name: question_type_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:38:23,956][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:38:23,956][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:38:23,956][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:38:23,961][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-08 18:38:23,962][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:38:25,652][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:38:28,756][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:38:28,756][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:38:28,821][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:34:19 2025).
[2025-04-08 18:38:28,854][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:34:19 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 78823.82 examples/s]
[2025-04-08 18:38:29,071][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-08 18:38:29,079][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:38:29,080][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-08 18:38:29,082][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:38:29,109][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:38:29,145][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:38:29,160][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-08 18:38:29,162][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:38:29,162][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-08 18:38:29,163][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:38:29,194][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:38:29,236][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:38:29,251][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-08 18:38:29,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:38:29,253][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-08 18:38:29,254][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:38:29,255][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-08 18:38:29,255][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-08 18:38:29,255][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:38:29,256][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:38:29,256][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-08 18:38:29,256][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:38:29,257][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:38:29,257][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:38:29,257][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-08 18:38:29,258][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:38:29,258][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:38:29,258][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:38:34,357][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:38:34,359][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:38:34,360][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:14,  1.26s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:33,  1.73it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:15,  3.62it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.23it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.54it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.55it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.33it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.91it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.33it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.63it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.85it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03, 10.01it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.12it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.19it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.24it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.31it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.37it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.37it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.37it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.37it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.75it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.61it/s]
[2025-04-08 18:38:43,528][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6918
[2025-04-08 18:38:43,781][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.74it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.92it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.02it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.54it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.37it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-08 18:38:50,163][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6934
[2025-04-08 18:38:50,410][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.67it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.99it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-08 18:38:56,329][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6946
[2025-04-08 18:38:56,581][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:11,  4.97it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  8.08it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.36it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.37it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.17it/s]
[2025-04-08 18:39:02,486][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6943
[2025-04-08 18:39:02,745][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:39:02,746][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▁▅█▇
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁█▆▆
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69363
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69426
wandb:           train_time 26.18497
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69373
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183824-jvzpgsxb
wandb: Find logs at: ./wandb/offline-run-20250408_183824-jvzpgsxb/logs
Control experiment for id (control=3) completed successfully
Running question type control=1 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:39:21,313][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ja/control1
experiment_name: question_type_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:39:21,314][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:39:21,314][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:39:21,314][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:39:21,329][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-04-08 18:39:21,332][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:39:23,106][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:39:26,021][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:39:26,021][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:39:26,080][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:35:38 2025).
[2025-04-08 18:39:26,119][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:35:38 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  94%|█████████▍| 7000/7460 [00:00<00:00, 63148.62 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 60898.22 examples/s]
[2025-04-08 18:39:26,362][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-08 18:39:26,373][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:39:26,374][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-08 18:39:26,375][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:39:26,405][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:39:26,441][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:39:26,457][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-08 18:39:26,458][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:39:26,458][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-08 18:39:26,460][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:39:26,488][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:39:26,525][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:39:26,539][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-08 18:39:26,541][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:39:26,541][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-08 18:39:26,542][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-08 18:39:26,543][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:39:26,543][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:39:26,543][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:39:26,543][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:39:26,543][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-08 18:39:26,543][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:39:26,544][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:39:26,544][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-08 18:39:26,544][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:39:26,545][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:39:26,545][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-08 18:39:26,545][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-08 18:39:26,546][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-08 18:39:26,546][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:39:26,546][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-08 18:39:26,546][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:39:26,546][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:39:26,547][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:39:31,196][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:39:31,199][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:39:31,199][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:24,  1.14s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:26,  2.73it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.40it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.81it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.97it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.87it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.56it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.07it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.45it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.72it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.91it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.07it/s]
[2025-04-08 18:39:42,578][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6946
[2025-04-08 18:39:42,769][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.92it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-08 18:39:50,572][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6936
[2025-04-08 18:39:50,774][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6949, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.19it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.19it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:39:58,108][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-08 18:39:58,324][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.36it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.34it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.28it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.71it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-08 18:40:05,649][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6952
[2025-04-08 18:40:05,845][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6946, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
[2025-04-08 18:40:05,846][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅▁▂█
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁█▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.47826
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69447
wandb:                epoch 4
wandb:  final_test_accuracy 0.40217
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.47826
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69518
wandb:           train_time 31.53763
wandb:         val_accuracy 0.47826
wandb:               val_f1 0
wandb:             val_loss 0.69455
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_183921-itq9aft0
wandb: Find logs at: ./wandb/offline-run-20250408_183921-itq9aft0/logs
Control experiment for ja (control=1) completed successfully
Running question type control=2 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:40:23,912][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ja/control2
experiment_name: question_type_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:40:23,912][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:40:23,912][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:40:23,912][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:40:23,917][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-04-08 18:40:23,917][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:40:25,634][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:40:28,499][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:40:28,499][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:40:28,587][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:36:48 2025).
[2025-04-08 18:40:28,621][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:36:48 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 74917.47 examples/s]
[2025-04-08 18:40:28,809][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-08 18:40:28,820][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:40:28,820][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-08 18:40:28,822][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:40:28,851][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:40:28,888][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:40:28,905][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-08 18:40:28,906][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:40:28,906][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-08 18:40:28,908][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:40:28,939][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:40:28,976][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:40:28,991][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-08 18:40:28,992][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:40:28,992][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-08 18:40:28,994][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-08 18:40:28,994][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:40:28,994][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:40:28,994][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:40:28,994][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:40:28,995][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-08 18:40:28,995][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:40:28,995][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-08 18:40:28,995][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:40:28,995][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:40:28,995][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:40:28,995][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:40:28,996][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-08 18:40:28,996][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:40:28,996][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:40:28,997][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:40:28,997][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-08 18:40:28,997][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-08 18:40:28,997][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-08 18:40:28,997][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:40:28,997][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-08 18:40:28,997][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:40:28,998][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:40:28,998][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:40:33,785][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:40:33,787][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:40:33,788][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:29,  1.21s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.59it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.22it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.63it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.81it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.74it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.45it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.99it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.39it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.67it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.34it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.36it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.36it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.36it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.91it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.99it/s]
[2025-04-08 18:40:45,716][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6948
[2025-04-08 18:40:45,909][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6951, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:40:53,719][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6960
[2025-04-08 18:40:53,915][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6948, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.60it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:41:01,733][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6925
[2025-04-08 18:41:01,944][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.31it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.33it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.34it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.35it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.35it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.36it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:41:09,713][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6959
[2025-04-08 18:41:09,936][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6947, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:24,  2.98it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:11,  6.30it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  7.90it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  8.78it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:07,  9.32it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.67it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.90it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.14it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:02<00:05, 10.22it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.27it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.29it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.31it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.33it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:03<00:04, 10.35it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-08 18:41:17,423][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6938
[2025-04-08 18:41:17,646][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6944, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:41:25,406][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6964
[2025-04-08 18:41:25,624][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:33,  2.23it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:13,  5.33it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:09,  7.10it/s]Epoch 7/10:   9%|▉         | 7/75 [00:01<00:08,  8.20it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:07,  8.90it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.37it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.67it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.89it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:02<00:05, 10.14it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.21it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.26it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.30it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.32it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:03<00:04, 10.33it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.34it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.30it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.32it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.34it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.34it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.35it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.36it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.36it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.37it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.37it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.88it/s]
[2025-04-08 18:41:33,216][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6937
[2025-04-08 18:41:33,444][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.78it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.36it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:41:40,802][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6951
[2025-04-08 18:41:41,018][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.50it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:41:48,809][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6946
[2025-04-08 18:41:49,031][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6946, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.86it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:41:56,388][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6939
[2025-04-08 18:41:56,611][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6942, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁
wandb:        best_val_loss █▆▄▄▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ▅▇▁▇▃█▃▆▅▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▆▄▅▄▄▄▁▅▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.47826
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69396
wandb:                epoch 10
wandb:  final_test_accuracy 0.40217
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.47826
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69386
wandb:           train_time 79.24408
wandb:         val_accuracy 0.47826
wandb:               val_f1 0
wandb:             val_loss 0.69423
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184023-oxrr699b
wandb: Find logs at: ./wandb/offline-run-20250408_184023-oxrr699b/logs
Control experiment for ja (control=2) completed successfully
Running question type control=3 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:42:15,911][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ja/control3
experiment_name: question_type_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:42:15,912][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:42:15,912][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:42:15,912][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:42:15,917][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-04-08 18:42:15,917][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:42:17,704][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:42:20,594][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:42:20,595][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:42:20,674][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:38:29 2025).
[2025-04-08 18:42:20,705][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:38:29 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 76593.48 examples/s]
[2025-04-08 18:42:20,898][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-08 18:42:20,909][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:42:20,909][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-08 18:42:20,911][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:42:20,938][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:42:20,968][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:42:20,982][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-08 18:42:20,983][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:42:20,984][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-08 18:42:20,985][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:42:21,011][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:42:21,045][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:42:21,061][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-08 18:42:21,062][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:42:21,062][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-08 18:42:21,064][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-08 18:42:21,064][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:42:21,064][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:42:21,065][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:42:21,065][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:42:21,065][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-08 18:42:21,065][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-08 18:42:21,065][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-08 18:42:21,065][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:42:21,065][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:42:21,066][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-08 18:42:21,066][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:42:21,066][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:42:21,067][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-08 18:42:21,067][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-08 18:42:21,067][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:42:21,068][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:42:21,068][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:42:25,911][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:42:25,913][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:42:25,914][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:34,  1.28s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.71it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.58it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.19it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.50it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.52it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.30it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.89it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.11it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.19it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.24it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.90it/s]
[2025-04-08 18:42:36,330][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-08 18:42:36,523][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6949, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.25it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-08 18:42:44,314][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6949
[2025-04-08 18:42:44,530][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6950, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-08 18:42:51,880][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6935
[2025-04-08 18:42:52,078][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6949, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.29it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.25it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.29it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.32it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.34it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.34it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.35it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.36it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:42:59,915][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6953
[2025-04-08 18:43:00,137][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6944, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:43:07,892][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6954
[2025-04-08 18:43:08,105][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.57it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:43:15,849][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6951
[2025-04-08 18:43:16,112][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:43:23,903][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6934
[2025-04-08 18:43:24,122][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.30it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.33it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.34it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.35it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:43:31,929][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6930
[2025-04-08 18:43:32,162][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.52it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.36it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:43:39,984][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6918
[2025-04-08 18:43:40,295][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.45652173913043476, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.32it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.33it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.36it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.36it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:43:48,102][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6935
[2025-04-08 18:43:48,338][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ███████▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁
wandb:        best_val_loss ██▆▅▄▄▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ▆▇▄██▇▄▃▁▄
wandb:           train_time ▁
wandb:         val_accuracy ████████▁█
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss ███▆▅▄▄▁▁▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.45652
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69269
wandb:                epoch 10
wandb:  final_test_accuracy 0.3913
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.50294
wandb:       final_train_f1 0.01661
wandb:   final_val_accuracy 0.45652
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69352
wandb:           train_time 80.43308
wandb:         val_accuracy 0.47826
wandb:               val_f1 0
wandb:             val_loss 0.6938
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184215-lx53he9q
wandb: Find logs at: ./wandb/offline-run-20250408_184215-lx53he9q/logs
Control experiment for ja (control=3) completed successfully
Running question type control=1 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:44:06,703][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ko/control1
experiment_name: question_type_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:44:06,704][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:44:06,704][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:44:06,704][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:44:06,755][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-04-08 18:44:06,769][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:44:08,551][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:44:11,423][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:44:11,424][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:44:11,511][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:39:26 2025).
[2025-04-08 18:44:11,538][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:39:26 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 42117.49 examples/s]
[2025-04-08 18:44:11,856][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-08 18:44:11,863][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:44:11,864][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-08 18:44:11,865][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:44:11,893][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:44:11,928][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:44:11,944][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-08 18:44:11,945][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:44:11,945][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-08 18:44:11,946][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:44:11,969][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:44:12,001][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:44:12,015][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-08 18:44:12,017][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:44:12,017][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-08 18:44:12,019][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-08 18:44:12,019][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:44:12,019][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:44:12,020][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:44:12,020][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:44:12,020][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-08 18:44:12,020][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-08 18:44:12,020][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-08 18:44:12,020][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:44:12,020][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:44:12,021][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:44:12,021][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:44:12,021][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:44:12,022][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:44:12,022][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-08 18:44:12,022][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:44:12,023][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:44:12,023][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:44:16,864][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:44:16,867][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:44:16,867][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:06,  1.44s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:29,  1.53it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:13,  3.29it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:08,  4.86it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:02<00:06,  6.18it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:05,  7.25it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.09it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.73it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.20it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:03<00:03,  9.54it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.79it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02,  9.97it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.09it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.17it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:04<00:01, 10.24it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.33it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  81%|████████  | 38/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.02it/s]
[2025-04-08 18:44:24,831][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6926
[2025-04-08 18:44:25,080][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:24,  1.90it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:09,  4.80it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:06,  6.64it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:01<00:05,  7.83it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:01<00:04,  8.63it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03,  9.17it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03,  9.55it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03,  9.81it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:02<00:03,  9.98it/s]Epoch 2/10:  40%|████      | 19/47 [00:02<00:02, 10.10it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.19it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.25it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.28it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.32it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.34it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00,  9.50it/s]
[2025-04-08 18:44:30,494][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6913
[2025-04-08 18:44:30,775][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.66it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-08 18:44:35,876][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6910
[2025-04-08 18:44:36,161][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.92it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  8.06it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.11it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.61it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.88it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-08 18:44:40,786][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6922
[2025-04-08 18:44:41,076][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  4.80it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.96it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  9.05it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.58it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-08 18:44:46,104][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6879
[2025-04-08 18:44:46,383][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.43it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.69it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-08 18:44:51,442][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6918
[2025-04-08 18:44:51,714][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.69it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.88it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.25it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-08 18:44:56,355][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6878
[2025-04-08 18:44:56,635][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.69it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.88it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-08 18:45:01,273][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6881
[2025-04-08 18:45:01,554][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:45:01,554][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁
wandb:        best_val_loss █▄▃▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▆▇▁▇▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁
wandb:             val_loss ▆▃▅▃▁▂▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69306
wandb:                epoch 8
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.53857
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.6881
wandb:           train_time 42.58915
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.694
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184406-hvv7nsjz
wandb: Find logs at: ./wandb/offline-run-20250408_184406-hvv7nsjz/logs
Control experiment for ko (control=1) completed successfully
Running question type control=2 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:45:20,229][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ko/control2
experiment_name: question_type_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:45:20,229][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:45:20,229][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:45:20,229][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:45:20,234][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-04-08 18:45:20,234][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:45:21,737][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:45:24,595][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:45:24,595][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:45:24,705][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:40:28 2025).
[2025-04-08 18:45:24,751][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:40:28 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 77060.73 examples/s]
[2025-04-08 18:45:24,983][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-08 18:45:24,991][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:45:24,991][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-08 18:45:24,993][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:45:25,024][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:45:25,060][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:45:25,075][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-08 18:45:25,077][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:45:25,077][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-08 18:45:25,078][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:45:25,108][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:45:25,145][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:45:25,157][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-08 18:45:25,159][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:45:25,159][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-08 18:45:25,160][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-08 18:45:25,161][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:45:25,161][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:45:25,161][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:45:25,161][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:45:25,162][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-08 18:45:25,162][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:45:25,162][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:45:25,163][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:45:25,163][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:45:25,163][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:45:25,164][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:45:25,164][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:45:25,164][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-08 18:45:25,164][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:45:25,164][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-08 18:45:25,164][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:45:25,164][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:45:25,165][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:45:30,018][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:45:30,021][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:45:30,021][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:08,  1.48s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:30,  1.49it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:13,  3.21it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:08,  4.76it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:02<00:06,  6.09it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:05,  7.17it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.02it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.66it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.15it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:03<00:03,  9.50it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.76it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02,  9.93it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.06it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.16it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:04<00:01, 10.22it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.26it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.30it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  81%|████████  | 38/47 [00:05<00:00, 10.35it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:05<00:00, 10.36it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.37it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  7.94it/s]
[2025-04-08 18:45:37,868][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6915
[2025-04-08 18:45:38,119][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6943, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.16it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.21it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.20it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.66it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.92it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.08it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.37it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.35it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.36it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.37it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.23it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-08 18:45:43,213][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6876
[2025-04-08 18:45:43,482][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6942, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.86it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  8.01it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.58it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.86it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.37it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.37it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.37it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-08 18:45:48,575][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6933
[2025-04-08 18:45:48,850][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6942, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.77it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.93it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.02it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.55it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.36it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.37it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.37it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-08 18:45:53,886][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6927
[2025-04-08 18:45:54,162][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.53it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.76it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.90it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.31it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.32it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.33it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.35it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.36it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.37it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.24it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-08 18:45:59,216][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6922
[2025-04-08 18:45:59,500][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.31it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.59it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.80it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.24it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.28it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.32it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.37it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.36it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.37it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.38it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.23it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.05it/s]
[2025-04-08 18:46:04,603][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6906
[2025-04-08 18:46:04,879][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6941, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.88it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  8.02it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  9.07it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.58it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.37it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.37it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-08 18:46:09,515][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6937
[2025-04-08 18:46:09,791][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6942, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  5.06it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  8.13it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  9.15it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.64it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.90it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.36it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.36it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.37it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.38it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-08 18:46:14,460][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6882
[2025-04-08 18:46:14,747][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6941, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:46:14,747][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁
wandb:        best_val_loss █▇▇▃▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss ▆▁█▇▆▄█▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁
wandb:             val_loss █▇▇▃▁▆▇▅
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69365
wandb:                epoch 8
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.53857
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68825
wandb:           train_time 42.80458
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69407
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184520-eo0er1te
wandb: Find logs at: ./wandb/offline-run-20250408_184520-eo0er1te/logs
Control experiment for ko (control=2) completed successfully
Running question type control=3 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:46:32,628][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ko/control3
experiment_name: question_type_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:46:32,628][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:46:32,629][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:46:32,629][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:46:32,652][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-04-08 18:46:32,656][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:46:34,031][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:46:36,929][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:46:36,930][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:46:37,019][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:42:20 2025).
[2025-04-08 18:46:37,049][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:42:20 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 75196.91 examples/s]
[2025-04-08 18:46:37,254][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-08 18:46:37,262][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:46:37,262][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-08 18:46:37,263][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:46:37,286][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:46:37,315][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:46:37,330][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-08 18:46:37,332][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:46:37,332][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-08 18:46:37,333][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:46:37,358][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:46:37,392][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:46:37,404][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-08 18:46:37,406][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:46:37,406][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-08 18:46:37,407][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-08 18:46:37,408][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:46:37,408][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:46:37,408][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:46:37,408][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:46:37,408][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-08 18:46:37,409][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:46:37,409][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:46:37,409][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:46:37,410][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:46:37,410][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:46:37,410][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:46:37,411][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:46:37,411][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-08 18:46:37,411][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:46:37,411][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-08 18:46:37,411][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:46:37,411][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:46:37,412][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:46:42,361][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:46:42,363][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:46:42,363][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:00<00:45,  1.01it/s]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:21,  2.14it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:10,  4.26it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:06,  5.91it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  7.14it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:01<00:04,  8.07it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.74it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  9.21it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.55it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:02,  9.73it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:02<00:02,  9.93it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02, 10.06it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.15it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.22it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.28it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:03<00:01, 10.31it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:03<00:01, 10.32it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.14it/s]
[2025-04-08 18:46:50,086][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-08 18:46:50,331][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6940, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.83it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.98it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.05it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.57it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-08 18:46:55,413][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6906
[2025-04-08 18:46:55,670][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.54it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.78it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.93it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.50it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-08 18:47:00,778][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-08 18:47:01,062][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6944, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.88it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  8.03it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.59it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-08 18:47:05,688][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6919
[2025-04-08 18:47:05,973][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6949, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  4.87it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  8.02it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.59it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.88it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.23it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-08 18:47:10,610][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6899
[2025-04-08 18:47:10,885][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6947, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:47:10,886][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁
wandb:           train_loss █▂▄▄▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁
wandb:             val_loss ▃▁▅█▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69378
wandb:                epoch 5
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.53857
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68988
wandb:           train_time 26.57945
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184632-do1ftogd
wandb: Find logs at: ./wandb/offline-run-20250408_184632-do1ftogd/logs
Control experiment for ko (control=3) completed successfully
Running question type control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:47:26,912][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ru/control1
experiment_name: question_type_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:47:26,912][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:47:26,912][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:47:26,912][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:47:26,932][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-04-08 18:47:26,935][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:47:28,867][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:47:31,941][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:47:31,942][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:47:31,981][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:44:11 2025).
[2025-04-08 18:47:32,011][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:44:11 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  94%|█████████▍| 7000/7460 [00:00<00:00, 65744.53 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 26419.25 examples/s]
[2025-04-08 18:47:32,376][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-08 18:47:32,387][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:47:32,388][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-08 18:47:32,389][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:47:32,420][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:47:32,457][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:47:32,472][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-08 18:47:32,474][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:47:32,474][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-08 18:47:32,476][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:47:32,503][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:47:32,539][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:47:32,554][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-08 18:47:32,556][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:47:32,556][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-08 18:47:32,557][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-08 18:47:32,558][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:47:32,558][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:47:32,558][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:47:32,558][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:47:32,559][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:47:32,559][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-08 18:47:32,559][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-08 18:47:32,559][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:47:32,559][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:47:32,559][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:47:32,559][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:47:32,560][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:47:32,560][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:47:32,560][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:47:32,561][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:47:32,561][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:47:32,561][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:47:32,561][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-08 18:47:32,561][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:47:32,561][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-08 18:47:32,561][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:47:32,562][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:47:32,562][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:47:37,147][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:47:37,150][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:47:37,150][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:26,  1.16s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:26,  2.68it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.33it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.75it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.91it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.82it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.53it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.05it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.43it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.70it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.90it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.76it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.02it/s]
[2025-04-08 18:47:48,053][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6946
[2025-04-08 18:47:48,327][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:47:56,153][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6924
[2025-04-08 18:47:56,443][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.85it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:48:03,800][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6943
[2025-04-08 18:48:04,069][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:48:11,444][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6926
[2025-04-08 18:48:11,701][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:48:11,702][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▇▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▂█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69256
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.6926
wandb:           train_time 31.96963
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69318
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184727-n6jjrro1
wandb: Find logs at: ./wandb/offline-run-20250408_184727-n6jjrro1/logs
Control experiment for ru (control=1) completed successfully
Running question type control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:48:31,118][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ru/control2
experiment_name: question_type_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:48:31,118][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:48:31,118][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:48:31,118][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:48:31,147][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-04-08 18:48:31,157][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:48:32,779][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:48:35,678][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:48:35,678][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:48:35,769][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:45:24 2025).
[2025-04-08 18:48:35,795][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:45:24 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 78429.84 examples/s]
[2025-04-08 18:48:35,967][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-08 18:48:35,978][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:48:35,978][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-08 18:48:35,980][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:48:36,001][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:48:36,029][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:48:36,041][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-08 18:48:36,043][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:48:36,043][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-08 18:48:36,044][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:48:36,066][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:48:36,095][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:48:36,108][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-08 18:48:36,110][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:48:36,110][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-08 18:48:36,111][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-08 18:48:36,111][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:48:36,111][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:48:36,111][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:48:36,112][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:48:36,112][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Sample label: 0
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:48:36,112][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:48:36,113][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:48:36,113][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:48:36,113][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:48:36,114][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:48:36,114][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:48:36,114][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:48:36,114][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-08 18:48:36,114][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:48:36,114][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-08 18:48:36,114][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:48:36,115][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:48:36,115][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:48:40,712][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:48:40,714][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:48:40,715][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:23,  1.12s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:38,  1.91it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.91it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.53it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.82it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.80it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.53it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.05it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.44it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.72it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.92it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.15it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.31it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.31it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.33it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.34it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.36it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.36it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.36it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.06it/s]
[2025-04-08 18:48:52,158][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6954
[2025-04-08 18:48:52,414][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-08 18:49:00,237][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6958
[2025-04-08 18:49:00,484][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.13it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.44it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:08,  8.71it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.71it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.12it/s]
[2025-04-08 18:49:08,620][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6943
[2025-04-08 18:49:08,922][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6921, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.16it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.48it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.71it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.31it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.32it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.34it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.35it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-08 18:49:16,733][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6953
[2025-04-08 18:49:17,029][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:49:24,391][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6921
[2025-04-08 18:49:24,694][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.31it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.33it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.35it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.36it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-08 18:49:32,069][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6942
[2025-04-08 18:49:32,360][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-08 18:49:32,361][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁
wandb:          best_val_f1 ▁▁▁
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss ▇█▅▇▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁
wandb:             val_loss ▂▂▁▂▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69207
wandb:                epoch 6
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69419
wandb:           train_time 48.48697
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69308
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184831-b3ijqko5
wandb: Find logs at: ./wandb/offline-run-20250408_184831-b3ijqko5/logs
Control experiment for ru (control=2) completed successfully
Running question type control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-08 18:49:51,303][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/question_type_output/ru/control3
experiment_name: question_type_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-08 18:49:51,303][__main__][INFO] - Normalized task: question_type
[2025-04-08 18:49:51,303][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-08 18:49:51,303][__main__][INFO] - Determined Task Type: classification
[2025-04-08 18:49:51,335][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-04-08 18:49:51,345][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-08 18:49:53,121][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-08 18:49:56,076][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-08 18:49:56,077][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:49:56,147][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:46:37 2025).
[2025-04-08 18:49:56,179][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:46:37 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 77615.46 examples/s]
[2025-04-08 18:49:56,376][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-08 18:49:56,387][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:49:56,388][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-08 18:49:56,389][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:49:56,412][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:49:56,443][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:49:56,456][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-08 18:49:56,458][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:49:56,458][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-08 18:49:56,459][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-08 18:49:56,483][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:49:56,516][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-08 18:49:56,529][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-08 18:49:56,531][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-08 18:49:56,531][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-08 18:49:56,532][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-08 18:49:56,533][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:49:56,533][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:49:56,533][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:49:56,533][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:49:56,533][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-08 18:49:56,533][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-08 18:49:56,533][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-08 18:49:56,534][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:49:56,534][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:49:56,534][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:49:56,534][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:49:56,534][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:49:56,534][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-08 18:49:56,534][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-08 18:49:56,535][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-08 18:49:56,535][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-08 18:49:56,535][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-08 18:49:56,536][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-08 18:49:56,536][src.data.datasets][INFO] - Sample label: 1
[2025-04-08 18:49:56,536][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-08 18:49:56,536][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-08 18:49:56,536][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-08 18:49:56,537][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-08 18:50:01,429][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-08 18:50:01,432][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-08 18:50:01,432][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:39,  1.35s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:44,  1.62it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.44it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.03it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:10,  6.34it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.39it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.20it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.81it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.25it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.58it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.81it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.09it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.24it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.36it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.36it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.36it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.29it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.32it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.34it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.35it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.36it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.80it/s]
[2025-04-08 18:50:11,999][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6953
[2025-04-08 18:50:12,239][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.26it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.29it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.31it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-08 18:50:20,070][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6948
[2025-04-08 18:50:20,332][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-08 18:50:28,350][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6930
[2025-04-08 18:50:28,640][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6921, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.56it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.36it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-08 18:50:36,450][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6934
[2025-04-08 18:50:36,761][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.36it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.36it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.36it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.36it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-08 18:50:44,146][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6929
[2025-04-08 18:50:44,430][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6916, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.23it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.53it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.75it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.37it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:50:52,251][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6915
[2025-04-08 18:50:52,551][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6919, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.16it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.47it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.71it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.93it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-08 18:50:59,957][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6912
[2025-04-08 18:51:00,253][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6914, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.77it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.37it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.36it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.29it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.32it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.34it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.34it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.35it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.11it/s]
[2025-04-08 18:51:08,099][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6932
[2025-04-08 18:51:08,395][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6914, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.36it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.37it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.78it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-08 18:51:15,759][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6876
[2025-04-08 18:51:16,052][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6919, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.36it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.79it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-08 18:51:23,444][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6929
[2025-04-08 18:51:23,744][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6921, Metrics: {'accuracy': 0.5277777777777778, 'f1': 0.10526315789473684}
[2025-04-08 18:51:23,745][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁
wandb:        best_val_loss █▅▅▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▆▆▅▄▆▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁█
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁█
wandb:             val_loss █▅▅▇▂▃▁▁▄▅
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69135
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69285
wandb:           train_time 80.27367
wandb:         val_accuracy 0.52778
wandb:               val_f1 0.10526
wandb:             val_loss 0.69208
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/wandb/offline-run-20250408_184951-lxwtn4fy
wandb: Find logs at: ./wandb/offline-run-20250408_184951-lxwtn4fy/logs
Control experiment for ru (control=3) completed successfully
Collecting results...
Traceback (most recent call last):
  File "/scratch/leuven/371/vsc37132/question_type_output/collect_results.py", line 5, in <module>
    with open('/tmp/languages.txt', 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/languages.txt'
Question type classification experiments completed
Question type classification experiments completed
