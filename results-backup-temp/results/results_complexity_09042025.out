SLURM_JOB_ID: 64308035
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: complexity_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100
SLURM_NNODES: 1
SLURM_NODELIST: k28g25
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed Apr  9 01:15:00 CEST 2025
Walltime: 00-03:00:00
========================================================================
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
HYDRA_JOB_CHDIR=False
GPU information:
Wed Apr  9 01:16:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             60W /  500W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running complexity regression for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:16:58,053][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ar
experiment_name: complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:16:58,053][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:16:58,053][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:16:58,053][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:16:58,059][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-09 01:16:58,059][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:17:01,452][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:17:04,320][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:17:04,321][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:17:04,490][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,558][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,717][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-09 01:17:04,727][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:17:04,727][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-09 01:17:04,729][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:17:04,761][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,804][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,823][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-09 01:17:04,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:17:04,825][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-09 01:17:04,826][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:17:04,858][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,905][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:17:04,922][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-09 01:17:04,923][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:17:04,923][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-09 01:17:04,925][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-09 01:17:04,926][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:17:04,926][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:17:04,926][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:17:04,926][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:17:04,926][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:17:04,927][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:17:04,927][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:17:04,927][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:17:04,928][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:17:04,928][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:17:04,929][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:17:04,929][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-09 01:17:04,929][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-09 01:17:04,929][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-09 01:17:04,929][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-09 01:17:04,929][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:17:04,930][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:17:04,930][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:17:11,409][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:17:11,411][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:17:11,412][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:49,  1.76s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:48,  1.27it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:28,  2.11it/s]Epoch 1/10:   8%|▊         | 5/63 [00:02<00:15,  3.84it/s]Epoch 1/10:  11%|█         | 7/63 [00:02<00:10,  5.35it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:02<00:08,  6.59it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  7.58it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.35it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:03<00:05,  8.92it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:03<00:04,  9.34it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.64it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.86it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.02it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:04<00:03, 10.13it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:03, 10.21it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.26it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.30it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:05<00:02, 10.35it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.14it/s]
[2025-04-09 01:17:23,064][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-09 01:17:23,240][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:11,  5.25it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.28it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.25it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.70it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.95it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.11it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.20it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.28it/s]
[2025-04-09 01:17:29,830][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-09 01:17:30,011][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.83it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.98it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.07it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:17:36,631][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-09 01:17:36,823][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.04it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.13it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.16it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.64it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.92it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.08it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:17:43,386][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-09 01:17:43,577][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:21,  2.86it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:09,  6.17it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:07,  7.80it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:06,  8.72it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:01<00:05,  9.29it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05,  9.65it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:05,  9.89it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.05it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.16it/s]Epoch 5/10:  30%|███       | 19/63 [00:02<00:04, 10.22it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.28it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.32it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.34it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00,  9.99it/s]
[2025-04-09 01:17:50,277][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-09 01:17:50,480][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  4.87it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  8.01it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.08it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.60it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:17:57,064][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-09 01:17:57,261][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  4.81it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.98it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  9.07it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.41it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-09 01:18:03,855][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-09 01:18:04,057][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  4.96it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  8.08it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  9.13it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.63it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.91it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:18:10,652][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-09 01:18:10,855][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:12,  4.83it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  8.00it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  9.07it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:18:17,437][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-09 01:18:17,646][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  4.92it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  8.05it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  9.11it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.40it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.41it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:18:23,800][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-09 01:18:24,003][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.04996
wandb:    final_test_r2 0.13866
wandb:  final_test_rmse 0.22352
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 69.12698
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_011658-o5purn8p
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_011658-o5purn8p/logs
Standard experiment for ar completed successfully
Running complexity regression for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:18:47,793][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/en
experiment_name: complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:18:47,793][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:18:47,793][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:18:47,793][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:18:47,799][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-09 01:18:47,799][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:18:49,888][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:18:52,716][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:18:52,716][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:18:52,874][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:52,919][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:53,052][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-09 01:18:53,063][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:18:53,063][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-09 01:18:53,065][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:18:53,099][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:53,149][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:53,168][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-09 01:18:53,169][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:18:53,169][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-09 01:18:53,171][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:18:53,206][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:53,253][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:18:53,272][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-09 01:18:53,274][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:18:53,274][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-09 01:18:53,276][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-09 01:18:53,277][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:18:53,277][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:18:53,277][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:18:53,277][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:18:53,277][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:18:53,278][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:18:53,278][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:18:53,279][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:18:53,279][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-09 01:18:53,279][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-09 01:18:53,279][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-09 01:18:53,279][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:18:53,279][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:18:53,279][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:18:53,280][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:18:53,280][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:18:53,280][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-09 01:18:53,280][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-09 01:18:53,280][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-09 01:18:53,280][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-09 01:18:53,280][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:18:53,281][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:18:53,281][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:18:59,248][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:18:59,251][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:18:59,252][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:35,  1.29s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.46it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.05it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.46it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.65it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.61it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.36it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.35it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.65it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.92it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-09 01:19:10,330][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-09 01:19:10,591][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.19it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.32it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:19:18,397][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-09 01:19:18,638][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.86it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:19:26,418][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-09 01:19:26,676][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:19:34,397][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-09 01:19:34,644][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.25it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.27it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.70it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.96it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:19:41,955][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-09 01:19:42,218][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.04it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.14it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:19:49,553][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-09 01:19:49,809][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-09 01:19:49,810][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.02591
wandb:    final_test_r2 0.32757
wandb:  final_test_rmse 0.16098
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.025
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_011847-64vm2bjd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_011847-64vm2bjd/logs
Standard experiment for en completed successfully
Running complexity regression for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:20:12,250][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/fi
experiment_name: complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:20:12,250][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:20:12,251][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:20:12,251][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:20:12,256][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-09 01:20:12,257][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:20:13,849][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:20:16,863][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:20:16,864][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:20:16,972][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,021][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,169][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-09 01:20:17,180][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:20:17,181][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-09 01:20:17,182][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:20:17,222][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,273][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,294][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-09 01:20:17,296][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:20:17,296][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-09 01:20:17,297][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:20:17,341][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,396][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:20:17,417][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-09 01:20:17,418][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:20:17,418][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-09 01:20:17,421][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-09 01:20:17,422][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:20:17,422][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:20:17,422][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:20:17,422][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:20:17,423][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:20:17,423][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-09 01:20:17,423][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-09 01:20:17,423][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-09 01:20:17,423][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:20:17,423][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:20:17,423][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:20:17,424][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:20:17,424][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:20:17,424][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:20:17,425][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:20:17,425][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:20:17,425][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-09 01:20:17,425][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-09 01:20:17,425][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-09 01:20:17,425][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-09 01:20:17,425][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:20:17,426][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:20:17,426][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:20:23,144][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:20:23,147][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:20:23,148][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:36,  1.31s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:43,  1.67it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.52it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.12it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.43it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.48it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.27it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.87it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.31it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.14it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.42it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.89it/s]
[2025-04-09 01:20:34,164][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-09 01:20:34,376][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:20:42,162][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-09 01:20:42,384][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:20:50,168][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-09 01:20:50,397][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.18it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.19it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:20:58,140][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-09 01:20:58,372][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.42it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.42it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:21:06,109][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-09 01:21:06,344][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:21:13,686][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-09 01:21:13,917][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:27,  2.69it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:12,  5.97it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:09,  7.64it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  8.61it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:07,  9.22it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.60it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.86it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.15it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:02<00:05, 10.23it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.28it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.42it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.42it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-09 01:21:21,422][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-09 01:21:21,656][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-09 01:21:21,657][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.02845
wandb:    final_test_r2 0.27943
wandb:  final_test_rmse 0.16868
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 55.93326
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012012-cdhalbgl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012012-cdhalbgl/logs
Standard experiment for fi completed successfully
Running complexity regression for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:21:43,436][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/id
experiment_name: complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:21:43,436][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:21:43,436][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:21:43,436][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:21:43,442][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-09 01:21:43,442][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:21:45,233][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:21:48,241][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:21:48,241][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:21:48,335][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,385][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,556][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-09 01:21:48,564][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:21:48,565][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-09 01:21:48,568][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:21:48,618][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,682][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,705][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-09 01:21:48,707][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:21:48,707][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-09 01:21:48,709][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:21:48,764][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,829][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:21:48,848][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-09 01:21:48,850][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:21:48,850][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-09 01:21:48,852][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-09 01:21:48,853][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:21:48,853][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:21:48,853][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:21:48,853][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:21:48,854][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:21:48,854][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-09 01:21:48,854][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-09 01:21:48,854][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-09 01:21:48,854][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:21:48,854][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:21:48,855][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:21:48,855][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:21:48,855][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:21:48,855][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-09 01:21:48,855][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-09 01:21:48,855][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-09 01:21:48,855][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:21:48,856][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:21:48,856][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-09 01:21:48,856][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:21:48,857][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:21:48,857][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:21:54,551][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:21:54,554][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:21:54,554][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:11,  1.22s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:32,  1.78it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:20,  2.85it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:11,  4.84it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:08,  6.36it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:06,  7.49it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:05,  8.33it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  8.93it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:04,  9.36it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.67it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:02<00:04,  9.89it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03, 10.04it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03, 10.15it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.23it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.28it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.32it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.42it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.69it/s]
[2025-04-09 01:22:03,829][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-09 01:22:04,054][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.18it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:06,  8.23it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:05,  9.22it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.69it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.95it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.11it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.21it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.35it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.21it/s]
[2025-04-09 01:22:10,403][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-09 01:22:10,635][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.78it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:06,  8.61it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:05,  9.45it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.84it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05, 10.04it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.17it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.30it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.33it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.36it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.42it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.26it/s]
[2025-04-09 01:22:16,488][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-09 01:22:16,722][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:11,  5.13it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:06,  8.20it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:05,  9.20it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.67it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.42it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.22it/s]
[2025-04-09 01:22:23,052][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-09 01:22:23,295][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.14it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:06,  8.21it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:05,  9.21it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.68it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.22it/s]
[2025-04-09 01:22:29,171][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-09 01:22:29,419][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:11,  5.06it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:06,  8.16it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:05,  9.18it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.66it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.93it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:22:35,302][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-09 01:22:35,563][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.43it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:06,  8.38it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:05,  9.31it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.75it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.99it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.13it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.22it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.28it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.35it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.22it/s]
[2025-04-09 01:22:41,849][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-09 01:22:42,101][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:11,  5.21it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:06,  8.25it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:05,  9.24it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.70it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.95it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.11it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.21it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.21it/s]
[2025-04-09 01:22:47,983][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-09 01:22:48,241][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:11,  5.11it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:06,  8.19it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:05,  9.20it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.68it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.42it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:22:54,124][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-09 01:22:54,371][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:11,  5.33it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:06,  8.33it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:05,  9.28it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.73it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.97it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.12it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.22it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.28it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.35it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.21it/s]
[2025-04-09 01:23:00,638][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-09 01:23:00,888][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.06184
wandb:    final_test_r2 -0.51671
wandb:  final_test_rmse 0.24868
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 63.96735
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012143-u9s95j07
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012143-u9s95j07/logs
Standard experiment for id completed successfully
Running complexity regression for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:23:21,907][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ja
experiment_name: complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:23:21,907][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:23:21,908][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:23:21,908][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:23:21,913][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-04-09 01:23:21,914][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:23:23,225][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:23:26,050][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:23:26,050][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:23:26,142][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,183][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,297][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-09 01:23:26,307][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:23:26,308][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-09 01:23:26,309][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:23:26,338][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,376][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,391][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-09 01:23:26,392][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:23:26,393][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-09 01:23:26,394][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:23:26,419][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,456][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:23:26,471][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-09 01:23:26,472][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:23:26,472][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-09 01:23:26,474][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-09 01:23:26,474][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:23:26,474][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:23:26,474][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:23:26,474][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:23:26,475][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:23:26,475][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-09 01:23:26,475][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-09 01:23:26,475][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-09 01:23:26,475][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:23:26,475][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:23:26,475][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:23:26,476][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:23:26,476][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:23:26,476][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-09 01:23:26,476][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-09 01:23:26,476][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-09 01:23:26,476][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:23:26,476][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:23:26,477][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:23:26,477][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-09 01:23:26,477][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:23:26,478][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:23:26,478][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:23:32,027][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:23:32,029][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:23:32,030][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:23,  1.13s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:38,  1.90it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.90it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.54it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.82it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.80it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.54it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.07it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.46it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.74it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.94it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.08it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.18it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.29it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.33it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.33it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.35it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.09it/s]
[2025-04-09 01:23:42,619][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-09 01:23:42,804][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.35it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:23:50,597][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-09 01:23:50,793][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:23:58,606][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-09 01:23:58,803][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.15it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:24:06,525][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-09 01:24:06,731][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:24:14,449][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-09 01:24:14,648][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:24:22,378][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-09 01:24:22,583][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:24:29,895][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-09 01:24:30,093][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:24:37,432][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-09 01:24:37,626][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-09 01:24:37,627][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.02282
wandb:    final_test_r2 0.56158
wandb:  final_test_rmse 0.15107
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 63.26189
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012321-7z89nf5i
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012321-7z89nf5i/logs
Standard experiment for ja completed successfully
Running complexity regression for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:25:00,461][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ko
experiment_name: complexity_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:25:00,461][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:25:00,461][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:25:00,461][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:25:00,467][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-04-09 01:25:00,467][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:25:01,923][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:25:04,751][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:25:04,751][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:25:04,844][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:04,890][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:05,028][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-09 01:25:05,035][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:25:05,035][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-09 01:25:05,037][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:25:05,074][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:05,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:05,141][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-09 01:25:05,142][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:25:05,142][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-09 01:25:05,144][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:25:05,179][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:05,225][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:25:05,244][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-09 01:25:05,247][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:25:05,247][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-09 01:25:05,248][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-09 01:25:05,249][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:25:05,249][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:25:05,249][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:25:05,249][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:25:05,249][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:25:05,249][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:25:05,250][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:25:05,250][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:25:05,251][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:25:05,251][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:25:05,251][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:25:05,252][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-09 01:25:05,252][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-09 01:25:05,252][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-09 01:25:05,252][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-09 01:25:05,252][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:25:05,252][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:25:05,253][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:25:11,390][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:25:11,393][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:25:11,393][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:55,  1.21s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:25,  1.79it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.72it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.34it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.64it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.65it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.41it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.98it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.39it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:02,  9.69it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.90it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02, 10.05it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.16it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.23it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.28it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.37it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.27it/s]
[2025-04-09 01:25:19,566][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-09 01:25:19,791][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.91it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.05it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.11it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.62it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.90it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.07it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.41it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.41it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s]
[2025-04-09 01:25:24,855][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-09 01:25:25,090][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:18,  2.43it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:07,  5.62it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:05,  7.36it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  8.41it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:04,  9.06it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.49it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03,  9.78it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03,  9.97it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.11it/s]Epoch 3/10:  40%|████      | 19/47 [00:02<00:02, 10.20it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.26it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.31it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00,  9.67it/s]
[2025-04-09 01:25:30,408][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-09 01:25:30,666][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.86it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  8.01it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.09it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.60it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.41it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.41it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.31it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.19it/s]
[2025-04-09 01:25:35,692][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-09 01:25:35,956][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  5.09it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  8.17it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  9.19it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.67it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.93it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.09it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.41it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-09 01:25:40,994][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-09 01:25:41,261][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  5.08it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  8.17it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.18it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.66it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.93it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.09it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.31it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.31it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.31it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s]
[2025-04-09 01:25:45,869][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-09 01:25:46,127][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.86it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  8.02it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  9.09it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.61it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.90it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.07it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.38it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.41it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.19it/s]
[2025-04-09 01:25:50,741][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-09 01:25:50,996][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.94it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  8.07it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  9.12it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.63it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.90it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.07it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.31it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s]
[2025-04-09 01:25:56,021][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-09 01:25:56,273][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:08,  5.15it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  8.21it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  9.21it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.68it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.94it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.10it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.27it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.40it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.41it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.41it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.31it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s]
[2025-04-09 01:26:00,892][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-09 01:26:01,155][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:09,  4.85it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  8.00it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.60it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-09 01:26:06,186][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-09 01:26:06,432][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.01562
wandb:    final_test_r2 0.51497
wandb:  final_test_rmse 0.12498
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 52.55182
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012500-uontzjsq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012500-uontzjsq/logs
Standard experiment for ko completed successfully
Running complexity regression for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:26:27,361][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ru
experiment_name: complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:26:27,361][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:26:27,361][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:26:27,361][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:26:27,367][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-04-09 01:26:27,368][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:26:28,977][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:26:31,880][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:26:31,880][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:26:31,958][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,003][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,146][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-09 01:26:32,157][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:26:32,158][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-09 01:26:32,159][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:26:32,199][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,250][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,270][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-09 01:26:32,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:26:32,271][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-09 01:26:32,275][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:26:32,321][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,373][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:26:32,393][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-09 01:26:32,395][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:26:32,395][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-09 01:26:32,396][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-09 01:26:32,397][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:26:32,397][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:26:32,397][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:26:32,398][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:26:32,398][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:26:32,398][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-09 01:26:32,398][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-09 01:26:32,398][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-09 01:26:32,398][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:26:32,399][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:26:32,399][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-09 01:26:32,399][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:26:32,400][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:26:32,400][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-09 01:26:32,400][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-09 01:26:32,401][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:26:32,401][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:26:32,401][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:26:38,219][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:26:38,222][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:26:38,222][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:29,  1.21s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.60it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.23it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.65it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.83it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.76it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.48it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.02it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.41it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.70it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.91it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.43it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.97it/s]
[2025-04-09 01:26:48,816][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-09 01:26:49,042][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:26:56,834][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-09 01:26:57,087][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.13it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:27:04,883][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-09 01:27:05,140][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.56it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.47it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.37it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06, 10.01it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.23it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.36it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-09 01:27:12,449][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-09 01:27:12,706][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.91it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:27:20,429][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-09 01:27:20,704][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.01it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.12it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:27:28,022][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-09 01:27:28,291][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.83it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:27:35,641][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-09 01:27:35,886][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-09 01:27:35,887][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.0249
wandb:    final_test_r2 0.37021
wandb:  final_test_rmse 0.15779
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 55.43521
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012627-oaayttkz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012627-oaayttkz/logs
Standard experiment for ru completed successfully
Running complexity control=1 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:27:58,641][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ar/control1
experiment_name: complexity_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:27:58,642][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:27:58,642][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:27:58,642][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:27:58,648][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-09 01:27:58,649][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:28:00,212][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:28:03,193][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:28:03,193][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:28:03,280][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:28:03,327][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:28:03,777][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-09 01:28:03,786][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:28:03,787][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-09 01:28:03,789][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:28:03,825][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:28:03,873][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:28:03,893][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-09 01:28:03,894][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:28:03,894][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-09 01:28:03,896][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:28:03,932][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:28:03,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:28:04,000][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-09 01:28:04,002][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:28:04,002][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-09 01:28:04,004][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-09 01:28:04,005][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:28:04,005][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:28:04,005][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:28:04,005][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:28:04,005][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:28:04,006][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Sample label: 0.20462249219417572
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:28:04,006][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:28:04,007][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:28:04,007][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:28:04,007][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:28:04,008][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:28:04,008][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-09 01:28:04,008][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-09 01:28:04,008][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-09 01:28:04,008][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-09 01:28:04,008][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:28:04,009][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:28:04,009][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:28:09,883][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:28:09,885][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:28:09,886][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:11,  1.15s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:22,  2.71it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:13,  4.37it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:09,  5.80it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:07,  6.96it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  7.87it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.56it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  9.08it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.46it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:04,  9.73it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.93it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.07it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.17it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.24it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.28it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.32it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.11it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.87it/s]
[2025-04-09 01:28:19,700][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1673
[2025-04-09 01:28:20,182][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1301, Metrics: {'mse': 0.13290393352508545, 'rmse': 0.3645599176062633, 'r2': -1.0484917163848877}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:12,  5.12it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.19it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.19it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.67it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.93it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.09it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:28:26,791][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1233
[2025-04-09 01:28:26,981][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1208, Metrics: {'mse': 0.12044896930456161, 'rmse': 0.3470575878792475, 'r2': -0.8565192222595215}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.90it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  8.04it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.10it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.25it/s]
[2025-04-09 01:28:33,588][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0716
[2025-04-09 01:28:33,793][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1108, Metrics: {'mse': 0.10865295678377151, 'rmse': 0.3296254795730626, 'r2': -0.6747034788131714}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.25it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.27it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.24it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.70it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.95it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.11it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.20it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:28:40,338][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0589
[2025-04-09 01:28:40,532][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0822, Metrics: {'mse': 0.08267040550708771, 'rmse': 0.28752461721927, 'r2': -0.27422595024108887}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.84it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  8.00it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.08it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 5/10:  13%|█▎        | 8/63 [00:00<00:06,  8.85it/s]Epoch 5/10:  16%|█▌        | 10/63 [00:01<00:05,  9.41it/s]Epoch 5/10:  19%|█▉        | 12/63 [00:01<00:05,  9.75it/s]Epoch 5/10:  22%|██▏       | 14/63 [00:01<00:04,  9.96it/s]Epoch 5/10:  25%|██▌       | 16/63 [00:01<00:04, 10.09it/s]Epoch 5/10:  29%|██▊       | 18/63 [00:01<00:04, 10.19it/s]Epoch 5/10:  32%|███▏      | 20/63 [00:02<00:04, 10.25it/s]Epoch 5/10:  35%|███▍      | 22/63 [00:02<00:03, 10.30it/s]Epoch 5/10:  38%|███▊      | 24/63 [00:02<00:03, 10.33it/s]Epoch 5/10:  41%|████▏     | 26/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  44%|████▍     | 28/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 30/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  51%|█████     | 32/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  54%|█████▍    | 34/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  57%|█████▋    | 36/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  60%|██████    | 38/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 40/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  67%|██████▋   | 42/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  70%|██████▉   | 44/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  73%|███████▎  | 46/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 48/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  79%|███████▉  | 50/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  83%|████████▎ | 52/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  86%|████████▌ | 54/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 56/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 58/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 60/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.15it/s]
[2025-04-09 01:28:47,140][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0544
[2025-04-09 01:28:47,347][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0730, Metrics: {'mse': 0.0731871947646141, 'rmse': 0.27053131937839303, 'r2': -0.1280580759048462}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-09 01:28:53,948][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0415
[2025-04-09 01:28:54,151][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0686, Metrics: {'mse': 0.06808673590421677, 'rmse': 0.26093435171363843, 'r2': -0.04944312572479248}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  4.80it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.97it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.33it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.36it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.37it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:29:00,734][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0381
[2025-04-09 01:29:00,947][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0661, Metrics: {'mse': 0.0657317191362381, 'rmse': 0.25638197896154497, 'r2': -0.013144373893737793}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.20it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  8.24it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  9.23it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.69it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.95it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.10it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.20it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.27it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:29:07,518][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0359
[2025-04-09 01:29:07,726][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0683, Metrics: {'mse': 0.06796708703041077, 'rmse': 0.2607049808316112, 'r2': -0.047598958015441895}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:12,  4.79it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.97it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.34it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.36it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.37it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-09 01:29:13,896][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0372
[2025-04-09 01:29:14,336][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0670, Metrics: {'mse': 0.06673403829336166, 'rmse': 0.258329321396859, 'r2': -0.02859342098236084}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.38it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  8.36it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  9.30it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.74it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.98it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.12it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.22it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.28it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.32it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.35it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.40it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  81%|████████  | 51/63 [00:04<00:01, 10.41it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.27it/s]
[2025-04-09 01:29:20,475][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0344
[2025-04-09 01:29:20,682][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0675, Metrics: {'mse': 0.06716548651456833, 'rmse': 0.2591630500564622, 'r2': -0.03524351119995117}
[2025-04-09 01:29:20,683][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▃▂▁▁
wandb:     best_val_mse █▇▅▃▂▁▁
wandb:      best_val_r2 ▁▂▄▆▇██
wandb:    best_val_rmse █▇▆▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▃▂▁▁▁▁▁
wandb:          val_mse █▇▅▃▂▁▁▁▁▁
wandb:           val_r2 ▁▂▄▆▇█████
wandb:         val_rmse █▇▆▃▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06609
wandb:     best_val_mse 0.06573
wandb:      best_val_r2 -0.01314
wandb:    best_val_rmse 0.25638
wandb:            epoch 10
wandb:   final_test_mse 0.06581
wandb:    final_test_r2 -0.13449
wandb:  final_test_rmse 0.25653
wandb:  final_train_mse 0.03279
wandb:   final_train_r2 -0.06832
wandb: final_train_rmse 0.18109
wandb:    final_val_mse 0.06573
wandb:     final_val_r2 -0.01314
wandb:   final_val_rmse 0.25638
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03436
wandb:       train_time 68.08942
wandb:         val_loss 0.06748
wandb:          val_mse 0.06717
wandb:           val_r2 -0.03524
wandb:         val_rmse 0.25916
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012758-jrm1ubwi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012758-jrm1ubwi/logs
Control experiment for ar (control=1) completed successfully
Running complexity control=2 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:29:42,257][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ar/control2
experiment_name: complexity_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:29:42,257][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:29:42,257][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:29:42,257][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:29:42,263][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-09 01:29:42,263][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:29:44,076][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:29:46,917][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:29:46,918][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:29:47,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:29:47,053][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:29:47,282][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-09 01:29:47,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:29:47,292][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-09 01:29:47,294][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:29:47,328][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:29:47,370][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:29:47,387][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-09 01:29:47,388][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:29:47,388][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-09 01:29:47,390][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:29:47,419][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:29:47,459][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:29:47,475][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-09 01:29:47,477][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:29:47,477][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-09 01:29:47,478][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-09 01:29:47,479][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:29:47,479][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:29:47,479][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:29:47,479][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:29:47,480][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:29:47,480][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-09 01:29:47,480][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-09 01:29:47,480][src.data.datasets][INFO] - Sample label: 0.443451464176178
[2025-04-09 01:29:47,480][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:29:47,480][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:29:47,481][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:29:47,481][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:29:47,481][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:29:47,482][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:29:47,482][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-09 01:29:47,482][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:29:47,483][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:29:47,483][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:29:53,158][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:29:53,161][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:29:53,161][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:19,  1.29s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:24,  2.47it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:14,  4.06it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:10,  5.47it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:02<00:08,  6.66it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  7.62it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.36it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.93it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.34it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.64it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.87it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.02it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.13it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.21it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.27it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.31it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.37it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.37it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.10it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.65it/s]
[2025-04-09 01:30:03,264][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1548
[2025-04-09 01:30:03,441][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1017, Metrics: {'mse': 0.1000465676188469, 'rmse': 0.31630138731729723, 'r2': -0.5420503616333008}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.66it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.52it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.39it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.79it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05, 10.01it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.14it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.23it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.27it/s]
[2025-04-09 01:30:10,031][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0863
[2025-04-09 01:30:10,331][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1003, Metrics: {'mse': 0.0992436632514, 'rmse': 0.31502962281569646, 'r2': -0.5296748876571655}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.29it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  8.29it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.25it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.71it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.96it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.11it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:30:16,939][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0702
[2025-04-09 01:30:17,150][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0773, Metrics: {'mse': 0.0788847804069519, 'rmse': 0.28086434520414283, 'r2': -0.21587681770324707}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.08it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.17it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.18it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.66it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.93it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.09it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:30:23,697][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0623
[2025-04-09 01:30:23,902][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0691, Metrics: {'mse': 0.06804449111223221, 'rmse': 0.2608533900723397, 'r2': -0.04879200458526611}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.80it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.97it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-09 01:30:30,457][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0495
[2025-04-09 01:30:30,657][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0719, Metrics: {'mse': 0.07216615974903107, 'rmse': 0.2686375992839258, 'r2': -0.11232054233551025}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.11it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  8.19it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.19it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.67it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.93it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.09it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.28it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.32it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.35it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.36it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.26it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:30:36,821][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0416
[2025-04-09 01:30:37,022][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0665, Metrics: {'mse': 0.06654361635446548, 'rmse': 0.25796049378628794, 'r2': -0.025658488273620605}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.74it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.93it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  9.03it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-09 01:30:43,615][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0392
[2025-04-09 01:30:43,816][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0642, Metrics: {'mse': 0.06404467672109604, 'rmse': 0.253070497532004, 'r2': 0.012858569622039795}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.24it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  8.26it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  9.24it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.70it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.95it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.10it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.20it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:30:50,372][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0352
[2025-04-09 01:30:50,581][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0633, Metrics: {'mse': 0.06293798983097076, 'rmse': 0.2508744503351642, 'r2': 0.029916226863861084}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:12,  4.79it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.96it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  9.05it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.17it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:30:57,152][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0346
[2025-04-09 01:30:57,359][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0641, Metrics: {'mse': 0.06373502314090729, 'rmse': 0.2524579631164509, 'r2': 0.01763129234313965}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  5.03it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  8.14it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  9.16it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.65it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.92it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.08it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.34it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.35it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.37it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.38it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s]
[2025-04-09 01:31:03,517][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0355
[2025-04-09 01:31:03,725][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0655, Metrics: {'mse': 0.06505019962787628, 'rmse': 0.2550494062488213, 'r2': -0.0026398897171020508}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▄▂▂▁▁
wandb:     best_val_mse ██▄▂▂▁▁
wandb:      best_val_r2 ▁▁▅▇▇██
wandb:    best_val_rmse ██▄▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▃▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▄▂▃▂▁▁▁▁
wandb:          val_mse ██▄▂▃▂▁▁▁▁
wandb:           val_r2 ▁▁▅▇▆▇████
wandb:         val_rmse ██▄▂▃▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06328
wandb:     best_val_mse 0.06294
wandb:      best_val_r2 0.02992
wandb:    best_val_rmse 0.25087
wandb:            epoch 10
wandb:   final_test_mse 0.05408
wandb:    final_test_r2 0.06765
wandb:  final_test_rmse 0.23255
wandb:  final_train_mse 0.03099
wandb:   final_train_r2 -0.00968
wandb: final_train_rmse 0.17605
wandb:    final_val_mse 0.06294
wandb:     final_val_r2 0.02992
wandb:   final_val_rmse 0.25087
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03546
wandb:       train_time 67.75108
wandb:         val_loss 0.06548
wandb:          val_mse 0.06505
wandb:           val_r2 -0.00264
wandb:         val_rmse 0.25505
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012942-cd77qdl6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_012942-cd77qdl6/logs
Control experiment for ar (control=2) completed successfully
Running complexity control=3 for ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:31:26,191][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ar/control3
experiment_name: complexity_control3_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:31:26,191][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:31:26,191][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:31:26,191][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:31:26,197][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-09 01:31:26,197][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:31:27,948][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:31:30,786][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:31:30,787][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:31:30,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:31:30,923][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:31:31,191][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-09 01:31:31,201][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:31:31,201][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-09 01:31:31,203][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:31:31,239][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:31:31,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:31:31,305][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-09 01:31:31,307][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:31:31,307][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-09 01:31:31,308][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:31:31,346][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:31:31,397][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:31:31,417][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-09 01:31:31,418][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:31:31,418][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-09 01:31:31,420][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-09 01:31:31,421][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:31:31,421][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:31:31,421][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:31:31,421][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:31:31,421][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:31:31,422][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Sample label: 0.31771957874298096
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:31:31,422][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:31:31,423][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:31:31,423][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-09 01:31:31,423][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-09 01:31:31,423][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-09 01:31:31,423][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:31:31,423][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:31:31,423][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:31:31,424][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:31:31,424][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:31:31,424][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-09 01:31:31,424][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-09 01:31:31,424][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-09 01:31:31,424][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-09 01:31:31,424][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:31:31,425][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:31:31,425][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:31:37,423][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:31:37,426][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:31:37,426][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:07,  1.09s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:31,  1.96it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:14,  3.99it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:10,  5.64it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:01<00:07,  6.91it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:01<00:06,  7.88it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:02<00:05,  8.59it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:05,  9.12it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:02<00:04,  9.49it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:02<00:04,  9.76it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:02<00:04,  9.95it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04, 10.09it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03, 10.18it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:03<00:03, 10.24it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:03<00:03, 10.29it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:03<00:03, 10.33it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  60%|██████    | 38/63 [00:04<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:04<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.40it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:05<00:01, 10.41it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:05<00:01, 10.41it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.95it/s]
[2025-04-09 01:31:46,829][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1755
[2025-04-09 01:31:47,011][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1408, Metrics: {'mse': 0.1418701857328415, 'rmse': 0.3766565885960864, 'r2': -1.1866912841796875}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:11,  5.17it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.23it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.22it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.69it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.95it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.11it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.21it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  81%|████████  | 51/63 [00:04<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.31it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.27it/s]
[2025-04-09 01:31:53,611][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0897
[2025-04-09 01:31:53,810][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0948, Metrics: {'mse': 0.09532200545072556, 'rmse': 0.30874262007491865, 'r2': -0.4692291021347046}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.68it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.88it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.00it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.41it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:32:00,428][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0517
[2025-04-09 01:32:00,623][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0624, Metrics: {'mse': 0.062148336321115494, 'rmse': 0.24929568051034398, 'r2': 0.042087435722351074}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.06it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  8.16it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.17it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.65it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.93it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.09it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.41it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.31it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:32:07,161][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0403
[2025-04-09 01:32:07,360][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0650, Metrics: {'mse': 0.064579539000988, 'rmse': 0.2541250459930859, 'r2': 0.004614472389221191}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.92it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  8.06it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.12it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.63it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.91it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.40it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.33it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.35it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.37it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-09 01:32:13,524][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0371
[2025-04-09 01:32:13,727][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0674, Metrics: {'mse': 0.06672171503305435, 'rmse': 0.2583054684536399, 'r2': -0.028403520584106445}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.11it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  8.19it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.20it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.68it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.94it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.09it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.19it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.34it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.36it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.41it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.26it/s]
[2025-04-09 01:32:19,871][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0366
[2025-04-09 01:32:20,226][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0645, Metrics: {'mse': 0.06422017514705658, 'rmse': 0.25341699853612143, 'r2': 0.010153532028198242}
[2025-04-09 01:32:20,227][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▄▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▁▁▁▁
wandb:          val_mse █▄▁▁▁▁
wandb:           val_r2 ▁▅████
wandb:         val_rmse █▄▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06236
wandb:     best_val_mse 0.06215
wandb:      best_val_r2 0.04209
wandb:    best_val_rmse 0.2493
wandb:            epoch 6
wandb:   final_test_mse 0.05724
wandb:    final_test_r2 0.01314
wandb:  final_test_rmse 0.23926
wandb:  final_train_mse 0.03188
wandb:   final_train_r2 -0.03851
wandb: final_train_rmse 0.17855
wandb:    final_val_mse 0.06215
wandb:     final_val_r2 0.04209
wandb:   final_val_rmse 0.2493
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03664
wandb:       train_time 40.44329
wandb:         val_loss 0.06449
wandb:          val_mse 0.06422
wandb:           val_r2 0.01015
wandb:         val_rmse 0.25342
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013126-8q4stof8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013126-8q4stof8/logs
Control experiment for ar (control=3) completed successfully
Running complexity control=1 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:32:41,632][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/en/control1
experiment_name: complexity_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:32:41,633][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:32:41,633][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:32:41,633][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:32:41,638][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-09 01:32:41,639][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:32:43,021][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:32:45,849][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:32:45,849][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:32:45,933][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:32:45,976][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:32:46,114][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-09 01:32:46,125][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:32:46,126][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-09 01:32:46,127][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:32:46,156][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:32:46,197][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:32:46,215][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-09 01:32:46,217][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:32:46,217][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-09 01:32:46,218][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:32:46,247][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:32:46,288][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:32:46,308][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-09 01:32:46,310][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:32:46,310][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-09 01:32:46,312][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-09 01:32:46,313][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:32:46,313][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:32:46,313][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:32:46,313][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:32:46,314][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:32:46,314][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-09 01:32:46,314][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-09 01:32:46,314][src.data.datasets][INFO] - Sample label: 0.4486629366874695
[2025-04-09 01:32:46,314][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:32:46,315][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:32:46,315][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-09 01:32:46,315][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:32:46,316][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:32:46,316][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-09 01:32:46,316][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:32:46,317][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:32:46,317][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:32:52,007][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:32:52,009][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:32:52,009][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:31,  1.23s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.55it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.17it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.59it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.77it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.71it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.44it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.98it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.38it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.67it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.91it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.95it/s]
[2025-04-09 01:33:02,716][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1349
[2025-04-09 01:33:02,947][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0509, Metrics: {'mse': 0.05204156041145325, 'rmse': 0.2281261940493753, 'r2': -0.24348998069763184}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.57it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.48it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.36it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:06,  9.78it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06, 10.00it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.23it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.29it/s]
[2025-04-09 01:33:10,691][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0642
[2025-04-09 01:33:10,931][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0371, Metrics: {'mse': 0.038908056914806366, 'rmse': 0.1972512532654897, 'r2': 0.07032418251037598}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.35it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.33it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.28it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:06,  9.73it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.98it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.22it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:33:18,726][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0402
[2025-04-09 01:33:18,993][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0410, Metrics: {'mse': 0.04224281385540962, 'rmse': 0.2055305667179693, 'r2': -0.009357094764709473}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.19it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.19it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:33:26,308][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0340
[2025-04-09 01:33:26,549][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0433, Metrics: {'mse': 0.043814584612846375, 'rmse': 0.20931933645233633, 'r2': -0.0469132661819458}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:33:33,876][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0309
[2025-04-09 01:33:34,134][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0500, Metrics: {'mse': 0.049888670444488525, 'rmse': 0.22335771856931322, 'r2': -0.19204843044281006}
[2025-04-09 01:33:34,135][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▄█
wandb:          val_mse █▁▃▄▇
wandb:           val_r2 ▁█▆▅▂
wandb:         val_rmse █▁▃▄▇
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03707
wandb:     best_val_mse 0.03891
wandb:      best_val_r2 0.07032
wandb:    best_val_rmse 0.19725
wandb:            epoch 5
wandb:   final_test_mse 0.04223
wandb:    final_test_r2 -0.09573
wandb:  final_test_rmse 0.20549
wandb:  final_train_mse 0.04021
wandb:   final_train_r2 -0.49888
wandb: final_train_rmse 0.20053
wandb:    final_val_mse 0.03891
wandb:     final_val_r2 0.07032
wandb:   final_val_rmse 0.19725
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0309
wandb:       train_time 39.80472
wandb:         val_loss 0.04996
wandb:          val_mse 0.04989
wandb:           val_r2 -0.19205
wandb:         val_rmse 0.22336
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013241-yjo36lz3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013241-yjo36lz3/logs
Control experiment for en (control=1) completed successfully
Running complexity control=2 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:33:56,787][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/en/control2
experiment_name: complexity_control2_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:33:56,787][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:33:56,787][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:33:56,787][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:33:56,792][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-09 01:33:56,793][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:33:58,259][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:34:01,252][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:34:01,253][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:34:01,344][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:34:01,379][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:34:01,502][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-09 01:34:01,513][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:34:01,514][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-09 01:34:01,516][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:34:01,543][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:34:01,575][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:34:01,590][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-09 01:34:01,591][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:34:01,591][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-09 01:34:01,593][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:34:01,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:34:01,658][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:34:01,672][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-09 01:34:01,674][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:34:01,674][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-09 01:34:01,675][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-09 01:34:01,676][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:34:01,676][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:34:01,676][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:34:01,676][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:34:01,676][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:34:01,676][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Sample label: 0.2855727970600128
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:34:01,677][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:34:01,677][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:34:01,678][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:34:01,678][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:34:01,678][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:34:01,679][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-09 01:34:01,679][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-09 01:34:01,679][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-09 01:34:01,679][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-09 01:34:01,679][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:34:01,679][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:34:01,680][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:34:06,716][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:34:06,719][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:34:06,719][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:40,  1.36s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:45,  1.61it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.60it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.51it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.04it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  7.22it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:07,  8.11it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.76it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.23it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.57it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.11it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.91it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-09 01:34:17,497][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1391
[2025-04-09 01:34:17,741][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0744, Metrics: {'mse': 0.07901505380868912, 'rmse': 0.28109616469935894, 'r2': -0.887999415397644}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:34:25,505][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0629
[2025-04-09 01:34:25,771][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0527, Metrics: {'mse': 0.053646136075258255, 'rmse': 0.23161635537081196, 'r2': -0.2818300724029541}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.95it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:34:33,562][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0485
[2025-04-09 01:34:33,819][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0559, Metrics: {'mse': 0.05655760318040848, 'rmse': 0.23781842481273077, 'r2': -0.3513970375061035}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.91it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:34:41,153][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0428
[2025-04-09 01:34:41,423][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0399, Metrics: {'mse': 0.04066835343837738, 'rmse': 0.2016639616748054, 'r2': 0.028263330459594727}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:34:49,139][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0383
[2025-04-09 01:34:49,419][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0378, Metrics: {'mse': 0.03836880251765251, 'rmse': 0.19587956125551362, 'r2': 0.08320927619934082}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:30,  2.46it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:12,  5.65it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:09,  7.38it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:08,  8.41it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:07,  9.07it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.49it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.78it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:02<00:05, 10.19it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.25it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.30it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-09 01:34:57,352][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0339
[2025-04-09 01:34:57,638][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0382, Metrics: {'mse': 0.03869502246379852, 'rmse': 0.1967105042030001, 'r2': 0.07541441917419434}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:35:04,987][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0314
[2025-04-09 01:35:05,250][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0421, Metrics: {'mse': 0.04220616817474365, 'rmse': 0.20544139839560976, 'r2': -0.008481502532958984}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.01it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.15it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:35:12,572][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0293
[2025-04-09 01:35:12,849][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0441, Metrics: {'mse': 0.044066473841667175, 'rmse': 0.20992016063653146, 'r2': -0.052931904792785645}
[2025-04-09 01:35:12,850][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁▁
wandb:     best_val_mse █▄▁▁
wandb:      best_val_r2 ▁▅██
wandb:    best_val_rmse █▄▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▁▁▁▂▂
wandb:          val_mse █▄▄▁▁▁▂▂
wandb:           val_r2 ▁▅▅███▇▇
wandb:         val_rmse █▄▄▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03778
wandb:     best_val_mse 0.03837
wandb:      best_val_r2 0.08321
wandb:    best_val_rmse 0.19588
wandb:            epoch 8
wandb:   final_test_mse 0.03967
wandb:    final_test_r2 -0.02939
wandb:  final_test_rmse 0.19918
wandb:  final_train_mse 0.02994
wandb:   final_train_r2 -0.11584
wandb: final_train_rmse 0.17302
wandb:    final_val_mse 0.03837
wandb:     final_val_r2 0.08321
wandb:   final_val_rmse 0.19588
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02927
wandb:       train_time 63.85687
wandb:         val_loss 0.04407
wandb:          val_mse 0.04407
wandb:           val_r2 -0.05293
wandb:         val_rmse 0.20992
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013356-emtv2j22
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013356-emtv2j22/logs
Control experiment for en (control=2) completed successfully
Running complexity control=3 for en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:35:36,141][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/en/control3
experiment_name: complexity_control3_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:35:36,141][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:35:36,141][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:35:36,141][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:35:36,147][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-09 01:35:36,147][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:35:38,029][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:35:40,984][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:35:40,984][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:35:41,066][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:35:41,112][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:35:41,267][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-09 01:35:41,279][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:35:41,279][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-09 01:35:41,281][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:35:41,315][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:35:41,355][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:35:41,372][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-09 01:35:41,373][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:35:41,374][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-09 01:35:41,375][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:35:41,408][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:35:41,448][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:35:41,464][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-09 01:35:41,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:35:41,465][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-09 01:35:41,467][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-09 01:35:41,467][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:35:41,468][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:35:41,468][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:35:41,468][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:35:41,468][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:35:41,468][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-09 01:35:41,468][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-09 01:35:41,469][src.data.datasets][INFO] - Sample label: 0.7702211737632751
[2025-04-09 01:35:41,469][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:35:41,469][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:35:41,469][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:35:41,469][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:35:41,469][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:35:41,469][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:35:41,470][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:35:41,470][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:35:41,470][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-09 01:35:41,471][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-09 01:35:41,471][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-09 01:35:41,471][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-09 01:35:41,471][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:35:41,471][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:35:41,472][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:35:46,689][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:35:46,692][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:35:46,692][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:44,  1.41s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:31,  2.29it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:18,  3.82it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:13,  5.22it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.43it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.42it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.20it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.81it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.25it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.58it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.42it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.92it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-09 01:35:57,626][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1346
[2025-04-09 01:35:57,872][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0513, Metrics: {'mse': 0.05481584370136261, 'rmse': 0.23412783623773276, 'r2': -0.30977916717529297}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.42it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.42it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-09 01:36:05,631][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0804
[2025-04-09 01:36:05,884][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0597, Metrics: {'mse': 0.059097521007061005, 'rmse': 0.24309981696221206, 'r2': -0.4120863676071167}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:36:13,197][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0548
[2025-04-09 01:36:13,442][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0430, Metrics: {'mse': 0.04476906359195709, 'rmse': 0.21158701186972015, 'r2': -0.06971967220306396}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 4/10:   3%|▎         | 2/75 [00:00<00:15,  4.68it/s]Epoch 4/10:   5%|▌         | 4/75 [00:00<00:09,  7.14it/s]Epoch 4/10:   8%|▊         | 6/75 [00:00<00:08,  8.38it/s]Epoch 4/10:  11%|█         | 8/75 [00:00<00:07,  9.10it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:06,  9.54it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.82it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06, 10.01it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05, 10.14it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.27it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.31it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:04, 10.34it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  80%|████████  | 60/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.42it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.09it/s]
[2025-04-09 01:36:21,343][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0485
[2025-04-09 01:36:21,588][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0408, Metrics: {'mse': 0.04184933751821518, 'rmse': 0.20457110626433828, 'r2': 4.470348358154297e-05}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:36:29,308][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0458
[2025-04-09 01:36:29,572][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0410, Metrics: {'mse': 0.042540840804576874, 'rmse': 0.2062543109963447, 'r2': -0.01647818088531494}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.67it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:36:36,887][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0350
[2025-04-09 01:36:37,141][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0408, Metrics: {'mse': 0.041340216994285583, 'rmse': 0.20332293769834622, 'r2': 0.012209773063659668}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.48it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.43it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.34it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:06,  9.77it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06, 10.00it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.14it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.23it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.28it/s]
[2025-04-09 01:36:44,442][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0322
[2025-04-09 01:36:44,692][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0444, Metrics: {'mse': 0.04447377473115921, 'rmse': 0.2108880620878271, 'r2': -0.06266403198242188}
[2025-04-09 01:36:44,693][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▃▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▂▁▁▁▂
wandb:          val_mse ▆█▂▁▁▁▂
wandb:           val_r2 ▃▁▇███▇
wandb:         val_rmse ▆█▂▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0408
wandb:     best_val_mse 0.04185
wandb:      best_val_r2 4e-05
wandb:    best_val_rmse 0.20457
wandb:            epoch 7
wandb:   final_test_mse 0.04041
wandb:    final_test_r2 -0.04844
wandb:  final_test_rmse 0.20101
wandb:  final_train_mse 0.03147
wandb:   final_train_r2 -0.17293
wandb: final_train_rmse 0.17739
wandb:    final_val_mse 0.04185
wandb:     final_val_r2 4e-05
wandb:   final_val_rmse 0.20457
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03219
wandb:       train_time 55.60835
wandb:         val_loss 0.04436
wandb:          val_mse 0.04447
wandb:           val_r2 -0.06266
wandb:         val_rmse 0.21089
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013536-ih2a9z9d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013536-ih2a9z9d/logs
Control experiment for en (control=3) completed successfully
Running complexity control=1 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:37:06,349][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/fi/control1
experiment_name: complexity_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:37:06,350][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:37:06,350][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:37:06,350][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:37:06,355][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-09 01:37:06,356][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:37:07,895][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:37:10,739][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:37:10,740][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:37:10,835][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:37:10,882][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:37:11,108][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-09 01:37:11,118][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:37:11,119][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-09 01:37:11,122][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:37:11,158][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:37:11,211][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:37:11,231][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-09 01:37:11,233][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:37:11,233][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-09 01:37:11,235][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:37:11,273][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:37:11,327][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:37:11,346][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-09 01:37:11,348][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:37:11,348][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-09 01:37:11,350][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-09 01:37:11,350][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:37:11,350][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:37:11,350][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:37:11,350][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:37:11,351][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:37:11,351][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-09 01:37:11,351][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-09 01:37:11,351][src.data.datasets][INFO] - Sample label: 0.21079079806804657
[2025-04-09 01:37:11,351][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:37:11,351][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:37:11,351][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:37:11,352][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:37:11,352][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:37:11,352][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-09 01:37:11,352][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-09 01:37:11,352][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-09 01:37:11,352][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:37:11,352][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:37:11,353][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:37:11,353][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-09 01:37:11,353][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:37:11,354][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:37:11,354][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:37:17,139][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:37:17,142][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:37:17,142][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:30,  1.23s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:41,  1.76it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.68it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.29it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.60it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.61it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.38it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.96it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.37it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.67it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.15it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.93it/s]
[2025-04-09 01:37:27,873][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1144
[2025-04-09 01:37:28,093][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1822, Metrics: {'mse': 0.18228554725646973, 'rmse': 0.426949115535411, 'r2': -1.7804155349731445}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:37:35,879][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0671
[2025-04-09 01:37:36,140][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1304, Metrics: {'mse': 0.13051049411296844, 'rmse': 0.3612623618825637, 'r2': -0.9906865358352661}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:37:43,944][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0296
[2025-04-09 01:37:44,317][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0899, Metrics: {'mse': 0.08982358872890472, 'rmse': 0.2997058369950521, 'r2': -0.37008607387542725}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.29it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:37:52,071][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0252
[2025-04-09 01:37:52,309][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0845, Metrics: {'mse': 0.08439705520868301, 'rmse': 0.2905117126876006, 'r2': -0.287314772605896}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.78it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 5/10:   5%|▌         | 4/75 [00:00<00:08,  8.55it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.38it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:06,  9.78it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  29%|██▉       | 22/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  32%|███▏      | 24/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  35%|███▍      | 26/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  37%|███▋      | 28/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  43%|████▎     | 32/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  45%|████▌     | 34/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  48%|████▊     | 36/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  51%|█████     | 38/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  80%|████████  | 60/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.33it/s]Epoch 5/10:  91%|█████████ | 68/75 [00:06<00:00, 10.35it/s]Epoch 5/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 5/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:38:00,051][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0239
[2025-04-09 01:38:00,295][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0859, Metrics: {'mse': 0.08582642674446106, 'rmse': 0.2929614765535924, 'r2': -0.30911707878112793}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 6/10:   8%|▊         | 6/75 [00:00<00:07,  9.34it/s]Epoch 6/10:  11%|█         | 8/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  13%|█▎        | 10/75 [00:01<00:06, 10.01it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  19%|█▊        | 14/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  21%|██▏       | 16/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  24%|██▍       | 18/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  27%|██▋       | 20/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  29%|██▉       | 22/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  32%|███▏      | 24/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  35%|███▍      | 26/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  37%|███▋      | 28/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  40%|████      | 30/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  45%|████▌     | 34/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  51%|█████     | 38/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  80%|████████  | 60/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:38:07,637][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0225
[2025-04-09 01:38:07,873][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0837, Metrics: {'mse': 0.08357636630535126, 'rmse': 0.28909577358610977, 'r2': -0.274796724319458}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:38:15,636][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0226
[2025-04-09 01:38:15,872][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0838, Metrics: {'mse': 0.08370497077703476, 'rmse': 0.28931811346169595, 'r2': -0.2767583131790161}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:38:23,216][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0227
[2025-04-09 01:38:23,456][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0836, Metrics: {'mse': 0.08347728103399277, 'rmse': 0.28892435174971454, 'r2': -0.27328526973724365}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.66it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-09 01:38:31,259][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0224
[2025-04-09 01:38:31,505][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0863, Metrics: {'mse': 0.08620559424161911, 'rmse': 0.2936078919947812, 'r2': -0.3149005174636841}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.05it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:38:38,843][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0217
[2025-04-09 01:38:39,075][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0835, Metrics: {'mse': 0.08335394412279129, 'rmse': 0.28871083132226144, 'r2': -0.2714040279388428}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁▁▁▁▁
wandb:     best_val_mse █▄▁▁▁▁▁
wandb:      best_val_r2 ▁▅█████
wandb:    best_val_rmse █▅▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▁▁▁▁▁▁▁▁
wandb:          val_mse █▄▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅████████
wandb:         val_rmse █▅▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08346
wandb:     best_val_mse 0.08335
wandb:      best_val_r2 -0.2714
wandb:    best_val_rmse 0.28871
wandb:            epoch 10
wandb:   final_test_mse 0.03854
wandb:    final_test_r2 0.02396
wandb:  final_test_rmse 0.19632
wandb:  final_train_mse 0.02
wandb:   final_train_r2 0.01021
wandb: final_train_rmse 0.14143
wandb:    final_val_mse 0.08335
wandb:     final_val_r2 -0.2714
wandb:   final_val_rmse 0.28871
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02165
wandb:       train_time 80.01445
wandb:         val_loss 0.08346
wandb:          val_mse 0.08335
wandb:           val_r2 -0.2714
wandb:         val_rmse 0.28871
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013706-28ppdqng
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013706-28ppdqng/logs
Control experiment for fi (control=1) completed successfully
Running complexity control=2 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:39:01,081][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/fi/control2
experiment_name: complexity_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:39:01,081][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:39:01,082][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:39:01,082][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:39:01,087][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-09 01:39:01,088][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:39:02,425][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:39:05,277][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:39:05,277][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:39:05,347][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:39:05,392][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:39:05,540][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-09 01:39:05,550][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:39:05,551][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-09 01:39:05,553][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:39:05,590][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:39:05,639][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:39:05,658][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-09 01:39:05,660][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:39:05,660][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-09 01:39:05,661][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:39:05,695][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:39:05,739][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:39:05,757][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-09 01:39:05,759][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:39:05,759][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-09 01:39:05,761][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-09 01:39:05,762][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:39:05,762][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:39:05,762][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:39:05,762][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:39:05,762][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:39:05,762][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Sample label: 0.6458609104156494
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:39:05,763][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:39:05,763][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:39:05,764][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:39:05,764][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:39:05,764][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:39:05,765][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-09 01:39:05,765][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-09 01:39:05,765][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-09 01:39:05,765][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-09 01:39:05,765][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:39:05,765][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:39:05,766][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:39:11,418][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:39:11,421][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:39:11,421][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:35,  1.29s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:43,  1.69it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.55it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.16it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.47it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.50it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.30it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.89it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.13it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.89it/s]
[2025-04-09 01:39:22,037][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1153
[2025-04-09 01:39:22,251][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1707, Metrics: {'mse': 0.1713147610425949, 'rmse': 0.41390187368819065, 'r2': -1.61307692527771}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.01it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.12it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:39:30,058][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0679
[2025-04-09 01:39:30,276][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1391, Metrics: {'mse': 0.13929420709609985, 'rmse': 0.3732213915306836, 'r2': -1.1246650218963623}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.31it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.26it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.71it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.96it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:39:38,059][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0358
[2025-04-09 01:39:38,282][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1050, Metrics: {'mse': 0.10498793423175812, 'rmse': 0.32401841650091145, 'r2': -0.6013889312744141}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.79it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-09 01:39:46,050][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0256
[2025-04-09 01:39:46,285][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0918, Metrics: {'mse': 0.09172739833593369, 'rmse': 0.30286531385408544, 'r2': -0.3991250991821289}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:39:54,025][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0230
[2025-04-09 01:39:54,265][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0839, Metrics: {'mse': 0.08382020890712738, 'rmse': 0.28951719967409084, 'r2': -0.2785160541534424}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.68it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:40:02,045][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0234
[2025-04-09 01:40:02,278][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0843, Metrics: {'mse': 0.08422894030809402, 'rmse': 0.29022222573072176, 'r2': -0.28475046157836914}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:40:09,617][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0224
[2025-04-09 01:40:09,855][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0844, Metrics: {'mse': 0.08426810801029205, 'rmse': 0.29028969670019644, 'r2': -0.28534793853759766}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.31it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.34it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:40:17,202][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0222
[2025-04-09 01:40:17,447][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0834, Metrics: {'mse': 0.08333708345890045, 'rmse': 0.28868162992975577, 'r2': -0.27114689350128174}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.33it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.35it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.36it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:40:25,222][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0228
[2025-04-09 01:40:25,495][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0868, Metrics: {'mse': 0.08673732727766037, 'rmse': 0.2945120155064312, 'r2': -0.3230111598968506}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:40:32,841][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0217
[2025-04-09 01:40:33,076][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0837, Metrics: {'mse': 0.08358558267354965, 'rmse': 0.2891117131379316, 'r2': -0.2749372720718384}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁
wandb:     best_val_mse █▅▃▂▁▁
wandb:      best_val_r2 ▁▄▆▇██
wandb:    best_val_rmse █▆▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁▁
wandb:          val_mse █▅▃▂▁▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇██████
wandb:         val_rmse █▆▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08344
wandb:     best_val_mse 0.08334
wandb:      best_val_r2 -0.27115
wandb:    best_val_rmse 0.28868
wandb:            epoch 10
wandb:   final_test_mse 0.03917
wandb:    final_test_r2 0.00795
wandb:  final_test_rmse 0.19792
wandb:  final_train_mse 0.02012
wandb:   final_train_r2 0.00456
wandb: final_train_rmse 0.14183
wandb:    final_val_mse 0.08334
wandb:     final_val_r2 -0.27115
wandb:   final_val_rmse 0.28868
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02171
wandb:       train_time 79.47609
wandb:         val_loss 0.08368
wandb:          val_mse 0.08359
wandb:           val_r2 -0.27494
wandb:         val_rmse 0.28911
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013901-c40qnd3e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_013901-c40qnd3e/logs
Control experiment for fi (control=2) completed successfully
Running complexity control=3 for fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:40:55,132][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/fi/control3
experiment_name: complexity_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:40:55,132][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:40:55,132][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:40:55,132][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:40:55,138][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-09 01:40:55,138][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:40:56,825][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:40:59,617][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:40:59,617][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:40:59,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:40:59,827][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:40:59,978][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-09 01:40:59,988][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:40:59,989][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-09 01:40:59,991][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:41:00,025][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:41:00,070][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:41:00,088][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-09 01:41:00,089][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:41:00,089][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-09 01:41:00,091][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:41:00,125][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:41:00,166][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:41:00,183][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-09 01:41:00,184][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:41:00,185][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-09 01:41:00,187][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-09 01:41:00,187][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:41:00,187][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:41:00,187][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:41:00,187][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:41:00,188][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:41:00,188][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-09 01:41:00,188][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-09 01:41:00,188][src.data.datasets][INFO] - Sample label: 0.3422375023365021
[2025-04-09 01:41:00,188][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:41:00,188][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:41:00,188][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:41:00,189][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:41:00,189][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:41:00,189][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-09 01:41:00,189][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-09 01:41:00,189][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-09 01:41:00,189][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:41:00,189][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:41:00,190][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:41:00,190][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-09 01:41:00,190][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:41:00,191][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:41:00,191][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:41:05,437][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:41:05,440][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:41:05,440][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:21,  1.11s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:37,  1.94it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:17,  3.97it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.61it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.89it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.86it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.58it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.10it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.48it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.75it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.95it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.08it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.18it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.30it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.34it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.36it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.40it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.10it/s]
[2025-04-09 01:41:16,160][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1074
[2025-04-09 01:41:16,390][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2625, Metrics: {'mse': 0.26307329535484314, 'rmse': 0.5129067121366644, 'r2': -3.0126771926879883}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.62it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.51it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.39it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:06,  9.79it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06, 10.02it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.24it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.39it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-09 01:41:24,152][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0542
[2025-04-09 01:41:24,372][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1159, Metrics: {'mse': 0.11595401167869568, 'rmse': 0.34052020744545497, 'r2': -0.7686554193496704}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.33it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:41:32,192][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0274
[2025-04-09 01:41:32,442][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0898, Metrics: {'mse': 0.08971168845891953, 'rmse': 0.2995190953160074, 'r2': -0.36837923526763916}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:41:40,167][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0236
[2025-04-09 01:41:40,398][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0860, Metrics: {'mse': 0.08595139533281326, 'rmse': 0.2931746839903017, 'r2': -0.3110232353210449}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:41:48,149][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0230
[2025-04-09 01:41:48,386][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0839, Metrics: {'mse': 0.08381495624780655, 'rmse': 0.2895081281204494, 'r2': -0.27843594551086426}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:41:56,146][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0226
[2025-04-09 01:41:56,410][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0851, Metrics: {'mse': 0.08502741903066635, 'rmse': 0.2915946142003764, 'r2': -0.2969297170639038}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:42:03,756][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0221
[2025-04-09 01:42:04,000][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0853, Metrics: {'mse': 0.08526482433080673, 'rmse': 0.29200141152194237, 'r2': -0.3005509376525879}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.88it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:42:11,342][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0226
[2025-04-09 01:42:11,588][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0827, Metrics: {'mse': 0.08262839168310165, 'rmse': 0.28745154667022, 'r2': -0.260337233543396}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.17it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:42:19,338][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0218
[2025-04-09 01:42:19,597][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0840, Metrics: {'mse': 0.08391574025154114, 'rmse': 0.28968213657652614, 'r2': -0.2799731492996216}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.77it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:42:26,941][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0226
[2025-04-09 01:42:27,191][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0834, Metrics: {'mse': 0.08328422158956528, 'rmse': 0.2885900580227345, 'r2': -0.27034056186676025}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁▁▁
wandb:     best_val_mse █▂▁▁▁▁
wandb:      best_val_r2 ▁▇████
wandb:    best_val_rmse █▃▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▁▁▁▁▁▁▁▁
wandb:          val_mse █▂▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▇████████
wandb:         val_rmse █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08271
wandb:     best_val_mse 0.08263
wandb:      best_val_r2 -0.26034
wandb:    best_val_rmse 0.28745
wandb:            epoch 10
wandb:   final_test_mse 0.03835
wandb:    final_test_r2 0.02875
wandb:  final_test_rmse 0.19584
wandb:  final_train_mse 0.02004
wandb:   final_train_r2 0.00837
wandb: final_train_rmse 0.14156
wandb:    final_val_mse 0.08263
wandb:     final_val_r2 -0.26034
wandb:   final_val_rmse 0.28745
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02257
wandb:       train_time 79.2722
wandb:         val_loss 0.08336
wandb:          val_mse 0.08328
wandb:           val_r2 -0.27034
wandb:         val_rmse 0.28859
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014055-rkumb6vr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014055-rkumb6vr/logs
Control experiment for fi (control=3) completed successfully
Running complexity control=1 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:42:50,468][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/id/control1
experiment_name: complexity_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:42:50,468][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:42:50,468][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:42:50,468][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:42:50,474][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-09 01:42:50,474][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:42:52,303][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:42:55,145][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:42:55,145][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:42:55,261][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:42:55,304][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:42:55,449][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-09 01:42:55,457][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:42:55,458][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-09 01:42:55,459][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:42:55,489][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:42:55,527][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:42:55,543][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-09 01:42:55,544][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:42:55,545][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-09 01:42:55,546][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:42:55,575][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:42:55,617][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:42:55,633][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-09 01:42:55,634][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:42:55,635][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-09 01:42:55,636][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-09 01:42:55,636][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:42:55,637][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:42:55,637][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:42:55,637][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:42:55,637][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:42:55,637][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-09 01:42:55,637][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-09 01:42:55,637][src.data.datasets][INFO] - Sample label: 0.41827768087387085
[2025-04-09 01:42:55,638][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:42:55,638][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:42:55,638][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:42:55,638][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:42:55,638][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:42:55,638][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-09 01:42:55,638][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-09 01:42:55,639][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-09 01:42:55,639][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:42:55,639][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:42:55,639][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:42:55,639][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:42:55,639][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:42:55,639][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-09 01:42:55,640][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-09 01:42:55,640][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-09 01:42:55,640][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-09 01:42:55,640][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:42:55,640][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:42:55,640][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:43:01,428][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:43:01,430][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:43:01,430][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:18,  1.33s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:23,  2.40it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:13,  3.98it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.39it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:02<00:07,  6.58it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:06,  7.55it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  8.31it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.89it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.31it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.63it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03,  9.85it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03, 10.01it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.12it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.20it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:04<00:03, 10.27it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.31it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.55it/s]
[2025-04-09 01:43:10,660][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1396
[2025-04-09 01:43:10,888][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1442, Metrics: {'mse': 0.13726666569709778, 'rmse': 0.37049516285249634, 'r2': -2.283172130584717}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.01it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  8.12it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.15it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.65it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.92it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:43:17,232][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0779
[2025-04-09 01:43:17,472][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1139, Metrics: {'mse': 0.11503913998603821, 'rmse': 0.33917420300789125, 'r2': -1.7515296936035156}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.89it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  8.02it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.09it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.61it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.42it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:43:23,824][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0660
[2025-04-09 01:43:24,110][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0710, Metrics: {'mse': 0.0698743611574173, 'rmse': 0.2643375893765722, 'r2': -0.6712691783905029}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:12,  4.65it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.00it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.54it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-09 01:43:30,418][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0600
[2025-04-09 01:43:30,694][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1269, Metrics: {'mse': 0.12411553412675858, 'rmse': 0.35230034647550174, 'r2': -1.968620777130127}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.15it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:06,  8.21it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:05,  9.21it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.68it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-09 01:43:36,583][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0505
[2025-04-09 01:43:36,854][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0726, Metrics: {'mse': 0.07074172049760818, 'rmse': 0.2659731574757276, 'r2': -0.6920149326324463}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:10,  5.49it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:06,  8.43it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:05,  9.34it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.77it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05, 10.00it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.14it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.23it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.29it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.35it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.23it/s]
[2025-04-09 01:43:42,723][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0435
[2025-04-09 01:43:43,004][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0617, Metrics: {'mse': 0.06038249284029007, 'rmse': 0.24572849415623346, 'r2': -0.4442408084869385}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.86it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  8.02it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  9.09it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-09 01:43:49,280][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0404
[2025-04-09 01:43:49,553][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0680, Metrics: {'mse': 0.06647095084190369, 'rmse': 0.2578196091105246, 'r2': -0.5898656845092773}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.37it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:06,  8.35it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:05,  9.30it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.73it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.98it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.12it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.21it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.28it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.32it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.35it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.40it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.22it/s]
[2025-04-09 01:43:55,424][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0404
[2025-04-09 01:43:55,687][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0607, Metrics: {'mse': 0.059236492961645126, 'rmse': 0.24338548223270245, 'r2': -0.41683053970336914}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:11,  4.94it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  8.08it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  9.13it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.63it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.91it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.08it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.42it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-09 01:44:02,038][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0390
[2025-04-09 01:44:02,307][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0601, Metrics: {'mse': 0.05878624692559242, 'rmse': 0.242458753039754, 'r2': -0.4060615301132202}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:11,  5.08it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:06,  8.17it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:05,  9.18it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.66it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.92it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-09 01:44:08,617][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0381
[2025-04-09 01:44:08,895][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0591, Metrics: {'mse': 0.05772320553660393, 'rmse': 0.24025654109015207, 'r2': -0.3806353807449341}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▂▁▁▁▁
wandb:     best_val_mse █▆▂▁▁▁▁
wandb:      best_val_r2 ▁▃▇████
wandb:    best_val_rmse █▆▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▃▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▂▇▂▁▂▁▁▁
wandb:          val_mse █▆▂▇▂▁▂▁▁▁
wandb:           val_r2 ▁▃▇▂▇█▇███
wandb:         val_rmse █▆▂▇▂▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05906
wandb:     best_val_mse 0.05772
wandb:      best_val_r2 -0.38064
wandb:    best_val_rmse 0.24026
wandb:            epoch 10
wandb:   final_test_mse 0.03945
wandb:    final_test_r2 0.03258
wandb:  final_test_rmse 0.19861
wandb:  final_train_mse 0.0361
wandb:   final_train_r2 0.00509
wandb: final_train_rmse 0.19001
wandb:    final_val_mse 0.05772
wandb:     final_val_r2 -0.38064
wandb:   final_val_rmse 0.24026
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03811
wandb:       train_time 65.69599
wandb:         val_loss 0.05906
wandb:          val_mse 0.05772
wandb:           val_r2 -0.38064
wandb:         val_rmse 0.24026
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014250-s6p939m5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014250-s6p939m5/logs
Control experiment for id (control=1) completed successfully
Running complexity control=2 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:44:31,929][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/id/control2
experiment_name: complexity_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:44:31,929][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:44:31,930][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:44:31,930][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:44:31,936][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-09 01:44:31,936][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:44:33,654][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:44:36,486][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:44:36,487][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:44:36,593][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
[2025-04-09 01:44:36,637][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:34:32 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 58885.04 examples/s]
[2025-04-09 01:44:36,885][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-09 01:44:36,893][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:44:36,894][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-09 01:44:36,896][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:44:36,933][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:44:36,976][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:44:36,997][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-09 01:44:36,999][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:44:36,999][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-09 01:44:37,000][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:44:37,036][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:44:37,082][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:44:37,101][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-09 01:44:37,103][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:44:37,103][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-09 01:44:37,105][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-09 01:44:37,106][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:44:37,106][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:44:37,106][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:44:37,106][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:44:37,106][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:44:37,106][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:44:37,107][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:44:37,107][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:44:37,108][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:44:37,108][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:44:37,108][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:44:37,109][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-09 01:44:37,109][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-09 01:44:37,109][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-09 01:44:37,109][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-09 01:44:37,109][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:44:37,109][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:44:37,110][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:44:43,266][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:44:43,269][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:44:43,269][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:09,  1.17s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:31,  1.84it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:14,  3.81it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:09,  5.44it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.73it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.72it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.47it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  9.02it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.42it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.70it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.91it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03, 10.05it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.15it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.23it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.28it/s]Epoch 1/10:  50%|█████     | 30/60 [00:03<00:02, 10.31it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:04<00:01, 10.38it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:05<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.77it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.74it/s]
[2025-04-09 01:44:52,753][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-09 01:44:52,999][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1684, Metrics: {'mse': 0.16143709421157837, 'rmse': 0.4017923521068792, 'r2': -2.8612852096557617}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.04it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  8.14it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.16it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.65it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.92it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.08it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.33it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.35it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-09 01:44:59,344][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0795
[2025-04-09 01:44:59,614][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1183, Metrics: {'mse': 0.11410392820835114, 'rmse': 0.33779272965585144, 'r2': -1.729161024093628}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:14,  3.94it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.27it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.60it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.28it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.67it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.91it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.07it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.18it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-09 01:45:06,011][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0677
[2025-04-09 01:45:06,257][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0896, Metrics: {'mse': 0.08835393190383911, 'rmse': 0.2972438929630668, 'r2': -1.113267421722412}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:11,  4.98it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  8.10it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.14it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.63it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.91it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-09 01:45:12,535][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0482
[2025-04-09 01:45:12,784][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0725, Metrics: {'mse': 0.07121923565864563, 'rmse': 0.2668693231876711, 'r2': -0.7034361362457275}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:12,  4.75it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.93it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  9.03it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.56it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.86it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-09 01:45:19,086][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0443
[2025-04-09 01:45:19,357][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0683, Metrics: {'mse': 0.0667848214507103, 'rmse': 0.25842759421298317, 'r2': -0.5973730087280273}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:25,  2.29it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:10,  5.42it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:07,  7.19it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:01<00:06,  8.27it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:01<00:05,  8.96it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:05,  9.42it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04,  9.72it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04,  9.93it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.07it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:02<00:04, 10.18it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.24it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.29it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.32it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:06<00:00,  9.79it/s]
[2025-04-09 01:45:25,929][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0416
[2025-04-09 01:45:26,183][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0666, Metrics: {'mse': 0.06515901535749435, 'rmse': 0.255262639956368, 'r2': -0.5584867000579834}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.76it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.94it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  9.04it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.57it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.87it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-09 01:45:32,514][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0405
[2025-04-09 01:45:32,777][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0617, Metrics: {'mse': 0.060433611273765564, 'rmse': 0.24583248620506926, 'r2': -0.44546353816986084}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:11,  4.92it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  8.06it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  9.11it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.62it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:45:39,110][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0388
[2025-04-09 01:45:39,383][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0601, Metrics: {'mse': 0.05885271355509758, 'rmse': 0.242595782228582, 'r2': -0.4076511859893799}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:11,  5.08it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:06,  8.17it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:05,  9.18it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.66it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.93it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:45:45,709][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0404
[2025-04-09 01:45:45,972][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0583, Metrics: {'mse': 0.057052984833717346, 'rmse': 0.23885766647465462, 'r2': -0.3646049499511719}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:12,  4.67it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.88it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  9.00it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.55it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-09 01:45:52,346][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0381
[2025-04-09 01:45:52,610][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0593, Metrics: {'mse': 0.05808030068874359, 'rmse': 0.24099854914240373, 'r2': -0.389176607131958}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▂▂▁▁▁
wandb:     best_val_mse █▅▃▂▂▂▁▁▁
wandb:      best_val_r2 ▁▄▆▇▇▇███
wandb:    best_val_rmse █▅▄▂▂▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▂▂▁▁▁▁
wandb:          val_mse █▅▃▂▂▂▁▁▁▁
wandb:           val_r2 ▁▄▆▇▇▇████
wandb:         val_rmse █▅▄▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05829
wandb:     best_val_mse 0.05705
wandb:      best_val_r2 -0.3646
wandb:    best_val_rmse 0.23886
wandb:            epoch 10
wandb:   final_test_mse 0.0414
wandb:    final_test_r2 -0.01525
wandb:  final_test_rmse 0.20346
wandb:  final_train_mse 0.03592
wandb:   final_train_r2 0.00997
wandb: final_train_rmse 0.18954
wandb:    final_val_mse 0.05705
wandb:     final_val_r2 -0.3646
wandb:   final_val_rmse 0.23886
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03808
wandb:       train_time 66.72176
wandb:         val_loss 0.05929
wandb:          val_mse 0.05808
wandb:           val_r2 -0.38918
wandb:         val_rmse 0.241
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014431-egkitmls
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014431-egkitmls/logs
Control experiment for id (control=2) completed successfully
Running complexity control=3 for id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:46:13,821][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/id/control3
experiment_name: complexity_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:46:13,821][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:46:13,821][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:46:13,821][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:46:13,827][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-09 01:46:13,827][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:46:15,403][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:46:18,418][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:46:18,418][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:46:18,513][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
[2025-04-09 01:46:18,560][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:11:22 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  94%|█████████▍| 7000/7460 [00:00<00:00, 63024.31 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 48311.92 examples/s]
[2025-04-09 01:46:18,862][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-09 01:46:18,872][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:46:18,872][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-09 01:46:18,874][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:46:18,911][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:46:18,961][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:46:18,982][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-09 01:46:18,983][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:46:18,984][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-09 01:46:18,985][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:46:19,024][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:46:19,078][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:46:19,097][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-09 01:46:19,098][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:46:19,098][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-09 01:46:19,100][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-09 01:46:19,100][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:46:19,100][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:46:19,100][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:46:19,101][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:46:19,101][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:46:19,101][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-09 01:46:19,101][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-09 01:46:19,101][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-04-09 01:46:19,101][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:46:19,102][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:46:19,102][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:46:19,102][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:46:19,102][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:46:19,102][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-09 01:46:19,102][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-09 01:46:19,102][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:46:19,103][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:46:19,103][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-09 01:46:19,103][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-09 01:46:19,104][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:46:19,104][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:46:19,104][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:46:24,339][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:46:24,342][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:46:24,342][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:21,  1.39s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:36,  1.58it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:16,  3.37it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  4.95it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:02<00:08,  6.27it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.34it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:05,  8.16it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.78it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  9.24it/s]Epoch 1/10:  30%|███       | 18/60 [00:03<00:04,  9.58it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.82it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03,  9.99it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.12it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.20it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.26it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00, 10.79it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.47it/s]
[2025-04-09 01:46:33,654][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1400
[2025-04-09 01:46:33,910][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1262, Metrics: {'mse': 0.12160767614841461, 'rmse': 0.3487229217421972, 'r2': -1.908637285232544}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.11it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:06,  8.19it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:05,  9.19it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.67it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.93it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:46:40,254][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0748
[2025-04-09 01:46:40,512][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1440, Metrics: {'mse': 0.14090204238891602, 'rmse': 0.3753692080990608, 'r2': -2.370123863220215}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.12it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:06,  8.19it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:05,  9.20it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.67it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.20it/s]
[2025-04-09 01:46:46,395][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0707
[2025-04-09 01:46:46,652][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0752, Metrics: {'mse': 0.07198783755302429, 'rmse': 0.268305492960961, 'r2': -0.7218198776245117}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:12,  4.88it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  8.04it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.61it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:46:53,016][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0613
[2025-04-09 01:46:53,265][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0782, Metrics: {'mse': 0.07501426339149475, 'rmse': 0.27388731878547196, 'r2': -0.7942066192626953}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.08it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:06,  8.16it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:05,  9.18it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.66it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.93it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-09 01:46:59,157][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0520
[2025-04-09 01:46:59,420][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0738, Metrics: {'mse': 0.07148999720811844, 'rmse': 0.2673761343278761, 'r2': -0.7099124193191528}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.69it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.89it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  9.01it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.55it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.86it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:47:05,709][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0509
[2025-04-09 01:47:05,964][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0989, Metrics: {'mse': 0.09571950882673264, 'rmse': 0.3093856958987158, 'r2': -1.2894387245178223}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:11,  5.29it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:06,  8.30it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:05,  9.26it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.72it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.97it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.12it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.21it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.27it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:04<00:00, 10.41it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.21it/s]
[2025-04-09 01:47:11,844][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0515
[2025-04-09 01:47:12,109][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0878, Metrics: {'mse': 0.0858406201004982, 'rmse': 0.29298569948121733, 'r2': -1.0531535148620605}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:12,  4.90it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  8.05it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.61it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:47:18,005][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0462
[2025-04-09 01:47:18,271][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0596, Metrics: {'mse': 0.05970890074968338, 'rmse': 0.24435404795027107, 'r2': -0.4281296730041504}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:12,  4.89it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  8.04it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  9.10it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.61it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.17it/s]
[2025-04-09 01:47:24,562][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0467
[2025-04-09 01:47:24,814][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0617, Metrics: {'mse': 0.060638513416051865, 'rmse': 0.24624888510621093, 'r2': -0.450364351272583}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:12,  4.87it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  8.02it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  9.09it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.60it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.89it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.40it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.34it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.36it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-09 01:47:30,714][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0434
[2025-04-09 01:47:30,986][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0583, Metrics: {'mse': 0.0572117418050766, 'rmse': 0.23918976107909928, 'r2': -0.36840224266052246}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▁▁
wandb:     best_val_mse █▃▃▁▁
wandb:      best_val_r2 ▁▆▆██
wandb:    best_val_rmse █▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▃▂▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▂▄▃▁▁▁
wandb:          val_mse ▆█▂▂▂▄▃▁▁▁
wandb:           val_r2 ▃▁▇▇▇▅▆███
wandb:         val_rmse ▇█▂▃▂▅▄▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0583
wandb:     best_val_mse 0.05721
wandb:      best_val_r2 -0.3684
wandb:    best_val_rmse 0.23919
wandb:            epoch 10
wandb:   final_test_mse 0.04732
wandb:    final_test_r2 -0.16061
wandb:  final_test_rmse 0.21754
wandb:  final_train_mse 0.03576
wandb:   final_train_r2 0.01445
wandb: final_train_rmse 0.18911
wandb:    final_val_mse 0.05721
wandb:     final_val_r2 -0.3684
wandb:   final_val_rmse 0.23919
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04337
wandb:       train_time 64.84404
wandb:         val_loss 0.0583
wandb:          val_mse 0.05721
wandb:           val_r2 -0.3684
wandb:         val_rmse 0.23919
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014613-s86ywlo7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014613-s86ywlo7/logs
Control experiment for id (control=3) completed successfully
Running complexity control=1 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:47:54,226][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ja/control1
experiment_name: complexity_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:47:54,226][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:47:54,226][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:47:54,226][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:47:54,232][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-04-09 01:47:54,232][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:47:55,766][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:47:58,818][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:47:58,819][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:47:58,909][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
[2025-04-09 01:47:58,954][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 13:35:44 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 73282.34 examples/s]
[2025-04-09 01:47:59,198][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-09 01:47:59,209][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:47:59,210][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-09 01:47:59,212][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:47:59,251][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:47:59,299][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:47:59,321][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-09 01:47:59,323][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:47:59,323][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-09 01:47:59,324][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:47:59,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:47:59,407][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:47:59,423][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-09 01:47:59,425][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:47:59,425][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-09 01:47:59,426][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-09 01:47:59,427][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:47:59,427][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:47:59,428][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:47:59,428][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:47:59,428][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:47:59,428][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-09 01:47:59,428][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-09 01:47:59,428][src.data.datasets][INFO] - Sample label: 0.5826417803764343
[2025-04-09 01:47:59,429][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:47:59,429][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:47:59,429][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:47:59,429][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:47:59,429][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:47:59,429][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-09 01:47:59,429][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:47:59,430][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:47:59,430][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-09 01:47:59,430][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-09 01:47:59,431][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-09 01:47:59,431][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-09 01:47:59,431][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:47:59,431][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:47:59,432][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:48:04,900][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:48:04,903][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:48:04,903][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:33,  1.26s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.50it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.11it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.52it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.71it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.66it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.40it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.96it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.37it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.67it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.15it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.95it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.96it/s]
[2025-04-09 01:48:15,761][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1530
[2025-04-09 01:48:15,980][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1774, Metrics: {'mse': 0.17585812509059906, 'rmse': 0.41935441465495393, 'r2': -1.8661224842071533}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-09 01:48:23,745][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0850
[2025-04-09 01:48:23,935][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1109, Metrics: {'mse': 0.11006293445825577, 'rmse': 0.3317573427344989, 'r2': -0.7937973737716675}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:48:31,900][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0692
[2025-04-09 01:48:32,116][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0755, Metrics: {'mse': 0.07433567941188812, 'rmse': 0.27264570308715325, 'r2': -0.211517333984375}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.06it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:48:39,853][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0546
[2025-04-09 01:48:40,080][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1047, Metrics: {'mse': 0.10378239303827286, 'rmse': 0.32215274799118643, 'r2': -0.6914377212524414}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:19,  3.85it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.21it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  8.56it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.25it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.66it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.91it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-09 01:48:47,463][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0583
[2025-04-09 01:48:47,687][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0799, Metrics: {'mse': 0.08001488447189331, 'rmse': 0.28286902352836957, 'r2': -0.30407655239105225}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.74it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.34it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:48:55,020][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0541
[2025-04-09 01:48:55,337][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0764, Metrics: {'mse': 0.07620631903409958, 'rmse': 0.27605492032220613, 'r2': -0.2420048713684082}
[2025-04-09 01:48:55,338][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▄▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▃▁▁
wandb:          val_mse █▃▁▃▁▁
wandb:           val_r2 ▁▆█▆██
wandb:         val_rmse █▄▁▃▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07555
wandb:     best_val_mse 0.07434
wandb:      best_val_r2 -0.21152
wandb:    best_val_rmse 0.27265
wandb:            epoch 6
wandb:   final_test_mse 0.05878
wandb:    final_test_r2 -0.12914
wandb:  final_test_rmse 0.24245
wandb:  final_train_mse 0.05309
wandb:   final_train_r2 -0.32501
wandb: final_train_rmse 0.23041
wandb:    final_val_mse 0.07434
wandb:     final_val_r2 -0.21152
wandb:   final_val_rmse 0.27265
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0541
wandb:       train_time 47.95263
wandb:         val_loss 0.07644
wandb:          val_mse 0.07621
wandb:           val_r2 -0.242
wandb:         val_rmse 0.27605
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014754-39w9j15z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014754-39w9j15z/logs
Control experiment for ja (control=1) completed successfully
Running complexity control=2 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:49:17,571][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ja/control2
experiment_name: complexity_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:49:17,571][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:49:17,572][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:49:17,572][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:49:17,577][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-04-09 01:49:17,577][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:49:19,217][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:49:22,103][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:49:22,104][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:49:22,211][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:44:36 2025).
[2025-04-09 01:49:22,253][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:44:36 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 60390.23 examples/s]
[2025-04-09 01:49:22,496][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-09 01:49:22,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:49:22,508][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-09 01:49:22,509][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:49:22,538][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:49:22,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:49:22,589][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-09 01:49:22,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:49:22,590][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-09 01:49:22,592][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:49:22,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:49:22,654][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:49:22,669][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-09 01:49:22,671][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:49:22,671][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-09 01:49:22,672][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-09 01:49:22,673][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:49:22,673][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:49:22,673][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:49:22,673][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:49:22,673][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:49:22,673][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Sample label: 0.5349239110946655
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:49:22,674][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:49:22,674][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:49:22,675][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:49:22,675][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:49:22,675][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:49:22,676][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-09 01:49:22,676][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-09 01:49:22,676][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-09 01:49:22,676][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-09 01:49:22,676][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:49:22,676][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:49:22,677][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:49:28,352][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:49:28,355][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:49:28,355][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:21,  1.10s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:37,  1.95it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:17,  3.98it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.62it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.90it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.86it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.58it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.11it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.48it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.75it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.94it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.08it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.18it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.29it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:04,  9.66it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03,  9.88it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.03it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.14it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.22it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.27it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.30it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.33it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.35it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.07it/s]
[2025-04-09 01:49:39,036][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1594
[2025-04-09 01:49:39,222][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1038, Metrics: {'mse': 0.10447928309440613, 'rmse': 0.3232325526527397, 'r2': -0.7027955055236816}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.93it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s]
[2025-04-09 01:49:46,995][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0873
[2025-04-09 01:49:47,225][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1057, Metrics: {'mse': 0.10702953487634659, 'rmse': 0.3271536869368074, 'r2': -0.7443592548370361}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.32it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.27it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:06,  9.72it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.97it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.27it/s]
[2025-04-09 01:49:54,533][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0706
[2025-04-09 01:49:54,731][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0629, Metrics: {'mse': 0.062481433153152466, 'rmse': 0.24996286354807282, 'r2': -0.018317699432373047}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:50:02,712][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0557
[2025-04-09 01:50:02,932][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0737, Metrics: {'mse': 0.07354927062988281, 'rmse': 0.2711996877392797, 'r2': -0.19870054721832275}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-09 01:50:10,269][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0493
[2025-04-09 01:50:10,500][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0711, Metrics: {'mse': 0.07109692692756653, 'rmse': 0.2666400699961777, 'r2': -0.15873241424560547}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.22it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.68it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:50:17,823][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0481
[2025-04-09 01:50:18,048][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0666, Metrics: {'mse': 0.06660663336515427, 'rmse': 0.2580826095752177, 'r2': -0.08554995059967041}
[2025-04-09 01:50:18,048][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▁▃▂▂
wandb:          val_mse ██▁▃▂▂
wandb:           val_r2 ▁▁█▆▇▇
wandb:         val_rmse ██▁▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06287
wandb:     best_val_mse 0.06248
wandb:      best_val_r2 -0.01832
wandb:    best_val_rmse 0.24996
wandb:            epoch 6
wandb:   final_test_mse 0.06219
wandb:    final_test_r2 -0.1946
wandb:  final_test_rmse 0.24938
wandb:  final_train_mse 0.04441
wandb:   final_train_r2 -0.10839
wandb: final_train_rmse 0.21073
wandb:    final_val_mse 0.06248
wandb:     final_val_r2 -0.01832
wandb:   final_val_rmse 0.24996
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04807
wandb:       train_time 47.28286
wandb:         val_loss 0.06657
wandb:          val_mse 0.06661
wandb:           val_r2 -0.08555
wandb:         val_rmse 0.25808
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014917-3yn1qneq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_014917-3yn1qneq/logs
Control experiment for ja (control=2) completed successfully
Running complexity control=3 for ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:50:41,063][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ja/control3
experiment_name: complexity_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:50:41,064][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:50:41,064][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:50:41,064][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:50:41,069][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-04-09 01:50:41,069][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:50:42,523][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:50:45,386][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:50:45,387][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:50:45,523][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:46:18 2025).
[2025-04-09 01:50:45,566][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:46:18 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter:  13%|█▎        | 1000/7460 [00:00<00:00, 7703.87 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 24886.47 examples/s]
[2025-04-09 01:50:46,047][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-09 01:50:46,058][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:50:46,058][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-09 01:50:46,060][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:50:46,095][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:50:46,143][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:50:46,162][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-09 01:50:46,163][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:50:46,163][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-09 01:50:46,165][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:50:46,198][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:50:46,248][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:50:46,267][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-09 01:50:46,268][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:50:46,268][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-09 01:50:46,270][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-09 01:50:46,271][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:50:46,272][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:50:46,272][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:50:46,272][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:50:46,272][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:50:46,272][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-09 01:50:46,272][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-09 01:50:46,272][src.data.datasets][INFO] - Sample label: 0.2807745635509491
[2025-04-09 01:50:46,273][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:50:46,273][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:50:46,273][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:50:46,273][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:50:46,273][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:50:46,273][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-09 01:50:46,273][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:50:46,274][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:50:46,274][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-09 01:50:46,274][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-09 01:50:46,275][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-09 01:50:46,275][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-09 01:50:46,275][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:50:46,275][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:50:46,276][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:50:52,067][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:50:52,070][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:50:52,070][__main__][INFO] - Successfully created model for ja
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:42,  1.38s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:30,  2.33it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:18,  3.87it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.27it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.48it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.46it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.24it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.83it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.28it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.42it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.95it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-09 01:51:03,099][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1547
[2025-04-09 01:51:03,333][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1602, Metrics: {'mse': 0.1598118543624878, 'rmse': 0.39976474877418566, 'r2': -1.6046016216278076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.16it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.94it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.10it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:51:11,158][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0883
[2025-04-09 01:51:11,357][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1914, Metrics: {'mse': 0.19101502001285553, 'rmse': 0.4370526513051437, 'r2': -2.1131484508514404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.37it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.31it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:06,  9.75it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.99it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.22it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.29it/s]
[2025-04-09 01:51:18,649][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0874
[2025-04-09 01:51:18,858][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1117, Metrics: {'mse': 0.11109200865030289, 'rmse': 0.3333046784104641, 'r2': -0.8105692863464355}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-09 01:51:26,840][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0607
[2025-04-09 01:51:27,060][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0607, Metrics: {'mse': 0.061474353075027466, 'rmse': 0.2479402207690948, 'r2': -0.0019044876098632812}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:51:34,786][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0586
[2025-04-09 01:51:34,995][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0717, Metrics: {'mse': 0.07178530842065811, 'rmse': 0.2679278044934085, 'r2': -0.16995155811309814}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.85it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:51:42,323][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0559
[2025-04-09 01:51:42,554][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0681, Metrics: {'mse': 0.06818072497844696, 'rmse': 0.26111439060007197, 'r2': -0.11120426654815674}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.87it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.42it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.05it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:51:49,901][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0503
[2025-04-09 01:51:50,190][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1047, Metrics: {'mse': 0.10431258380413055, 'rmse': 0.3229745869323631, 'r2': -0.7000787258148193}
[2025-04-09 01:51:50,191][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁
wandb:     best_val_mse █▅▁
wandb:      best_val_r2 ▁▄█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▄▁▂▁▃
wandb:          val_mse ▆█▄▁▂▁▃
wandb:           val_r2 ▃▁▅█▇█▆
wandb:         val_rmse ▇█▄▁▂▁▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06074
wandb:     best_val_mse 0.06147
wandb:      best_val_r2 -0.0019
wandb:    best_val_rmse 0.24794
wandb:            epoch 7
wandb:   final_test_mse 0.05565
wandb:    final_test_r2 -0.06891
wandb:  final_test_rmse 0.23589
wandb:  final_train_mse 0.05768
wandb:   final_train_r2 -0.43963
wandb: final_train_rmse 0.24017
wandb:    final_val_mse 0.06147
wandb:     final_val_r2 -0.0019
wandb:   final_val_rmse 0.24794
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05035
wandb:       train_time 55.59346
wandb:         val_loss 0.10467
wandb:          val_mse 0.10431
wandb:           val_r2 -0.70008
wandb:         val_rmse 0.32297
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015041-biginmax
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015041-biginmax/logs
Control experiment for ja (control=3) completed successfully
Running complexity control=1 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:52:12,809][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ko/control1
experiment_name: complexity_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:52:12,810][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:52:12,810][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:52:12,810][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:52:12,815][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-04-09 01:52:12,816][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:52:14,471][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:52:17,281][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:52:17,282][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:52:17,385][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:47:59 2025).
[2025-04-09 01:52:17,431][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:47:59 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 75347.75 examples/s]
[2025-04-09 01:52:17,694][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-09 01:52:17,701][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:52:17,701][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-09 01:52:17,703][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:52:17,736][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:52:17,782][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:52:17,801][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-09 01:52:17,802][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:52:17,803][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-09 01:52:17,804][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:52:17,836][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:52:17,881][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:52:17,897][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-09 01:52:17,899][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:52:17,899][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-09 01:52:17,901][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-09 01:52:17,901][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:52:17,901][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:52:17,901][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:52:17,901][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:52:17,901][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:52:17,902][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Sample label: 0.2160857915878296
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:52:17,902][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:52:17,903][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:52:17,903][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-09 01:52:17,903][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-09 01:52:17,903][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-09 01:52:17,903][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:52:17,903][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:52:17,903][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:52:17,904][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:52:17,904][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:52:17,904][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-09 01:52:17,904][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-09 01:52:17,904][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-09 01:52:17,904][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-09 01:52:17,904][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:52:17,905][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:52:17,905][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:52:23,400][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:52:23,403][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:52:23,403][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:53,  1.16s/it]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:16,  2.68it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:09,  4.34it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:06,  5.76it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:01<00:05,  6.92it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:04,  7.84it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:03,  8.54it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  9.06it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.45it/s]Epoch 1/10:  40%|████      | 19/47 [00:02<00:02,  9.72it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.92it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02, 10.06it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.16it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.23it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.28it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.10it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.42it/s]
[2025-04-09 01:52:31,366][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1660
[2025-04-09 01:52:31,617][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2270, Metrics: {'mse': 0.23508699238300323, 'rmse': 0.48485770323158034, 'r2': -3.9900760650634766}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.17it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.23it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.21it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.68it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.94it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.10it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.31it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s]
[2025-04-09 01:52:36,682][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0979
[2025-04-09 01:52:36,928][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1295, Metrics: {'mse': 0.13444162905216217, 'rmse': 0.3666628274752735, 'r2': -1.8537261486053467}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.63it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.84it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.53it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-09 01:52:42,203][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0742
[2025-04-09 01:52:42,461][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1024, Metrics: {'mse': 0.10668165236711502, 'rmse': 0.326621573640069, 'r2': -1.2644789218902588}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.71it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.90it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.02it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.56it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.86it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:52:47,488][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0483
[2025-04-09 01:52:47,761][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0596, Metrics: {'mse': 0.062543123960495, 'rmse': 0.25008623304871264, 'r2': -0.3275719881057739}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  4.66it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:52:52,826][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0376
[2025-04-09 01:52:53,104][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0712, Metrics: {'mse': 0.07466952502727509, 'rmse': 0.27325725063989625, 'r2': -0.5849733352661133}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  4.89it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  8.04it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.10it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.61it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.07it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:52:57,737][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-09 01:52:58,024][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0584, Metrics: {'mse': 0.06114935129880905, 'rmse': 0.24728394872860035, 'r2': -0.2979872226715088}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.49it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.74it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.91it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:53:03,072][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0259
[2025-04-09 01:53:03,394][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0581, Metrics: {'mse': 0.06089998781681061, 'rmse': 0.2467792289006727, 'r2': -0.292694091796875}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.53it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.77it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.93it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.50it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-09 01:53:08,468][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0254
[2025-04-09 01:53:08,735][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0581, Metrics: {'mse': 0.06096866354346275, 'rmse': 0.24691833375321232, 'r2': -0.29415178298950195}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.44it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.70it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-09 01:53:13,809][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0257
[2025-04-09 01:53:14,088][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0577, Metrics: {'mse': 0.06048855558037758, 'rmse': 0.24594421233356475, 'r2': -0.28396081924438477}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.48it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.73it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.90it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-09 01:53:19,158][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0248
[2025-04-09 01:53:19,435][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0551, Metrics: {'mse': 0.05772566795349121, 'rmse': 0.24026166559293477, 'r2': -0.22531437873840332}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▁▁▁▁▁▁
wandb:     best_val_mse █▄▃▁▁▁▁▁▁
wandb:      best_val_r2 ▁▅▆██████
wandb:    best_val_rmse █▅▃▁▁▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▃▁▂▁▁▁▁▁
wandb:          val_mse █▄▃▁▂▁▁▁▁▁
wandb:           val_r2 ▁▅▆█▇█████
wandb:         val_rmse █▅▃▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05512
wandb:     best_val_mse 0.05773
wandb:      best_val_r2 -0.22531
wandb:    best_val_rmse 0.24026
wandb:            epoch 10
wandb:   final_test_mse 0.03688
wandb:    final_test_r2 -0.14528
wandb:  final_test_rmse 0.19205
wandb:  final_train_mse 0.02208
wandb:   final_train_r2 0.00807
wandb: final_train_rmse 0.1486
wandb:    final_val_mse 0.05773
wandb:     final_val_r2 -0.22531
wandb:   final_val_rmse 0.24026
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02477
wandb:       train_time 54.08813
wandb:         val_loss 0.05512
wandb:          val_mse 0.05773
wandb:           val_r2 -0.22531
wandb:         val_rmse 0.24026
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015212-22i4hu6t
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015212-22i4hu6t/logs
Control experiment for ko (control=1) completed successfully
Running complexity control=2 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:53:41,994][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ko/control2
experiment_name: complexity_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:53:41,994][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:53:41,994][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:53:41,994][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:53:42,000][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-04-09 01:53:42,000][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:53:43,621][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:53:46,686][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:53:46,687][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:53:46,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:49:22 2025).
[2025-04-09 01:53:46,823][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:49:22 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 56851.56 examples/s]
[2025-04-09 01:53:47,103][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-09 01:53:47,110][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:53:47,111][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-09 01:53:47,112][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:53:47,147][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:53:47,196][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:53:47,215][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-09 01:53:47,216][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:53:47,216][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-09 01:53:47,218][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:53:47,255][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:53:47,306][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:53:47,326][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-09 01:53:47,327][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:53:47,328][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-09 01:53:47,329][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-09 01:53:47,330][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:53:47,330][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:53:47,330][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:53:47,331][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:53:47,331][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:53:47,331][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-09 01:53:47,331][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-09 01:53:47,331][src.data.datasets][INFO] - Sample label: 0.4206434190273285
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:53:47,332][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:53:47,332][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-09 01:53:47,332][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:53:47,333][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:53:47,333][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-09 01:53:47,333][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-09 01:53:47,334][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-09 01:53:47,334][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:53:47,334][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:53:47,334][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:53:52,997][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:53:53,000][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:53:53,000][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:53,  1.17s/it]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:16,  2.66it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:09,  4.31it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:06,  5.73it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:01<00:05,  6.90it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:04,  7.82it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:03,  8.52it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  9.05it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.44it/s]Epoch 1/10:  40%|████      | 19/47 [00:02<00:02,  9.71it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.85it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02, 10.01it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.13it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.21it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.27it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.10it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.41it/s]
[2025-04-09 01:54:01,316][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1751
[2025-04-09 01:54:01,555][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2316, Metrics: {'mse': 0.24002525210380554, 'rmse': 0.489923720699259, 'r2': -4.094897747039795}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.24it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.27it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.24it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.70it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.95it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.11it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.23it/s]
[2025-04-09 01:54:06,614][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1035
[2025-04-09 01:54:06,890][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0931, Metrics: {'mse': 0.09557691961526871, 'rmse': 0.3091551707723303, 'r2': -1.028764247894287}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.65it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:54:12,131][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0666
[2025-04-09 01:54:12,400][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0446, Metrics: {'mse': 0.04665413498878479, 'rmse': 0.21599568280126524, 'r2': 0.009695649147033691}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.64it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.85it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.98it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.53it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-09 01:54:17,457][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0449
[2025-04-09 01:54:17,729][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0407, Metrics: {'mse': 0.04226531460881233, 'rmse': 0.20558529764750283, 'r2': 0.10285496711730957}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.50it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.74it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.91it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.49it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-09 01:54:22,792][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0372
[2025-04-09 01:54:23,056][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0428, Metrics: {'mse': 0.04330994188785553, 'rmse': 0.20811040792775246, 'r2': 0.0806812047958374}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  4.81it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.98it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.06it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.59it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.88it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-09 01:54:27,702][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0318
[2025-04-09 01:54:27,995][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0407, Metrics: {'mse': 0.04145064949989319, 'rmse': 0.20359432580475614, 'r2': 0.1201474666595459}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.87it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  8.02it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  9.09it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.60it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s]
[2025-04-09 01:54:32,614][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0327
[2025-04-09 01:54:32,879][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0442, Metrics: {'mse': 0.046118225902318954, 'rmse': 0.21475154458657325, 'r2': 0.021071135997772217}
[2025-04-09 01:54:32,880][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁
wandb:     best_val_mse █▃▁▁
wandb:      best_val_r2 ▁▆██
wandb:    best_val_rmse █▄▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁
wandb:           val_r2 ▁▆█████
wandb:         val_rmse █▄▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04068
wandb:     best_val_mse 0.04227
wandb:      best_val_r2 0.10285
wandb:    best_val_rmse 0.20559
wandb:            epoch 7
wandb:   final_test_mse 0.02789
wandb:    final_test_r2 0.13401
wandb:  final_test_rmse 0.167
wandb:  final_train_mse 0.02915
wandb:   final_train_r2 -0.30953
wandb: final_train_rmse 0.17074
wandb:    final_val_mse 0.04227
wandb:     final_val_r2 0.10285
wandb:   final_val_rmse 0.20559
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0327
wandb:       train_time 37.15531
wandb:         val_loss 0.04417
wandb:          val_mse 0.04612
wandb:           val_r2 0.02107
wandb:         val_rmse 0.21475
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015342-gjwj5jfd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015342-gjwj5jfd/logs
Control experiment for ko (control=2) completed successfully
Running complexity control=3 for ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:54:54,274][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ko/control3
experiment_name: complexity_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:54:54,274][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:54:54,274][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:54:54,274][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:54:54,280][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-04-09 01:54:54,280][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:54:56,001][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:54:58,888][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:54:58,889][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:54:58,972][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:50:46 2025).
[2025-04-09 01:54:59,015][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:50:46 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 57487.57 examples/s]
[2025-04-09 01:54:59,299][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-09 01:54:59,307][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:54:59,307][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-09 01:54:59,309][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:54:59,346][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:54:59,395][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:54:59,414][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-09 01:54:59,416][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:54:59,416][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-09 01:54:59,418][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:54:59,452][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:54:59,504][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:54:59,523][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-09 01:54:59,525][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:54:59,525][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-09 01:54:59,527][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-09 01:54:59,528][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:54:59,528][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:54:59,528][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:54:59,528][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:54:59,529][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:54:59,529][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-09 01:54:59,529][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-09 01:54:59,529][src.data.datasets][INFO] - Sample label: 0.2868632674217224
[2025-04-09 01:54:59,529][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:54:59,529][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:54:59,529][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:54:59,530][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:54:59,530][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:54:59,530][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-09 01:54:59,530][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-09 01:54:59,530][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-09 01:54:59,530][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:54:59,530][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:54:59,531][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:54:59,531][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-09 01:54:59,531][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:54:59,532][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:54:59,532][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:55:05,151][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:55:05,154][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:55:05,154][__main__][INFO] - Successfully created model for ko
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:55,  1.22s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:25,  1.78it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.70it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.32it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.63it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.64it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.40it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.97it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.38it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:02,  9.68it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.89it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02, 10.04it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.15it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.22it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.27it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.37it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.33it/s]
[2025-04-09 01:55:13,361][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1675
[2025-04-09 01:55:13,587][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2094, Metrics: {'mse': 0.21731001138687134, 'rmse': 0.4661652189802145, 'r2': -3.6127328872680664}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:11,  4.10it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.42it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.70it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.35it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:01<00:03,  9.72it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03,  9.95it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-09 01:55:18,711][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0828
[2025-04-09 01:55:18,956][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0532, Metrics: {'mse': 0.05528143420815468, 'rmse': 0.2351200421234963, 'r2': -0.17343175411224365}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.96it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  8.08it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  9.12it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.62it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.90it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.07it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.19it/s]
[2025-04-09 01:55:24,205][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0516
[2025-04-09 01:55:24,522][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0578, Metrics: {'mse': 0.061145901679992676, 'rmse': 0.2472769736145941, 'r2': -0.29791390895843506}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  5.02it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  8.13it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  9.16it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.64it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.92it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.08it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.20it/s]
[2025-04-09 01:55:29,133][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0365
[2025-04-09 01:55:29,410][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0503, Metrics: {'mse': 0.05279560759663582, 'rmse': 0.22977294792171646, 'r2': -0.12066638469696045}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  5.03it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  8.13it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  9.16it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.65it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.91it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.08it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.18it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.20it/s]
[2025-04-09 01:55:34,424][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0352
[2025-04-09 01:55:34,705][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0494, Metrics: {'mse': 0.05096913129091263, 'rmse': 0.22576344099723636, 'r2': -0.08189666271209717}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  4.89it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  8.03it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.60it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-09 01:55:39,751][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0348
[2025-04-09 01:55:40,036][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0510, Metrics: {'mse': 0.05220767483115196, 'rmse': 0.22848998847028718, 'r2': -0.10818660259246826}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:15,  2.88it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:07,  6.19it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:05,  7.82it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  8.73it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:01<00:04,  9.29it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.65it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03,  9.89it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.05it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.16it/s]Epoch 7/10:  40%|████      | 19/47 [00:02<00:02, 10.23it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.28it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.31it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00,  9.86it/s]
[2025-04-09 01:55:44,805][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0319
[2025-04-09 01:55:45,091][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0491, Metrics: {'mse': 0.05043980851769447, 'rmse': 0.22458808632181376, 'r2': -0.07066094875335693}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.77it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.95it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  9.05it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.57it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-09 01:55:50,133][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0297
[2025-04-09 01:55:50,439][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0512, Metrics: {'mse': 0.05414120480418205, 'rmse': 0.23268262677772497, 'r2': -0.14922869205474854}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:09,  4.60it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-09 01:55:55,071][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0270
[2025-04-09 01:55:55,365][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0495, Metrics: {'mse': 0.0516187958419323, 'rmse': 0.22719770210530804, 'r2': -0.09568679332733154}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:09,  4.74it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.93it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  9.04it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.57it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-09 01:55:59,991][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0255
[2025-04-09 01:56:00,263][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0576, Metrics: {'mse': 0.06063530221581459, 'rmse': 0.2462423647868388, 'r2': -0.2870757579803467}
[2025-04-09 01:56:00,264][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁▁▁
wandb:     best_val_mse █▁▁▁▁
wandb:      best_val_r2 ▁████
wandb:    best_val_rmse █▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁█████████
wandb:         val_rmse █▁▂▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04907
wandb:     best_val_mse 0.05044
wandb:      best_val_r2 -0.07066
wandb:    best_val_rmse 0.22459
wandb:            epoch 10
wandb:   final_test_mse 0.03297
wandb:    final_test_r2 -0.02382
wandb:  final_test_rmse 0.18159
wandb:  final_train_mse 0.03687
wandb:   final_train_r2 -0.65623
wandb: final_train_rmse 0.19202
wandb:    final_val_mse 0.05044
wandb:     final_val_r2 -0.07066
wandb:   final_val_rmse 0.22459
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02554
wandb:       train_time 52.54569
wandb:         val_loss 0.05763
wandb:          val_mse 0.06064
wandb:           val_r2 -0.28708
wandb:         val_rmse 0.24624
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015454-s3bgwh7b
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015454-s3bgwh7b/logs
Control experiment for ko (control=3) completed successfully
Running complexity control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:56:21,508][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ru/control1
experiment_name: complexity_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:56:21,509][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:56:21,509][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:56:21,509][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:56:21,514][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-04-09 01:56:21,515][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:56:22,924][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:56:25,765][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:56:25,765][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:56:25,845][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:52:17 2025).
[2025-04-09 01:56:25,894][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:52:17 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 78949.71 examples/s]
[2025-04-09 01:56:26,136][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-09 01:56:26,147][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:56:26,148][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-09 01:56:26,149][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:56:26,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:56:26,220][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:56:26,237][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-09 01:56:26,238][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:56:26,239][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-09 01:56:26,240][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:56:26,267][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:56:26,307][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:56:26,324][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-09 01:56:26,325][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:56:26,326][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-09 01:56:26,327][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-09 01:56:26,327][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:56:26,327][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:56:26,328][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:56:26,328][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:56:26,328][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:56:26,328][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-09 01:56:26,328][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-09 01:56:26,328][src.data.datasets][INFO] - Sample label: 0.639226496219635
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:56:26,329][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:56:26,329][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-09 01:56:26,329][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-09 01:56:26,330][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:56:26,330][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:56:26,330][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:56:26,330][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:56:26,330][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:56:26,330][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-09 01:56:26,330][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-09 01:56:26,331][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-09 01:56:26,331][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-09 01:56:26,331][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:56:26,331][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:56:26,331][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:56:31,788][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:56:31,791][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:56:31,791][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:27,  1.18s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:39,  1.83it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.78it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.41it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.70it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.71it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.46it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.01it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.42it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.71it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.91it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.15it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.23it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.42it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.02it/s]
[2025-04-09 01:56:42,538][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1437
[2025-04-09 01:56:42,816][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1207, Metrics: {'mse': 0.12614041566848755, 'rmse': 0.3551625200784671, 'r2': -1.7115800380706787}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:56:50,602][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0650
[2025-04-09 01:56:50,864][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1020, Metrics: {'mse': 0.1069374531507492, 'rmse': 0.3270129250515172, 'r2': -1.298783302307129}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.84it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:08,  8.45it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:07,  9.32it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:06,  9.74it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  80%|████████  | 60/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-09 01:56:58,850][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0467
[2025-04-09 01:56:59,137][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0915, Metrics: {'mse': 0.09676602482795715, 'rmse': 0.31107237876088767, 'r2': -1.0801329612731934}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.42it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:57:06,883][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0531
[2025-04-09 01:57:07,163][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1024, Metrics: {'mse': 0.10600091516971588, 'rmse': 0.3255778173796794, 'r2': -1.2786509990692139}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.26it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.70it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.96it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 01:57:14,486][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0309
[2025-04-09 01:57:14,796][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0755, Metrics: {'mse': 0.07587692141532898, 'rmse': 0.27545765811704886, 'r2': -0.6310899257659912}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.55it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:57:22,572][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0265
[2025-04-09 01:57:22,848][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0676, Metrics: {'mse': 0.06673749536275864, 'rmse': 0.25833601251617755, 'r2': -0.43462395668029785}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.58it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-09 01:57:30,642][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0246
[2025-04-09 01:57:30,922][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0608, Metrics: {'mse': 0.05929015576839447, 'rmse': 0.2434956996917902, 'r2': -0.2745321989059448}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:57:38,708][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0268
[2025-04-09 01:57:38,993][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0580, Metrics: {'mse': 0.0607198029756546, 'rmse': 0.24641388551714086, 'r2': -0.3052644729614258}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.15it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.08it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.42it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-09 01:57:46,826][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0247
[2025-04-09 01:57:47,113][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0548, Metrics: {'mse': 0.05491937696933746, 'rmse': 0.2343488360742111, 'r2': -0.18057548999786377}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.55it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:57:54,887][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0238
[2025-04-09 01:57:55,176][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0617, Metrics: {'mse': 0.06042063236236572, 'rmse': 0.24580608691073075, 'r2': -0.2988334894180298}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▂▂▁▁
wandb:     best_val_mse █▆▅▃▂▁▂▁
wandb:      best_val_r2 ▁▃▄▆▇█▇█
wandb:    best_val_rmse █▆▅▃▂▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▃▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▆▃▂▂▁▁▂
wandb:          val_mse █▆▅▆▃▂▁▂▁▂
wandb:           val_r2 ▁▃▄▃▆▇█▇█▇
wandb:         val_rmse █▆▅▆▃▂▂▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05483
wandb:     best_val_mse 0.05492
wandb:      best_val_r2 -0.18058
wandb:    best_val_rmse 0.23435
wandb:            epoch 10
wandb:   final_test_mse 0.05829
wandb:    final_test_r2 -0.47428
wandb:  final_test_rmse 0.24142
wandb:  final_train_mse 0.02035
wandb:   final_train_r2 -0.02077
wandb: final_train_rmse 0.14266
wandb:    final_val_mse 0.05492
wandb:     final_val_r2 -0.18058
wandb:   final_val_rmse 0.23435
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02383
wandb:       train_time 80.95443
wandb:         val_loss 0.06169
wandb:          val_mse 0.06042
wandb:           val_r2 -0.29883
wandb:         val_rmse 0.24581
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015621-dxh3d5ah
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015621-dxh3d5ah/logs
Control experiment for ru (control=1) completed successfully
Running complexity control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:58:17,832][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ru/control2
experiment_name: complexity_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:58:17,832][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:58:17,832][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:58:17,832][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:58:17,838][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-04-09 01:58:17,838][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:58:19,350][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:58:22,220][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:58:22,221][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:58:22,378][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:53:47 2025).
[2025-04-09 01:58:22,425][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:53:47 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 56122.26 examples/s]
[2025-04-09 01:58:22,700][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-09 01:58:22,711][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:58:22,712][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-09 01:58:22,713][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:58:22,746][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:58:22,793][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:58:22,812][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-09 01:58:22,813][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:58:22,814][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-09 01:58:22,815][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:58:22,850][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:58:22,894][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:58:22,911][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-09 01:58:22,912][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:58:22,913][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-09 01:58:22,914][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-09 01:58:22,915][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:58:22,915][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:58:22,915][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:58:22,915][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:58:22,916][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:58:22,916][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-09 01:58:22,916][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-09 01:58:22,916][src.data.datasets][INFO] - Sample label: 0.28674033284187317
[2025-04-09 01:58:22,916][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:58:22,916][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:58:22,916][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:58:22,917][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:58:22,917][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:58:22,917][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-09 01:58:22,917][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-09 01:58:22,917][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-09 01:58:22,917][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:58:22,917][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:58:22,918][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:58:22,918][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-09 01:58:22,918][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:58:22,919][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:58:22,919][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 01:58:28,453][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 01:58:28,456][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 01:58:28,457][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:18,  1.07s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:36,  2.00it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:17,  4.05it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.69it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.96it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.92it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.63it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.14it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.51it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.78it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.96it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.10it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.19it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.30it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.34it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.36it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.40it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.41it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.42it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.42it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.42it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.42it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.16it/s]
[2025-04-09 01:58:39,018][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1351
[2025-04-09 01:58:39,264][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0632, Metrics: {'mse': 0.062113769352436066, 'rmse': 0.2492263416102641, 'r2': -0.3352299928665161}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.54it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.45it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.36it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:06,  9.77it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06, 10.00it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.23it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.28it/s]
[2025-04-09 01:58:47,030][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0545
[2025-04-09 01:58:47,276][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0496, Metrics: {'mse': 0.04574199393391609, 'rmse': 0.21387378037972793, 'r2': 0.016706228256225586}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:58:55,245][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0357
[2025-04-09 01:58:55,521][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0693, Metrics: {'mse': 0.06941376626491547, 'rmse': 0.2634649241643287, 'r2': -0.492154598236084}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 01:59:02,873][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0329
[2025-04-09 01:59:03,147][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0487, Metrics: {'mse': 0.04444827139377594, 'rmse': 0.21082758688979947, 'r2': 0.044516801834106445}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 01:59:10,890][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0280
[2025-04-09 01:59:11,180][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0590, Metrics: {'mse': 0.05572126805782318, 'rmse': 0.23605352795038498, 'r2': -0.1978135108947754}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.25it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.70it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.96it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:59:18,507][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0281
[2025-04-09 01:59:18,787][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0548, Metrics: {'mse': 0.05177903175354004, 'rmse': 0.22755006427935817, 'r2': -0.11306905746459961}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.42it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.83it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-09 01:59:26,116][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0274
[2025-04-09 01:59:26,411][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0552, Metrics: {'mse': 0.05206696689128876, 'rmse': 0.22818187239850748, 'r2': -0.11925876140594482}
[2025-04-09 01:59:26,412][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▁█▁▄▃▃
wandb:          val_mse ▆▁█▁▄▃▃
wandb:           val_r2 ▃█▁█▅▆▆
wandb:         val_rmse ▆▁█▁▄▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04872
wandb:     best_val_mse 0.04445
wandb:      best_val_r2 0.04452
wandb:    best_val_rmse 0.21083
wandb:            epoch 7
wandb:   final_test_mse 0.04714
wandb:    final_test_r2 -0.19231
wandb:  final_test_rmse 0.21711
wandb:  final_train_mse 0.02907
wandb:   final_train_r2 -0.45784
wandb: final_train_rmse 0.17049
wandb:    final_val_mse 0.04445
wandb:     final_val_r2 0.04452
wandb:   final_val_rmse 0.21083
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02742
wandb:       train_time 55.58315
wandb:         val_loss 0.05523
wandb:          val_mse 0.05207
wandb:           val_r2 -0.11926
wandb:         val_rmse 0.22818
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015817-eoa3ecwh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015817-eoa3ecwh/logs
Control experiment for ru (control=2) completed successfully
Running complexity control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-09 01:59:50,288][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/complexity_output/ru/control3
experiment_name: complexity_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-09 01:59:50,289][__main__][INFO] - Normalized task: complexity
[2025-04-09 01:59:50,289][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-09 01:59:50,289][__main__][INFO] - Determined Task Type: regression
[2025-04-09 01:59:50,294][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-04-09 01:59:50,295][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-09 01:59:51,979][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-09 01:59:54,826][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-09 01:59:54,826][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:59:54,933][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-04-09 01:59:54,979][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-04-09 01:59:55,152][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-09 01:59:55,163][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:59:55,164][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-09 01:59:55,166][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:59:55,204][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:59:55,259][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:59:55,281][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-09 01:59:55,282][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:59:55,283][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-09 01:59:55,285][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-09 01:59:55,324][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:59:55,373][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-09 01:59:55,393][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-09 01:59:55,395][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-09 01:59:55,395][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-09 01:59:55,397][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-09 01:59:55,397][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:59:55,397][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:59:55,397][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:59:55,397][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:59:55,398][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:59:55,398][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-09 01:59:55,398][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-09 01:59:55,398][src.data.datasets][INFO] - Sample label: 0.4091160297393799
[2025-04-09 01:59:55,398][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:59:55,398][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:59:55,398][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:59:55,399][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:59:55,399][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:59:55,399][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-09 01:59:55,399][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-09 01:59:55,399][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-09 01:59:55,399][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-09 01:59:55,399][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-09 01:59:55,400][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-09 01:59:55,400][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-09 01:59:55,400][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-09 01:59:55,401][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-09 01:59:55,401][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-09 02:00:01,139][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-09 02:00:01,142][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-09 02:00:01,142][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:25,  1.16s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:39,  1.85it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.82it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.45it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.74it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.74it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.48it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.03it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.43it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.71it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.92it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.06it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.17it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.24it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.29it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.04it/s]
[2025-04-09 02:00:12,716][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1397
[2025-04-09 02:00:12,974][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1279, Metrics: {'mse': 0.129598930478096, 'rmse': 0.35999851454984644, 'r2': -1.785926342010498}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.25it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.23it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.95it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.11it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.21it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.35it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.38it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.25it/s]
[2025-04-09 02:00:20,755][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0646
[2025-04-09 02:00:21,017][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0672, Metrics: {'mse': 0.06472566723823547, 'rmse': 0.2544123959995571, 'r2': -0.3913766145706177}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.50it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-09 02:00:29,007][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0423
[2025-04-09 02:00:29,271][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0841, Metrics: {'mse': 0.08658056706190109, 'rmse': 0.2942457596328299, 'r2': -0.8611810207366943}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.88it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-09 02:00:36,616][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0319
[2025-04-09 02:00:36,903][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0801, Metrics: {'mse': 0.08197289705276489, 'rmse': 0.2863090935558368, 'r2': -0.7621321678161621}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.34it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.40it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.41it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-09 02:00:44,292][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0256
[2025-04-09 02:00:44,569][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0677, Metrics: {'mse': 0.06765207648277283, 'rmse': 0.2601001278023001, 'r2': -0.4542844295501709}
[2025-04-09 02:00:44,570][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▂▁
wandb:          val_mse █▁▃▃▁
wandb:           val_r2 ▁█▆▆█
wandb:         val_rmse █▁▄▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06718
wandb:     best_val_mse 0.06473
wandb:      best_val_r2 -0.39138
wandb:    best_val_rmse 0.25441
wandb:            epoch 5
wandb:   final_test_mse 0.06686
wandb:    final_test_r2 -0.69105
wandb:  final_test_rmse 0.25856
wandb:  final_train_mse 0.02487
wandb:   final_train_r2 -0.24737
wandb: final_train_rmse 0.1577
wandb:    final_val_mse 0.06473
wandb:     final_val_r2 -0.39138
wandb:   final_val_rmse 0.25441
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02561
wandb:       train_time 40.1482
wandb:         val_loss 0.06773
wandb:          val_mse 0.06765
wandb:           val_r2 -0.45428
wandb:         val_rmse 0.2601
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015950-17vcgmbg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250409_015950-17vcgmbg/logs
Control experiment for ru (control=3) completed successfully
Complexity regression experiments completed
