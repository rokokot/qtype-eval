SLURM_JOB_ID: 64466950
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 20:44:18 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja/ja/results.json for layer 2
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja/ja/results.json for layer 2
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Experiment probe_layer2_avg_links_len_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ja"         "wandb.mode=offline" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:44:47,221][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ja
experiment_name: probe_layer2_avg_max_depth_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:44:47,221][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:44:47,221][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:44:47,221][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:44:47,221][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:44:47,226][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:44:47,226][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:44:47,226][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:44:49,943][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:44:52,399][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:44:52,400][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:44:52,617][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:52,686][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:52,947][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:44:52,957][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:44:52,958][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:44:52,960][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:44:53,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:53,085][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:53,100][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:44:53,101][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:44:53,101][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:44:53,112][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:44:53,166][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:53,239][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:44:53,261][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:44:53,263][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:44:53,263][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:44:53,274][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:44:53,275][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:44:53,275][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:44:53,275][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:44:53,276][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 20:44:53,276][src.data.datasets][INFO] -   Mean: 0.2860, Std: 0.1307
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Sample label: 0.16699999570846558
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:44:53,276][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:44:53,277][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:44:53,277][src.data.datasets][INFO] -   Mean: 0.2943, Std: 0.1709
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:44:53,277][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:44:53,277][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:44:53,278][src.data.datasets][INFO] -   Mean: 0.4112, Std: 0.2602
[2025-05-07 20:44:53,278][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:44:53,278][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 20:44:53,278][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:44:53,278][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:44:53,278][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:44:53,278][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:44:53,279][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:45:00,020][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:45:00,021][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:45:00,022][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:45:00,022][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:45:00,025][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:45:00,025][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:45:00,025][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:45:00,025][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:45:00,025][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:45:00,026][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:45:00,026][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4344Epoch 1/15: [                              ] 2/75 batches, loss: 0.4521Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4467Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4606Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4413Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4087Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4043Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4219Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4066Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3867Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3808Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3978Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3808Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4020Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3954Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4058Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4069Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4197Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4095Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4080Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3983Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3975Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3849Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3744Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3695Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3641Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3587Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3525Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3492Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3426Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3381Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3348Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3321Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3311Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3289Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3300Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3259Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3212Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3210Epoch 1/15: [================              ] 40/75 batches, loss: 0.3152Epoch 1/15: [================              ] 41/75 batches, loss: 0.3116Epoch 1/15: [================              ] 42/75 batches, loss: 0.3119Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3111Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3133Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3114Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3072Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3041Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3014Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2995Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2973Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2970Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3018Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2976Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2991Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2985Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2940Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2911Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2900Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2881Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2859Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2848Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2832Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2841Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2843Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2836Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2811Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2798Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2767Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2752Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2726Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2716Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2728Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2709Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2709Epoch 1/15: [==============================] 75/75 batches, loss: 0.2684
[2025-05-07 20:45:07,271][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2684
[2025-05-07 20:45:07,481][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0369, Metrics: {'mse': 0.03610684722661972, 'rmse': 0.19001801816306715, 'r2': -0.23696064949035645}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1398Epoch 2/15: [                              ] 2/75 batches, loss: 0.1410Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1614Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1720Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1634Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1526Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1628Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1583Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1690Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1744Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1670Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1682Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1658Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1617Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1668Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1640Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1625Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1612Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1586Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1544Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1516Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1497Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1481Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1444Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1398Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1452Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1454Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1441Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1457Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1444Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1428Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1449Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1433Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1426Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1430Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1419Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1429Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1412Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1411Epoch 2/15: [================              ] 40/75 batches, loss: 0.1409Epoch 2/15: [================              ] 41/75 batches, loss: 0.1398Epoch 2/15: [================              ] 42/75 batches, loss: 0.1418Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1417Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1449Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1430Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1412Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1398Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1387Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1389Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1380Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1373Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1361Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1355Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1363Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1352Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1357Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1347Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1336Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1329Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1328Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1329Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1330Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1331Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1317Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1306Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1294Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1305Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1303Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1296Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1293Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1285Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1279Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1265Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1261Epoch 2/15: [==============================] 75/75 batches, loss: 0.1263
[2025-05-07 20:45:10,151][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1263
[2025-05-07 20:45:10,352][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0356, Metrics: {'mse': 0.034761253744363785, 'rmse': 0.18644370127296814, 'r2': -0.190862774848938}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1194Epoch 3/15: [                              ] 2/75 batches, loss: 0.1246Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0989Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0977Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1035Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1022Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0976Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1007Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1002Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0980Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0973Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0948Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0941Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0924Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0896Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0891Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0882Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0886Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0877Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0911Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0945Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0925Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0937Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0916Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0902Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0915Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0937Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0932Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0922Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0904Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0910Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0952Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0952Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0948Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0946Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0942Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0952Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0938Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0928Epoch 3/15: [================              ] 40/75 batches, loss: 0.0919Epoch 3/15: [================              ] 41/75 batches, loss: 0.0920Epoch 3/15: [================              ] 42/75 batches, loss: 0.0933Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0935Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0938Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0935Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0932Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0934Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0921Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0918Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0921Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0927Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0925Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0926Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0917Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0910Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0905Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0903Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0910Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0903Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0894Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0888Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0882Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0876Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0873Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0880Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0878Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0872Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0873Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0876Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0875Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0870Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0875Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0881Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0877Epoch 3/15: [==============================] 75/75 batches, loss: 0.0883
[2025-05-07 20:45:13,032][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0883
[2025-05-07 20:45:13,243][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0337, Metrics: {'mse': 0.03284044191241264, 'rmse': 0.181219319920401, 'r2': -0.12505900859832764}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1108Epoch 4/15: [                              ] 2/75 batches, loss: 0.0898Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1112Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1043Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0868Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0819Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0836Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0793Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0807Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0797Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0859Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0849Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0824Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0821Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0796Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0824Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0822Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0805Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0806Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0819Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0806Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0804Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0808Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0818Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0812Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0793Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0802Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0797Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0785Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0787Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0790Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0785Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0778Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0777Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0767Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0764Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0756Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0759Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0756Epoch 4/15: [================              ] 40/75 batches, loss: 0.0758Epoch 4/15: [================              ] 41/75 batches, loss: 0.0753Epoch 4/15: [================              ] 42/75 batches, loss: 0.0755Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0762Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0753Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0749Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0751Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0747Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0756Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0751Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0741Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0738Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0735Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0735Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0725Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0718Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0722Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0719Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0722Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0724Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0719Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0721Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0717Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0716Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0714Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0717Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0714Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0713Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0710Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0712Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0708Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0707Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0705Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0703Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0709Epoch 4/15: [==============================] 75/75 batches, loss: 0.0705
[2025-05-07 20:45:15,937][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0705
[2025-05-07 20:45:16,140][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0321, Metrics: {'mse': 0.03125926852226257, 'rmse': 0.17680290869287918, 'r2': -0.07089054584503174}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0367Epoch 5/15: [                              ] 2/75 batches, loss: 0.0499Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0442Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0479Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0459Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0580Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0569Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0569Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0586Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0585Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0606Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0605Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0614Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0627Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0605Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0601Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0585Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0584Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0575Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0564Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0559Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0585Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0582Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0589Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0600Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0601Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0614Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0602Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0609Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0602Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0602Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0600Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0603Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0599Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0606Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0612Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0616Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0621Epoch 5/15: [================              ] 40/75 batches, loss: 0.0614Epoch 5/15: [================              ] 41/75 batches, loss: 0.0610Epoch 5/15: [================              ] 42/75 batches, loss: 0.0625Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0620Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0613Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0609Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0614Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0611Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0608Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0610Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0614Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0606Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0606Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0603Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0599Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0601Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0597Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0594Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0605Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0602Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0612Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0617Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0612Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0608Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0612Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0618Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0612Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0610Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0612Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0611Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0606Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0603Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0598Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0597Epoch 5/15: [==============================] 75/75 batches, loss: 0.0597
[2025-05-07 20:45:18,790][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0597
[2025-05-07 20:45:19,006][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0318, Metrics: {'mse': 0.030966704711318016, 'rmse': 0.17597359094852277, 'r2': -0.060867905616760254}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0332Epoch 6/15: [                              ] 2/75 batches, loss: 0.0433Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0394Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0421Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0509Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0532Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0525Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0529Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0526Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0513Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0492Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0528Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0509Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0522Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0509Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0516Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0533Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0531Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0515Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0508Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0512Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0501Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0494Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0515Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0510Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0525Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0525Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0532Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0527Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0525Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0515Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0510Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0518Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0531Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0529Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0528Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0528Epoch 6/15: [================              ] 40/75 batches, loss: 0.0538Epoch 6/15: [================              ] 41/75 batches, loss: 0.0545Epoch 6/15: [================              ] 42/75 batches, loss: 0.0541Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0528Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0526Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0528Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0528Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0525Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0522Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0523Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0520Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0516Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0525Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0522Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0517Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0517Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0515Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0515Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0517Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0515Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0515Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0515Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0513Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0512Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0508Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0506Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0503Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0500Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0500Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0503Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0505Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0503Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0500Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0498Epoch 6/15: [==============================] 75/75 batches, loss: 0.0500
[2025-05-07 20:45:21,652][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0500
[2025-05-07 20:45:21,857][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0264, Metrics: {'mse': 0.0256582573056221, 'rmse': 0.16018195062372695, 'r2': 0.12099069356918335}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0312Epoch 7/15: [                              ] 2/75 batches, loss: 0.0438Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0465Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0439Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0446Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0444Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0443Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0500Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0491Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0477Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0465Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0485Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0469Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0462Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0469Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0466Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0493Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0507Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0513Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0502Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0493Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0492Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0492Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0488Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0482Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0484Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0484Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0494Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0507Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0511Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0505Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0507Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0517Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0510Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0510Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0511Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0509Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0513Epoch 7/15: [================              ] 40/75 batches, loss: 0.0511Epoch 7/15: [================              ] 41/75 batches, loss: 0.0505Epoch 7/15: [================              ] 42/75 batches, loss: 0.0501Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0500Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0497Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0497Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0493Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0494Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0492Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0489Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0485Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0483Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0478Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0474Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0473Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0472Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0471Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0469Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0472Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0472Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0474Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0468Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0467Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0465Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0464Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0460Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0462Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0461Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0461Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0457Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0460Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0458Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0456Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0454Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0453Epoch 7/15: [==============================] 75/75 batches, loss: 0.0455
[2025-05-07 20:45:24,539][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0455
[2025-05-07 20:45:24,767][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0295, Metrics: {'mse': 0.028636384755373, 'rmse': 0.16922288484532166, 'r2': 0.01896500587463379}
[2025-05-07 20:45:24,768][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0391Epoch 8/15: [                              ] 2/75 batches, loss: 0.0288Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0362Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0472Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0445Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0457Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0436Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0422Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0417Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0412Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0453Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0436Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0424Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0410Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0416Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0397Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0402Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0409Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0434Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0429Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0430Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0433Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0439Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0439Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0439Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0428Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0433Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0429Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0422Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0431Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0433Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0432Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0431Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0434Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0434Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0436Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0434Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0432Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0439Epoch 8/15: [================              ] 40/75 batches, loss: 0.0436Epoch 8/15: [================              ] 41/75 batches, loss: 0.0429Epoch 8/15: [================              ] 42/75 batches, loss: 0.0431Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0428Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0423Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0423Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0420Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0416Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0417Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0413Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0415Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0421Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0418Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0416Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0414Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0414Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0421Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0422Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0422Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0421Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0418Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0414Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0413Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0414Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0412Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0409Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0406Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0408Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0408Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0406Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0405Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0408Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0409Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0408Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0408Epoch 8/15: [==============================] 75/75 batches, loss: 0.0405
[2025-05-07 20:45:27,080][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0405
[2025-05-07 20:45:27,313][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0342, Metrics: {'mse': 0.03327661380171776, 'rmse': 0.18241878686614973, 'r2': -0.14000153541564941}
[2025-05-07 20:45:27,313][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0184Epoch 9/15: [                              ] 2/75 batches, loss: 0.0324Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0371Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0389Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0389Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0405Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0410Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0380Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0385Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0390Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0388Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0377Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0372Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0377Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0385Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0398Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0399Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0394Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0391Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0382Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0375Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0380Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0374Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0373Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0370Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0369Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0363Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0369Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0372Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0376Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0370Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0373Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0389Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0390Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0386Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0381Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0382Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0388Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0385Epoch 9/15: [================              ] 40/75 batches, loss: 0.0384Epoch 9/15: [================              ] 41/75 batches, loss: 0.0379Epoch 9/15: [================              ] 42/75 batches, loss: 0.0375Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0381Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0380Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0377Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0385Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0385Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0382Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0381Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0380Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0380Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0377Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0377Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0386Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0384Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0382Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0384Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0384Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0381Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0383Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0384Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0384Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0384Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0391Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0392Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0392Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0390Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0390Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0389Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0387Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0385Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0384Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0383Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0381Epoch 9/15: [==============================] 75/75 batches, loss: 0.0379
[2025-05-07 20:45:29,612][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0379
[2025-05-07 20:45:29,827][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0291, Metrics: {'mse': 0.028255077078938484, 'rmse': 0.16809246586012855, 'r2': 0.032028019428253174}
[2025-05-07 20:45:29,828][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0452Epoch 10/15: [                              ] 2/75 batches, loss: 0.0378Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0417Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0387Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0385Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0370Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0376Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0377Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0376Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0391Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0385Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0406Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0390Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0408Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0403Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0405Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0407Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0401Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0390Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0385Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0383Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0381Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0377Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0376Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0376Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0377Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0372Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0365Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0361Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0359Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0356Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0353Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0355Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0355Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0353Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0351Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0352Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0354Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0348Epoch 10/15: [================              ] 40/75 batches, loss: 0.0343Epoch 10/15: [================              ] 41/75 batches, loss: 0.0342Epoch 10/15: [================              ] 42/75 batches, loss: 0.0341Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0346Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0345Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0343Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0345Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0343Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0349Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0349Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0349Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0348Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0349Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0347Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0345Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0346Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0347Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0347Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0355Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0349Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0349Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0347Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0348Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0347Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0348Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0345Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0348Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0347Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0348Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0347Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0346Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0344Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0344Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0343Epoch 10/15: [==============================] 75/75 batches, loss: 0.0341
[2025-05-07 20:45:32,101][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0341
[2025-05-07 20:45:32,321][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0298, Metrics: {'mse': 0.02898082695901394, 'rmse': 0.17023756036496157, 'r2': 0.007164955139160156}
[2025-05-07 20:45:32,322][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:45:32,322][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:45:32,322][src.training.lm_trainer][INFO] - Training completed in 28.23 seconds
[2025-05-07 20:45:32,322][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:45:35,152][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.015386554412543774, 'rmse': 0.12404255081440309, 'r2': 0.09971708059310913}
[2025-05-07 20:45:35,153][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0256582573056221, 'rmse': 0.16018195062372695, 'r2': 0.12099069356918335}
[2025-05-07 20:45:35,153][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.08216237276792526, 'rmse': 0.28663979620409524, 'r2': -0.2137969732284546}
[2025-05-07 20:45:36,820][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ja/ja/model.pt
[2025-05-07 20:45:36,822][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▅▁
wandb:     best_val_mse █▇▆▅▅▁
wandb:      best_val_r2 ▁▂▃▄▄█
wandb:    best_val_rmse █▇▆▅▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▂▃▃▄▄▂▄
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▅▅▁▃▆▃▃
wandb:          val_mse █▇▆▅▅▁▃▆▃▃
wandb:           val_r2 ▁▂▃▄▄█▆▃▆▆
wandb:         val_rmse █▇▆▅▅▁▃▆▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02639
wandb:     best_val_mse 0.02566
wandb:      best_val_r2 0.12099
wandb:    best_val_rmse 0.16018
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.08216
wandb:    final_test_r2 -0.2138
wandb:  final_test_rmse 0.28664
wandb:  final_train_mse 0.01539
wandb:   final_train_r2 0.09972
wandb: final_train_rmse 0.12404
wandb:    final_val_mse 0.02566
wandb:     final_val_r2 0.12099
wandb:   final_val_rmse 0.16018
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03408
wandb:       train_time 28.23019
wandb:         val_loss 0.02978
wandb:          val_mse 0.02898
wandb:           val_r2 0.00716
wandb:         val_rmse 0.17024
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204447-87ibtbv6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204447-87ibtbv6/logs
Experiment probe_layer2_avg_max_depth_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ja"         "wandb.mode=offline" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:45:59,961][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ja
experiment_name: probe_layer2_avg_subordinate_chain_len_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:45:59,962][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:45:59,962][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:45:59,962][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:45:59,962][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:45:59,966][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:45:59,966][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:45:59,966][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:46:02,794][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:46:05,230][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:46:05,231][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:46:05,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:05,556][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:05,798][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:46:05,807][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:46:05,808][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:46:05,811][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:46:05,899][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:05,995][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:06,032][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:46:06,034][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:46:06,034][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:46:06,047][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:46:06,145][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:06,269][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:46:06,303][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:46:06,305][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:46:06,305][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:46:06,308][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:46:06,308][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:46:06,308][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:46:06,308][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:46:06,308][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:46:06,308][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:46:06,309][src.data.datasets][INFO] -   Mean: 0.1797, Std: 0.2539
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:46:06,309][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:46:06,309][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5000
[2025-05-07 20:46:06,310][src.data.datasets][INFO] -   Mean: 0.1848, Std: 0.2413
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:46:06,310][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:46:06,310][src.data.datasets][INFO] -   Mean: 0.3168, Std: 0.2713
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:46:06,310][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:46:06,311][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:46:06,311][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:46:06,311][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:46:06,311][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:46:06,311][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:46:13,571][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:46:13,572][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:46:13,572][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:46:13,573][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:46:13,575][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:46:13,576][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:46:13,576][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:46:13,576][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:46:13,576][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:46:13,577][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:46:13,577][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5402Epoch 1/15: [                              ] 2/75 batches, loss: 0.5097Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5046Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5199Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4916Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4774Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4823Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5140Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4914Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4821Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4778Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4984Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4752Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4953Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4857Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4881Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4830Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4996Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4850Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4829Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4749Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4745Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4614Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4474Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4396Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4315Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4296Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4209Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4172Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4100Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4059Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4000Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3962Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3953Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3927Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3965Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3908Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3865Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3880Epoch 1/15: [================              ] 40/75 batches, loss: 0.3815Epoch 1/15: [================              ] 41/75 batches, loss: 0.3781Epoch 1/15: [================              ] 42/75 batches, loss: 0.3775Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3778Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3788Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3754Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3705Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3689Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3668Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3632Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3612Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3604Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3624Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3579Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3596Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3589Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3536Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3511Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3518Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3495Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3474Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3457Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3454Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3464Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3472Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3470Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3439Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3427Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3394Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3376Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3364Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3349Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3357Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3339Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3338Epoch 1/15: [==============================] 75/75 batches, loss: 0.3299
[2025-05-07 20:46:19,631][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3299
[2025-05-07 20:46:19,834][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0579, Metrics: {'mse': 0.05746077001094818, 'rmse': 0.23970976202680647, 'r2': 0.013492882251739502}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2145Epoch 2/15: [                              ] 2/75 batches, loss: 0.1972Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1977Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2428Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2585Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2372Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2345Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2277Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2409Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2437Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2318Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2263Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2206Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2173Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2249Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2219Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2208Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2170Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2106Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2055Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2035Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2022Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1999Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1970Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1927Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1966Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1923Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1898Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1926Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1925Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1914Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1972Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1943Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1929Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1942Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1919Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1918Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1906Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1909Epoch 2/15: [================              ] 40/75 batches, loss: 0.1905Epoch 2/15: [================              ] 41/75 batches, loss: 0.1887Epoch 2/15: [================              ] 42/75 batches, loss: 0.1900Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1885Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1917Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1897Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1871Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1856Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1846Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1844Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1830Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1814Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1799Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1782Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1779Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1783Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1788Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1775Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1758Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1757Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1751Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1748Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1741Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1739Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1735Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1724Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1712Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1724Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1729Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1725Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1723Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1716Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1705Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1691Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1682Epoch 2/15: [==============================] 75/75 batches, loss: 0.1675
[2025-05-07 20:46:22,499][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1675
[2025-05-07 20:46:22,745][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0493, Metrics: {'mse': 0.04877324774861336, 'rmse': 0.22084666116700374, 'r2': 0.16264337301254272}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1498Epoch 3/15: [                              ] 2/75 batches, loss: 0.1858Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1681Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1533Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1457Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1467Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1397Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1464Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1427Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1416Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1440Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1354Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1330Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1313Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1272Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1267Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1229Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1236Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1227Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1277Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1313Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1285Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1305Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1291Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1271Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1273Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1292Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1292Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1289Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1272Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1292Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1351Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1347Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1343Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1339Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1331Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1373Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1373Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1368Epoch 3/15: [================              ] 40/75 batches, loss: 0.1366Epoch 3/15: [================              ] 41/75 batches, loss: 0.1368Epoch 3/15: [================              ] 42/75 batches, loss: 0.1388Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1390Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1394Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1382Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1377Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1387Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1373Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1361Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1354Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1357Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1368Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1359Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1344Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1337Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1328Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1333Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1337Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1334Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1323Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1326Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1317Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1319Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1313Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1316Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1317Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1313Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1320Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1316Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1319Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1313Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1317Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1325Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1320Epoch 3/15: [==============================] 75/75 batches, loss: 0.1326
[2025-05-07 20:46:25,436][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1326
[2025-05-07 20:46:25,680][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0498, Metrics: {'mse': 0.04915471747517586, 'rmse': 0.22170863193654833, 'r2': 0.15609419345855713}
[2025-05-07 20:46:25,680][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1431Epoch 4/15: [                              ] 2/75 batches, loss: 0.1262Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1644Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1502Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1297Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1232Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1282Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1206Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1201Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1209Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1264Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1213Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1182Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1214Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1224Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1227Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1212Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1208Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1240Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1258Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1243Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1225Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1220Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1235Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1234Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1207Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1250Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1250Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1232Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1217Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1218Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1219Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1200Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1224Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1213Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1218Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1225Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1231Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1222Epoch 4/15: [================              ] 40/75 batches, loss: 0.1236Epoch 4/15: [================              ] 41/75 batches, loss: 0.1232Epoch 4/15: [================              ] 42/75 batches, loss: 0.1235Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1255Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1248Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1245Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1244Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1246Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1249Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1245Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1234Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1226Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1228Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1225Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1218Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1213Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1213Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1216Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1224Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1225Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1216Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1221Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1214Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1216Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1224Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1224Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1215Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1216Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1209Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1211Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1212Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1215Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1219Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1220Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1223Epoch 4/15: [==============================] 75/75 batches, loss: 0.1214
[2025-05-07 20:46:27,950][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1214
[2025-05-07 20:46:28,147][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0438, Metrics: {'mse': 0.04315297678112984, 'rmse': 0.20773294582499388, 'r2': 0.2591341733932495}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0831Epoch 5/15: [                              ] 2/75 batches, loss: 0.0947Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0977Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0994Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0955Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0961Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1059Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1081Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1057Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1076Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1103Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1128Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1161Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1161Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1117Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1113Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1117Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1107Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1090Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1067Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1057Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1089Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1090Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1099Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1103Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1094Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1106Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1096Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1091Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1090Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1127Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1135Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1111Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1098Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1106Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1090Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1094Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1104Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1103Epoch 5/15: [================              ] 40/75 batches, loss: 0.1092Epoch 5/15: [================              ] 41/75 batches, loss: 0.1085Epoch 5/15: [================              ] 42/75 batches, loss: 0.1092Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1086Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1085Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1076Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1072Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1066Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1066Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1066Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1064Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1060Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1057Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1057Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1057Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1051Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1051Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1044Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1033Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1028Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1024Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1030Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1036Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1033Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1028Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1038Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1049Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1042Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1033Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1042Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1042Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1038Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1032Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1022Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1025Epoch 5/15: [==============================] 75/75 batches, loss: 0.1040
[2025-05-07 20:46:30,827][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1040
[2025-05-07 20:46:31,042][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0454, Metrics: {'mse': 0.04466095566749573, 'rmse': 0.21133138826851, 'r2': 0.2332446575164795}
[2025-05-07 20:46:31,042][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1196Epoch 6/15: [                              ] 2/75 batches, loss: 0.1260Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1088Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0971Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1048Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1045Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1040Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1051Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1041Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0992Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0940Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0936Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0910Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0928Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0918Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0914Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0905Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0928Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0929Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0926Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0924Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0913Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0902Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0908Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0902Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0926Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0914Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0931Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0927Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0941Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0935Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0941Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0932Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0925Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0939Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0946Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0941Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0947Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0947Epoch 6/15: [================              ] 40/75 batches, loss: 0.0960Epoch 6/15: [================              ] 41/75 batches, loss: 0.0967Epoch 6/15: [================              ] 42/75 batches, loss: 0.0967Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0957Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0956Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0968Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0963Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0959Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0965Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0964Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0982Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0977Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0972Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0977Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0968Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0967Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0965Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0967Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0962Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0967Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0964Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0960Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0959Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0962Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0965Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0961Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0955Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0954Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0955Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0954Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0950Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0958Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0956Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0948Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0942Epoch 6/15: [==============================] 75/75 batches, loss: 0.0954
[2025-05-07 20:46:33,323][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0954
[2025-05-07 20:46:33,527][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0421, Metrics: {'mse': 0.04142709821462631, 'rmse': 0.20353647883027334, 'r2': 0.28876471519470215}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0625Epoch 7/15: [                              ] 2/75 batches, loss: 0.0575Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0684Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0684Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0676Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0754Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0733Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0786Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0779Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0802Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0800Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0786Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0836Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0821Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0827Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0835Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0830Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0845Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0858Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0872Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0871Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0848Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0838Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0837Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0828Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0822Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0847Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0854Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0861Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0879Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0878Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0869Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0867Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0897Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0893Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0888Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0900Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0887Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0902Epoch 7/15: [================              ] 40/75 batches, loss: 0.0900Epoch 7/15: [================              ] 41/75 batches, loss: 0.0894Epoch 7/15: [================              ] 42/75 batches, loss: 0.0900Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0896Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0897Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0903Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0892Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0895Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0892Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0886Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0880Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0876Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0870Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0862Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0852Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0860Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0856Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0854Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0857Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0858Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0857Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0849Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0847Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0846Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0851Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0854Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0860Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0857Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0862Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0864Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0867Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0862Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0859Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0855Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0850Epoch 7/15: [==============================] 75/75 batches, loss: 0.0847
[2025-05-07 20:46:36,263][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0847
[2025-05-07 20:46:36,536][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0442, Metrics: {'mse': 0.04343103617429733, 'rmse': 0.20840114244959726, 'r2': 0.2543603777885437}
[2025-05-07 20:46:36,536][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1047Epoch 8/15: [                              ] 2/75 batches, loss: 0.0817Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0814Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0854Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0763Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0762Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0746Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0777Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0763Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0745Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0751Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0759Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0765Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0766Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0763Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0759Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0763Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0776Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0823Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0833Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0841Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0853Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0851Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0849Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0859Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0853Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0866Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0850Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0844Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0849Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0841Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0832Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0816Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0809Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0805Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0804Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0810Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0807Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0814Epoch 8/15: [================              ] 40/75 batches, loss: 0.0814Epoch 8/15: [================              ] 41/75 batches, loss: 0.0828Epoch 8/15: [================              ] 42/75 batches, loss: 0.0833Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0828Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0817Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0819Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0816Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0806Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0808Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0798Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0801Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0796Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0790Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0790Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0787Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0786Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0790Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0791Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0791Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0788Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0786Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0782Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0783Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0785Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0782Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0780Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0779Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0779Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0782Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0783Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0780Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0784Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0785Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0781Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0781Epoch 8/15: [==============================] 75/75 batches, loss: 0.0783
[2025-05-07 20:46:38,823][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0783
[2025-05-07 20:46:39,069][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0502, Metrics: {'mse': 0.049252867698669434, 'rmse': 0.2219298711275015, 'r2': 0.15440911054611206}
[2025-05-07 20:46:39,069][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0342Epoch 9/15: [                              ] 2/75 batches, loss: 0.0554Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0574Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0596Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0625Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0638Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0650Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0666Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0681Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0703Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0745Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0742Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0729Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0787Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0791Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0833Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0830Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0816Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0831Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0819Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0814Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0809Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0823Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0825Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0841Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0844Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0836Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0833Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0835Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0836Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0826Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0823Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0831Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0817Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0817Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0807Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0812Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0802Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0795Epoch 9/15: [================              ] 40/75 batches, loss: 0.0794Epoch 9/15: [================              ] 41/75 batches, loss: 0.0790Epoch 9/15: [================              ] 42/75 batches, loss: 0.0783Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0787Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0786Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0784Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0788Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0784Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0785Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0787Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0792Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0795Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0791Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0790Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0797Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0796Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0797Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0792Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0792Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0794Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0792Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0794Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0793Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0792Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0796Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0795Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0799Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0796Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0793Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0793Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0788Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0785Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0781Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0776Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0771Epoch 9/15: [==============================] 75/75 batches, loss: 0.0773
[2025-05-07 20:46:41,393][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0773
[2025-05-07 20:46:41,597][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0412, Metrics: {'mse': 0.040473051369190216, 'rmse': 0.2011791524218904, 'r2': 0.30514413118362427}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.1325Epoch 10/15: [                              ] 2/75 batches, loss: 0.0874Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0819Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0876Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0902Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0829Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0760Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0815Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0776Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0754Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0722Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0729Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0722Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0740Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0747Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0740Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0727Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0722Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0708Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0732Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0728Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0720Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0713Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0710Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0715Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0711Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0712Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0712Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0718Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0707Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0704Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0717Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0721Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0715Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0714Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0710Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0703Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0694Epoch 10/15: [================              ] 40/75 batches, loss: 0.0693Epoch 10/15: [================              ] 41/75 batches, loss: 0.0699Epoch 10/15: [================              ] 42/75 batches, loss: 0.0700Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0712Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0722Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0727Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0738Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0735Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0741Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0740Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0741Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0746Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0744Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0741Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0740Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0740Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0735Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0738Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0744Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0739Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0735Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0733Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0733Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0739Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0733Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0735Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0729Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0730Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0727Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0726Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0731Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0727Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0727Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0727Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0725Epoch 10/15: [==============================] 75/75 batches, loss: 0.0717
[2025-05-07 20:46:44,294][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0717
[2025-05-07 20:46:44,560][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0403, Metrics: {'mse': 0.03956281393766403, 'rmse': 0.19890403197940465, 'r2': 0.32077139616012573}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0402Epoch 11/15: [                              ] 2/75 batches, loss: 0.0612Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0705Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0752Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0870Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0813Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0793Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0765Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0766Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0713Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0710Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0731Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0728Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0721Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0704Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0692Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0690Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0705Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0720Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0728Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0730Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0738Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0745Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0745Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0751Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0752Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0741Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0725Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0718Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0713Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0715Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0707Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0707Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0706Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0716Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0718Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0710Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0706Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0701Epoch 11/15: [================              ] 40/75 batches, loss: 0.0699Epoch 11/15: [================              ] 41/75 batches, loss: 0.0696Epoch 11/15: [================              ] 42/75 batches, loss: 0.0700Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0701Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0702Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0711Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0705Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0698Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0697Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0699Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0693Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0686Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0675Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0673Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0667Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0669Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0668Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0662Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0662Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0659Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0661Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0670Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0666Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0667Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0663Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0667Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0671Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0667Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0664Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0662Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0663Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0663Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0670Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0668Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0669Epoch 11/15: [==============================] 75/75 batches, loss: 0.0667
[2025-05-07 20:46:47,340][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0667
[2025-05-07 20:46:47,578][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0380, Metrics: {'mse': 0.0375174917280674, 'rmse': 0.19369432549268809, 'r2': 0.35588622093200684}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0312Epoch 12/15: [                              ] 2/75 batches, loss: 0.0337Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0523Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0621Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0662Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0678Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0659Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0642Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0632Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0683Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0677Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0667Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0682Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0682Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0686Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0674Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0656Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0657Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0661Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0659Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0656Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0660Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0667Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0654Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0646Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0655Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0653Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0653Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0649Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0674Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0675Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0672Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0665Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0661Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0658Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0656Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0652Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0653Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0650Epoch 12/15: [================              ] 40/75 batches, loss: 0.0644Epoch 12/15: [================              ] 41/75 batches, loss: 0.0655Epoch 12/15: [================              ] 42/75 batches, loss: 0.0649Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0646Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0645Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0639Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0637Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0650Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0662Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0656Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0652Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0653Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0651Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0657Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0654Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0656Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0656Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0651Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0657Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0655Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0658Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0665Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0664Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0664Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0662Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0660Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0659Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0658Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0655Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0652Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0654Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0652Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0651Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0652Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0650Epoch 12/15: [==============================] 75/75 batches, loss: 0.0649
[2025-05-07 20:46:50,309][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0649
[2025-05-07 20:46:50,614][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0394, Metrics: {'mse': 0.03864245489239693, 'rmse': 0.19657684220781685, 'r2': 0.33657246828079224}
[2025-05-07 20:46:50,615][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.1112Epoch 13/15: [                              ] 2/75 batches, loss: 0.0911Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0751Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0679Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0647Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0571Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0611Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0671Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0717Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0704Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0687Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0682Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0674Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0653Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0683Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0661Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0665Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0665Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0670Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0678Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0679Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0679Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0667Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0670Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0667Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0659Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0662Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0664Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0664Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0660Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0656Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0647Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0638Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0643Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0655Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0657Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0662Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0675Epoch 13/15: [================              ] 40/75 batches, loss: 0.0675Epoch 13/15: [================              ] 41/75 batches, loss: 0.0674Epoch 13/15: [================              ] 42/75 batches, loss: 0.0670Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0676Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0675Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0673Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0668Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0662Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0659Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0664Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0663Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0655Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0659Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0653Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0651Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0648Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0650Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0651Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0649Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0649Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0648Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0649Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0654Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0653Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0652Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0651Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0652Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0650Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0650Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0647Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0648Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0654Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0653Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0658Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0659Epoch 13/15: [==============================] 75/75 batches, loss: 0.0664
[2025-05-07 20:46:52,952][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0664
[2025-05-07 20:46:53,211][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0358, Metrics: {'mse': 0.035199809819459915, 'rmse': 0.18761612355941032, 'r2': 0.39567703008651733}
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0551Epoch 14/15: [                              ] 2/75 batches, loss: 0.0622Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0848Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0768Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0693Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0637Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0601Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0592Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0605Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0620Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0638Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0611Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0604Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0607Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0609Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0585Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0583Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0600Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0587Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0582Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0588Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0607Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0595Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0589Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0589Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0585Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0598Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0592Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0586Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0594Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0586Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0602Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0600Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0615Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0612Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0615Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0626Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0627Epoch 14/15: [================              ] 40/75 batches, loss: 0.0627Epoch 14/15: [================              ] 41/75 batches, loss: 0.0629Epoch 14/15: [================              ] 42/75 batches, loss: 0.0622Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0619Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0616Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0632Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0630Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0628Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0628Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0628Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0639Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0647Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0644Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0647Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0642Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0639Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0637Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0632Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0633Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0631Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0626Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0624Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0619Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0619Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0619Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0614Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0624Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0622Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0621Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0623Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0622Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0623Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0620Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0617Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0614Epoch 14/15: [==============================] 75/75 batches, loss: 0.0615
[2025-05-07 20:46:55,990][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0615
[2025-05-07 20:46:56,262][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0364, Metrics: {'mse': 0.03568483516573906, 'rmse': 0.18890430160729285, 'r2': 0.3873499035835266}
[2025-05-07 20:46:56,263][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0666Epoch 15/15: [                              ] 2/75 batches, loss: 0.0526Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0487Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0437Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0499Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0489Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0486Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0496Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0499Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0507Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0496Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0474Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0511Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0494Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0481Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0492Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0485Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0510Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0513Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0498Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0500Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0489Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0488Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0479Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0477Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0486Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0484Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0485Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0484Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0492Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0497Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0504Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0511Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0518Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0514Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0522Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0517Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0515Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0512Epoch 15/15: [================              ] 40/75 batches, loss: 0.0511Epoch 15/15: [================              ] 41/75 batches, loss: 0.0512Epoch 15/15: [================              ] 42/75 batches, loss: 0.0516Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0526Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0524Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0526Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0525Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0522Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0522Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0530Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0534Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0533Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0531Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0529Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0537Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0539Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0534Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0538Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0542Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0539Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0541Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0542Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0541Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0543Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0542Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0546Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0542Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0545Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0545Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0549Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0553Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0553Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0553Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0550Epoch 15/15: [==============================] 75/75 batches, loss: 0.0546
[2025-05-07 20:46:58,639][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0546
[2025-05-07 20:46:58,880][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0379, Metrics: {'mse': 0.03711877763271332, 'rmse': 0.19266234098212687, 'r2': 0.3627315163612366}
[2025-05-07 20:46:58,881][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-07 20:46:58,881][src.training.lm_trainer][INFO] - Training completed in 42.45 seconds
[2025-05-07 20:46:58,881][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:47:01,798][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03706395626068115, 'rmse': 0.19252001522096646, 'r2': 0.4252062439918518}
[2025-05-07 20:47:01,798][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.035199809819459915, 'rmse': 0.18761612355941032, 'r2': 0.39567703008651733}
[2025-05-07 20:47:01,798][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06357602775096893, 'rmse': 0.2521428717036612, 'r2': 0.1359999179840088}
[2025-05-07 20:47:03,435][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ja/ja/model.pt
[2025-05-07 20:47:03,436][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▃▃▂▂▁
wandb:     best_val_mse █▅▄▃▃▂▂▁
wandb:      best_val_r2 ▁▄▅▆▆▇▇█
wandb:    best_val_rmse █▅▄▃▃▃▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▄▄▄▄▃▄▅▅▅▅▅
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▅▄▄▃▄▆▃▂▂▂▁▁▂
wandb:          val_mse █▅▅▄▄▃▄▅▃▂▂▂▁▁▂
wandb:           val_r2 ▁▄▄▅▅▆▅▄▆▇▇▇██▇
wandb:         val_rmse █▅▆▄▄▃▄▆▃▃▂▂▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03584
wandb:     best_val_mse 0.0352
wandb:      best_val_r2 0.39568
wandb:    best_val_rmse 0.18762
wandb:            epoch 15
wandb:   final_test_mse 0.06358
wandb:    final_test_r2 0.136
wandb:  final_test_rmse 0.25214
wandb:  final_train_mse 0.03706
wandb:   final_train_r2 0.42521
wandb: final_train_rmse 0.19252
wandb:    final_val_mse 0.0352
wandb:     final_val_r2 0.39568
wandb:   final_val_rmse 0.18762
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0546
wandb:       train_time 42.44696
wandb:         val_loss 0.03793
wandb:          val_mse 0.03712
wandb:           val_r2 0.36273
wandb:         val_rmse 0.19266
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204600-c7ilqsi4
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204600-c7ilqsi4/logs
Experiment probe_layer2_avg_subordinate_chain_len_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ja"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:47:32,924][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ja
experiment_name: probe_layer2_avg_verb_edges_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:47:32,924][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:47:32,924][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:47:32,924][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:47:32,924][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:47:32,928][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:47:32,928][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 20:47:32,929][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:47:35,760][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:47:38,193][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:47:38,193][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:47:38,375][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:38,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:38,747][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:47:38,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:47:38,756][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:47:38,759][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:47:38,854][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:38,942][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:38,958][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:47:38,959][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:47:38,959][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:47:38,971][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:47:39,048][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:39,152][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:47:39,182][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:47:39,183][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:47:39,183][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:47:39,187][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:47:39,188][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:47:39,188][src.data.datasets][INFO] -   Mean: 0.2629, Std: 0.2549
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-05-07 20:47:39,188][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:47:39,189][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:47:39,189][src.data.datasets][INFO] -   Mean: 0.3093, Std: 0.2933
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Sample label: 0.800000011920929
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 20:47:39,189][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 20:47:39,190][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:47:39,190][src.data.datasets][INFO] -   Mean: 0.3726, Std: 0.3091
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:47:39,190][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:47:39,191][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 20:47:39,191][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:47:46,293][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:47:46,295][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:47:46,295][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:47:46,295][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:47:46,298][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:47:46,298][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:47:46,299][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:47:46,299][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:47:46,299][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:47:46,300][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:47:46,300][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5408Epoch 1/15: [                              ] 2/75 batches, loss: 0.5213Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4781Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4765Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4759Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4510Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4629Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4849Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4645Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4560Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4583Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4712Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4495Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4530Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4478Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4599Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4522Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4707Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4600Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4621Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4567Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4570Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4440Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4330Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4282Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4173Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4126Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4048Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4005Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3931Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3880Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3827Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3786Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3787Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3750Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3778Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3738Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3688Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3703Epoch 1/15: [================              ] 40/75 batches, loss: 0.3664Epoch 1/15: [================              ] 41/75 batches, loss: 0.3629Epoch 1/15: [================              ] 42/75 batches, loss: 0.3620Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3610Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3608Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3621Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3572Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3568Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3540Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3518Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3504Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3496Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3508Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3457Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3471Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3479Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3435Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3415Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3413Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3399Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3381Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3351Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3338Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3329Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3331Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3329Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3305Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3301Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3272Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3254Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3235Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3223Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3264Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3253Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3245Epoch 1/15: [==============================] 75/75 batches, loss: 0.3204
[2025-05-07 20:47:52,415][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3204
[2025-05-07 20:47:52,611][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0965, Metrics: {'mse': 0.0973326563835144, 'rmse': 0.31198182059779445, 'r2': -0.13146591186523438}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2630Epoch 2/15: [                              ] 2/75 batches, loss: 0.2423Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2289Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2163Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2146Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2014Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1943Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1918Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1951Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2079Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2038Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2033Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2046Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2028Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2030Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2061Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2054Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2041Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1970Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1913Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1900Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1900Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1911Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1893Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1885Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1896Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1906Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1895Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1914Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1921Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1932Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1966Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1940Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1924Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1935Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1913Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1926Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1930Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1918Epoch 2/15: [================              ] 40/75 batches, loss: 0.1915Epoch 2/15: [================              ] 41/75 batches, loss: 0.1895Epoch 2/15: [================              ] 42/75 batches, loss: 0.1918Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1906Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1910Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1904Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1884Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1863Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1836Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1848Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1831Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1820Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1815Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1808Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1818Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1812Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1822Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1807Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1789Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1775Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1765Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1763Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1767Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1762Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1757Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1746Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1738Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1745Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1745Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1739Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1740Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1738Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1732Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1717Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1698Epoch 2/15: [==============================] 75/75 batches, loss: 0.1700
[2025-05-07 20:47:55,275][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1700
[2025-05-07 20:47:55,536][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0865, Metrics: {'mse': 0.08687679469585419, 'rmse': 0.29474869753037786, 'r2': -0.009919285774230957}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1465Epoch 3/15: [                              ] 2/75 batches, loss: 0.1380Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1343Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1190Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1235Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1267Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1240Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1373Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1431Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1432Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1455Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1368Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1346Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1310Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1286Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1258Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1231Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1253Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1248Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1298Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1360Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1340Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1371Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1361Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1341Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1339Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1374Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1379Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1346Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1327Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1324Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1400Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1389Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1404Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1386Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1380Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1407Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1395Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1386Epoch 3/15: [================              ] 40/75 batches, loss: 0.1376Epoch 3/15: [================              ] 41/75 batches, loss: 0.1391Epoch 3/15: [================              ] 42/75 batches, loss: 0.1386Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1376Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1369Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1387Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1385Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1378Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1378Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1386Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1371Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1374Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1385Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1376Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1359Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1353Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1356Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1351Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1353Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1359Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1347Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1352Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1341Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1339Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1331Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1325Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1313Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1306Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1299Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1299Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1301Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1296Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1293Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1306Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1300Epoch 3/15: [==============================] 75/75 batches, loss: 0.1292
[2025-05-07 20:47:58,240][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1292
[2025-05-07 20:47:58,471][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0857, Metrics: {'mse': 0.0860777497291565, 'rmse': 0.29339009821252743, 'r2': -0.0006306171417236328}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1623Epoch 4/15: [                              ] 2/75 batches, loss: 0.1485Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1525Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1512Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1363Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1311Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1268Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1228Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1155Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1106Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1169Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1194Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1197Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1198Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1183Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1157Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1164Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1131Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1156Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1190Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1175Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1161Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1154Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1156Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1140Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1129Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1138Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1141Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1146Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1151Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1163Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1179Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1172Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1171Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1167Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1156Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1135Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1122Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1118Epoch 4/15: [================              ] 40/75 batches, loss: 0.1108Epoch 4/15: [================              ] 41/75 batches, loss: 0.1101Epoch 4/15: [================              ] 42/75 batches, loss: 0.1103Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1105Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1096Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1093Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1096Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1100Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1106Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1104Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1102Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1101Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1101Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1105Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1105Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1101Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1104Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1105Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1131Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1126Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1122Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1128Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1126Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1130Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1129Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1126Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1124Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1127Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1125Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1129Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1127Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1121Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1119Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1130Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1136Epoch 4/15: [==============================] 75/75 batches, loss: 0.1132
[2025-05-07 20:48:01,130][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1132
[2025-05-07 20:48:01,351][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0707, Metrics: {'mse': 0.07121686637401581, 'rmse': 0.2668648841155685, 'r2': 0.17212307453155518}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1270Epoch 5/15: [                              ] 2/75 batches, loss: 0.1227Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1101Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1170Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1103Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1135Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1184Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1225Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1243Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1178Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1233Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1212Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1195Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1164Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1142Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1120Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1113Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1074Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1070Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1057Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1033Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1096Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1104Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1097Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1097Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1095Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1109Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1095Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1098Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1104Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1103Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1091Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1074Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1067Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1054Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1039Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1042Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1033Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1018Epoch 5/15: [================              ] 40/75 batches, loss: 0.1016Epoch 5/15: [================              ] 41/75 batches, loss: 0.1015Epoch 5/15: [================              ] 42/75 batches, loss: 0.1028Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1025Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1013Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1008Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1021Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1012Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1004Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1011Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1020Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1019Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1016Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1014Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1013Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1012Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1007Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0998Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0992Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0990Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0988Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0990Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0995Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0995Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0993Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1005Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1001Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0996Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0990Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0986Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0991Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0990Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0988Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0984Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0979Epoch 5/15: [==============================] 75/75 batches, loss: 0.0998
[2025-05-07 20:48:03,977][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0998
[2025-05-07 20:48:04,187][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0660, Metrics: {'mse': 0.06652170419692993, 'rmse': 0.2579180183642274, 'r2': 0.22670310735702515}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1161Epoch 6/15: [                              ] 2/75 batches, loss: 0.1051Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1144Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1084Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1210Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1141Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1088Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1105Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1053Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1021Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0979Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1059Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1023Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1026Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0979Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0972Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0980Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0962Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0949Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0940Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0928Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0922Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0912Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0909Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0927Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0936Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0923Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0920Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0923Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0918Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0912Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0916Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0907Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0901Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0897Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0907Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0904Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0905Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0910Epoch 6/15: [================              ] 40/75 batches, loss: 0.0910Epoch 6/15: [================              ] 41/75 batches, loss: 0.0911Epoch 6/15: [================              ] 42/75 batches, loss: 0.0910Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0902Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0902Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0899Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0900Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0890Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0886Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0887Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0888Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0890Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0886Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0888Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0881Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0879Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0876Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0878Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0871Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0876Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0872Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0875Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0876Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0878Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0878Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0874Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0865Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0867Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0868Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0865Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0862Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0865Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0865Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0870Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0868Epoch 6/15: [==============================] 75/75 batches, loss: 0.0881
[2025-05-07 20:48:06,817][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0881
[2025-05-07 20:48:07,016][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0616, Metrics: {'mse': 0.06211564317345619, 'rmse': 0.24923010085753325, 'r2': 0.2779223322868347}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0644Epoch 7/15: [                              ] 2/75 batches, loss: 0.0789Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0788Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0756Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0755Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0753Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0787Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0853Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0869Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0882Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0879Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0866Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0922Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0898Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0873Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0868Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0871Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0869Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0864Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0854Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0855Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0838Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0830Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0821Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0814Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0807Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0821Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0820Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0828Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0834Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0831Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0821Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0827Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0835Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0830Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0826Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0830Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0830Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0834Epoch 7/15: [================              ] 40/75 batches, loss: 0.0833Epoch 7/15: [================              ] 41/75 batches, loss: 0.0826Epoch 7/15: [================              ] 42/75 batches, loss: 0.0824Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0824Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0820Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0819Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0816Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0816Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0812Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0808Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0804Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0800Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0795Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0792Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0788Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0791Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0795Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0799Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0802Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0794Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0801Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0799Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0798Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0794Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0795Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0798Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0798Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0804Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0801Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0798Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0815Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0814Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0808Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0803Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0801Epoch 7/15: [==============================] 75/75 batches, loss: 0.0797
[2025-05-07 20:48:09,737][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0797
[2025-05-07 20:48:09,964][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0700, Metrics: {'mse': 0.07018309831619263, 'rmse': 0.26492092842241177, 'r2': 0.18414032459259033}
[2025-05-07 20:48:09,964][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0515Epoch 8/15: [                              ] 2/75 batches, loss: 0.0473Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0635Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0698Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0661Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0729Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0711Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0738Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0717Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0747Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0725Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0722Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0728Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0725Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0728Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0713Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0742Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0742Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0746Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0764Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0768Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0780Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0780Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0796Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0804Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0799Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0800Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0789Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0800Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0791Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0795Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0802Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0787Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0774Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0781Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0777Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0774Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0776Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0774Epoch 8/15: [================              ] 40/75 batches, loss: 0.0772Epoch 8/15: [================              ] 41/75 batches, loss: 0.0774Epoch 8/15: [================              ] 42/75 batches, loss: 0.0770Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0766Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0764Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0766Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0771Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0765Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0771Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0764Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0764Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0761Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0755Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0748Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0744Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0743Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0736Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0733Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0739Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0733Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0734Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0733Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0732Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0733Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0739Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0740Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0746Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0743Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0746Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0748Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0747Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0749Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0744Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0748Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0744Epoch 8/15: [==============================] 75/75 batches, loss: 0.0736
[2025-05-07 20:48:12,264][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0736
[2025-05-07 20:48:12,460][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0588, Metrics: {'mse': 0.0591035820543766, 'rmse': 0.2431122828126473, 'r2': 0.31293678283691406}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0109Epoch 9/15: [                              ] 2/75 batches, loss: 0.0392Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0580Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0614Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0680Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0685Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0677Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0685Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0686Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0728Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0802Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0783Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0747Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0756Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0784Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0768Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0758Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0750Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0762Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0752Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0745Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0748Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0741Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0765Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0763Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0769Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0759Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0755Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0760Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0761Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0746Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0751Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0751Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0747Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0747Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0747Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0746Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0754Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0753Epoch 9/15: [================              ] 40/75 batches, loss: 0.0749Epoch 9/15: [================              ] 41/75 batches, loss: 0.0741Epoch 9/15: [================              ] 42/75 batches, loss: 0.0737Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0740Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0744Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0736Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0743Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0738Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0733Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0732Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0732Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0735Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0730Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0729Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0740Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0735Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0733Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0726Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0722Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0721Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0720Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0717Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0715Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0715Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0711Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0710Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0709Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0710Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0707Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0704Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0704Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0702Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0704Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0703Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0700Epoch 9/15: [==============================] 75/75 batches, loss: 0.0700
[2025-05-07 20:48:15,151][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0700
[2025-05-07 20:48:15,376][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0656, Metrics: {'mse': 0.06560149788856506, 'rmse': 0.25612789361677313, 'r2': 0.23740023374557495}
[2025-05-07 20:48:15,377][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0573Epoch 10/15: [                              ] 2/75 batches, loss: 0.0598Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0692Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0621Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0599Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0603Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0582Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0683Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0690Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0645Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0645Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0622Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0644Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0639Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0641Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0635Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0639Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0629Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0624Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0619Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0626Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0628Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0618Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0631Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0633Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0626Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0638Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0636Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0633Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0643Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0644Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0639Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0636Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0630Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0631Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0643Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0637Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0632Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0634Epoch 10/15: [================              ] 40/75 batches, loss: 0.0630Epoch 10/15: [================              ] 41/75 batches, loss: 0.0635Epoch 10/15: [================              ] 42/75 batches, loss: 0.0635Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0638Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0642Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0644Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0639Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0636Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0650Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0652Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0649Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0649Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0647Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0647Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0644Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0646Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0642Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0642Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0646Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0646Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0648Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0652Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0653Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0654Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0651Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0657Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0654Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0649Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0649Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0653Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0651Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0649Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0649Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0648Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0649Epoch 10/15: [==============================] 75/75 batches, loss: 0.0655
[2025-05-07 20:48:17,661][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0655
[2025-05-07 20:48:17,866][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0512, Metrics: {'mse': 0.051519326865673065, 'rmse': 0.22697869253670722, 'r2': 0.4011017084121704}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0568Epoch 11/15: [                              ] 2/75 batches, loss: 0.0648Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0579Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0613Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0562Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0580Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0601Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0637Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0625Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0607Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0587Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0630Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0652Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0623Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0620Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0605Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0623Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0656Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0662Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0667Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0659Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0670Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0670Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0676Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0664Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0663Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0656Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0663Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0661Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0654Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0651Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0647Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0646Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0643Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0640Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0644Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0636Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0635Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0634Epoch 11/15: [================              ] 40/75 batches, loss: 0.0630Epoch 11/15: [================              ] 41/75 batches, loss: 0.0629Epoch 11/15: [================              ] 42/75 batches, loss: 0.0631Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0632Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0634Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0628Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0630Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0636Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0631Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0631Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0631Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0624Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0619Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0627Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0630Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0631Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0634Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0631Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0635Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0635Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0644Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0646Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0639Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0641Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0639Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0637Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0642Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0635Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0632Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0629Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0630Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0629Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0640Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0638Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0639Epoch 11/15: [==============================] 75/75 batches, loss: 0.0638
[2025-05-07 20:48:20,532][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0638
[2025-05-07 20:48:20,750][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0497, Metrics: {'mse': 0.04989884793758392, 'rmse': 0.22338050035216575, 'r2': 0.4199393391609192}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0901Epoch 12/15: [                              ] 2/75 batches, loss: 0.0864Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0801Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0721Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0644Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0651Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0666Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0628Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0659Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0669Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0690Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0684Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0693Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0686Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0671Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0673Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0687Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0682Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0682Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0664Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0670Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0667Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0677Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0686Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0676Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0673Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0664Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0664Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0654Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0651Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0645Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0640Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0637Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0632Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0630Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0624Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0636Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0630Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0625Epoch 12/15: [================              ] 40/75 batches, loss: 0.0624Epoch 12/15: [================              ] 41/75 batches, loss: 0.0624Epoch 12/15: [================              ] 42/75 batches, loss: 0.0631Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0629Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0627Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0620Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0623Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0624Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0621Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0617Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0612Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0617Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0617Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0617Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0619Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0619Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0612Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0607Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0609Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0605Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0606Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0607Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0602Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0607Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0604Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0603Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0600Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0596Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0594Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0589Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0589Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0584Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0582Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0580Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0580Epoch 12/15: [==============================] 75/75 batches, loss: 0.0578
[2025-05-07 20:48:23,617][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0578
[2025-05-07 20:48:23,843][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0599, Metrics: {'mse': 0.05985337868332863, 'rmse': 0.24464950170259622, 'r2': 0.30422061681747437}
[2025-05-07 20:48:23,843][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0626Epoch 13/15: [                              ] 2/75 batches, loss: 0.0591Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0535Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0523Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0518Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0511Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0527Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0557Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0539Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0570Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0587Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0571Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0565Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0552Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0537Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0530Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0552Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0548Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0540Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0543Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0540Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0552Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0546Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0542Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0552Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0560Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0549Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0547Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0545Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0550Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0561Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0562Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0563Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0556Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0556Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0553Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0555Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0560Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0565Epoch 13/15: [================              ] 40/75 batches, loss: 0.0567Epoch 13/15: [================              ] 41/75 batches, loss: 0.0568Epoch 13/15: [================              ] 42/75 batches, loss: 0.0571Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0570Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0572Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0578Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0575Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0570Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0565Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0565Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0561Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0561Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0555Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0555Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0556Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0565Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0565Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0564Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0567Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0570Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0569Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0569Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0569Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0571Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0570Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0572Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0570Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0565Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0566Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0570Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0574Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0572Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0573Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0576Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0573Epoch 13/15: [==============================] 75/75 batches, loss: 0.0577
[2025-05-07 20:48:26,169][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0577
[2025-05-07 20:48:26,387][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0501, Metrics: {'mse': 0.05012855306267738, 'rmse': 0.22389406660891525, 'r2': 0.4172690510749817}
[2025-05-07 20:48:26,388][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0577Epoch 14/15: [                              ] 2/75 batches, loss: 0.0635Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0627Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0657Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0640Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0583Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0595Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0564Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0550Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0581Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0579Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0558Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0556Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0574Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0585Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0578Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0579Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0562Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0586Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0573Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0576Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0573Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0576Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0594Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0595Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0600Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0608Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0599Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0599Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0587Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0584Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0579Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0580Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0590Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0587Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0590Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0585Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0584Epoch 14/15: [================              ] 40/75 batches, loss: 0.0584Epoch 14/15: [================              ] 41/75 batches, loss: 0.0579Epoch 14/15: [================              ] 42/75 batches, loss: 0.0575Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0572Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0572Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0577Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0569Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0570Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0568Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0568Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0577Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0578Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0573Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0572Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0565Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0563Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0566Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0567Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0564Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0564Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0565Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0564Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0560Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0562Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0562Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0560Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0564Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0561Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0561Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0562Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0557Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0554Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0551Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0550Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0552Epoch 14/15: [==============================] 75/75 batches, loss: 0.0557
[2025-05-07 20:48:28,683][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0557
[2025-05-07 20:48:28,899][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0525, Metrics: {'mse': 0.05240213871002197, 'rmse': 0.22891513429658156, 'r2': 0.39083921909332275}
[2025-05-07 20:48:28,900][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0371Epoch 15/15: [                              ] 2/75 batches, loss: 0.0367Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0346Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0345Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0384Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0386Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0381Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0403Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0408Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0443Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0448Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0439Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0467Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0460Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0452Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0464Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0452Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0451Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0451Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0446Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0446Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0440Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0461Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0457Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0467Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0467Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0465Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0470Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0470Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0476Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0477Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0494Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0490Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0487Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0486Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0484Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0483Epoch 15/15: [================              ] 40/75 batches, loss: 0.0487Epoch 15/15: [================              ] 41/75 batches, loss: 0.0480Epoch 15/15: [================              ] 42/75 batches, loss: 0.0480Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0489Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0489Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0489Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0492Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0493Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0491Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0491Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0490Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0487Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0485Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0484Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0486Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0490Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0485Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0484Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0481Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0482Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0483Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0485Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0483Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0481Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0484Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0485Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0483Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0483Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0485Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0485Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0481Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0485Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0487Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0485Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0484Epoch 15/15: [==============================] 75/75 batches, loss: 0.0479
[2025-05-07 20:48:31,238][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0479
[2025-05-07 20:48:31,468][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0535, Metrics: {'mse': 0.053363773971796036, 'rmse': 0.23100600418992584, 'r2': 0.3796604871749878}
[2025-05-07 20:48:31,468][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:48:31,469][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 20:48:31,469][src.training.lm_trainer][INFO] - Training completed in 42.10 seconds
[2025-05-07 20:48:31,469][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:48:34,339][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.030118102207779884, 'rmse': 0.17354567758310746, 'r2': 0.5363925695419312}
[2025-05-07 20:48:34,339][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04989884793758392, 'rmse': 0.22338050035216575, 'r2': 0.4199393391609192}
[2025-05-07 20:48:34,339][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0828094407916069, 'rmse': 0.2877662954406004, 'r2': 0.13312900066375732}
[2025-05-07 20:48:36,009][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ja/ja/model.pt
[2025-05-07 20:48:36,011][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▄▃▃▂▁▁
wandb:     best_val_mse █▆▆▄▃▃▂▁▁
wandb:      best_val_r2 ▁▃▃▅▆▆▇██
wandb:    best_val_rmse █▇▇▄▄▃▃▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▂▄▅▅▄▅▅▆▆▅▆▆
wandb:       train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▄▃▃▄▂▃▁▁▃▁▁▂
wandb:          val_mse █▆▆▄▃▃▄▂▃▁▁▂▁▁▂
wandb:           val_r2 ▁▃▃▅▆▆▅▇▆██▇██▇
wandb:         val_rmse █▇▇▄▄▃▄▃▄▁▁▃▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04974
wandb:     best_val_mse 0.0499
wandb:      best_val_r2 0.41994
wandb:    best_val_rmse 0.22338
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.08281
wandb:    final_test_r2 0.13313
wandb:  final_test_rmse 0.28777
wandb:  final_train_mse 0.03012
wandb:   final_train_r2 0.53639
wandb: final_train_rmse 0.17355
wandb:    final_val_mse 0.0499
wandb:     final_val_r2 0.41994
wandb:   final_val_rmse 0.22338
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04786
wandb:       train_time 42.09911
wandb:         val_loss 0.05353
wandb:          val_mse 0.05336
wandb:           val_r2 0.37966
wandb:         val_rmse 0.23101
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204732-jf5ny9hl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204732-jf5ny9hl/logs
Experiment probe_layer2_avg_verb_edges_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_lexical_density_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ja"         "wandb.mode=offline" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:49:03,309][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ja
experiment_name: probe_layer2_lexical_density_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:49:03,309][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:49:03,309][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:49:03,309][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:49:03,309][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:49:03,314][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:49:03,314][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 20:49:03,314][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:49:06,840][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:49:09,285][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:49:09,285][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:49:09,488][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:09,559][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:09,806][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:49:09,814][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:49:09,815][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:49:09,816][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:49:09,923][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:10,001][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:10,068][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:49:10,069][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:49:10,070][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:49:10,070][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:49:10,170][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:10,273][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:49:10,314][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:49:10,315][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:49:10,315][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:49:10,316][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:49:10,317][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:49:10,317][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:49:10,317][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:49:10,317][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:49:10,318][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:49:10,318][src.data.datasets][INFO] -   Mean: 0.4139, Std: 0.1614
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Sample label: 0.5239999890327454
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:49:10,318][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:49:10,318][src.data.datasets][INFO] -   Min: 0.0670, Max: 0.7920
[2025-05-07 20:49:10,319][src.data.datasets][INFO] -   Mean: 0.4093, Std: 0.1643
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 20:49:10,319][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 20:49:10,319][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:49:10,319][src.data.datasets][INFO] -   Mean: 0.5256, Std: 0.2289
[2025-05-07 20:49:10,320][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:49:10,320][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:49:10,320][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:49:10,320][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:49:10,320][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:49:10,320][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 20:49:10,321][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:49:16,602][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:49:16,603][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:49:16,603][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:49:16,603][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:49:16,606][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:49:16,606][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:49:16,606][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:49:16,606][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:49:16,607][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:49:16,607][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:49:16,607][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4519Epoch 1/15: [                              ] 2/75 batches, loss: 0.5011Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4867Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4884Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4687Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4164Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4105Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4165Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4036Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3845Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3647Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3742Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3603Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3751Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3700Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3818Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3835Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3945Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3861Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3865Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3775Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3734Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3622Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3529Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3479Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3471Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3436Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3397Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3391Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3359Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3305Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3272Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3250Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3249Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3225Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3245Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3209Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3161Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3149Epoch 1/15: [================              ] 40/75 batches, loss: 0.3100Epoch 1/15: [================              ] 41/75 batches, loss: 0.3067Epoch 1/15: [================              ] 42/75 batches, loss: 0.3070Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3067Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3080Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3057Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3036Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2998Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2970Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2933Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2913Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2903Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2966Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2932Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2950Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2944Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2904Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2867Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2846Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2832Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2820Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2816Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2799Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2816Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2810Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2801Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2776Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2760Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2733Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2717Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2698Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2691Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2692Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2667Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2655Epoch 1/15: [==============================] 75/75 batches, loss: 0.2641
[2025-05-07 20:49:22,281][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2641
[2025-05-07 20:49:22,491][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0376, Metrics: {'mse': 0.03712155669927597, 'rmse': 0.1926695531195211, 'r2': -0.3749206066131592}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1497Epoch 2/15: [                              ] 2/75 batches, loss: 0.1706Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1714Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1957Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1850Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1693Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1876Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1852Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1937Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1945Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1836Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1844Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1819Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1816Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1863Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1851Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1795Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1755Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1724Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1676Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1644Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1617Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1599Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1576Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1532Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1576Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1605Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1586Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1605Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1599Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1602Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1605Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1585Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1558Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1561Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1538Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1550Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1533Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1519Epoch 2/15: [================              ] 40/75 batches, loss: 0.1521Epoch 2/15: [================              ] 41/75 batches, loss: 0.1507Epoch 2/15: [================              ] 42/75 batches, loss: 0.1513Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1494Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1530Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1513Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1507Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1495Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1493Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1472Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1467Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1472Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1466Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1466Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1462Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1448Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1453Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1435Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1429Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1424Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1421Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1423Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1425Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1418Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1407Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1394Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1386Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1393Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1396Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1392Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1388Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1381Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1379Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1371Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1365Epoch 2/15: [==============================] 75/75 batches, loss: 0.1371
[2025-05-07 20:49:25,165][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1371
[2025-05-07 20:49:25,395][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0329, Metrics: {'mse': 0.03265904262661934, 'rmse': 0.1807181303207272, 'r2': -0.20963644981384277}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1097Epoch 3/15: [                              ] 2/75 batches, loss: 0.1150Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0961Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0958Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1018Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1031Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1028Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1136Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1129Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1079Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1090Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1090Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1102Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1057Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1040Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1047Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1027Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1013Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1004Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1010Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1037Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1032Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1040Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1018Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0995Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0993Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1018Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1022Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1017Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1003Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1010Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1054Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1059Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1059Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1049Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1064Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1080Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1071Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1062Epoch 3/15: [================              ] 40/75 batches, loss: 0.1047Epoch 3/15: [================              ] 41/75 batches, loss: 0.1041Epoch 3/15: [================              ] 42/75 batches, loss: 0.1046Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1050Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1047Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1041Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1030Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1031Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1019Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1019Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1027Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1025Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1018Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1013Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1005Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0997Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1000Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0995Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1007Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0996Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0986Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0983Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0980Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0974Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0967Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0971Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0971Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0962Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0965Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0967Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0962Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0958Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0959Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0963Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0963Epoch 3/15: [==============================] 75/75 batches, loss: 0.0954
[2025-05-07 20:49:28,109][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0954
[2025-05-07 20:49:28,323][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0267, Metrics: {'mse': 0.02640634961426258, 'rmse': 0.16250030650513425, 'r2': 0.02195274829864502}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0836Epoch 4/15: [                              ] 2/75 batches, loss: 0.0714Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0945Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1068Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0954Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0941Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0915Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0897Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0905Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0864Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0894Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0856Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0829Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0809Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0807Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0839Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0811Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0787Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0804Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0810Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0787Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0801Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0810Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0840Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0840Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0824Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0824Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0812Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0805Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0807Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0801Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0802Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0806Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0810Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0808Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0797Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0787Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0784Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0781Epoch 4/15: [================              ] 40/75 batches, loss: 0.0786Epoch 4/15: [================              ] 41/75 batches, loss: 0.0777Epoch 4/15: [================              ] 42/75 batches, loss: 0.0775Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0790Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0789Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0780Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0787Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0788Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0793Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0792Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0784Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0784Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0783Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0792Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0782Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0774Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0776Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0773Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0770Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0770Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0768Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0769Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0766Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0766Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0769Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0769Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0764Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0764Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0763Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0766Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0759Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0759Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0756Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0751Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0755Epoch 4/15: [==============================] 75/75 batches, loss: 0.0750
[2025-05-07 20:49:30,986][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0750
[2025-05-07 20:49:31,244][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0284, Metrics: {'mse': 0.028085781261324883, 'rmse': 0.16758812983420063, 'r2': -0.040250539779663086}
[2025-05-07 20:49:31,245][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0605Epoch 5/15: [                              ] 2/75 batches, loss: 0.0548Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0484Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0496Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0574Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0638Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0627Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0623Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0642Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0637Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0705Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0690Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0692Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0670Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0661Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0677Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0664Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0704Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0688Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0682Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0673Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0686Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0690Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0685Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0702Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0701Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0712Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0709Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0723Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0715Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0714Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0706Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0700Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0702Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0696Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0695Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0698Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0693Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0688Epoch 5/15: [================              ] 40/75 batches, loss: 0.0684Epoch 5/15: [================              ] 41/75 batches, loss: 0.0683Epoch 5/15: [================              ] 42/75 batches, loss: 0.0692Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0689Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0681Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0686Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0680Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0675Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0680Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0675Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0677Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0673Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0677Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0676Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0676Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0676Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0679Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0676Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0678Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0690Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0686Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0693Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0707Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0701Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0693Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0697Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0697Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0696Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0699Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0701Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0699Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0695Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0692Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0692Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0691Epoch 5/15: [==============================] 75/75 batches, loss: 0.0695
[2025-05-07 20:49:33,517][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0695
[2025-05-07 20:49:33,731][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0272, Metrics: {'mse': 0.026883697137236595, 'rmse': 0.163962486981738, 'r2': 0.004272580146789551}
[2025-05-07 20:49:33,731][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0342Epoch 6/15: [                              ] 2/75 batches, loss: 0.0445Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0412Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0488Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0556Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0563Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0563Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0604Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0610Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0585Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0565Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0572Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0574Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0578Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0573Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0583Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0609Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0599Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0598Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0587Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0584Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0569Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0579Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0593Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0595Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0591Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0586Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0593Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0603Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0612Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0603Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0610Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0602Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0601Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0594Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0603Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0594Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0595Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0588Epoch 6/15: [================              ] 40/75 batches, loss: 0.0590Epoch 6/15: [================              ] 41/75 batches, loss: 0.0591Epoch 6/15: [================              ] 42/75 batches, loss: 0.0585Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0585Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0585Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0580Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0577Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0586Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0586Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0585Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0589Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0582Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0579Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0588Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0588Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0584Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0582Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0582Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0583Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0582Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0575Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0572Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0570Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0572Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0571Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0571Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0573Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0569Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0565Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0565Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0562Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0561Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0557Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0553Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0549Epoch 6/15: [==============================] 75/75 batches, loss: 0.0550
[2025-05-07 20:49:36,031][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0550
[2025-05-07 20:49:36,243][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0260, Metrics: {'mse': 0.02573288045823574, 'rmse': 0.16041471397049506, 'r2': 0.04689699411392212}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0605Epoch 7/15: [                              ] 2/75 batches, loss: 0.0671Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0536Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0524Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0575Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0573Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0613Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0643Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0604Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0609Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0616Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0610Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0605Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0613Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0600Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0594Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0582Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0588Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0584Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0580Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0567Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0562Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0577Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0572Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0582Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0577Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0577Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0574Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0572Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0574Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0577Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0570Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0568Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0578Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0581Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0580Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0573Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0576Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0583Epoch 7/15: [================              ] 40/75 batches, loss: 0.0584Epoch 7/15: [================              ] 41/75 batches, loss: 0.0580Epoch 7/15: [================              ] 42/75 batches, loss: 0.0572Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0568Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0565Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0564Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0564Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0571Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0573Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0573Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0570Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0568Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0562Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0570Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0569Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0567Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0561Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0559Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0556Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0558Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0559Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0555Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0553Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0552Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0550Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0546Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0546Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0543Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0542Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0544Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0541Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0535Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0533Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0533Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0532Epoch 7/15: [==============================] 75/75 batches, loss: 0.0535
[2025-05-07 20:49:38,942][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0535
[2025-05-07 20:49:39,158][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0273, Metrics: {'mse': 0.026987308636307716, 'rmse': 0.16427814412242342, 'r2': 0.0004349946975708008}
[2025-05-07 20:49:39,158][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0412Epoch 8/15: [                              ] 2/75 batches, loss: 0.0451Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0405Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0430Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0437Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0439Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0458Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0455Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0435Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0421Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0442Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0441Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0447Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0448Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0470Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0471Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0467Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0468Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0469Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0473Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0462Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0462Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0471Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0468Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0471Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0467Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0465Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0462Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0464Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0472Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0466Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0467Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0468Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0477Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0470Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0466Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0461Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0462Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0467Epoch 8/15: [================              ] 40/75 batches, loss: 0.0463Epoch 8/15: [================              ] 41/75 batches, loss: 0.0465Epoch 8/15: [================              ] 42/75 batches, loss: 0.0471Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0468Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0475Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0476Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0472Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0472Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0473Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0472Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0468Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0468Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0468Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0469Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0473Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0469Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0471Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0476Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0473Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0472Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0468Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0467Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0468Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0467Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0465Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0468Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0469Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0467Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0464Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0461Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0462Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0465Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0465Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0465Epoch 8/15: [==============================] 75/75 batches, loss: 0.0463
[2025-05-07 20:49:41,448][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0463
[2025-05-07 20:49:41,669][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0310, Metrics: {'mse': 0.030659444630146027, 'rmse': 0.1750983855726432, 'r2': -0.1355748176574707}
[2025-05-07 20:49:41,670][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0517Epoch 9/15: [                              ] 2/75 batches, loss: 0.0691Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0592Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0575Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0520Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0501Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0507Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0505Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0513Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0515Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0508Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0505Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0517Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0503Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0506Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0514Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0502Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0488Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0491Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0486Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0484Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0475Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0469Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0460Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0453Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0452Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0446Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0461Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0459Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0457Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0459Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0473Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0472Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0468Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0462Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0475Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0483Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0478Epoch 9/15: [================              ] 40/75 batches, loss: 0.0477Epoch 9/15: [================              ] 41/75 batches, loss: 0.0473Epoch 9/15: [================              ] 42/75 batches, loss: 0.0481Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0486Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0483Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0479Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0474Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0471Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0466Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0461Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0461Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0460Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0460Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0456Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0459Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0455Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0454Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0450Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0451Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0450Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0449Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0451Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0452Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0451Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0456Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0456Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0457Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0454Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0453Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0454Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0453Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0454Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0454Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0457Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0459Epoch 9/15: [==============================] 75/75 batches, loss: 0.0460
[2025-05-07 20:49:43,947][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0460
[2025-05-07 20:49:44,158][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0297, Metrics: {'mse': 0.02939181961119175, 'rmse': 0.1714404258370579, 'r2': -0.08862411975860596}
[2025-05-07 20:49:44,158][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0220Epoch 10/15: [                              ] 2/75 batches, loss: 0.0309Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0356Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0344Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0368Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0383Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0394Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0382Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0386Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0416Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0407Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0438Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0426Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0430Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0424Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0418Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0430Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0444Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0444Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0450Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0460Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0460Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0459Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0462Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0471Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0477Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0468Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0465Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0463Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0458Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0454Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0444Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0443Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0445Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0448Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0459Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0454Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0453Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0446Epoch 10/15: [================              ] 40/75 batches, loss: 0.0442Epoch 10/15: [================              ] 41/75 batches, loss: 0.0439Epoch 10/15: [================              ] 42/75 batches, loss: 0.0437Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0435Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0429Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0427Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0427Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0427Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0431Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0427Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0429Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0426Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0425Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0426Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0425Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0433Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0433Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0434Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0433Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0427Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0426Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0424Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0428Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0427Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0428Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0428Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0428Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0428Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0431Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0430Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0429Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0424Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0424Epoch 10/15: [==============================] 75/75 batches, loss: 0.0422
[2025-05-07 20:49:46,468][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0422
[2025-05-07 20:49:46,720][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0302, Metrics: {'mse': 0.02987680211663246, 'rmse': 0.17284907323047025, 'r2': -0.10658705234527588}
[2025-05-07 20:49:46,720][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:49:46,721][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:49:46,721][src.training.lm_trainer][INFO] - Training completed in 27.46 seconds
[2025-05-07 20:49:46,721][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:49:49,588][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021702788770198822, 'rmse': 0.14731866402529864, 'r2': 0.16655749082565308}
[2025-05-07 20:49:49,589][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02573288045823574, 'rmse': 0.16041471397049506, 'r2': 0.04689699411392212}
[2025-05-07 20:49:49,589][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0737929716706276, 'rmse': 0.27164861801715023, 'r2': -0.4082522392272949}
[2025-05-07 20:49:51,315][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ja/ja/model.pt
[2025-05-07 20:49:51,316][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▄▄▅▄▃▄
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▂▂▁▂▄▃▄
wandb:          val_mse █▅▁▂▂▁▂▄▃▄
wandb:           val_r2 ▁▄█▇▇█▇▅▆▅
wandb:         val_rmse █▅▁▃▂▁▂▄▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02604
wandb:     best_val_mse 0.02573
wandb:      best_val_r2 0.0469
wandb:    best_val_rmse 0.16041
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.07379
wandb:    final_test_r2 -0.40825
wandb:  final_test_rmse 0.27165
wandb:  final_train_mse 0.0217
wandb:   final_train_r2 0.16656
wandb: final_train_rmse 0.14732
wandb:    final_val_mse 0.02573
wandb:     final_val_r2 0.0469
wandb:   final_val_rmse 0.16041
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04218
wandb:       train_time 27.45794
wandb:         val_loss 0.03021
wandb:          val_mse 0.02988
wandb:           val_r2 -0.10659
wandb:         val_rmse 0.17285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204903-ve0hi6ig
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_204903-ve0hi6ig/logs
Experiment probe_layer2_lexical_density_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_n_tokens_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ja"         "wandb.mode=offline" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:50:15,065][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ja
experiment_name: probe_layer2_n_tokens_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:50:15,065][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:50:15,066][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:50:15,066][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:50:15,066][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:50:15,070][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:50:15,070][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 20:50:15,070][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:50:17,940][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:50:20,177][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:50:20,177][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:50:20,334][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,545][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:50:20,553][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:50:20,553][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:50:20,555][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:50:20,618][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,664][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,680][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:50:20,681][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:50:20,681][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:50:20,682][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:50:20,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,807][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:50:20,864][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:50:20,865][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:50:20,865][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:50:20,876][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:50:20,877][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:50:20,877][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:50:20,877][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:50:20,877][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:50:20,878][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9500
[2025-05-07 20:50:20,878][src.data.datasets][INFO] -   Mean: 0.2931, Std: 0.1442
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Sample label: 0.39100000262260437
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:50:20,878][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:50:20,878][src.data.datasets][INFO] -   Min: 0.0500, Max: 0.5430
[2025-05-07 20:50:20,879][src.data.datasets][INFO] -   Mean: 0.2933, Std: 0.1414
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Sample label: 0.27900001406669617
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 20:50:20,879][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:50:20,879][src.data.datasets][INFO] -   Mean: 0.3277, Std: 0.2443
[2025-05-07 20:50:20,879][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:50:20,880][src.data.datasets][INFO] - Sample label: 0.17100000381469727
[2025-05-07 20:50:20,880][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:50:20,880][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:50:20,880][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:50:20,880][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 20:50:20,880][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:50:27,216][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:50:27,217][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:50:27,217][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:50:27,217][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:50:27,220][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:50:27,221][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:50:27,221][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:50:27,221][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:50:27,221][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:50:27,222][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:50:27,222][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4006Epoch 1/15: [                              ] 2/75 batches, loss: 0.4423Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4537Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4752Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4476Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4208Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4161Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4289Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4183Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3998Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3961Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4122Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3936Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4095Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4047Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4155Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4155Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4290Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4173Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4168Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4081Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4067Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3937Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3831Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3776Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3719Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3675Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3599Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3566Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3497Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3454Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3418Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3397Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3397Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3376Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3379Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3340Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3292Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3297Epoch 1/15: [================              ] 40/75 batches, loss: 0.3242Epoch 1/15: [================              ] 41/75 batches, loss: 0.3209Epoch 1/15: [================              ] 42/75 batches, loss: 0.3218Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3203Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3221Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3210Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3162Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3136Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3108Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3081Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3057Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3057Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3084Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3046Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3066Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3064Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3019Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2997Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2985Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2965Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2948Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2930Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2910Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2911Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2902Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2902Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2874Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2860Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2829Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2810Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2791Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2781Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2802Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2781Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2775Epoch 1/15: [==============================] 75/75 batches, loss: 0.2747
[2025-05-07 20:50:33,314][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2747
[2025-05-07 20:50:33,540][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0253, Metrics: {'mse': 0.02535821497440338, 'rmse': 0.1592426292624038, 'r2': -0.26913559436798096}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1657Epoch 2/15: [                              ] 2/75 batches, loss: 0.1638Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1680Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1757Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1742Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1634Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1692Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1629Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1754Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1794Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1731Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1736Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1733Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1684Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1702Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1697Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1679Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1664Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1630Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1591Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1561Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1546Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1527Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1502Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1461Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1491Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1485Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1470Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1464Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1452Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1436Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1471Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1452Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1434Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1440Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1425Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1423Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1416Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1405Epoch 2/15: [================              ] 40/75 batches, loss: 0.1400Epoch 2/15: [================              ] 41/75 batches, loss: 0.1389Epoch 2/15: [================              ] 42/75 batches, loss: 0.1404Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1406Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1425Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1411Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1391Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1381Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1367Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1367Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1361Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1349Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1341Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1334Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1340Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1332Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1338Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1325Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1312Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1306Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1301Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1299Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1305Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1306Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1294Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1284Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1275Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1282Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1283Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1276Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1275Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1280Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1277Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1264Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1255Epoch 2/15: [==============================] 75/75 batches, loss: 0.1263
[2025-05-07 20:50:36,225][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1263
[2025-05-07 20:50:36,427][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0203, Metrics: {'mse': 0.020076533779501915, 'rmse': 0.1416916856399906, 'r2': -0.004796385765075684}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1267Epoch 3/15: [                              ] 2/75 batches, loss: 0.1125Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0981Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0930Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1014Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1024Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0997Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1066Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1052Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1041Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1050Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0997Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0995Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0976Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0951Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0940Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0913Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0917Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0906Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0936Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0990Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0970Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0993Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0970Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0952Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0950Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0985Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0979Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0970Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0953Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0955Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1004Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0998Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0995Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0989Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0982Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1002Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0991Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0979Epoch 3/15: [================              ] 40/75 batches, loss: 0.0969Epoch 3/15: [================              ] 41/75 batches, loss: 0.0977Epoch 3/15: [================              ] 42/75 batches, loss: 0.0998Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0996Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0995Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0991Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0992Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0989Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0980Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0980Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0978Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0983Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0984Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0983Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0972Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0967Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0961Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0956Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0962Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0958Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0947Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0944Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0939Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0936Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0929Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0937Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0926Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0924Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0925Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0923Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0917Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0917Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0926Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0924Epoch 3/15: [==============================] 75/75 batches, loss: 0.0926
[2025-05-07 20:50:39,111][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0926
[2025-05-07 20:50:39,315][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0221, Metrics: {'mse': 0.021844685077667236, 'rmse': 0.14779947590457565, 'r2': -0.09328937530517578}
[2025-05-07 20:50:39,316][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1270Epoch 4/15: [                              ] 2/75 batches, loss: 0.1006Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1299Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1202Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1029Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0955Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0944Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0895Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0886Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0864Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0930Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0903Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0879Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0879Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0862Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0864Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0855Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0834Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0854Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0859Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0847Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0839Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0839Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0848Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0837Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0817Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0834Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0822Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0812Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0810Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0811Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0811Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0802Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0799Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0790Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0784Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0778Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0779Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0775Epoch 4/15: [================              ] 40/75 batches, loss: 0.0779Epoch 4/15: [================              ] 41/75 batches, loss: 0.0774Epoch 4/15: [================              ] 42/75 batches, loss: 0.0771Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0770Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0764Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0761Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0772Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0774Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0773Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0769Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0764Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0761Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0758Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0758Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0753Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0747Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0747Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0745Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0751Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0749Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0749Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0753Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0749Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0751Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0750Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0750Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0747Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0751Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0748Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0752Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0750Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0746Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0748Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0749Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0751Epoch 4/15: [==============================] 75/75 batches, loss: 0.0748
[2025-05-07 20:50:41,616][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0748
[2025-05-07 20:50:41,819][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0167, Metrics: {'mse': 0.01642604172229767, 'rmse': 0.12816412026108426, 'r2': 0.17790448665618896}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0476Epoch 5/15: [                              ] 2/75 batches, loss: 0.0569Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0517Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0545Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0547Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0632Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0652Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0674Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0690Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0643Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0699Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0701Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0730Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0714Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0689Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0692Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0689Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0670Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0662Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0650Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0635Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0672Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0669Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0666Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0670Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0664Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0673Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0658Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0668Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0671Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0678Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0669Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0661Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0657Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0662Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0655Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0656Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0651Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0662Epoch 5/15: [================              ] 40/75 batches, loss: 0.0659Epoch 5/15: [================              ] 41/75 batches, loss: 0.0652Epoch 5/15: [================              ] 42/75 batches, loss: 0.0663Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0656Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0652Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0647Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0647Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0647Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0642Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0641Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0635Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0634Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0631Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0628Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0627Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0621Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0615Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0623Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0621Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0633Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0642Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0639Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0635Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0638Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0636Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0631Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0629Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0630Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0629Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0626Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0622Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0617Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0612Epoch 5/15: [==============================] 75/75 batches, loss: 0.0612
[2025-05-07 20:50:44,509][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0612
[2025-05-07 20:50:44,803][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0166, Metrics: {'mse': 0.016378561034798622, 'rmse': 0.12797875227864436, 'r2': 0.180280864238739}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0482Epoch 6/15: [                              ] 2/75 batches, loss: 0.0612Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0552Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0513Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0613Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0623Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0626Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0629Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0595Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0576Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0553Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0580Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0556Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0561Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0553Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0554Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0552Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0541Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0525Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0522Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0522Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0513Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0502Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0508Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0506Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0502Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0509Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0511Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0515Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0508Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0514Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0516Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0515Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0519Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0527Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0533Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0537Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0538Epoch 6/15: [================              ] 40/75 batches, loss: 0.0541Epoch 6/15: [================              ] 41/75 batches, loss: 0.0546Epoch 6/15: [================              ] 42/75 batches, loss: 0.0542Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0538Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0537Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0531Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0531Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0528Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0526Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0525Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0529Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0525Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0522Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0531Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0526Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0524Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0521Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0521Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0518Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0520Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0516Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0514Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0515Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0513Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0512Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0509Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0507Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0506Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0504Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0504Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0501Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0503Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0501Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0502Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0499Epoch 6/15: [==============================] 75/75 batches, loss: 0.0504
[2025-05-07 20:50:47,455][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0504
[2025-05-07 20:50:47,656][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0121, Metrics: {'mse': 0.012021418660879135, 'rmse': 0.10964223028048606, 'r2': 0.39834845066070557}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0410Epoch 7/15: [                              ] 2/75 batches, loss: 0.0513Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0465Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0488Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0490Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0499Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0552Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0524Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0516Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0508Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0490Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0522Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0518Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0510Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0503Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0510Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0514Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0519Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0507Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0496Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0490Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0485Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0480Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0476Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0485Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0484Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0491Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0501Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0498Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0490Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0493Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0511Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0513Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0510Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0509Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0507Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0512Epoch 7/15: [================              ] 40/75 batches, loss: 0.0511Epoch 7/15: [================              ] 41/75 batches, loss: 0.0505Epoch 7/15: [================              ] 42/75 batches, loss: 0.0504Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0500Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0499Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0499Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0496Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0497Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0496Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0493Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0488Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0484Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0478Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0475Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0470Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0469Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0473Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0470Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0471Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0468Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0470Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0465Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0464Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0462Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0462Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0460Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0463Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0464Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0465Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0462Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0465Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0463Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0462Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0459Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0458Epoch 7/15: [==============================] 75/75 batches, loss: 0.0458
[2025-05-07 20:50:50,354][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0458
[2025-05-07 20:50:50,594][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0166, Metrics: {'mse': 0.01626688987016678, 'rmse': 0.12754171815593038, 'r2': 0.18586981296539307}
[2025-05-07 20:50:50,595][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0557Epoch 8/15: [                              ] 2/75 batches, loss: 0.0407Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0459Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0472Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0446Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0439Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0421Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0432Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0409Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0404Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0424Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0410Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0411Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0409Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0436Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0421Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0428Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0440Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0448Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0458Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0458Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0454Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0460Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0460Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0459Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0448Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0455Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0451Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0452Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0458Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0452Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0457Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0453Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0452Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0456Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0454Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0448Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0444Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0442Epoch 8/15: [================              ] 40/75 batches, loss: 0.0442Epoch 8/15: [================              ] 41/75 batches, loss: 0.0440Epoch 8/15: [================              ] 42/75 batches, loss: 0.0442Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0436Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0432Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0431Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0429Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0426Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0424Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0418Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0424Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0424Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0420Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0416Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0413Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0412Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0414Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0412Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0412Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0411Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0409Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0407Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0403Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0404Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0403Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0401Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0401Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0402Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0402Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0402Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0399Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0399Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0399Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0399Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0398Epoch 8/15: [==============================] 75/75 batches, loss: 0.0398
[2025-05-07 20:50:52,879][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0398
[2025-05-07 20:50:53,103][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0172, Metrics: {'mse': 0.01689680479466915, 'rmse': 0.12998771016780453, 'r2': 0.15434366464614868}
[2025-05-07 20:50:53,104][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0180Epoch 9/15: [                              ] 2/75 batches, loss: 0.0252Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0374Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0368Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0374Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0401Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0403Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0393Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0380Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0401Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0403Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0392Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0377Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0377Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0390Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0391Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0389Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0384Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0389Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0379Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0373Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0378Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0374Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0374Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0376Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0378Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0378Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0377Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0377Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0380Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0374Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0372Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0379Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0377Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0378Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0372Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0374Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0384Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0381Epoch 9/15: [================              ] 40/75 batches, loss: 0.0380Epoch 9/15: [================              ] 41/75 batches, loss: 0.0374Epoch 9/15: [================              ] 42/75 batches, loss: 0.0372Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0378Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0384Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0381Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0387Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0386Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0382Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0381Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0381Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0381Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0377Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0375Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0379Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0377Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0379Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0377Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0375Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0376Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0377Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0377Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0375Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0375Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0378Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0379Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0380Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0378Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0377Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0376Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0375Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0374Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0374Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0374Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0373Epoch 9/15: [==============================] 75/75 batches, loss: 0.0371
[2025-05-07 20:50:55,448][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0371
[2025-05-07 20:50:55,710][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0138, Metrics: {'mse': 0.013553141616284847, 'rmse': 0.11641796088355459, 'r2': 0.32168835401535034}
[2025-05-07 20:50:55,711][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0494Epoch 10/15: [                              ] 2/75 batches, loss: 0.0410Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0420Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0422Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0368Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0341Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0360Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0394Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0391Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0394Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0382Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0382Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0372Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0361Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0359Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0356Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0366Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0363Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0352Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0350Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0354Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0353Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0345Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0347Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0356Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0356Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0359Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0355Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0354Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0356Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0352Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0347Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0343Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0341Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0342Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0341Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0342Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0342Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0339Epoch 10/15: [================              ] 40/75 batches, loss: 0.0337Epoch 10/15: [================              ] 41/75 batches, loss: 0.0337Epoch 10/15: [================              ] 42/75 batches, loss: 0.0339Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0339Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0337Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0339Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0344Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0344Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0346Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0344Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0346Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0346Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0347Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0345Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0348Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0348Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0349Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0347Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0350Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0349Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0345Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0348Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0349Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0351Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0348Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0349Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0345Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0344Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0343Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0347Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0347Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0347Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0344Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0344Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0342Epoch 10/15: [==============================] 75/75 batches, loss: 0.0345
[2025-05-07 20:50:58,065][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0345
[2025-05-07 20:50:58,292][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0119, Metrics: {'mse': 0.011694831773638725, 'rmse': 0.10814264549029086, 'r2': 0.41469353437423706}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0172Epoch 11/15: [                              ] 2/75 batches, loss: 0.0258Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0269Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0294Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0287Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0299Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0330Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0346Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0343Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0320Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0318Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0320Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0326Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0316Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0312Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0314Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0312Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0317Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0325Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0330Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0339Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0338Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0336Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0335Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0342Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0349Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0341Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0342Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0342Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0338Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0338Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0342Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0343Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0340Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0338Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0338Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0335Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0329Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0328Epoch 11/15: [================              ] 40/75 batches, loss: 0.0326Epoch 11/15: [================              ] 41/75 batches, loss: 0.0325Epoch 11/15: [================              ] 42/75 batches, loss: 0.0327Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0331Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0329Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0333Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0331Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0329Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0330Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0331Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0330Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0326Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0322Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0320Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0319Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0318Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0319Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0317Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0317Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0318Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0320Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0319Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0317Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0318Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0317Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0314Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0318Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0318Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0317Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0318Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0318Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0317Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0321Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0324Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0328Epoch 11/15: [==============================] 75/75 batches, loss: 0.0329
[2025-05-07 20:51:00,988][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0329
[2025-05-07 20:51:01,209][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0109, Metrics: {'mse': 0.010771894827485085, 'rmse': 0.1037877392926789, 'r2': 0.4608849883079529}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0279Epoch 12/15: [                              ] 2/75 batches, loss: 0.0286Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0268Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0239Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0264Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0258Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0289Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0281Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0273Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0293Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0296Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0300Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0317Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0310Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0304Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0306Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0304Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0302Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0300Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0296Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0296Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0294Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0292Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0288Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0288Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0286Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0285Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0289Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0292Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0296Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0299Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0300Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0301Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0300Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0299Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0300Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0299Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0298Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0296Epoch 12/15: [================              ] 40/75 batches, loss: 0.0301Epoch 12/15: [================              ] 41/75 batches, loss: 0.0303Epoch 12/15: [================              ] 42/75 batches, loss: 0.0304Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0303Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0301Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0301Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0299Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0301Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0302Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0301Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0297Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0298Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0298Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0306Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0305Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0307Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0306Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0303Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0305Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0304Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0303Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0305Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0304Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0305Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0305Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0304Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0302Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0302Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0300Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0297Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0298Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0296Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0296Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0295Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0295Epoch 12/15: [==============================] 75/75 batches, loss: 0.0296
[2025-05-07 20:51:03,897][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0296
[2025-05-07 20:51:04,146][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0130, Metrics: {'mse': 0.012746304273605347, 'rmse': 0.11289953176876044, 'r2': 0.36206912994384766}
[2025-05-07 20:51:04,147][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0626Epoch 13/15: [                              ] 2/75 batches, loss: 0.0504Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0426Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0377Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0371Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0334Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0308Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0344Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0338Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0338Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0335Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0330Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0322Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0327Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0326Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0315Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0310Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0301Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0324Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0332Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0339Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0334Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0330Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0324Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0324Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0326Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0323Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0318Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0317Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0318Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0318Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0318Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0312Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0309Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0310Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0317Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0318Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0317Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0318Epoch 13/15: [================              ] 40/75 batches, loss: 0.0315Epoch 13/15: [================              ] 41/75 batches, loss: 0.0312Epoch 13/15: [================              ] 42/75 batches, loss: 0.0311Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0314Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0313Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0311Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0307Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0305Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0304Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0310Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0310Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0306Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0303Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0299Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0297Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0298Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0297Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0298Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0298Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0303Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0300Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0302Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0302Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0303Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0303Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0301Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0301Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0299Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0300Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0300Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0298Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0297Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0296Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0298Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0297Epoch 13/15: [==============================] 75/75 batches, loss: 0.0297
[2025-05-07 20:51:06,432][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0297
[2025-05-07 20:51:06,640][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0098, Metrics: {'mse': 0.009703745134174824, 'rmse': 0.09850758922121089, 'r2': 0.5143440961837769}
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0116Epoch 14/15: [                              ] 2/75 batches, loss: 0.0255Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0232Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0239Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0226Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0217Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0212Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0241Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0261Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0282Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0284Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0269Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0262Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0281Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0278Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0275Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0275Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0275Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0273Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0266Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0261Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0261Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0264Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0269Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0267Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0267Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0264Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0263Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0261Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0258Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0254Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0253Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0262Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0260Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0267Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0263Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0265Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0269Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0267Epoch 14/15: [================              ] 40/75 batches, loss: 0.0268Epoch 14/15: [================              ] 41/75 batches, loss: 0.0268Epoch 14/15: [================              ] 42/75 batches, loss: 0.0269Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0271Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0269Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0277Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0273Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0271Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0270Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0267Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0274Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0278Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0276Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0275Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0272Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0270Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0273Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0272Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0271Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0270Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0271Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0271Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0270Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0269Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0271Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0269Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0269Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0268Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0269Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0270Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0271Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0269Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0267Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0266Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0265Epoch 14/15: [==============================] 75/75 batches, loss: 0.0269
[2025-05-07 20:51:09,398][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0269
[2025-05-07 20:51:09,628][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0095, Metrics: {'mse': 0.009406407363712788, 'rmse': 0.09698663497468497, 'r2': 0.5292253494262695}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0239Epoch 15/15: [                              ] 2/75 batches, loss: 0.0206Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0209Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0217Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0214Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0207Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0216Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0217Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0211Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0206Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0207Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0205Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0228Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0223Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0213Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0215Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0214Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0220Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0225Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0229Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0238Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0234Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0232Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0230Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0230Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0227Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0227Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0227Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0225Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0225Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0226Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0227Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0236Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0236Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0234Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0233Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0234Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0232Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0234Epoch 15/15: [================              ] 40/75 batches, loss: 0.0233Epoch 15/15: [================              ] 41/75 batches, loss: 0.0230Epoch 15/15: [================              ] 42/75 batches, loss: 0.0232Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0238Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0237Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0237Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0240Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0239Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0240Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0240Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0241Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0240Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0242Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0241Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0244Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0243Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0242Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0241Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0243Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0241Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0241Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0243Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0242Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0242Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0241Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0243Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0242Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0241Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0243Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0244Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0243Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0244Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0243Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0242Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0241Epoch 15/15: [==============================] 75/75 batches, loss: 0.0239
[2025-05-07 20:51:12,301][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0239
[2025-05-07 20:51:12,511][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0094, Metrics: {'mse': 0.00929378904402256, 'rmse': 0.09640429992496476, 'r2': 0.53486168384552}
[2025-05-07 20:51:12,910][src.training.lm_trainer][INFO] - Training completed in 42.91 seconds
[2025-05-07 20:51:12,910][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:51:15,755][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.010003352537751198, 'rmse': 0.10001676128405278, 'r2': 0.5186362266540527}
[2025-05-07 20:51:15,756][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.00929378904402256, 'rmse': 0.09640429992496476, 'r2': 0.53486168384552}
[2025-05-07 20:51:15,756][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.029042011126875877, 'rmse': 0.17041716793467693, 'r2': 0.513195276260376}
[2025-05-07 20:51:17,446][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ja/ja/model.pt
[2025-05-07 20:51:17,447][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▄▂▂▂▁▁▁
wandb:     best_val_mse █▆▄▄▂▂▂▁▁▁
wandb:      best_val_r2 ▁▃▅▅▇▇▇███
wandb:    best_val_rmse █▆▅▅▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▃▅▅▆▅▅▆▆▇▆▇▇
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▇▄▄▂▄▄▃▂▂▃▁▁▁
wandb:          val_mse █▆▆▄▄▂▄▄▃▂▂▃▁▁▁
wandb:           val_r2 ▁▃▃▅▅▇▅▅▆▇▇▆███
wandb:         val_rmse █▆▇▅▅▂▄▅▃▂▂▃▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.00943
wandb:     best_val_mse 0.00929
wandb:      best_val_r2 0.53486
wandb:    best_val_rmse 0.0964
wandb:            epoch 15
wandb:   final_test_mse 0.02904
wandb:    final_test_r2 0.5132
wandb:  final_test_rmse 0.17042
wandb:  final_train_mse 0.01
wandb:   final_train_r2 0.51864
wandb: final_train_rmse 0.10002
wandb:    final_val_mse 0.00929
wandb:     final_val_r2 0.53486
wandb:   final_val_rmse 0.0964
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02387
wandb:       train_time 42.90764
wandb:         val_loss 0.00943
wandb:          val_mse 0.00929
wandb:           val_r2 0.53486
wandb:         val_rmse 0.0964
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205015-ml265hv2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205015-ml265hv2/logs
Experiment probe_layer2_n_tokens_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/layer2/ja/ja/results.json for layer 2
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:51:40,147][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja
experiment_name: probe_layer2_avg_links_len_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:51:40,147][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:51:40,147][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:51:40,148][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:51:40,148][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:51:40,153][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:51:40,153][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:51:40,153][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:51:43,052][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:51:45,279][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:51:45,280][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:51:45,463][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 20:51:45,534][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 20:51:45,748][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:51:45,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:51:45,757][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:51:45,758][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:51:45,797][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:51:45,846][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:51:45,861][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:51:45,862][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:51:45,862][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:51:45,864][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:51:45,903][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:51:45,973][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:51:46,000][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:51:46,002][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:51:46,002][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:51:46,003][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:51:46,004][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:51:46,004][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:51:46,004][src.data.datasets][INFO] - Sample label: 0.23199999332427979
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:51:46,005][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 20:51:46,005][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 20:51:46,005][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:51:46,006][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:51:46,006][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:51:46,006][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:51:46,007][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:51:46,007][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:51:46,007][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:51:51,767][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:51:51,768][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:51:51,768][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:51:51,768][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:51:51,771][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:51:51,771][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:51:51,771][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:51:51,771][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:51:51,772][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:51:51,772][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:51:51,773][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5818Epoch 1/15: [                              ] 2/75 batches, loss: 0.5458Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4949Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4910Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4652Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4275Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4167Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4213Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3992Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3795Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3682Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3855Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3678Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3828Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3785Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3870Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3827Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3989Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3909Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3916Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3815Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3836Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3711Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3612Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3564Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3512Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3476Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3423Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3417Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3357Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3316Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3287Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3301Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3288Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3258Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3286Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3249Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3203Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3203Epoch 1/15: [================              ] 40/75 batches, loss: 0.3150Epoch 1/15: [================              ] 41/75 batches, loss: 0.3113Epoch 1/15: [================              ] 42/75 batches, loss: 0.3114Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3095Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3105Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3092Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3052Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3009Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2977Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2953Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2927Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2933Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2978Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2939Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2949Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2947Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2908Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2875Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2865Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2846Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2832Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2815Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2803Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2808Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2810Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2805Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2783Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2774Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2749Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2738Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2715Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2703Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2710Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2688Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2678Epoch 1/15: [==============================] 75/75 batches, loss: 0.2651
[2025-05-07 20:51:57,709][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2651
[2025-05-07 20:51:57,968][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0385, Metrics: {'mse': 0.038955315947532654, 'rmse': 0.19737101090973988, 'r2': -1.185093641281128}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1208Epoch 2/15: [                              ] 2/75 batches, loss: 0.1580Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1729Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2011Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1802Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1653Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1787Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1724Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1976Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2017Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1913Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1893Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1894Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1834Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1837Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1826Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1794Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1772Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1728Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1686Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1659Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1652Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1618Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1591Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1543Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1570Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1568Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1550Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1548Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1534Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1538Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1552Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1536Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1518Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1504Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1484Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1498Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1481Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1465Epoch 2/15: [================              ] 40/75 batches, loss: 0.1458Epoch 2/15: [================              ] 41/75 batches, loss: 0.1446Epoch 2/15: [================              ] 42/75 batches, loss: 0.1457Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1443Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1463Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1448Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1431Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1420Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1404Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1395Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1388Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1382Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1375Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1368Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1365Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1350Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1355Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1339Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1327Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1318Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1314Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1314Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1320Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1317Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1306Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1292Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1282Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1293Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1293Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1286Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1287Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1284Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1283Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1271Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1266Epoch 2/15: [==============================] 75/75 batches, loss: 0.1271
[2025-05-07 20:52:00,661][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1271
[2025-05-07 20:52:00,852][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0434, Metrics: {'mse': 0.04384840652346611, 'rmse': 0.20940011108752093, 'r2': -1.4595582485198975}
[2025-05-07 20:52:00,853][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1741Epoch 3/15: [                              ] 2/75 batches, loss: 0.1333Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1126Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1039Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0991Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1029Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0975Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1011Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1022Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0974Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0961Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0932Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0939Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0910Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0899Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0875Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0847Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0855Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0854Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0876Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0910Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0900Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0903Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0883Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0858Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0855Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0888Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0895Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0885Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0871Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0878Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0929Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0936Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0946Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0941Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0937Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0950Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0937Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0926Epoch 3/15: [================              ] 40/75 batches, loss: 0.0915Epoch 3/15: [================              ] 41/75 batches, loss: 0.0915Epoch 3/15: [================              ] 42/75 batches, loss: 0.0911Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0908Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0907Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0905Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0900Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0896Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0886Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0881Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0878Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0871Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0871Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0867Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0857Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0851Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0850Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0846Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0857Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0856Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0844Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0840Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0834Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0830Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0827Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0832Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0825Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0816Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0823Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0825Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0823Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0818Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0819Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0826Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0825Epoch 3/15: [==============================] 75/75 batches, loss: 0.0826
[2025-05-07 20:52:03,115][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0826
[2025-05-07 20:52:03,324][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0318, Metrics: {'mse': 0.03223707899451256, 'rmse': 0.17954687130248903, 'r2': -0.808252215385437}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0813Epoch 4/15: [                              ] 2/75 batches, loss: 0.0785Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0979Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1006Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0849Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0811Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0797Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0766Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0738Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0688Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0757Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0737Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0714Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0706Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0681Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0703Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0699Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0717Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0743Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0753Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0734Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0727Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0722Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0733Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0721Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0710Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0710Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0698Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0693Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0687Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0685Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0684Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0680Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0671Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0669Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0662Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0650Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0649Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0649Epoch 4/15: [================              ] 40/75 batches, loss: 0.0655Epoch 4/15: [================              ] 41/75 batches, loss: 0.0648Epoch 4/15: [================              ] 42/75 batches, loss: 0.0650Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0656Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0653Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0650Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0653Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0651Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0657Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0650Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0639Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0636Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0640Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0633Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0628Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0635Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0632Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0638Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0636Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0630Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0631Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0629Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0631Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0632Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0631Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0630Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0634Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0634Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0635Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0635Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0632Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0632Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0631Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0638Epoch 4/15: [==============================] 75/75 batches, loss: 0.0637
[2025-05-07 20:52:06,002][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0637
[2025-05-07 20:52:06,210][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0317, Metrics: {'mse': 0.03218964487314224, 'rmse': 0.17941472869623118, 'r2': -0.8055914640426636}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0615Epoch 5/15: [                              ] 2/75 batches, loss: 0.0619Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0522Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0520Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0519Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0594Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0598Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0564Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0573Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0558Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0621Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0605Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0591Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0577Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0569Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0577Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0577Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0579Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0569Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0556Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0544Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0560Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0575Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0567Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0574Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0579Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0575Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0582Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0574Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0573Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0566Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0565Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0564Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0571Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0565Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0562Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0562Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0559Epoch 5/15: [================              ] 40/75 batches, loss: 0.0552Epoch 5/15: [================              ] 41/75 batches, loss: 0.0552Epoch 5/15: [================              ] 42/75 batches, loss: 0.0567Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0563Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0556Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0553Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0554Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0551Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0551Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0548Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0548Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0543Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0554Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0559Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0561Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0561Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0560Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0556Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0556Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0565Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0561Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0567Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0572Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0570Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0564Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0571Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0572Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0569Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0567Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0567Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0566Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0563Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0561Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0559Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0557Epoch 5/15: [==============================] 75/75 batches, loss: 0.0563
[2025-05-07 20:52:08,840][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0563
[2025-05-07 20:52:09,049][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0242, Metrics: {'mse': 0.02461838163435459, 'rmse': 0.1569024589812237, 'r2': -0.38090193271636963}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0356Epoch 6/15: [                              ] 2/75 batches, loss: 0.0481Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0453Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0479Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0587Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0581Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0570Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0552Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0531Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0524Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0498Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0523Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0506Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0516Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0503Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0514Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0527Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0521Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0518Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0515Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0513Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0501Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0499Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0497Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0512Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0508Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0501Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0502Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0502Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0504Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0497Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0500Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0489Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0484Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0480Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0483Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0477Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0482Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0482Epoch 6/15: [================              ] 40/75 batches, loss: 0.0486Epoch 6/15: [================              ] 41/75 batches, loss: 0.0492Epoch 6/15: [================              ] 42/75 batches, loss: 0.0493Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0489Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0490Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0486Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0485Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0482Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0482Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0482Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0492Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0490Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0483Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0486Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0481Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0477Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0478Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0477Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0480Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0477Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0480Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0479Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0477Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0476Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0475Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0472Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0470Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0476Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0473Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0471Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0469Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0468Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0466Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0462Epoch 6/15: [==============================] 75/75 batches, loss: 0.0466
[2025-05-07 20:52:11,678][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0466
[2025-05-07 20:52:11,883][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0210, Metrics: {'mse': 0.021447187289595604, 'rmse': 0.14644858240896566, 'r2': -0.20302224159240723}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0263Epoch 7/15: [                              ] 2/75 batches, loss: 0.0346Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0335Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0301Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0362Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0370Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0384Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0418Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0417Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0430Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0414Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0398Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0411Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0404Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0394Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0388Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0375Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0382Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0382Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0380Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0371Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0361Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0369Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0376Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0382Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0381Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0386Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0382Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0391Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0401Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0398Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0394Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0392Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0395Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0391Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0392Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0391Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0388Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0394Epoch 7/15: [================              ] 40/75 batches, loss: 0.0397Epoch 7/15: [================              ] 41/75 batches, loss: 0.0397Epoch 7/15: [================              ] 42/75 batches, loss: 0.0395Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0395Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0392Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0388Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0385Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0387Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0384Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0387Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0383Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0378Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0378Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0375Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0383Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0384Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0384Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0384Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0383Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0386Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0384Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0381Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0381Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0379Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0378Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0379Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0378Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0375Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0374Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0374Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0372Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0369Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0368Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0366Epoch 7/15: [==============================] 75/75 batches, loss: 0.0363
[2025-05-07 20:52:14,602][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0363
[2025-05-07 20:52:14,808][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0281, Metrics: {'mse': 0.028506271541118622, 'rmse': 0.16883800384131123, 'r2': -0.598982572555542}
[2025-05-07 20:52:14,809][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0263Epoch 8/15: [                              ] 2/75 batches, loss: 0.0230Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0283Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0311Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0298Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0317Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0310Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0309Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0295Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0296Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0299Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0297Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0310Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0314Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0323Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0317Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0315Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0317Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0322Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0326Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0323Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0329Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0332Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0328Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0330Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0330Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0343Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0347Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0343Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0339Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0337Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0339Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0339Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0347Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0344Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0340Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0337Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0335Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0338Epoch 8/15: [================              ] 40/75 batches, loss: 0.0335Epoch 8/15: [================              ] 41/75 batches, loss: 0.0330Epoch 8/15: [================              ] 42/75 batches, loss: 0.0332Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0329Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0330Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0333Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0332Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0328Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0329Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0327Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0340Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0338Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0335Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0335Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0334Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0332Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0331Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0332Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0332Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0329Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0328Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0326Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0325Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0323Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0322Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0322Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0320Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0320Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0319Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0318Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0316Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0321Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0324Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0327Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0329Epoch 8/15: [==============================] 75/75 batches, loss: 0.0327
[2025-05-07 20:52:17,086][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0327
[2025-05-07 20:52:17,291][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0278, Metrics: {'mse': 0.02822018787264824, 'rmse': 0.16798865399975155, 'r2': -0.5829354524612427}
[2025-05-07 20:52:17,292][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0332Epoch 9/15: [                              ] 2/75 batches, loss: 0.0324Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0341Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0312Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0328Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0321Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0330Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0315Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0313Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0312Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0322Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0321Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0330Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0339Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0355Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0364Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0365Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0360Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0354Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0350Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0344Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0341Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0341Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0343Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0336Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0332Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0322Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0320Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0326Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0327Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0320Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0320Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0328Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0325Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0327Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0323Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0322Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0331Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0330Epoch 9/15: [================              ] 40/75 batches, loss: 0.0327Epoch 9/15: [================              ] 41/75 batches, loss: 0.0324Epoch 9/15: [================              ] 42/75 batches, loss: 0.0323Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0327Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0324Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0321Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0323Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0320Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0319Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0318Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0316Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0314Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0314Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0311Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0315Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0315Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0315Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0314Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0313Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0311Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0310Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0310Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0307Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0307Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0307Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0309Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0307Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0305Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0304Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0305Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0302Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0301Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0301Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0301Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0300Epoch 9/15: [==============================] 75/75 batches, loss: 0.0297
[2025-05-07 20:52:19,590][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0297
[2025-05-07 20:52:19,799][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0274, Metrics: {'mse': 0.02785332314670086, 'rmse': 0.1668931488908423, 'r2': -0.5623571872711182}
[2025-05-07 20:52:19,799][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0493Epoch 10/15: [                              ] 2/75 batches, loss: 0.0417Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0430Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0389Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0348Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0312Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0302Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0290Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0279Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0286Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0274Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0288Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0297Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0304Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0303Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0313Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0312Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0315Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0311Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0308Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0314Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0314Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0311Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0314Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0312Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0312Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0308Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0308Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0305Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0320Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0316Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0314Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0317Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0316Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0313Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0310Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0310Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0306Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0303Epoch 10/15: [================              ] 40/75 batches, loss: 0.0300Epoch 10/15: [================              ] 41/75 batches, loss: 0.0299Epoch 10/15: [================              ] 42/75 batches, loss: 0.0299Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0302Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0303Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0307Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0303Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0301Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0306Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0304Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0306Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0304Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0302Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0301Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0301Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0302Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0305Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0306Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0308Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0307Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0304Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0303Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0303Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0302Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0301Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0302Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0302Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0303Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0304Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0305Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0305Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0305Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0304Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0304Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0304Epoch 10/15: [==============================] 75/75 batches, loss: 0.0301
[2025-05-07 20:52:22,087][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0301
[2025-05-07 20:52:22,298][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0278, Metrics: {'mse': 0.02824043110013008, 'rmse': 0.16804889496848852, 'r2': -0.5840710401535034}
[2025-05-07 20:52:22,299][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:52:22,299][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:52:22,299][src.training.lm_trainer][INFO] - Training completed in 27.81 seconds
[2025-05-07 20:52:22,299][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:52:25,143][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.00963132455945015, 'rmse': 0.0981393119980477, 'r2': -0.03543508052825928}
[2025-05-07 20:52:25,144][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.021447187289595604, 'rmse': 0.14644858240896566, 'r2': -0.20302224159240723}
[2025-05-07 20:52:25,144][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.1185237392783165, 'rmse': 0.34427276871445484, 'r2': -1.787543535232544}
[2025-05-07 20:52:26,845][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja/ja/model.pt
[2025-05-07 20:52:26,846][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▂▁
wandb:     best_val_mse █▅▅▂▁
wandb:      best_val_r2 ▁▄▄▇█
wandb:    best_val_rmse █▆▆▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▄▆▆▅▅▅
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▄▄▂▁▃▃▃▃
wandb:          val_mse ▆█▄▄▂▁▃▃▃▃
wandb:           val_r2 ▃▁▅▅▇█▆▆▆▆
wandb:         val_rmse ▇█▅▅▂▁▃▃▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02104
wandb:     best_val_mse 0.02145
wandb:      best_val_r2 -0.20302
wandb:    best_val_rmse 0.14645
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.11852
wandb:    final_test_r2 -1.78754
wandb:  final_test_rmse 0.34427
wandb:  final_train_mse 0.00963
wandb:   final_train_r2 -0.03544
wandb: final_train_rmse 0.09814
wandb:    final_val_mse 0.02145
wandb:     final_val_r2 -0.20302
wandb:   final_val_rmse 0.14645
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03014
wandb:       train_time 27.81128
wandb:         val_loss 0.02777
wandb:          val_mse 0.02824
wandb:           val_r2 -0.58407
wandb:         val_rmse 0.16805
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205140-67o0d984
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205140-67o0d984/logs
Experiment probe_layer2_avg_links_len_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:52:49,248][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ja
experiment_name: probe_layer2_avg_links_len_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:52:49,248][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:52:49,248][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:52:49,248][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:52:49,248][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:52:49,253][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:52:49,253][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:52:49,253][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:52:51,468][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:52:53,772][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:52:53,772][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:52:53,877][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 20:52:53,948][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 20:52:54,080][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:52:54,090][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:52:54,092][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:52:54,093][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:52:54,132][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:52:54,181][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:52:54,199][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:52:54,200][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:52:54,200][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:52:54,203][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:52:54,242][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:52:54,284][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:52:54,300][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:52:54,302][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:52:54,302][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:52:54,303][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:52:54,304][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:52:54,304][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:52:54,304][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:52:54,304][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:52:54,304][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:52:54,304][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Sample label: 0.1899999976158142
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:52:54,305][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 20:52:54,305][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:52:54,305][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:52:54,306][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:52:54,306][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:52:54,306][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:52:54,307][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:52:54,307][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:52:54,307][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:53:00,022][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:53:00,023][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:53:00,023][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:53:00,023][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:53:00,026][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:53:00,027][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:53:00,027][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:53:00,027][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:53:00,027][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:53:00,028][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:53:00,028][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5046Epoch 1/15: [                              ] 2/75 batches, loss: 0.4786Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4632Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4509Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4408Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4101Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4000Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4180Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3968Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3801Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3686Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3799Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3633Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3729Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3700Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3809Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3798Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3972Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3865Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3858Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3799Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3790Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3667Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3560Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3526Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3463Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3420Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3363Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3350Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3288Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3236Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3207Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3191Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3188Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3177Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3199Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3161Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3143Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3133Epoch 1/15: [================              ] 40/75 batches, loss: 0.3079Epoch 1/15: [================              ] 41/75 batches, loss: 0.3046Epoch 1/15: [================              ] 42/75 batches, loss: 0.3049Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3042Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3075Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3048Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3007Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2975Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2956Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2938Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2910Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2908Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2955Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2919Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2926Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2917Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2876Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2845Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2835Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2818Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2809Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2789Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2775Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2769Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2765Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2765Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2738Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2736Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2707Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2691Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2672Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2665Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2673Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2649Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2643Epoch 1/15: [==============================] 75/75 batches, loss: 0.2615
[2025-05-07 20:53:05,951][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2615
[2025-05-07 20:53:06,151][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0424, Metrics: {'mse': 0.04288017377257347, 'rmse': 0.20707528527705443, 'r2': -1.4052479267120361}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1457Epoch 2/15: [                              ] 2/75 batches, loss: 0.1588Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1541Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1833Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1714Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1552Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1680Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1646Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1753Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1784Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1715Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1690Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1689Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1639Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1645Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1644Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1613Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1600Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1565Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1521Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1493Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1480Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1461Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1444Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1409Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1435Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1445Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1433Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1441Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1436Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1432Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1448Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1430Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1415Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1411Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1396Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1411Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1396Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1388Epoch 2/15: [================              ] 40/75 batches, loss: 0.1380Epoch 2/15: [================              ] 41/75 batches, loss: 0.1365Epoch 2/15: [================              ] 42/75 batches, loss: 0.1379Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1371Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1401Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1381Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1364Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1354Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1342Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1337Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1327Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1325Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1317Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1311Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1315Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1303Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1315Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1303Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1289Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1281Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1278Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1275Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1277Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1274Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1266Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1254Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1244Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1255Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1256Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1250Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1250Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1244Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1240Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1228Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1225Epoch 2/15: [==============================] 75/75 batches, loss: 0.1235
[2025-05-07 20:53:08,838][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1235
[2025-05-07 20:53:09,040][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0420, Metrics: {'mse': 0.042507804930210114, 'rmse': 0.20617421014814175, 'r2': -1.3843610286712646}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1293Epoch 3/15: [                              ] 2/75 batches, loss: 0.1072Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0993Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0898Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1000Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1034Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0995Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0988Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0977Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0932Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0954Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0955Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0956Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0910Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0899Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0901Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0868Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0862Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0854Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0872Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0914Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0901Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0902Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0884Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0856Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0860Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0884Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0892Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0892Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0876Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0873Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0920Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0925Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0928Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0924Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0917Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0923Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0911Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0900Epoch 3/15: [================              ] 40/75 batches, loss: 0.0887Epoch 3/15: [================              ] 41/75 batches, loss: 0.0883Epoch 3/15: [================              ] 42/75 batches, loss: 0.0881Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0882Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0874Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0876Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0873Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0870Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0861Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0856Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0859Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0856Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0857Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0854Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0846Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0842Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0837Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0833Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0843Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0836Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0826Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0824Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0818Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0816Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0812Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0815Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0811Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0802Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0801Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0801Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0801Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0796Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0799Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0805Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0804Epoch 3/15: [==============================] 75/75 batches, loss: 0.0801
[2025-05-07 20:53:11,750][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0801
[2025-05-07 20:53:11,947][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0358, Metrics: {'mse': 0.03630097210407257, 'rmse': 0.19052813992707893, 'r2': -1.036205530166626}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0780Epoch 4/15: [                              ] 2/75 batches, loss: 0.0612Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0795Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0830Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0703Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0668Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0659Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0613Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0625Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0631Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0669Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0668Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0668Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0656Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0643Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0664Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0657Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0646Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0662Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0682Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0664Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0664Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0674Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0665Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0656Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0662Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0655Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0653Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0656Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0654Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0653Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0656Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0660Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0661Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0654Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0652Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0648Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0659Epoch 4/15: [================              ] 40/75 batches, loss: 0.0665Epoch 4/15: [================              ] 41/75 batches, loss: 0.0659Epoch 4/15: [================              ] 42/75 batches, loss: 0.0660Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0668Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0660Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0655Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0658Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0657Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0664Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0656Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0648Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0645Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0643Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0648Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0643Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0639Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0641Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0638Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0647Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0647Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0646Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0650Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0649Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0654Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0654Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0650Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0649Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0651Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0649Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0652Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0649Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0648Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0647Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0647Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0650Epoch 4/15: [==============================] 75/75 batches, loss: 0.0649
[2025-05-07 20:53:14,584][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0649
[2025-05-07 20:53:14,801][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0322, Metrics: {'mse': 0.03270915523171425, 'rmse': 0.18085672570218186, 'r2': -0.8347320556640625}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0591Epoch 5/15: [                              ] 2/75 batches, loss: 0.0547Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0505Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0504Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0484Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0524Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0555Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0524Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0550Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0532Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0582Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0572Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0603Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0586Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0575Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0567Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0557Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0568Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0564Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0558Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0542Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0559Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0570Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0568Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0577Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0571Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0573Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0562Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0567Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0561Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0565Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0562Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0556Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0555Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0552Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0549Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0544Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0548Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0548Epoch 5/15: [================              ] 40/75 batches, loss: 0.0545Epoch 5/15: [================              ] 41/75 batches, loss: 0.0543Epoch 5/15: [================              ] 42/75 batches, loss: 0.0554Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0549Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0540Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0538Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0538Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0537Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0541Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0539Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0537Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0531Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0532Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0535Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0540Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0541Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0539Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0536Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0534Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0545Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0543Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0548Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0557Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0554Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0549Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0552Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0553Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0552Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0554Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0554Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0556Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0552Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0550Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0547Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0546Epoch 5/15: [==============================] 75/75 batches, loss: 0.0550
[2025-05-07 20:53:17,444][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0550
[2025-05-07 20:53:17,656][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0305, Metrics: {'mse': 0.030970731750130653, 'rmse': 0.17598503274463614, 'r2': -0.7372199296951294}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0445Epoch 6/15: [                              ] 2/75 batches, loss: 0.0679Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0584Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0594Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0671Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0607Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0576Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0575Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0555Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0537Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0513Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0517Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0493Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0491Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0486Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0488Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0495Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0479Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0474Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0464Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0467Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0462Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0460Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0455Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0459Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0457Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0454Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0460Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0475Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0477Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0477Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0471Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0466Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0464Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0483Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0476Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0482Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0476Epoch 6/15: [================              ] 40/75 batches, loss: 0.0480Epoch 6/15: [================              ] 41/75 batches, loss: 0.0482Epoch 6/15: [================              ] 42/75 batches, loss: 0.0485Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0478Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0477Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0471Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0468Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0468Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0468Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0470Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0474Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0470Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0464Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0464Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0461Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0459Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0460Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0461Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0460Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0458Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0456Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0457Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0459Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0456Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0454Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0453Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0454Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0449Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0448Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0448Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0446Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0444Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0443Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0444Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0441Epoch 6/15: [==============================] 75/75 batches, loss: 0.0442
[2025-05-07 20:53:20,313][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0442
[2025-05-07 20:53:20,509][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0219, Metrics: {'mse': 0.022355729714035988, 'rmse': 0.1495183256796169, 'r2': -0.25398457050323486}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0343Epoch 7/15: [                              ] 2/75 batches, loss: 0.0385Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0379Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0328Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0385Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0375Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0416Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0458Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0428Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0456Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0440Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0433Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0444Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0440Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0463Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0458Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0448Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0440Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0443Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0450Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0441Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0430Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0432Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0428Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0422Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0419Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0418Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0422Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0427Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0428Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0422Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0424Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0436Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0431Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0426Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0422Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0422Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0429Epoch 7/15: [================              ] 40/75 batches, loss: 0.0434Epoch 7/15: [================              ] 41/75 batches, loss: 0.0433Epoch 7/15: [================              ] 42/75 batches, loss: 0.0427Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0423Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0422Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0421Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0419Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0425Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0426Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0421Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0418Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0415Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0410Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0411Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0406Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0407Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0406Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0406Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0406Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0404Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0404Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0401Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0397Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0395Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0393Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0391Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0391Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0391Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0389Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0388Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0391Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0388Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0386Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0385Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0383Epoch 7/15: [==============================] 75/75 batches, loss: 0.0384
[2025-05-07 20:53:23,215][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0384
[2025-05-07 20:53:23,433][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0258, Metrics: {'mse': 0.026198705658316612, 'rmse': 0.16186014227819218, 'r2': -0.46954596042633057}
[2025-05-07 20:53:23,434][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0420Epoch 8/15: [                              ] 2/75 batches, loss: 0.0357Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0350Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0349Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0326Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0318Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0338Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0333Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0343Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0337Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0329Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0325Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0324Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0332Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0359Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0352Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0361Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0362Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0366Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0372Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0366Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0366Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0365Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0364Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0363Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0359Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0360Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0359Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0356Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0361Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0357Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0360Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0355Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0364Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0363Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0359Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0357Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0355Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0367Epoch 8/15: [================              ] 40/75 batches, loss: 0.0366Epoch 8/15: [================              ] 41/75 batches, loss: 0.0360Epoch 8/15: [================              ] 42/75 batches, loss: 0.0364Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0361Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0366Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0370Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0368Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0363Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0362Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0360Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0362Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0360Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0357Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0354Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0354Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0352Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0351Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0349Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0348Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0352Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0350Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0349Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0345Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0344Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0341Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0343Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0343Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0343Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0340Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0340Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0341Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0341Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0340Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0338Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0337Epoch 8/15: [==============================] 75/75 batches, loss: 0.0333
[2025-05-07 20:53:25,777][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0333
[2025-05-07 20:53:26,001][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0302, Metrics: {'mse': 0.030679108574986458, 'rmse': 0.17515452770335815, 'r2': -0.7208621501922607}
[2025-05-07 20:53:26,002][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0248Epoch 9/15: [                              ] 2/75 batches, loss: 0.0514Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0433Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0387Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0374Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0346Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0329Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0317Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0345Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0354Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0348Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0342Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0346Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0344Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0336Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0335Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0333Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0326Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0325Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0323Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0319Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0321Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0320Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0324Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0318Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0319Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0315Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0312Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0317Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0314Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0309Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0317Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0320Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0314Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0310Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0305Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0306Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0312Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0310Epoch 9/15: [================              ] 40/75 batches, loss: 0.0313Epoch 9/15: [================              ] 41/75 batches, loss: 0.0309Epoch 9/15: [================              ] 42/75 batches, loss: 0.0310Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0314Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0315Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0314Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0315Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0311Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0309Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0310Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0309Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0307Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0304Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0301Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0303Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0299Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0302Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0301Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0301Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0302Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0303Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0305Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0303Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0305Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0307Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0308Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0307Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0305Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0307Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0310Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0308Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0309Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0308Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0309Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0307Epoch 9/15: [==============================] 75/75 batches, loss: 0.0307
[2025-05-07 20:53:28,301][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0307
[2025-05-07 20:53:28,526][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0300, Metrics: {'mse': 0.030459463596343994, 'rmse': 0.17452639799280795, 'r2': -0.7085416316986084}
[2025-05-07 20:53:28,527][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0198Epoch 10/15: [                              ] 2/75 batches, loss: 0.0200Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0198Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0241Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0270Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0263Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0246Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0267Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0264Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0271Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0267Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0282Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0285Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0299Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0307Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0324Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0321Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0318Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0314Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0312Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0313Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0309Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0314Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0318Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0319Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0320Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0314Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0313Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0311Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0307Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0302Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0298Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0297Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0296Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0295Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0291Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0295Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0292Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0290Epoch 10/15: [================              ] 40/75 batches, loss: 0.0288Epoch 10/15: [================              ] 41/75 batches, loss: 0.0287Epoch 10/15: [================              ] 42/75 batches, loss: 0.0287Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0287Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0289Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0286Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0285Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0284Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0291Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0289Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0291Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0289Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0286Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0287Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0295Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0294Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0297Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0299Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0298Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0296Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0294Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0294Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0295Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0295Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0295Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0298Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0295Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0295Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0297Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0298Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0299Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0299Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0299Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0297Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0298Epoch 10/15: [==============================] 75/75 batches, loss: 0.0295
[2025-05-07 20:53:30,816][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0295
[2025-05-07 20:53:31,023][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0257, Metrics: {'mse': 0.026188088580965996, 'rmse': 0.16182734188315026, 'r2': -0.46895039081573486}
[2025-05-07 20:53:31,024][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:53:31,024][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:53:31,024][src.training.lm_trainer][INFO] - Training completed in 28.18 seconds
[2025-05-07 20:53:31,025][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:53:33,892][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.009675145149230957, 'rmse': 0.09836231569676955, 'r2': -0.04014623165130615}
[2025-05-07 20:53:33,893][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.022355729714035988, 'rmse': 0.1495183256796169, 'r2': -0.25398457050323486}
[2025-05-07 20:53:33,893][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12044607102870941, 'rmse': 0.3470534123571031, 'r2': -1.8327546119689941}
[2025-05-07 20:53:35,564][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ja/ja/model.pt
[2025-05-07 20:53:35,565][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▆▅▄▁
wandb:     best_val_mse ██▆▅▄▁
wandb:      best_val_r2 ▁▁▃▄▅█
wandb:    best_val_rmse ██▆▅▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▄▄▆▅▄▅
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▆▅▄▁▂▄▄▂
wandb:          val_mse ██▆▅▄▁▂▄▄▂
wandb:           val_r2 ▁▁▃▄▅█▇▅▅▇
wandb:         val_rmse ██▆▅▄▁▃▄▄▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02192
wandb:     best_val_mse 0.02236
wandb:      best_val_r2 -0.25398
wandb:    best_val_rmse 0.14952
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.12045
wandb:    final_test_r2 -1.83275
wandb:  final_test_rmse 0.34705
wandb:  final_train_mse 0.00968
wandb:   final_train_r2 -0.04015
wandb: final_train_rmse 0.09836
wandb:    final_val_mse 0.02236
wandb:     final_val_r2 -0.25398
wandb:   final_val_rmse 0.14952
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02954
wandb:       train_time 28.17608
wandb:         val_loss 0.02573
wandb:          val_mse 0.02619
wandb:           val_r2 -0.46895
wandb:         val_rmse 0.16183
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205249-h8gywpiw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205249-h8gywpiw/logs
Experiment probe_layer2_avg_links_len_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:53:57,465][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ja
experiment_name: probe_layer2_avg_links_len_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:53:57,465][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:53:57,465][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:53:57,465][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:53:57,465][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:53:57,470][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:53:57,470][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 20:53:57,470][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:54:00,282][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:54:02,534][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:54:02,535][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:54:02,695][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 20:54:02,767][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 20:54:03,001][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:54:03,009][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:54:03,010][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:54:03,011][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:54:03,064][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:54:03,129][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:54:03,144][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:54:03,145][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:54:03,146][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:54:03,146][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:54:03,232][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:54:03,314][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:54:03,351][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:54:03,353][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:54:03,353][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:54:03,364][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:54:03,365][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:54:03,365][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:54:03,365][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:54:03,365][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:54:03,365][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:54:03,366][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Sample label: 0.13500000536441803
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:54:03,366][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:54:03,366][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 20:54:03,367][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 20:54:03,367][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:54:03,367][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:54:03,367][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 20:54:03,368][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:54:03,368][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:54:03,368][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:54:03,368][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 20:54:03,368][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:54:09,967][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:54:09,968][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:54:09,968][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:54:09,969][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:54:09,971][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:54:09,972][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:54:09,972][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:54:09,972][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:54:09,972][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:54:09,973][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:54:09,973][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5180Epoch 1/15: [                              ] 2/75 batches, loss: 0.5166Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4813Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4738Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4604Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4144Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4058Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4229Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4058Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3838Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3759Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3900Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3727Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3883Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3862Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3961Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3927Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4082Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3977Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3953Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3881Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3889Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3752Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3645Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3596Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3550Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3529Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3478Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3444Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3383Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3329Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3315Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3296Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3296Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3270Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3338Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3305Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3257Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3236Epoch 1/15: [================              ] 40/75 batches, loss: 0.3181Epoch 1/15: [================              ] 41/75 batches, loss: 0.3147Epoch 1/15: [================              ] 42/75 batches, loss: 0.3142Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3140Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3164Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3145Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3101Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3065Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3049Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3031Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3003Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3003Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3051Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3014Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3026Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3026Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2981Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2950Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2944Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2927Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2910Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2894Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2874Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2878Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2873Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2867Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2841Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2829Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2801Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2784Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2761Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2748Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2761Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2737Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2731Epoch 1/15: [==============================] 75/75 batches, loss: 0.2702
[2025-05-07 20:54:15,736][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2702
[2025-05-07 20:54:15,935][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0425, Metrics: {'mse': 0.04289213567972183, 'rmse': 0.20710416625389705, 'r2': -1.405918836593628}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1440Epoch 2/15: [                              ] 2/75 batches, loss: 0.1815Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1703Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1916Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1724Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1556Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1638Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1614Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1714Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1705Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1650Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1644Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1642Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1599Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1632Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1623Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1592Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1572Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1550Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1510Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1489Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1491Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1462Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1440Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1409Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1436Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1468Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1458Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1464Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1455Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1448Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1476Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1462Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1450Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1451Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1437Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1446Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1434Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1425Epoch 2/15: [================              ] 40/75 batches, loss: 0.1414Epoch 2/15: [================              ] 41/75 batches, loss: 0.1396Epoch 2/15: [================              ] 42/75 batches, loss: 0.1411Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1399Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1422Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1406Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1387Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1371Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1359Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1356Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1346Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1339Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1328Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1326Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1321Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1310Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1320Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1309Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1295Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1285Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1289Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1288Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1293Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1287Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1277Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1268Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1260Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1269Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1267Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1263Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1264Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1256Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1252Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1238Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1233Epoch 2/15: [==============================] 75/75 batches, loss: 0.1231
[2025-05-07 20:54:18,624][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1231
[2025-05-07 20:54:18,823][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0410, Metrics: {'mse': 0.04142439737915993, 'rmse': 0.20352984395208465, 'r2': -1.3235902786254883}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1475Epoch 3/15: [                              ] 2/75 batches, loss: 0.1182Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1120Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1047Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1042Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1059Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1003Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1021Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1002Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0969Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0988Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0952Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0959Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0915Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0897Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0896Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0862Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0851Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0841Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0861Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0896Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0897Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0911Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0888Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0868Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0862Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0894Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0888Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0876Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0859Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0858Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0925Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0936Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0933Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0927Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0929Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0940Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0933Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0921Epoch 3/15: [================              ] 40/75 batches, loss: 0.0907Epoch 3/15: [================              ] 41/75 batches, loss: 0.0909Epoch 3/15: [================              ] 42/75 batches, loss: 0.0913Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0910Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0912Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0913Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0910Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0906Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0897Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0891Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0890Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0887Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0887Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0885Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0876Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0868Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0864Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0857Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0861Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0858Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0846Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0844Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0841Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0841Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0834Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0833Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0824Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0819Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0819Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0825Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0824Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0819Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0824Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0827Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0826Epoch 3/15: [==============================] 75/75 batches, loss: 0.0823
[2025-05-07 20:54:21,508][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0823
[2025-05-07 20:54:21,718][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0339, Metrics: {'mse': 0.034404706209897995, 'rmse': 0.18548505656763295, 'r2': -0.9298394918441772}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1184Epoch 4/15: [                              ] 2/75 batches, loss: 0.0896Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0953Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0948Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0814Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0782Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0773Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0733Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0828Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0803Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0830Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0811Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0796Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0768Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0734Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0753Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0752Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0734Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0734Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0731Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0716Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0710Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0712Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0722Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0714Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0705Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0703Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0692Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0690Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0682Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0686Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0682Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0680Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0678Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0674Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0664Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0656Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0657Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0649Epoch 4/15: [================              ] 40/75 batches, loss: 0.0656Epoch 4/15: [================              ] 41/75 batches, loss: 0.0655Epoch 4/15: [================              ] 42/75 batches, loss: 0.0650Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0657Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0651Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0644Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0647Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0645Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0646Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0634Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0629Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0625Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0625Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0618Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0612Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0613Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0611Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0614Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0613Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0611Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0610Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0610Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0611Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0612Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0611Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0609Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0609Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0607Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0608Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0604Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0606Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0607Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0605Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0606Epoch 4/15: [==============================] 75/75 batches, loss: 0.0603
[2025-05-07 20:54:24,336][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0603
[2025-05-07 20:54:24,539][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0275, Metrics: {'mse': 0.02792956493794918, 'rmse': 0.16712140777874385, 'r2': -0.5666338205337524}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0508Epoch 5/15: [                              ] 2/75 batches, loss: 0.0473Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0487Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0516Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0541Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0611Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0598Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0565Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0604Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0598Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0652Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0647Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0633Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0618Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0607Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0601Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0595Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0606Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0592Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0578Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0561Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0582Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0584Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0577Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0584Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0581Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0583Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0574Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0577Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0576Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0578Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0567Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0564Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0563Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0565Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0560Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0556Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0557Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0552Epoch 5/15: [================              ] 40/75 batches, loss: 0.0546Epoch 5/15: [================              ] 41/75 batches, loss: 0.0545Epoch 5/15: [================              ] 42/75 batches, loss: 0.0554Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0551Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0543Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0540Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0539Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0536Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0541Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0543Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0541Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0536Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0537Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0535Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0534Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0532Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0533Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0528Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0538Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0534Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0542Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0551Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0545Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0541Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0543Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0544Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0543Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0544Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0540Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0542Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0541Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0537Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0535Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0541Epoch 5/15: [==============================] 75/75 batches, loss: 0.0545
[2025-05-07 20:54:27,146][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0545
[2025-05-07 20:54:27,351][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0241, Metrics: {'mse': 0.024521589279174805, 'rmse': 0.15659370766149835, 'r2': -0.3754725456237793}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0624Epoch 6/15: [                              ] 2/75 batches, loss: 0.0538Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0483Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0565Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0527Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0495Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0505Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0503Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0469Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0474Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0457Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0470Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0485Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0502Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0495Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0484Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0476Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0471Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0463Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0457Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0461Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0466Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0460Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0460Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0478Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0481Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0489Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0483Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0486Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0479Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0480Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0481Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0482Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0476Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0479Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0476Epoch 6/15: [================              ] 40/75 batches, loss: 0.0478Epoch 6/15: [================              ] 41/75 batches, loss: 0.0476Epoch 6/15: [================              ] 42/75 batches, loss: 0.0475Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0472Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0473Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0470Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0467Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0466Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0464Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0467Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0474Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0480Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0474Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0480Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0476Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0474Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0476Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0476Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0475Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0470Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0473Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0472Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0470Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0469Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0466Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0462Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0460Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0456Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0456Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0453Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0453Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0452Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0449Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0446Epoch 6/15: [==============================] 75/75 batches, loss: 0.0449
[2025-05-07 20:54:30,015][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0449
[2025-05-07 20:54:30,236][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0251, Metrics: {'mse': 0.025511031970381737, 'rmse': 0.15972173293068712, 'r2': -0.430972695350647}
[2025-05-07 20:54:30,237][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0239Epoch 7/15: [                              ] 2/75 batches, loss: 0.0367Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0374Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0338Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0340Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0340Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0392Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0438Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0419Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0434Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0418Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0404Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0419Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0414Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0398Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0406Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0399Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0400Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0400Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0395Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0386Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0378Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0378Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0371Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0370Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0367Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0373Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0371Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0375Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0381Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0382Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0375Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0376Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0389Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0385Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0383Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0381Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0383Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0390Epoch 7/15: [================              ] 40/75 batches, loss: 0.0391Epoch 7/15: [================              ] 41/75 batches, loss: 0.0390Epoch 7/15: [================              ] 42/75 batches, loss: 0.0389Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0386Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0384Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0384Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0381Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0387Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0384Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0382Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0384Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0380Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0377Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0377Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0374Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0375Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0375Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0374Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0375Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0374Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0376Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0373Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0371Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0371Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0368Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0367Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0370Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0374Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0371Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0375Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0377Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0378Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0375Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0372Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0371Epoch 7/15: [==============================] 75/75 batches, loss: 0.0370
[2025-05-07 20:54:32,501][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0370
[2025-05-07 20:54:32,708][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0224, Metrics: {'mse': 0.02284780517220497, 'rmse': 0.1511549045588828, 'r2': -0.2815861701965332}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0302Epoch 8/15: [                              ] 2/75 batches, loss: 0.0263Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0253Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0295Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0285Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0286Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0315Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0304Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0316Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0308Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0326Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0324Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0330Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0331Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0347Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0337Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0347Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0347Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0354Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0348Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0346Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0352Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0352Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0346Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0341Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0349Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0354Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0351Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0352Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0347Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0348Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0345Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0347Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0346Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0345Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0340Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0339Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0340Epoch 8/15: [================              ] 40/75 batches, loss: 0.0338Epoch 8/15: [================              ] 41/75 batches, loss: 0.0338Epoch 8/15: [================              ] 42/75 batches, loss: 0.0338Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0346Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0347Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0348Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0347Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0343Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0342Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0340Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0341Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0340Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0338Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0335Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0334Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0334Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0330Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0331Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0330Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0333Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0330Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0329Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0328Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0328Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0326Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0324Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0322Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0323Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0320Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0319Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0318Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0321Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0320Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0320Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0318Epoch 8/15: [==============================] 75/75 batches, loss: 0.0315
[2025-05-07 20:54:35,435][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0315
[2025-05-07 20:54:35,648][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0311, Metrics: {'mse': 0.031585194170475006, 'rmse': 0.1777222388179797, 'r2': -0.7716865539550781}
[2025-05-07 20:54:35,649][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0871Epoch 9/15: [                              ] 2/75 batches, loss: 0.0564Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0448Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0406Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0388Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0360Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0367Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0339Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0328Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0326Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0338Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0339Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0336Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0339Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0335Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0332Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0327Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0321Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0321Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0321Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0314Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0315Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0311Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0312Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0314Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0317Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0310Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0313Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0316Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0314Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0308Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0311Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0313Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0309Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0309Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0305Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0308Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0311Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0313Epoch 9/15: [================              ] 40/75 batches, loss: 0.0311Epoch 9/15: [================              ] 41/75 batches, loss: 0.0307Epoch 9/15: [================              ] 42/75 batches, loss: 0.0308Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0308Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0308Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0307Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0309Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0306Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0303Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0302Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0305Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0303Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0301Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0300Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0304Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0303Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0303Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0300Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0300Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0300Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0301Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0303Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0300Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0301Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0302Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0303Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0302Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0300Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0300Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0300Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0300Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0298Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0296Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0296Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0294Epoch 9/15: [==============================] 75/75 batches, loss: 0.0295
[2025-05-07 20:54:37,935][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0295
[2025-05-07 20:54:38,147][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0302, Metrics: {'mse': 0.03066222369670868, 'rmse': 0.175106321121508, 'r2': -0.7199149131774902}
[2025-05-07 20:54:38,148][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0271Epoch 10/15: [                              ] 2/75 batches, loss: 0.0293Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0284Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0313Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0313Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0289Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0283Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0286Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0299Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0289Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0276Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0289Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0287Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0307Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0308Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0309Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0306Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0311Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0304Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0303Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0298Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0293Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0292Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0296Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0299Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0298Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0299Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0299Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0295Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0293Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0293Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0295Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0294Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0292Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0288Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0288Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0284Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0284Epoch 10/15: [================              ] 40/75 batches, loss: 0.0286Epoch 10/15: [================              ] 41/75 batches, loss: 0.0288Epoch 10/15: [================              ] 42/75 batches, loss: 0.0289Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0286Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0286Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0287Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0286Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0284Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0288Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0286Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0289Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0286Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0284Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0284Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0283Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0283Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0283Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0286Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0286Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0284Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0281Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0282Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0284Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0283Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0283Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0283Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0283Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0284Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0285Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0287Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0289Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0292Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0292Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0291Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0290Epoch 10/15: [==============================] 75/75 batches, loss: 0.0291
[2025-05-07 20:54:40,447][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0291
[2025-05-07 20:54:40,664][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0239, Metrics: {'mse': 0.024343501776456833, 'rmse': 0.1560240423026427, 'r2': -0.3654831647872925}
[2025-05-07 20:54:40,665][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0204Epoch 11/15: [                              ] 2/75 batches, loss: 0.0296Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0257Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0258Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0255Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0271Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0257Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0271Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0269Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0256Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0265Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0259Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0270Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0277Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0280Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0274Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0274Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0290Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0288Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0287Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0284Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0282Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0278Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0281Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0280Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0274Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0272Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0273Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0274Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0276Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0275Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0273Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0270Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0268Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0268Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0267Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0267Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0266Epoch 11/15: [================              ] 40/75 batches, loss: 0.0262Epoch 11/15: [================              ] 41/75 batches, loss: 0.0262Epoch 11/15: [================              ] 42/75 batches, loss: 0.0262Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0265Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0262Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0260Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0260Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0258Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0257Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0259Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0259Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0258Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0254Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0252Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0250Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0251Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0249Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0247Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0248Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0247Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0247Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0246Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0245Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0244Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0242Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0241Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0239Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0240Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0243Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0243Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0255Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0254Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0269Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0270Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0271Epoch 11/15: [==============================] 75/75 batches, loss: 0.0269
[2025-05-07 20:54:42,947][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0269
[2025-05-07 20:54:43,163][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0208, Metrics: {'mse': 0.021206900477409363, 'rmse': 0.1456258921943806, 'r2': -0.18954408168792725}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0296Epoch 12/15: [                              ] 2/75 batches, loss: 0.0270Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0303Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0290Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0285Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0257Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0254Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0242Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0250Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0254Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0242Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0245Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0251Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0247Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0237Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0238Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0242Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0240Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0237Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0231Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0227Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0236Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0235Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0232Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0230Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0234Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0231Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0234Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0239Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0244Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0249Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0252Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0252Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0250Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0249Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0245Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0244Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0243Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0243Epoch 12/15: [================              ] 40/75 batches, loss: 0.0247Epoch 12/15: [================              ] 41/75 batches, loss: 0.0247Epoch 12/15: [================              ] 42/75 batches, loss: 0.0249Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0247Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0246Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0252Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0252Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0252Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0249Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0247Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0245Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0247Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0248Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0258Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0259Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0258Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0257Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0256Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0254Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0252Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0252Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0251Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0250Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0249Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0248Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0248Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0249Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0251Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0248Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0247Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0248Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0246Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0245Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0245Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0244Epoch 12/15: [==============================] 75/75 batches, loss: 0.0241
[2025-05-07 20:54:45,886][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0241
[2025-05-07 20:54:46,102][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0388, Metrics: {'mse': 0.03932949900627136, 'rmse': 0.1983166634609189, 'r2': -1.2060825824737549}
[2025-05-07 20:54:46,103][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0311Epoch 13/15: [                              ] 2/75 batches, loss: 0.0278Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0282Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0247Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0254Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0257Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0248Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0250Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0239Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0246Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0245Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0235Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0234Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0229Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0230Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0227Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0238Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0238Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0235Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0247Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0245Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0249Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0247Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0242Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0241Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0242Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0238Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0234Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0246Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0248Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0248Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0249Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0244Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0241Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0244Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0241Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0244Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0244Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0243Epoch 13/15: [================              ] 40/75 batches, loss: 0.0241Epoch 13/15: [================              ] 41/75 batches, loss: 0.0242Epoch 13/15: [================              ] 42/75 batches, loss: 0.0242Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0238Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0237Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0236Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0233Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0233Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0231Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0234Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0233Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0232Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0232Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0229Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0229Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0230Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0229Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0229Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0228Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0228Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0227Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0227Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0226Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0224Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0224Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0224Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0223Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0223Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0223Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0222Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0222Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0223Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0222Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0223Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0223Epoch 13/15: [==============================] 75/75 batches, loss: 0.0225
[2025-05-07 20:54:48,407][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0225
[2025-05-07 20:54:48,621][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0249, Metrics: {'mse': 0.025347484275698662, 'rmse': 0.15920893277608095, 'r2': -0.4217989444732666}
[2025-05-07 20:54:48,621][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0085Epoch 14/15: [                              ] 2/75 batches, loss: 0.0113Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0125Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0137Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0136Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0145Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0142Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0138Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0146Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0161Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0168Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0171Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0177Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0188Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0180Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0174Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0179Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0175Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0182Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0182Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0184Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0186Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0186Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0186Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0186Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0188Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0185Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0188Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0186Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0191Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0190Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0190Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0190Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0188Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0191Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0191Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0193Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0194Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0193Epoch 14/15: [================              ] 40/75 batches, loss: 0.0202Epoch 14/15: [================              ] 41/75 batches, loss: 0.0202Epoch 14/15: [================              ] 42/75 batches, loss: 0.0203Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0202Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0203Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0207Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0209Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0210Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0209Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0211Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0216Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0217Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0218Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0217Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0217Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0217Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0218Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0216Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0216Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0215Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0219Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0221Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0221Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0220Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0222Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0221Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0221Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0220Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0219Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0218Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0221Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0220Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0219Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0220Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0220Epoch 14/15: [==============================] 75/75 batches, loss: 0.0221
[2025-05-07 20:54:50,913][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0221
[2025-05-07 20:54:51,120][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0205, Metrics: {'mse': 0.02096107415854931, 'rmse': 0.1447793982531676, 'r2': -0.17575502395629883}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0145Epoch 15/15: [                              ] 2/75 batches, loss: 0.0123Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0152Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0164Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0185Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0190Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0175Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0169Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0170Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0185Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0195Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0191Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0202Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0201Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0191Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0196Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0192Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0193Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0193Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0190Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0193Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0189Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0188Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0189Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0192Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0191Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0191Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0189Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0187Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0190Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0190Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0188Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0190Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0190Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0189Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0189Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0190Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0191Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0189Epoch 15/15: [================              ] 40/75 batches, loss: 0.0189Epoch 15/15: [================              ] 41/75 batches, loss: 0.0191Epoch 15/15: [================              ] 42/75 batches, loss: 0.0189Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0190Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0189Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0189Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0188Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0189Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0189Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0190Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0195Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0195Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0196Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0196Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0197Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0196Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0198Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0198Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0197Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0196Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0195Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0195Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0194Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0195Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0194Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0193Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0192Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0191Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0192Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0193Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0196Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0195Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0194Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0193Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0192Epoch 15/15: [==============================] 75/75 batches, loss: 0.0192
[2025-05-07 20:54:53,838][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0192
[2025-05-07 20:54:54,057][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0261, Metrics: {'mse': 0.026550861075520515, 'rmse': 0.1629443496274741, 'r2': -0.48929905891418457}
[2025-05-07 20:54:54,058][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 20:54:54,058][src.training.lm_trainer][INFO] - Training completed in 41.46 seconds
[2025-05-07 20:54:54,058][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:54:56,962][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.009403495118021965, 'rmse': 0.09697162016807787, 'r2': -0.010941863059997559}
[2025-05-07 20:54:56,962][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02096107415854931, 'rmse': 0.1447793982531676, 'r2': -0.17575502395629883}
[2025-05-07 20:54:56,962][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.11403900384902954, 'rmse': 0.33769661509856674, 'r2': -1.682067632675171}
[2025-05-07 20:54:58,602][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ja/ja/model.pt
[2025-05-07 20:54:58,604][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▅▃▂▂▁▁
wandb:     best_val_mse ██▅▃▂▂▁▁
wandb:      best_val_r2 ▁▁▄▆▇▇██
wandb:    best_val_rmse ██▆▄▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▆▆▆▄▄▆▆▂▆▆
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▅▃▂▂▂▄▄▂▁▇▂▁▃
wandb:          val_mse ██▅▃▂▂▂▄▄▂▁▇▂▁▃
wandb:           val_r2 ▁▁▄▆▇▇▇▅▅▇█▂▇█▆
wandb:         val_rmse ██▆▄▂▃▂▅▄▂▁▇▃▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02054
wandb:     best_val_mse 0.02096
wandb:      best_val_r2 -0.17576
wandb:    best_val_rmse 0.14478
wandb:            epoch 15
wandb:   final_test_mse 0.11404
wandb:    final_test_r2 -1.68207
wandb:  final_test_rmse 0.3377
wandb:  final_train_mse 0.0094
wandb:   final_train_r2 -0.01094
wandb: final_train_rmse 0.09697
wandb:    final_val_mse 0.02096
wandb:     final_val_r2 -0.17576
wandb:   final_val_rmse 0.14478
wandb:    learning_rate 0.0001
wandb:       train_loss 0.01919
wandb:       train_time 41.4649
wandb:         val_loss 0.0261
wandb:          val_mse 0.02655
wandb:           val_r2 -0.4893
wandb:         val_rmse 0.16294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205357-x2fkprkx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205357-x2fkprkx/logs
Experiment probe_layer2_avg_links_len_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:55:21,780][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ja
experiment_name: probe_layer2_avg_max_depth_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:55:21,780][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:55:21,780][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:55:21,780][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:55:21,780][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:55:21,785][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:55:21,785][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:55:21,785][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:55:24,492][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:55:26,763][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:55:26,763][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:55:26,897][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 20:55:26,964][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:19:23 2025).
[2025-05-07 20:55:27,193][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:55:27,203][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:55:27,203][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:55:27,205][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:55:27,238][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:55:27,293][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:55:27,307][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:55:27,309][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:55:27,309][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:55:27,310][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:55:27,387][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:55:27,446][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:55:27,484][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:55:27,485][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:55:27,485][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:55:27,486][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:55:27,487][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:55:27,488][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 20:55:27,488][src.data.datasets][INFO] -   Mean: 0.2860, Std: 0.1307
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 20:55:27,488][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:55:27,489][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:55:27,489][src.data.datasets][INFO] -   Mean: 0.2943, Std: 0.1709
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:55:27,489][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:55:27,490][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:55:27,490][src.data.datasets][INFO] -   Mean: 0.4112, Std: 0.2602
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:55:27,490][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:55:27,491][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:55:27,491][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:55:33,550][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:55:33,551][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:55:33,551][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:55:33,551][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:55:33,554][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:55:33,554][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:55:33,555][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:55:33,555][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:55:33,555][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:55:33,556][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:55:33,556][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4555Epoch 1/15: [                              ] 2/75 batches, loss: 0.4895Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4861Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4898Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4612Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4335Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4237Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4382Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4197Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4024Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3862Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4000Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3819Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3911Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3879Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4070Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4004Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4152Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4057Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4075Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3977Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3957Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3834Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3727Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3696Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3649Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3594Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3532Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3495Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3441Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3390Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3340Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3325Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3323Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3302Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3332Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3310Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3267Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3262Epoch 1/15: [================              ] 40/75 batches, loss: 0.3214Epoch 1/15: [================              ] 41/75 batches, loss: 0.3175Epoch 1/15: [================              ] 42/75 batches, loss: 0.3179Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3167Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3197Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3171Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3125Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3096Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3071Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3051Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3027Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3013Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3053Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3009Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3014Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3012Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2978Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2943Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2932Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2905Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2894Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2877Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2859Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2864Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2859Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2850Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2829Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2819Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2794Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2775Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2751Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2734Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2747Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2723Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2719Epoch 1/15: [==============================] 75/75 batches, loss: 0.2694
[2025-05-07 20:55:39,259][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2694
[2025-05-07 20:55:39,451][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0405, Metrics: {'mse': 0.03956259787082672, 'rmse': 0.19890348883523065, 'r2': -0.35534894466400146}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1606Epoch 2/15: [                              ] 2/75 batches, loss: 0.1969Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1964Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2061Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1965Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1819Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1880Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1810Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1834Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1845Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1771Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1761Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1737Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1712Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1756Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1729Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1690Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1670Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1638Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1584Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1544Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1540Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1506Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1492Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1451Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1490Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1510Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1504Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1522Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1513Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1495Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1517Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1504Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1490Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1501Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1480Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1488Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1480Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1464Epoch 2/15: [================              ] 40/75 batches, loss: 0.1463Epoch 2/15: [================              ] 41/75 batches, loss: 0.1455Epoch 2/15: [================              ] 42/75 batches, loss: 0.1457Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1446Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1464Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1454Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1441Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1424Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1410Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1414Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1404Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1405Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1403Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1399Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1397Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1386Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1398Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1389Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1380Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1374Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1364Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1374Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1379Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1378Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1365Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1355Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1348Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1369Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1368Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1361Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1365Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1357Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1353Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1340Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1331Epoch 2/15: [==============================] 75/75 batches, loss: 0.1339
[2025-05-07 20:55:42,136][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1339
[2025-05-07 20:55:42,336][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0397, Metrics: {'mse': 0.03872332721948624, 'rmse': 0.19678243625762498, 'r2': -0.32659685611724854}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0894Epoch 3/15: [                              ] 2/75 batches, loss: 0.1141Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0950Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0902Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0918Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0955Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0934Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1029Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1037Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1014Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1046Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1011Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1011Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0974Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0969Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0966Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0938Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0932Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0932Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0956Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1006Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0985Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0999Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0981Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0966Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0969Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1000Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1005Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0992Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0982Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0982Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1019Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1019Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1013Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1005Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1001Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1011Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1000Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0994Epoch 3/15: [================              ] 40/75 batches, loss: 0.0981Epoch 3/15: [================              ] 41/75 batches, loss: 0.0981Epoch 3/15: [================              ] 42/75 batches, loss: 0.0987Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0984Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0975Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0972Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0964Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0960Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0951Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0944Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0948Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0950Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0949Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0944Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0941Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0936Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0929Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0925Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0930Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0924Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0914Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0922Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0921Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0919Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0912Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0912Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0907Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0900Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0901Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0901Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0896Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0892Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0896Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0899Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0901Epoch 3/15: [==============================] 75/75 batches, loss: 0.0898
[2025-05-07 20:55:45,027][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0898
[2025-05-07 20:55:45,235][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0335, Metrics: {'mse': 0.032627448439598083, 'rmse': 0.1806306962827694, 'r2': -0.11776220798492432}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0997Epoch 4/15: [                              ] 2/75 batches, loss: 0.0869Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0950Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0988Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0826Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0809Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0804Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0787Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0768Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0739Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0798Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0808Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0785Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0769Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0752Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0763Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0760Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0747Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0771Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0782Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0761Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0761Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0761Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0771Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0765Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0751Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0777Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0766Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0764Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0772Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0778Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0778Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0772Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0771Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0768Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0766Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0764Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0765Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0761Epoch 4/15: [================              ] 40/75 batches, loss: 0.0761Epoch 4/15: [================              ] 41/75 batches, loss: 0.0756Epoch 4/15: [================              ] 42/75 batches, loss: 0.0760Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0777Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0769Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0767Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0776Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0776Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0776Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0771Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0765Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0759Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0758Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0764Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0757Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0748Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0757Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0751Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0754Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0750Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0749Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0747Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0743Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0743Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0742Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0743Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0744Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0746Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0740Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0748Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0743Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0739Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0737Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0736Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0740Epoch 4/15: [==============================] 75/75 batches, loss: 0.0738
[2025-05-07 20:55:47,872][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0738
[2025-05-07 20:55:48,163][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0329, Metrics: {'mse': 0.032045938074588776, 'rmse': 0.17901379297302422, 'r2': -0.09784066677093506}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0691Epoch 5/15: [                              ] 2/75 batches, loss: 0.0646Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0545Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0527Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0562Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0633Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0629Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0586Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0599Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0595Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0671Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0670Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0662Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0665Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0660Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0671Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0662Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0673Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0665Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0654Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0641Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0654Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0672Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0669Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0683Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0681Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0680Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0668Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0668Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0666Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0660Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0651Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0644Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0639Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0636Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0628Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0631Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0623Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0617Epoch 5/15: [================              ] 40/75 batches, loss: 0.0615Epoch 5/15: [================              ] 41/75 batches, loss: 0.0615Epoch 5/15: [================              ] 42/75 batches, loss: 0.0624Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0614Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0614Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0613Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0617Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0615Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0615Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0611Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0607Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0603Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0603Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0601Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0601Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0606Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0607Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0600Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0601Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0605Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0601Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0609Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0614Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0610Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0618Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0618Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0614Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0618Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0618Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0621Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0614Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0611Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0612Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0614Epoch 5/15: [==============================] 75/75 batches, loss: 0.0621
[2025-05-07 20:55:50,809][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0621
[2025-05-07 20:55:51,024][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0352, Metrics: {'mse': 0.034228961914777756, 'rmse': 0.1850107075679074, 'r2': -0.17262744903564453}
[2025-05-07 20:55:51,024][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0602Epoch 6/15: [                              ] 2/75 batches, loss: 0.0649Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0537Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0532Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0666Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0624Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0610Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0612Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0631Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0627Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0591Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0603Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0579Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0577Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0556Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0556Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0574Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0571Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0565Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0569Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0578Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0570Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0557Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0568Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0571Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0577Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0564Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0566Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0563Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0564Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0558Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0570Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0558Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0558Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0555Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0567Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0572Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0572Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0568Epoch 6/15: [================              ] 40/75 batches, loss: 0.0567Epoch 6/15: [================              ] 41/75 batches, loss: 0.0569Epoch 6/15: [================              ] 42/75 batches, loss: 0.0569Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0564Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0568Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0566Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0561Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0555Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0557Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0561Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0569Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0561Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0554Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0554Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0549Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0545Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0542Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0543Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0541Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0540Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0539Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0538Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0535Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0531Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0530Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0534Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0532Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0528Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0527Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0526Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0524Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0521Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0520Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0517Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0514Epoch 6/15: [==============================] 75/75 batches, loss: 0.0517
[2025-05-07 20:55:53,298][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0517
[2025-05-07 20:55:53,499][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0302, Metrics: {'mse': 0.029361829161643982, 'rmse': 0.1713529374176118, 'r2': -0.005887508392333984}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0331Epoch 7/15: [                              ] 2/75 batches, loss: 0.0389Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0403Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0377Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0427Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0448Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0462Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0483Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0478Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0463Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0442Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0464Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0472Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0465Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0465Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0453Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0471Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0474Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0475Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0462Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0450Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0454Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0448Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0454Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0454Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0461Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0454Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0471Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0485Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0484Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0477Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0479Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0490Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0488Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0484Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0483Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0482Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0489Epoch 7/15: [================              ] 40/75 batches, loss: 0.0487Epoch 7/15: [================              ] 41/75 batches, loss: 0.0485Epoch 7/15: [================              ] 42/75 batches, loss: 0.0482Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0481Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0480Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0480Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0476Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0472Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0472Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0469Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0466Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0463Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0460Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0459Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0455Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0454Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0457Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0457Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0457Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0456Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0459Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0454Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0451Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0449Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0447Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0449Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0449Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0452Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0451Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0451Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0454Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0456Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0455Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0453Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0451Epoch 7/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-07 20:55:56,246][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0452
[2025-05-07 20:55:56,478][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0315, Metrics: {'mse': 0.03066466934978962, 'rmse': 0.17511330431977354, 'r2': -0.05052065849304199}
[2025-05-07 20:55:56,479][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0459Epoch 8/15: [                              ] 2/75 batches, loss: 0.0454Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0409Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0462Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0408Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0405Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0422Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0404Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0429Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0423Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0441Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0428Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0444Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0429Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0430Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0422Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0421Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0429Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0429Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0433Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0428Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0433Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0445Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0441Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0441Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0433Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0441Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0434Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0439Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0440Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0436Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0443Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0437Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0449Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0448Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0445Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0439Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0437Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0437Epoch 8/15: [================              ] 40/75 batches, loss: 0.0435Epoch 8/15: [================              ] 41/75 batches, loss: 0.0434Epoch 8/15: [================              ] 42/75 batches, loss: 0.0435Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0434Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0434Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0434Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0430Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0426Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0428Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0423Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0419Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0415Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0411Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0411Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0413Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0410Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0415Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0414Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0412Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0410Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0410Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0412Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0411Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0412Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0415Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0411Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0415Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0419Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0417Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0416Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0414Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0419Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0419Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0419Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0417Epoch 8/15: [==============================] 75/75 batches, loss: 0.0414
[2025-05-07 20:55:58,768][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0414
[2025-05-07 20:55:58,991][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0317, Metrics: {'mse': 0.030813533812761307, 'rmse': 0.17553784154068122, 'r2': -0.055620431900024414}
[2025-05-07 20:55:58,992][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0164Epoch 9/15: [                              ] 2/75 batches, loss: 0.0490Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0407Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0366Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0392Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0369Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0359Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0349Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0351Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0338Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0371Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0370Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0373Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0374Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0374Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0371Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0368Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0365Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0387Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0399Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0388Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0391Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0383Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0382Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0375Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0382Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0387Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0390Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0398Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0403Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0400Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0402Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0405Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0401Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0402Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0399Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0399Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0407Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0409Epoch 9/15: [================              ] 40/75 batches, loss: 0.0408Epoch 9/15: [================              ] 41/75 batches, loss: 0.0404Epoch 9/15: [================              ] 42/75 batches, loss: 0.0404Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0404Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0402Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0396Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0401Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0404Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0405Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0404Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0403Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0399Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0397Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0395Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0399Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0398Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0396Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0393Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0391Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0390Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0389Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0387Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0385Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0386Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0389Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0391Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0390Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0389Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0390Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0389Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0389Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0387Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0387Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0388Epoch 9/15: [==============================] 75/75 batches, loss: 0.0387
[2025-05-07 20:56:01,279][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0387
[2025-05-07 20:56:01,499][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0379, Metrics: {'mse': 0.036976300179958344, 'rmse': 0.19229222599980048, 'r2': -0.2667466402053833}
[2025-05-07 20:56:01,500][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0398Epoch 10/15: [                              ] 2/75 batches, loss: 0.0398Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0416Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0379Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0399Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0402Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0398Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0401Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0392Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0385Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0372Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0403Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0413Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0411Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0428Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0427Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0424Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0426Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0418Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0420Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0425Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0420Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0419Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0413Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0420Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0420Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0417Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0423Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0420Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0413Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0404Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0403Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0401Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0397Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0395Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0392Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0393Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0390Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0385Epoch 10/15: [================              ] 40/75 batches, loss: 0.0385Epoch 10/15: [================              ] 41/75 batches, loss: 0.0383Epoch 10/15: [================              ] 42/75 batches, loss: 0.0380Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0381Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0379Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0382Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0383Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0387Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0388Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0388Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0388Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0384Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0392Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0390Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0388Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0388Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0391Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0395Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0395Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0391Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0388Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0389Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0396Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0396Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0393Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0394Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0390Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0390Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0391Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0394Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0393Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0389Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0387Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0388Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0386Epoch 10/15: [==============================] 75/75 batches, loss: 0.0382
[2025-05-07 20:56:03,799][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0382
[2025-05-07 20:56:04,022][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0345, Metrics: {'mse': 0.03360218182206154, 'rmse': 0.18330897910921204, 'r2': -0.15115487575531006}
[2025-05-07 20:56:04,023][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:56:04,023][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:56:04,023][src.training.lm_trainer][INFO] - Training completed in 27.80 seconds
[2025-05-07 20:56:04,023][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:56:06,918][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.017484743148088455, 'rmse': 0.13222988749934128, 'r2': -0.02305018901824951}
[2025-05-07 20:56:06,919][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.029361829161643982, 'rmse': 0.1713529374176118, 'r2': -0.005887508392333984}
[2025-05-07 20:56:06,919][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09252770245075226, 'rmse': 0.30418366565407856, 'r2': -0.36692559719085693}
[2025-05-07 20:56:08,595][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ja/ja/model.pt
[2025-05-07 20:56:08,596][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▃▃▁
wandb:     best_val_mse █▇▃▃▁
wandb:      best_val_r2 ▁▂▆▆█
wandb:    best_val_rmse █▇▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▃▃▄▄▄▂
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▃▃▄▁▂▂▆▄
wandb:          val_mse █▇▃▃▄▁▂▂▆▄
wandb:           val_r2 ▁▂▆▆▅█▇▇▃▅
wandb:         val_rmse █▇▃▃▄▁▂▂▆▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03016
wandb:     best_val_mse 0.02936
wandb:      best_val_r2 -0.00589
wandb:    best_val_rmse 0.17135
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.09253
wandb:    final_test_r2 -0.36693
wandb:  final_test_rmse 0.30418
wandb:  final_train_mse 0.01748
wandb:   final_train_r2 -0.02305
wandb: final_train_rmse 0.13223
wandb:    final_val_mse 0.02936
wandb:     final_val_r2 -0.00589
wandb:   final_val_rmse 0.17135
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03824
wandb:       train_time 27.80213
wandb:         val_loss 0.03451
wandb:          val_mse 0.0336
wandb:           val_r2 -0.15115
wandb:         val_rmse 0.18331
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205521-rmpxwwzj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205521-rmpxwwzj/logs
Experiment probe_layer2_avg_max_depth_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:56:31,365][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ja
experiment_name: probe_layer2_avg_max_depth_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:56:31,365][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:56:31,365][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:56:31,365][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:56:31,365][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:56:31,370][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:56:31,370][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:56:31,370][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:56:34,001][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:56:36,315][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:56:36,315][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:56:36,515][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 20:56:36,590][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:21:04 2025).
[2025-05-07 20:56:36,828][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:56:36,836][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:56:36,837][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:56:36,838][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:56:36,884][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:56:36,929][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:56:36,944][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:56:36,945][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:56:36,945][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:56:36,947][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:56:37,018][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:56:37,109][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:56:37,126][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:56:37,127][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:56:37,127][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:56:37,129][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:56:37,129][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:56:37,130][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 20:56:37,130][src.data.datasets][INFO] -   Mean: 0.2860, Std: 0.1307
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 20:56:37,130][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:56:37,131][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:56:37,131][src.data.datasets][INFO] -   Mean: 0.2943, Std: 0.1709
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:56:37,131][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:56:37,132][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:56:37,132][src.data.datasets][INFO] -   Mean: 0.4112, Std: 0.2602
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:56:37,132][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:56:37,133][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:56:37,133][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:56:43,043][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:56:43,044][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:56:43,044][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:56:43,044][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:56:43,047][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:56:43,048][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:56:43,048][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:56:43,048][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:56:43,048][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:56:43,049][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:56:43,049][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4088Epoch 1/15: [                              ] 2/75 batches, loss: 0.3959Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4002Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4221Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4136Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3768Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3756Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3977Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3810Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3657Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3525Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3662Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3504Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3738Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3705Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3867Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3831Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3948Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3873Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3858Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3797Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3777Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3650Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3554Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3525Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3484Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3447Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3384Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3362Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3307Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3265Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3229Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3225Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3232Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3213Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3227Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3185Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3139Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3130Epoch 1/15: [================              ] 40/75 batches, loss: 0.3093Epoch 1/15: [================              ] 41/75 batches, loss: 0.3056Epoch 1/15: [================              ] 42/75 batches, loss: 0.3063Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3053Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3072Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3065Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3024Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2996Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2982Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2964Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2945Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2949Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2988Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2946Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2950Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2942Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2903Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2870Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2851Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2823Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2810Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2796Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2779Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2796Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2789Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2777Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2767Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2755Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2738Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2732Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2713Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2706Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2721Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2700Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2692Epoch 1/15: [==============================] 75/75 batches, loss: 0.2666
[2025-05-07 20:56:48,851][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2666
[2025-05-07 20:56:49,032][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0412, Metrics: {'mse': 0.04035871848464012, 'rmse': 0.20089479456830164, 'r2': -0.3826225996017456}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1921Epoch 2/15: [                              ] 2/75 batches, loss: 0.1893Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1861Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2100Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1951Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1806Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1923Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1883Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1939Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1967Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1858Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1807Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1770Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1724Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1741Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1737Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1691Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1669Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1617Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1570Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1535Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1526Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1510Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1499Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1454Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1485Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1511Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1503Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1504Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1501Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1501Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1514Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1493Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1483Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1474Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1448Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1451Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1431Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1414Epoch 2/15: [================              ] 40/75 batches, loss: 0.1421Epoch 2/15: [================              ] 41/75 batches, loss: 0.1410Epoch 2/15: [================              ] 42/75 batches, loss: 0.1426Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1422Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1459Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1438Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1417Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1397Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1393Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1391Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1385Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1385Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1379Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1374Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1379Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1366Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1368Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1361Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1346Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1331Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1330Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1324Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1333Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1331Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1323Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1314Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1307Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1310Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1314Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1314Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1320Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1311Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1306Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1297Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1290Epoch 2/15: [==============================] 75/75 batches, loss: 0.1288
[2025-05-07 20:56:51,747][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1288
[2025-05-07 20:56:51,953][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0358, Metrics: {'mse': 0.03485835716128349, 'rmse': 0.1867039291533081, 'r2': -0.19418954849243164}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0930Epoch 3/15: [                              ] 2/75 batches, loss: 0.0960Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0852Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0861Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0929Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0981Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0957Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1041Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1086Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1066Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1060Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1034Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1021Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0994Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0977Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0966Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0940Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0950Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0927Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0953Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0975Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0954Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0954Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0932Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0917Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0916Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0943Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0950Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0935Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0923Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0936Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1001Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0998Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1005Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1001Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1011Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1018Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1018Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1004Epoch 3/15: [================              ] 40/75 batches, loss: 0.0995Epoch 3/15: [================              ] 41/75 batches, loss: 0.0994Epoch 3/15: [================              ] 42/75 batches, loss: 0.0989Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0987Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0985Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0986Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0984Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0986Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0976Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0973Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0968Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0965Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0966Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0962Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0954Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0946Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0942Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0939Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0943Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0940Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0930Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0925Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0921Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0913Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0913Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0916Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0909Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0905Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0907Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0912Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0910Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0906Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0909Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0906Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0906Epoch 3/15: [==============================] 75/75 batches, loss: 0.0901
[2025-05-07 20:56:54,709][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0901
[2025-05-07 20:56:54,994][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0341, Metrics: {'mse': 0.03327663615345955, 'rmse': 0.18241884813105128, 'r2': -0.14000225067138672}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1423Epoch 4/15: [                              ] 2/75 batches, loss: 0.1053Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1066Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1166Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1004Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0931Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0897Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0887Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0870Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0841Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0867Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0850Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0830Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0824Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0797Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0822Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0812Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0799Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0801Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0816Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0798Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0791Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0794Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0802Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0790Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0778Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0779Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0775Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0771Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0765Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0763Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0761Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0760Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0748Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0755Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0744Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0739Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0736Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0731Epoch 4/15: [================              ] 40/75 batches, loss: 0.0738Epoch 4/15: [================              ] 41/75 batches, loss: 0.0727Epoch 4/15: [================              ] 42/75 batches, loss: 0.0721Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0724Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0719Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0714Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0718Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0716Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0724Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0721Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0712Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0706Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0701Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0717Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0712Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0704Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0715Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0710Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0723Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0721Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0716Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0714Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0712Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0718Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0717Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0716Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0712Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0714Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0707Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0715Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0712Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0711Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0709Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0707Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0712Epoch 4/15: [==============================] 75/75 batches, loss: 0.0708
[2025-05-07 20:56:57,759][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0708
[2025-05-07 20:56:58,048][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0342, Metrics: {'mse': 0.03334279730916023, 'rmse': 0.1826001021608702, 'r2': -0.14226889610290527}
[2025-05-07 20:56:58,048][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0624Epoch 5/15: [                              ] 2/75 batches, loss: 0.0559Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0584Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0629Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0616Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0648Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0656Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0637Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0680Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0648Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0672Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0660Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0675Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0672Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0657Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0660Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0654Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0658Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0636Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0620Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0610Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0627Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0629Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0629Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0632Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0621Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0630Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0625Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0625Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0617Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0617Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0609Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0602Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0603Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0601Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0599Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0601Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0604Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0603Epoch 5/15: [================              ] 40/75 batches, loss: 0.0593Epoch 5/15: [================              ] 41/75 batches, loss: 0.0588Epoch 5/15: [================              ] 42/75 batches, loss: 0.0608Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0601Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0598Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0595Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0601Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0595Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0595Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0593Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0592Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0586Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0587Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0586Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0586Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0586Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0591Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0586Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0585Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0588Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0590Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0597Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0610Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0611Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0607Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0609Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0609Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0609Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0606Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0604Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0608Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0603Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0600Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0599Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0595Epoch 5/15: [==============================] 75/75 batches, loss: 0.0602
[2025-05-07 20:57:00,318][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0602
[2025-05-07 20:57:00,528][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0347, Metrics: {'mse': 0.03380259871482849, 'rmse': 0.18385483054526605, 'r2': -0.1580209732055664}
[2025-05-07 20:57:00,529][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0784Epoch 6/15: [                              ] 2/75 batches, loss: 0.0699Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0631Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0627Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0652Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0620Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0578Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0585Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0579Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0607Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0593Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0607Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0590Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0598Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0604Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0609Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0619Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0607Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0602Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0601Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0611Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0593Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0583Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0584Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0594Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0599Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0594Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0595Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0599Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0603Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0593Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0596Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0596Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0595Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0592Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0598Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0592Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0599Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0594Epoch 6/15: [================              ] 40/75 batches, loss: 0.0596Epoch 6/15: [================              ] 41/75 batches, loss: 0.0598Epoch 6/15: [================              ] 42/75 batches, loss: 0.0601Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0593Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0589Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0585Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0583Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0579Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0580Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0583Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0584Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0582Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0574Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0575Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0572Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0566Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0565Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0564Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0560Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0563Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0557Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0555Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0555Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0556Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0559Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0557Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0558Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0558Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0555Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0554Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0549Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0547Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0548Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0547Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0543Epoch 6/15: [==============================] 75/75 batches, loss: 0.0547
[2025-05-07 20:57:02,834][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0547
[2025-05-07 20:57:03,059][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0302, Metrics: {'mse': 0.029433226212859154, 'rmse': 0.17156114423976995, 'r2': -0.008333444595336914}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0171Epoch 7/15: [                              ] 2/75 batches, loss: 0.0410Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0394Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0372Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0394Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0392Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0448Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0494Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0502Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0522Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0509Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0494Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0511Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0503Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0491Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0500Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0511Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0522Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0518Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0518Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0512Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0503Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0498Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0502Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0503Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0511Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0502Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0503Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0506Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0507Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0501Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0498Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0511Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0505Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0504Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0500Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0503Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0506Epoch 7/15: [================              ] 40/75 batches, loss: 0.0504Epoch 7/15: [================              ] 41/75 batches, loss: 0.0503Epoch 7/15: [================              ] 42/75 batches, loss: 0.0497Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0494Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0492Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0491Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0487Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0484Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0482Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0480Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0477Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0474Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0468Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0466Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0466Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0465Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0465Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0464Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0467Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0476Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0484Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0478Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0479Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0475Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0473Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0470Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0470Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0469Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0469Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0469Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0471Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0470Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0468Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0466Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0467Epoch 7/15: [==============================] 75/75 batches, loss: 0.0466
[2025-05-07 20:57:05,776][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0466
[2025-05-07 20:57:06,029][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0311, Metrics: {'mse': 0.03025570884346962, 'rmse': 0.17394168230608104, 'r2': -0.036510348320007324}
[2025-05-07 20:57:06,029][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0408Epoch 8/15: [                              ] 2/75 batches, loss: 0.0394Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0356Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0357Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0308Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0338Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0347Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0376Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0367Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0389Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0406Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0397Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0408Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0406Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0418Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0407Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0417Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0421Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0431Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0434Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0429Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0432Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0433Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0435Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0431Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0444Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0443Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0444Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0441Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0443Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0447Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0443Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0450Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0452Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0448Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0445Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0446Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0443Epoch 8/15: [================              ] 40/75 batches, loss: 0.0440Epoch 8/15: [================              ] 41/75 batches, loss: 0.0437Epoch 8/15: [================              ] 42/75 batches, loss: 0.0441Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0437Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0439Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0439Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0434Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0433Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0434Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0435Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0434Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0436Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0436Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0432Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0434Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0430Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0426Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0426Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0422Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0418Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0413Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0411Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0409Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0408Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0409Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0405Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0403Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0408Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0406Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0407Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0405Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0404Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0405Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0405Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0409Epoch 8/15: [==============================] 75/75 batches, loss: 0.0405
[2025-05-07 20:57:08,336][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0405
[2025-05-07 20:57:08,547][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0354, Metrics: {'mse': 0.03443830832839012, 'rmse': 0.18557561350670546, 'r2': -0.17979919910430908}
[2025-05-07 20:57:08,547][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0247Epoch 9/15: [                              ] 2/75 batches, loss: 0.0299Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0359Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0322Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0347Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0372Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0392Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0412Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0415Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0395Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0414Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0398Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0404Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0403Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0413Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0415Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0409Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0400Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0408Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0401Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0395Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0399Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0402Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0391Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0388Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0379Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0371Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0377Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0380Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0379Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0377Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0377Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0381Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0373Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0374Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0374Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0375Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0387Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0383Epoch 9/15: [================              ] 40/75 batches, loss: 0.0388Epoch 9/15: [================              ] 41/75 batches, loss: 0.0384Epoch 9/15: [================              ] 42/75 batches, loss: 0.0384Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0382Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0380Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0376Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0375Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0379Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0376Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0378Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0375Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0377Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0377Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0377Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0378Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0374Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0373Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0372Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0369Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0369Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0367Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0369Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0366Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0365Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0368Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0365Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0364Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0366Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0365Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0367Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0365Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0365Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0362Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0362Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0360Epoch 9/15: [==============================] 75/75 batches, loss: 0.0360
[2025-05-07 20:57:10,846][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0360
[2025-05-07 20:57:11,101][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0339, Metrics: {'mse': 0.032998401671648026, 'rmse': 0.1816546219385789, 'r2': -0.1304703950881958}
[2025-05-07 20:57:11,102][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0447Epoch 10/15: [                              ] 2/75 batches, loss: 0.0385Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0364Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0418Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0365Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0333Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0324Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0326Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0326Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0323Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0312Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0339Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0337Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0337Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0331Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0351Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0357Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0357Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0363Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0356Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0354Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0345Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0378Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0378Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0376Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0378Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0373Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0373Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0367Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0364Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0364Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0363Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0358Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0366Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0360Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0363Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0362Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0359Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0357Epoch 10/15: [================              ] 40/75 batches, loss: 0.0358Epoch 10/15: [================              ] 41/75 batches, loss: 0.0355Epoch 10/15: [================              ] 42/75 batches, loss: 0.0352Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0355Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0353Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0351Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0349Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0350Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0356Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0353Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0351Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0353Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0355Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0352Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0351Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0351Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0353Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0355Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0355Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0354Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0354Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0357Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0359Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0358Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0360Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0359Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0358Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0358Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0364Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0366Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0365Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0364Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0361Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0362Epoch 10/15: [==============================] 75/75 batches, loss: 0.0360
[2025-05-07 20:57:13,387][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0360
[2025-05-07 20:57:13,625][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0349, Metrics: {'mse': 0.03400350734591484, 'rmse': 0.18440039952753584, 'r2': -0.16490375995635986}
[2025-05-07 20:57:13,625][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:57:13,626][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:57:13,626][src.training.lm_trainer][INFO] - Training completed in 27.93 seconds
[2025-05-07 20:57:13,626][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:57:16,503][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.017684567719697952, 'rmse': 0.13298333624818545, 'r2': -0.03474223613739014}
[2025-05-07 20:57:16,503][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.029433226212859154, 'rmse': 0.17156114423976995, 'r2': -0.008333444595336914}
[2025-05-07 20:57:16,503][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09298183768987656, 'rmse': 0.3049292339049776, 'r2': -0.37363457679748535}
[2025-05-07 20:57:18,202][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ja/ja/model.pt
[2025-05-07 20:57:18,204][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▁
wandb:     best_val_mse █▄▃▁
wandb:      best_val_r2 ▁▅▆█
wandb:    best_val_rmse █▅▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▃▃▄▄▃▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▄▄▁▂▄▃▄
wandb:          val_mse █▄▃▄▄▁▂▄▃▄
wandb:           val_r2 ▁▅▆▅▅█▇▅▆▅
wandb:         val_rmse █▅▄▄▄▁▂▄▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03025
wandb:     best_val_mse 0.02943
wandb:      best_val_r2 -0.00833
wandb:    best_val_rmse 0.17156
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.09298
wandb:    final_test_r2 -0.37363
wandb:  final_test_rmse 0.30493
wandb:  final_train_mse 0.01768
wandb:   final_train_r2 -0.03474
wandb: final_train_rmse 0.13298
wandb:    final_val_mse 0.02943
wandb:     final_val_r2 -0.00833
wandb:   final_val_rmse 0.17156
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03601
wandb:       train_time 27.92647
wandb:         val_loss 0.03492
wandb:          val_mse 0.034
wandb:           val_r2 -0.1649
wandb:         val_rmse 0.1844
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205631-d3e4c3rh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205631-d3e4c3rh/logs
Experiment probe_layer2_avg_max_depth_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:57:40,410][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ja
experiment_name: probe_layer2_avg_max_depth_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:57:40,410][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:57:40,410][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:57:40,410][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:57:40,410][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:57:40,415][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:57:40,415][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 20:57:40,415][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:57:43,042][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:57:45,437][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:57:45,437][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:57:45,631][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 20:57:45,692][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:22:13 2025).
[2025-05-07 20:57:45,887][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:57:45,896][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:57:45,896][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:57:45,898][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:57:45,972][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:57:46,015][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:57:46,032][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:57:46,033][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:57:46,033][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:57:46,034][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:57:46,124][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:57:46,209][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:57:46,225][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:57:46,227][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:57:46,227][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:57:46,228][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:57:46,229][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:57:46,229][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:57:46,229][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:57:46,229][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:57:46,229][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 20:57:46,229][src.data.datasets][INFO] -   Mean: 0.2860, Std: 0.1307
[2025-05-07 20:57:46,229][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:57:46,230][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:57:46,230][src.data.datasets][INFO] -   Mean: 0.2943, Std: 0.1709
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:57:46,230][src.data.datasets][INFO] - Sample label: 0.28600001335144043
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 20:57:46,231][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:57:46,231][src.data.datasets][INFO] -   Mean: 0.4112, Std: 0.2602
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:57:46,231][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:57:46,232][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:57:46,232][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 20:57:46,232][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:57:52,722][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:57:52,723][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:57:52,723][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:57:52,723][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:57:52,726][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:57:52,727][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:57:52,727][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:57:52,727][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:57:52,727][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:57:52,728][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:57:52,728][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4321Epoch 1/15: [                              ] 2/75 batches, loss: 0.4683Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4535Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4557Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4467Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4236Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4096Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4175Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4005Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3796Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3709Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3798Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3660Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3715Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3695Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3827Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3801Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4013Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3932Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3963Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3892Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3870Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3747Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3657Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3624Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3572Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3545Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3479Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3441Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3393Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3342Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3316Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3317Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3293Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3275Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3294Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3255Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3200Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3196Epoch 1/15: [================              ] 40/75 batches, loss: 0.3151Epoch 1/15: [================              ] 41/75 batches, loss: 0.3105Epoch 1/15: [================              ] 42/75 batches, loss: 0.3099Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3086Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3109Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3103Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3064Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3040Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3009Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2993Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2968Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2995Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3029Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2989Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3001Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2999Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2959Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2930Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2909Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2889Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2884Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2869Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2864Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2862Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2864Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2860Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2836Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2825Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2794Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2776Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2749Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2741Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2752Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2729Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2724Epoch 1/15: [==============================] 75/75 batches, loss: 0.2699
[2025-05-07 20:57:58,860][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2699
[2025-05-07 20:57:59,039][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0400, Metrics: {'mse': 0.03907639905810356, 'rmse': 0.19767751277801823, 'r2': -0.33869266510009766}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1425Epoch 2/15: [                              ] 2/75 batches, loss: 0.1957Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1883Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1854Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1728Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1592Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1696Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1684Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1764Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1783Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1702Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1695Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1669Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1610Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1636Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1627Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1599Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1588Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1549Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1497Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1470Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1468Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1440Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1436Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1393Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1437Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1427Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1426Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1424Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1413Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1424Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1456Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1446Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1435Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1439Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1422Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1449Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1447Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1436Epoch 2/15: [================              ] 40/75 batches, loss: 0.1437Epoch 2/15: [================              ] 41/75 batches, loss: 0.1437Epoch 2/15: [================              ] 42/75 batches, loss: 0.1453Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1436Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1470Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1452Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1437Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1427Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1423Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1413Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1403Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1398Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1390Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1381Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1376Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1364Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1369Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1355Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1347Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1336Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1334Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1328Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1333Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1328Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1318Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1306Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1298Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1310Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1307Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1304Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1303Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1300Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1300Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1289Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1287Epoch 2/15: [==============================] 75/75 batches, loss: 0.1290
[2025-05-07 20:58:01,713][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1290
[2025-05-07 20:58:01,914][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0377, Metrics: {'mse': 0.03679153323173523, 'rmse': 0.19181119162273932, 'r2': -0.26041674613952637}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1174Epoch 3/15: [                              ] 2/75 batches, loss: 0.1200Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1021Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1027Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1052Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1096Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1063Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1054Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1104Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1097Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1113Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1072Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1046Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0998Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1001Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0997Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0970Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0958Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0949Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0975Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1012Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0997Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0998Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0979Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0952Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0950Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0975Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0967Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0954Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0937Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0942Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0989Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1009Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1012Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1009Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1003Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1012Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0995Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0986Epoch 3/15: [================              ] 40/75 batches, loss: 0.0978Epoch 3/15: [================              ] 41/75 batches, loss: 0.0974Epoch 3/15: [================              ] 42/75 batches, loss: 0.0973Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0964Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0967Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0974Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0970Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0968Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0966Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0964Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0961Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0970Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0970Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0964Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0956Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0946Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0939Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0934Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0937Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0935Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0922Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0920Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0915Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0917Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0911Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0911Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0903Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0897Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0897Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0899Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0894Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0888Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0891Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0901Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0898Epoch 3/15: [==============================] 75/75 batches, loss: 0.0892
[2025-05-07 20:58:04,603][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0892
[2025-05-07 20:58:04,811][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0343, Metrics: {'mse': 0.03342382237315178, 'rmse': 0.18282183232084667, 'r2': -0.14504468441009521}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0857Epoch 4/15: [                              ] 2/75 batches, loss: 0.0809Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0830Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0936Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0825Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0813Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0768Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0750Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0758Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0754Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0789Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0753Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0733Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0737Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0719Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0736Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0760Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0767Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0788Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0791Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0777Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0767Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0768Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0780Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0765Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0753Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0760Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0764Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0754Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0746Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0751Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0745Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0744Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0740Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0749Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0747Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0744Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0744Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0735Epoch 4/15: [================              ] 40/75 batches, loss: 0.0740Epoch 4/15: [================              ] 41/75 batches, loss: 0.0732Epoch 4/15: [================              ] 42/75 batches, loss: 0.0727Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0727Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0723Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0718Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0722Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0723Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0723Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0718Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0709Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0706Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0702Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0708Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0704Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0704Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0705Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0702Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0713Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0714Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0708Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0710Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0711Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0712Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0711Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0710Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0712Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0714Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0711Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0713Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0709Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0707Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0704Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0703Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0716Epoch 4/15: [==============================] 75/75 batches, loss: 0.0715
[2025-05-07 20:58:07,458][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0715
[2025-05-07 20:58:07,669][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0352, Metrics: {'mse': 0.034319326281547546, 'rmse': 0.18525476048282147, 'r2': -0.17572307586669922}
[2025-05-07 20:58:07,670][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0267Epoch 5/15: [                              ] 2/75 batches, loss: 0.0593Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0588Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0556Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0577Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0665Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0724Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0673Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0718Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0695Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0729Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0703Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0696Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0697Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0685Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0695Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0694Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0686Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0668Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0652Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0649Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0670Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0683Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0686Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0689Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0686Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0688Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0674Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0671Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0675Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0678Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0667Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0658Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0660Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0654Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0651Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0653Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0650Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0651Epoch 5/15: [================              ] 40/75 batches, loss: 0.0645Epoch 5/15: [================              ] 41/75 batches, loss: 0.0645Epoch 5/15: [================              ] 42/75 batches, loss: 0.0662Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0655Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0648Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0648Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0655Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0654Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0657Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0656Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0649Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0650Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0650Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0645Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0643Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0653Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0647Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0646Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0659Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0659Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0668Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0669Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0665Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0659Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0663Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0666Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0665Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0664Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0664Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0665Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0662Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0659Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0656Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0655Epoch 5/15: [==============================] 75/75 batches, loss: 0.0653
[2025-05-07 20:58:09,963][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0653
[2025-05-07 20:58:10,171][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0314, Metrics: {'mse': 0.030550280585885048, 'rmse': 0.17478638558504792, 'r2': -0.04660189151763916}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0656Epoch 6/15: [                              ] 2/75 batches, loss: 0.0670Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0577Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0573Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0654Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0666Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0647Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0658Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0623Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0596Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0577Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0568Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0548Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0547Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0529Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0526Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0529Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0522Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0513Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0508Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0501Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0496Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0491Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0501Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0513Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0514Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0510Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0514Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0518Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0527Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0516Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0518Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0512Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0514Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0518Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0526Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0527Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0530Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0537Epoch 6/15: [================              ] 40/75 batches, loss: 0.0540Epoch 6/15: [================              ] 41/75 batches, loss: 0.0544Epoch 6/15: [================              ] 42/75 batches, loss: 0.0541Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0535Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0528Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0528Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0525Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0526Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0525Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0529Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0530Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0525Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0527Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0525Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0530Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0531Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0530Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0529Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0526Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0528Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0529Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0526Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0527Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0529Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0527Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0523Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0521Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0522Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0523Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0522Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0519Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0520Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0518Epoch 6/15: [==============================] 75/75 batches, loss: 0.0518
[2025-05-07 20:58:12,817][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0518
[2025-05-07 20:58:13,047][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0297, Metrics: {'mse': 0.028925906866788864, 'rmse': 0.17007617959840485, 'r2': 0.009046494960784912}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0223Epoch 7/15: [                              ] 2/75 batches, loss: 0.0452Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0481Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0422Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0473Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0455Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0468Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0545Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0524Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0532Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0517Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0494Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0509Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0497Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0480Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0479Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0482Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0492Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0485Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0474Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0470Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0460Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0458Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0459Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0462Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0465Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0479Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0471Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0471Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0474Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0475Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0466Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0466Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0475Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0470Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0474Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0470Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0475Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0475Epoch 7/15: [================              ] 40/75 batches, loss: 0.0471Epoch 7/15: [================              ] 41/75 batches, loss: 0.0464Epoch 7/15: [================              ] 42/75 batches, loss: 0.0461Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0458Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0457Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0454Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0453Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0451Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0456Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0456Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0457Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0456Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0451Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0463Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0463Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0463Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0459Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0459Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0459Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0457Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0454Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0449Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0447Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0447Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0449Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0447Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0450Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0450Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0450Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0449Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0456Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0455Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0452Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0450Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0450Epoch 7/15: [==============================] 75/75 batches, loss: 0.0450
[2025-05-07 20:58:15,753][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0450
[2025-05-07 20:58:15,971][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0342, Metrics: {'mse': 0.03327690437436104, 'rmse': 0.18241958330826502, 'r2': -0.14001142978668213}
[2025-05-07 20:58:15,972][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0471Epoch 8/15: [                              ] 2/75 batches, loss: 0.0499Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0434Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0410Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0382Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0447Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0442Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0425Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0440Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0427Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0430Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0422Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0424Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0430Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0422Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0416Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0420Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0429Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0427Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0423Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0427Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0437Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0435Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0429Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0417Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0430Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0431Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0424Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0435Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0436Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0439Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0440Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0437Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0435Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0436Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0433Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0433Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0436Epoch 8/15: [================              ] 40/75 batches, loss: 0.0432Epoch 8/15: [================              ] 41/75 batches, loss: 0.0428Epoch 8/15: [================              ] 42/75 batches, loss: 0.0432Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0428Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0429Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0435Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0433Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0434Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0436Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0430Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0431Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0433Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0432Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0435Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0435Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0434Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0435Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0435Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0437Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0433Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0431Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0426Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0423Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0420Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0419Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0416Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0420Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0418Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0420Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0420Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0421Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0421Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0424Epoch 8/15: [==============================] 75/75 batches, loss: 0.0421
[2025-05-07 20:58:18,259][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0421
[2025-05-07 20:58:18,475][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0346, Metrics: {'mse': 0.03363430127501488, 'rmse': 0.1833965683294398, 'r2': -0.15225529670715332}
[2025-05-07 20:58:18,476][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0401Epoch 9/15: [                              ] 2/75 batches, loss: 0.0374Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0416Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0440Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0441Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0399Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0391Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0392Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0378Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0434Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0408Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0410Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0396Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0399Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0386Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0388Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0395Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0414Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0404Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0391Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0395Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0399Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0397Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0387Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0378Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0369Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0362Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0371Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0371Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0368Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0381Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0384Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0383Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0383Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0386Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0390Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0392Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0391Epoch 9/15: [================              ] 40/75 batches, loss: 0.0399Epoch 9/15: [================              ] 41/75 batches, loss: 0.0393Epoch 9/15: [================              ] 42/75 batches, loss: 0.0392Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0389Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0390Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0385Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0387Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0386Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0381Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0383Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0388Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0384Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0380Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0382Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0378Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0376Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0374Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0373Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0372Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0369Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0369Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0365Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0364Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0362Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0362Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0362Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0361Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0362Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0361Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0362Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0364Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0364Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0361Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0359Epoch 9/15: [==============================] 75/75 batches, loss: 0.0356
[2025-05-07 20:58:20,771][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0356
[2025-05-07 20:58:20,993][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0333, Metrics: {'mse': 0.03240681067109108, 'rmse': 0.18001891753671634, 'r2': -0.11020350456237793}
[2025-05-07 20:58:20,993][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0387Epoch 10/15: [                              ] 2/75 batches, loss: 0.0387Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0424Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0378Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0339Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0329Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0340Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0355Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0341Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0353Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0351Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0347Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0349Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0356Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0351Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0353Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0356Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0357Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0366Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0371Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0370Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0375Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0371Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0376Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0373Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0375Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0367Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0362Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0359Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0352Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0347Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0346Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0343Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0344Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0340Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0336Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0343Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0342Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0340Epoch 10/15: [================              ] 40/75 batches, loss: 0.0340Epoch 10/15: [================              ] 41/75 batches, loss: 0.0337Epoch 10/15: [================              ] 42/75 batches, loss: 0.0344Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0352Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0352Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0352Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0352Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0357Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0360Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0358Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0358Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0357Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0361Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0358Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0356Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0352Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0355Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0354Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0356Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0351Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0349Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0353Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0356Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0356Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0356Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0355Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0357Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0361Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0361Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0362Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0360Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0361Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0361Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0361Epoch 10/15: [==============================] 75/75 batches, loss: 0.0359
[2025-05-07 20:58:23,277][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0359
[2025-05-07 20:58:23,492][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0347, Metrics: {'mse': 0.03382433205842972, 'rmse': 0.18391392567837195, 'r2': -0.15876543521881104}
[2025-05-07 20:58:23,493][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:58:23,493][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:58:23,493][src.training.lm_trainer][INFO] - Training completed in 27.92 seconds
[2025-05-07 20:58:23,493][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:58:26,338][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.017755959182977676, 'rmse': 0.13325148848315982, 'r2': -0.03891944885253906}
[2025-05-07 20:58:26,339][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.028925906866788864, 'rmse': 0.17007617959840485, 'r2': 0.009046494960784912}
[2025-05-07 20:58:26,339][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09101542085409164, 'rmse': 0.301687621313987, 'r2': -0.34458446502685547}
[2025-05-07 20:58:27,973][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ja/ja/model.pt
[2025-05-07 20:58:27,975][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▂▁
wandb:     best_val_mse █▆▄▂▁
wandb:      best_val_r2 ▁▃▅▇█
wandb:    best_val_rmse █▇▄▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▃▄▄▃▃▃
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▅▂▁▄▄▃▄
wandb:          val_mse █▆▄▅▂▁▄▄▃▄
wandb:           val_r2 ▁▃▅▄▇█▅▅▆▅
wandb:         val_rmse █▇▄▅▂▁▄▄▄▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02972
wandb:     best_val_mse 0.02893
wandb:      best_val_r2 0.00905
wandb:    best_val_rmse 0.17008
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.09102
wandb:    final_test_r2 -0.34458
wandb:  final_test_rmse 0.30169
wandb:  final_train_mse 0.01776
wandb:   final_train_r2 -0.03892
wandb: final_train_rmse 0.13325
wandb:    final_val_mse 0.02893
wandb:     final_val_r2 0.00905
wandb:   final_val_rmse 0.17008
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03593
wandb:       train_time 27.91773
wandb:         val_loss 0.03474
wandb:          val_mse 0.03382
wandb:           val_r2 -0.15877
wandb:         val_rmse 0.18391
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205740-3boktjva
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205740-3boktjva/logs
Experiment probe_layer2_avg_max_depth_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 20:58:49,374][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ja
experiment_name: probe_layer2_avg_subordinate_chain_len_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 20:58:49,374][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 20:58:49,374][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:58:49,374][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 20:58:49,374][__main__][INFO] - Determined Task Type: regression
[2025-05-07 20:58:49,379][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 20:58:49,379][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 20:58:49,379][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 20:58:52,018][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 20:58:54,324][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 20:58:54,324][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:58:54,517][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 20:58:54,615][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:23:31 2025).
[2025-05-07 20:58:54,886][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 20:58:54,894][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:58:54,895][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 20:58:54,897][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:58:54,979][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:58:55,058][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:58:55,077][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 20:58:55,079][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:58:55,079][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 20:58:55,081][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 20:58:55,188][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:58:55,290][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 20:58:55,317][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 20:58:55,319][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 20:58:55,319][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 20:58:55,320][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 20:58:55,320][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:58:55,321][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:58:55,321][src.data.datasets][INFO] -   Mean: 0.1797, Std: 0.2539
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 20:58:55,321][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:58:55,322][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5000
[2025-05-07 20:58:55,322][src.data.datasets][INFO] -   Mean: 0.1848, Std: 0.2413
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 20:58:55,322][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 20:58:55,323][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 20:58:55,323][src.data.datasets][INFO] -   Mean: 0.3168, Std: 0.2713
[2025-05-07 20:58:55,323][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 20:58:55,323][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 20:58:55,323][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 20:58:55,323][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 20:58:55,323][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 20:58:55,323][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 20:58:55,324][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 20:59:02,044][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 20:59:02,044][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 20:59:02,044][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 20:59:02,045][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 20:59:02,047][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 20:59:02,048][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 20:59:02,048][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 20:59:02,048][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 20:59:02,048][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 20:59:02,049][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 20:59:02,049][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4982Epoch 1/15: [                              ] 2/75 batches, loss: 0.6034Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5347Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5138Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4968Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4601Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4420Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4614Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4404Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4399Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4252Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4338Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4128Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4265Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4243Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4311Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4288Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4566Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4484Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4489Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4394Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4411Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4283Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4162Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4171Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4162Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4084Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4036Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4024Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3962Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3899Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3859Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3853Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3850Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3821Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3830Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3793Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3757Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3745Epoch 1/15: [================              ] 40/75 batches, loss: 0.3707Epoch 1/15: [================              ] 41/75 batches, loss: 0.3673Epoch 1/15: [================              ] 42/75 batches, loss: 0.3651Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3627Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3624Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3621Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3577Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3556Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3530Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3515Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3494Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3491Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3505Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3463Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3483Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3501Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3476Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3440Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3418Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3400Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3392Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3376Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3367Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3388Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3384Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3370Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3363Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3350Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3324Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3310Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3286Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3272Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3264Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3237Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3239Epoch 1/15: [==============================] 75/75 batches, loss: 0.3224
[2025-05-07 20:59:07,901][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3224
[2025-05-07 20:59:08,135][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0725, Metrics: {'mse': 0.07239063829183578, 'rmse': 0.26905508412188717, 'r2': -0.24282848834991455}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1320Epoch 2/15: [                              ] 2/75 batches, loss: 0.2107Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2564Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2647Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2595Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2446Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2532Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2431Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2514Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2477Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2384Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2368Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2383Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2346Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2365Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2331Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2358Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2393Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2361Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2305Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2270Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2291Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2233Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2208Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2160Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2151Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2156Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2146Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2179Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2149Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2121Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2132Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2114Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2103Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2086Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2070Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2079Epoch 2/15: [===============               ] 38/75 batches, loss: 0.2056Epoch 2/15: [===============               ] 39/75 batches, loss: 0.2033Epoch 2/15: [================              ] 40/75 batches, loss: 0.2072Epoch 2/15: [================              ] 41/75 batches, loss: 0.2058Epoch 2/15: [================              ] 42/75 batches, loss: 0.2040Epoch 2/15: [=================             ] 43/75 batches, loss: 0.2021Epoch 2/15: [=================             ] 44/75 batches, loss: 0.2024Epoch 2/15: [==================            ] 45/75 batches, loss: 0.2005Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1991Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1986Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1961Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1961Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1953Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1963Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1961Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1968Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1958Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1936Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1956Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1955Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1948Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1933Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1917Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1917Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1937Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1929Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1910Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1890Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1876Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1886Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1881Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1869Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1866Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1864Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1858Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1852Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1838Epoch 2/15: [==============================] 75/75 batches, loss: 0.1839
[2025-05-07 20:59:10,847][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1839
[2025-05-07 20:59:11,041][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0712, Metrics: {'mse': 0.07058774679899216, 'rmse': 0.26568354634600944, 'r2': -0.2118757963180542}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2297Epoch 3/15: [                              ] 2/75 batches, loss: 0.2058Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1868Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1707Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1674Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1682Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1547Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1579Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1570Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1521Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1534Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1472Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1564Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1504Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1466Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1480Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1453Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1465Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1459Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1498Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1510Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1477Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1501Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1490Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1470Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1458Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1523Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1518Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1511Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1505Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1508Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1558Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1558Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1550Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1558Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1554Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1591Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1564Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1558Epoch 3/15: [================              ] 40/75 batches, loss: 0.1549Epoch 3/15: [================              ] 41/75 batches, loss: 0.1548Epoch 3/15: [================              ] 42/75 batches, loss: 0.1548Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1528Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1521Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1525Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1530Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1515Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1495Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1487Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1473Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1480Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1481Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1475Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1462Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1452Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1440Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1430Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1442Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1438Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1427Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1426Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1424Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1412Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1407Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1422Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1414Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1406Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1404Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1399Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1393Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1386Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1395Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1395Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1390Epoch 3/15: [==============================] 75/75 batches, loss: 0.1403
[2025-05-07 20:59:13,742][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1403
[2025-05-07 20:59:13,952][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0636, Metrics: {'mse': 0.06314177811145782, 'rmse': 0.2512802779994041, 'r2': -0.08404064178466797}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1349Epoch 4/15: [                              ] 2/75 batches, loss: 0.1199Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1576Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1603Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1380Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1321Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1318Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1383Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1332Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1284Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1362Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1340Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1301Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1267Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1226Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1235Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1233Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1231Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1311Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1314Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1284Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1294Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1282Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1299Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1280Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1270Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1286Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1261Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1283Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1279Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1293Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1294Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1310Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1303Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1300Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1310Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1294Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1291Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1281Epoch 4/15: [================              ] 40/75 batches, loss: 0.1283Epoch 4/15: [================              ] 41/75 batches, loss: 0.1278Epoch 4/15: [================              ] 42/75 batches, loss: 0.1288Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1288Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1280Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1268Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1281Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1287Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1290Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1277Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1264Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1268Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1258Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1254Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1246Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1243Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1243Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1233Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1236Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1227Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1218Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1218Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1210Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1206Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1206Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1199Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1198Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1209Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1205Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1208Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1205Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1198Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1214Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1214Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1228Epoch 4/15: [==============================] 75/75 batches, loss: 0.1231
[2025-05-07 20:59:16,657][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1231
[2025-05-07 20:59:16,886][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0587, Metrics: {'mse': 0.05835643410682678, 'rmse': 0.2415707641806574, 'r2': -0.0018842220306396484}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0887Epoch 5/15: [                              ] 2/75 batches, loss: 0.1132Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0946Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1062Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1018Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1188Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1167Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1134Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1145Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1172Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1309Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1277Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1239Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1236Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1224Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1199Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1198Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1195Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1201Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1184Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1172Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1168Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1197Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1175Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1190Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1182Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1197Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1186Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1215Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1205Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1190Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1177Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1172Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1171Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1182Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1165Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1163Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1160Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1144Epoch 5/15: [================              ] 40/75 batches, loss: 0.1150Epoch 5/15: [================              ] 41/75 batches, loss: 0.1144Epoch 5/15: [================              ] 42/75 batches, loss: 0.1160Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1157Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1160Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1156Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1161Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1155Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1150Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1159Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1150Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1141Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1138Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1143Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1147Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1149Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1149Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1140Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1144Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1147Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1140Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1143Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1149Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1146Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1141Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1148Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1151Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1143Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1134Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1129Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1130Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1122Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1112Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1114Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1117Epoch 5/15: [==============================] 75/75 batches, loss: 0.1130
[2025-05-07 20:59:19,533][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1130
[2025-05-07 20:59:19,744][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0645, Metrics: {'mse': 0.0639607310295105, 'rmse': 0.25290458878697814, 'r2': -0.09810078144073486}
[2025-05-07 20:59:19,744][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1009Epoch 6/15: [                              ] 2/75 batches, loss: 0.1361Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1252Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1103Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1283Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1256Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1160Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1133Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1148Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1159Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1099Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1091Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1078Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1115Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1095Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1079Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1115Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1152Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1135Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1144Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1160Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1162Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1132Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1144Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1180Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1181Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1164Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1175Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1151Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1154Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1148Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1144Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1123Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1123Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1116Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1131Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1125Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1129Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1132Epoch 6/15: [================              ] 40/75 batches, loss: 0.1128Epoch 6/15: [================              ] 41/75 batches, loss: 0.1131Epoch 6/15: [================              ] 42/75 batches, loss: 0.1126Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1127Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1127Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1118Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1109Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1100Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1093Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1106Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1111Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1104Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1093Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1087Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1079Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1073Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1067Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1068Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1074Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1074Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1063Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1063Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1064Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1058Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1054Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1060Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1056Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1051Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1043Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1038Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1033Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1037Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1043Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1038Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1032Epoch 6/15: [==============================] 75/75 batches, loss: 0.1032
[2025-05-07 20:59:22,033][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1032
[2025-05-07 20:59:22,257][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0562, Metrics: {'mse': 0.05577777698636055, 'rmse': 0.23617319277674287, 'r2': 0.04238706827163696}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0564Epoch 7/15: [                              ] 2/75 batches, loss: 0.0619Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0689Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0747Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0888Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0934Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0971Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0968Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0944Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0971Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0943Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0932Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0975Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0990Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0961Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0940Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0913Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0923Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0908Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0924Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0920Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0900Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0902Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0915Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0984Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0981Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0992Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0970Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0979Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0984Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0981Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0979Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0986Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0995Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0993Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0990Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0988Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0993Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0991Epoch 7/15: [================              ] 40/75 batches, loss: 0.1013Epoch 7/15: [================              ] 41/75 batches, loss: 0.1016Epoch 7/15: [================              ] 42/75 batches, loss: 0.1013Epoch 7/15: [=================             ] 43/75 batches, loss: 0.1014Epoch 7/15: [=================             ] 44/75 batches, loss: 0.1010Epoch 7/15: [==================            ] 45/75 batches, loss: 0.1004Epoch 7/15: [==================            ] 46/75 batches, loss: 0.1005Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0996Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0983Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0977Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0983Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0980Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0973Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0973Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0972Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0964Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0969Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0971Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0964Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0965Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0971Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0964Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0964Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0967Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0968Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0987Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0992Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0986Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0980Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0976Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0974Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0974Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0973Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0970Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0970Epoch 7/15: [==============================] 75/75 batches, loss: 0.0966
[2025-05-07 20:59:24,982][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0966
[2025-05-07 20:59:25,222][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0578, Metrics: {'mse': 0.05733589082956314, 'rmse': 0.2394491403817586, 'r2': 0.015636861324310303}
[2025-05-07 20:59:25,223][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0690Epoch 8/15: [                              ] 2/75 batches, loss: 0.0815Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0986Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0964Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0904Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0975Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0942Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0914Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0906Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0889Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0921Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0902Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0914Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0905Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0888Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0875Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0860Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0852Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0856Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0862Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0865Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0856Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0860Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0870Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0877Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0859Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0880Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0869Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0872Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0861Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0853Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0877Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0873Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0909Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0917Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0911Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0906Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0908Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0902Epoch 8/15: [================              ] 40/75 batches, loss: 0.0908Epoch 8/15: [================              ] 41/75 batches, loss: 0.0914Epoch 8/15: [================              ] 42/75 batches, loss: 0.0902Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0899Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0922Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0920Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0917Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0912Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0913Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0906Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0910Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0906Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0898Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0904Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0903Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0902Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0904Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0905Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0913Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0906Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0906Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0912Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0910Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0905Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0911Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0911Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0911Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0908Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0902Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0900Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0897Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0901Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0900Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0911Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0908Epoch 8/15: [==============================] 75/75 batches, loss: 0.0907
[2025-05-07 20:59:27,517][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0907
[2025-05-07 20:59:27,728][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0596, Metrics: {'mse': 0.05915403738617897, 'rmse': 0.24321603028209093, 'r2': -0.01557779312133789}
[2025-05-07 20:59:27,729][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0422Epoch 9/15: [                              ] 2/75 batches, loss: 0.0760Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0695Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0746Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0804Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0774Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0854Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0859Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0852Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0859Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0911Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0905Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0920Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0920Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0927Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0896Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0901Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0913Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0917Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0949Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0939Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0930Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0926Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0920Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0910Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0899Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0899Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0895Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0897Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0900Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0882Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0875Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0877Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0876Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0869Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0862Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0858Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0884Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0876Epoch 9/15: [================              ] 40/75 batches, loss: 0.0868Epoch 9/15: [================              ] 41/75 batches, loss: 0.0871Epoch 9/15: [================              ] 42/75 batches, loss: 0.0867Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0890Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0887Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0882Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0890Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0901Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0906Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0905Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0902Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0893Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0892Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0893Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0898Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0905Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0900Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0892Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0887Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0887Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0891Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0892Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0891Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0888Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0887Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0890Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0886Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0885Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0882Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0885Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0884Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0879Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0880Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0884Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0887Epoch 9/15: [==============================] 75/75 batches, loss: 0.0881
[2025-05-07 20:59:30,019][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0881
[2025-05-07 20:59:30,239][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0571, Metrics: {'mse': 0.056745514273643494, 'rmse': 0.23821316981569993, 'r2': 0.02577263116836548}
[2025-05-07 20:59:30,240][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0447Epoch 10/15: [                              ] 2/75 batches, loss: 0.0804Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0908Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0798Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0804Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0789Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0740Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0794Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0800Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0781Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0801Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0795Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0817Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0829Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0829Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0830Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0810Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0840Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0848Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0847Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0864Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0860Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0874Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0862Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0860Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0862Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0873Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0878Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0884Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0868Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0857Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0851Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0854Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0847Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0850Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0846Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0848Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0842Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0848Epoch 10/15: [================              ] 40/75 batches, loss: 0.0853Epoch 10/15: [================              ] 41/75 batches, loss: 0.0851Epoch 10/15: [================              ] 42/75 batches, loss: 0.0863Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0866Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0864Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0869Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0861Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0865Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0873Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0868Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0882Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0879Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0882Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0881Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0880Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0879Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0882Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0887Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0887Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0879Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0874Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0875Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0880Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0882Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0881Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0881Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0875Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0880Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0883Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0886Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0884Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0882Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0885Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0888Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0883Epoch 10/15: [==============================] 75/75 batches, loss: 0.0878
[2025-05-07 20:59:32,547][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0878
[2025-05-07 20:59:32,758][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0572, Metrics: {'mse': 0.05689374357461929, 'rmse': 0.23852409432721738, 'r2': 0.023227810859680176}
[2025-05-07 20:59:32,759][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 20:59:32,759][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 20:59:32,759][src.training.lm_trainer][INFO] - Training completed in 27.94 seconds
[2025-05-07 20:59:32,759][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 20:59:35,605][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06507916748523712, 'rmse': 0.2551061886455072, 'r2': -0.009258151054382324}
[2025-05-07 20:59:35,605][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05577777698636055, 'rmse': 0.23617319277674287, 'r2': 0.04238706827163696}
[2025-05-07 20:59:35,606][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10940519720315933, 'rmse': 0.33076456461229237, 'r2': -0.486819863319397}
[2025-05-07 20:59:37,250][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ja/ja/model.pt
[2025-05-07 20:59:37,251][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▂▁
wandb:     best_val_mse █▇▄▂▁
wandb:      best_val_r2 ▁▂▅▇█
wandb:    best_val_rmse █▇▄▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▃▂▄▄▃▄
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▂▅▁▂▂▁▁
wandb:          val_mse █▇▄▂▄▁▂▂▁▁
wandb:           val_r2 ▁▂▅▇▅█▇▇██
wandb:         val_rmse █▇▄▂▅▁▂▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05615
wandb:     best_val_mse 0.05578
wandb:      best_val_r2 0.04239
wandb:    best_val_rmse 0.23617
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.10941
wandb:    final_test_r2 -0.48682
wandb:  final_test_rmse 0.33076
wandb:  final_train_mse 0.06508
wandb:   final_train_r2 -0.00926
wandb: final_train_rmse 0.25511
wandb:    final_val_mse 0.05578
wandb:     final_val_r2 0.04239
wandb:   final_val_rmse 0.23617
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08783
wandb:       train_time 27.93877
wandb:         val_loss 0.0572
wandb:          val_mse 0.05689
wandb:           val_r2 0.02323
wandb:         val_rmse 0.23852
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205849-sk3r8kgi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_205849-sk3r8kgi/logs
Experiment probe_layer2_avg_subordinate_chain_len_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:00:00,212][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ja
experiment_name: probe_layer2_avg_subordinate_chain_len_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:00:00,213][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:00:00,213][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 21:00:00,213][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:00:00,213][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:00:00,217][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:00:00,217][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 21:00:00,218][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:00:03,483][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:00:05,811][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:00:05,812][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:00:05,968][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 21:00:06,054][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:25:20 2025).
[2025-05-07 21:00:06,316][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:00:06,325][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:00:06,325][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:00:06,328][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:00:06,385][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:00:06,480][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:00:06,506][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:00:06,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:00:06,507][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:00:06,510][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:00:06,571][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:00:06,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:00:06,671][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:00:06,672][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:00:06,672][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:00:06,674][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:00:06,675][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:00:06,675][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:00:06,676][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:00:06,676][src.data.datasets][INFO] -   Mean: 0.1797, Std: 0.2539
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:00:06,676][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:00:06,677][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5000
[2025-05-07 21:00:06,677][src.data.datasets][INFO] -   Mean: 0.1848, Std: 0.2413
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:00:06,677][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:00:06,678][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:00:06,678][src.data.datasets][INFO] -   Mean: 0.3168, Std: 0.2713
[2025-05-07 21:00:06,678][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:00:06,678][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:00:06,678][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:00:06,678][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:00:06,678][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:00:06,678][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 21:00:06,679][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:00:14,072][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:00:14,073][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:00:14,073][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:00:14,074][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:00:14,076][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:00:14,077][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:00:14,077][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:00:14,077][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:00:14,077][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:00:14,078][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:00:14,078][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5520Epoch 1/15: [                              ] 2/75 batches, loss: 0.5280Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4710Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4696Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4521Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4313Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4250Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4415Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4252Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4091Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4026Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4140Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3980Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4161Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4182Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4382Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4379Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4519Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4447Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4401Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4355Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4317Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4175Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4066Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4018Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3964Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3933Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3876Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3848Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3788Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3731Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3691Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3682Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3695Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3684Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3696Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3676Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3630Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3626Epoch 1/15: [================              ] 40/75 batches, loss: 0.3583Epoch 1/15: [================              ] 41/75 batches, loss: 0.3563Epoch 1/15: [================              ] 42/75 batches, loss: 0.3583Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3607Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3653Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3646Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3624Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3587Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3563Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3532Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3518Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3512Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3561Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3522Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3502Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3491Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3456Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3421Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3395Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3363Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3351Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3329Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3322Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3336Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3321Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3316Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3313Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3333Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3319Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3305Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3289Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3296Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3306Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3294Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3281Epoch 1/15: [==============================] 75/75 batches, loss: 0.3250
[2025-05-07 21:00:20,643][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3250
[2025-05-07 21:00:20,840][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0704, Metrics: {'mse': 0.07035071402788162, 'rmse': 0.2652370902190748, 'r2': -0.20780634880065918}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2878Epoch 2/15: [                              ] 2/75 batches, loss: 0.2807Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2582Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2884Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2646Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2423Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2472Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2460Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2537Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2618Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2487Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2402Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2421Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2381Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2347Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2347Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2308Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2257Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2198Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2126Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2100Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2069Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2094Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2080Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2046Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2051Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2086Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2061Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2069Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2048Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2034Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2056Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2034Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2016Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1998Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1978Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1992Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1963Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1942Epoch 2/15: [================              ] 40/75 batches, loss: 0.1926Epoch 2/15: [================              ] 41/75 batches, loss: 0.1909Epoch 2/15: [================              ] 42/75 batches, loss: 0.1915Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1917Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1975Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1943Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1913Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1888Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1882Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1894Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1887Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1883Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1877Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1881Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1889Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1883Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1878Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1866Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1847Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1847Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1843Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1840Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1853Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1845Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1835Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1821Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1807Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1827Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1828Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1819Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1828Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1816Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1805Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1797Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1795Epoch 2/15: [==============================] 75/75 batches, loss: 0.1790
[2025-05-07 21:00:23,565][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1790
[2025-05-07 21:00:23,780][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0664, Metrics: {'mse': 0.06602632254362106, 'rmse': 0.2569558766473751, 'r2': -0.13356363773345947}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1721Epoch 3/15: [                              ] 2/75 batches, loss: 0.1561Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1346Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1424Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1473Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1585Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1594Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1582Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1644Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1641Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1642Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1622Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1581Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1552Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1538Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1535Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1507Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1539Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1490Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1506Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1543Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1508Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1509Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1484Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1473Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1488Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1516Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1558Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1553Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1534Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1565Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1614Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1630Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1630Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1616Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1601Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1605Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1594Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1574Epoch 3/15: [================              ] 40/75 batches, loss: 0.1566Epoch 3/15: [================              ] 41/75 batches, loss: 0.1560Epoch 3/15: [================              ] 42/75 batches, loss: 0.1535Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1517Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1507Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1512Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1514Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1516Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1509Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1495Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1488Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1486Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1488Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1480Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1466Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1460Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1453Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1459Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1454Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1446Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1438Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1432Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1426Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1416Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1414Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1426Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1418Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1412Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1410Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1405Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1406Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1395Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1394Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1386Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1396Epoch 3/15: [==============================] 75/75 batches, loss: 0.1393
[2025-05-07 21:00:26,467][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1393
[2025-05-07 21:00:26,695][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0651, Metrics: {'mse': 0.06469104439020157, 'rmse': 0.25434434216274904, 'r2': -0.11063909530639648}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.2397Epoch 4/15: [                              ] 2/75 batches, loss: 0.1703Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1849Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1940Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1698Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1575Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1514Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1457Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1403Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1413Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1417Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1390Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1395Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1366Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1337Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1388Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1387Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1360Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1362Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1378Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1354Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1348Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1357Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1343Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1334Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1310Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1318Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1345Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1334Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1319Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1300Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1295Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1300Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1279Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1283Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1284Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1280Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1267Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1258Epoch 4/15: [================              ] 40/75 batches, loss: 0.1278Epoch 4/15: [================              ] 41/75 batches, loss: 0.1272Epoch 4/15: [================              ] 42/75 batches, loss: 0.1275Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1292Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1294Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1285Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1282Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1288Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1304Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1299Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1285Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1273Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1265Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1279Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1276Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1267Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1277Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1270Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1285Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1282Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1286Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1300Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1295Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1310Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1312Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1308Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1303Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1303Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1306Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1309Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1305Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1297Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1297Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1290Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1297Epoch 4/15: [==============================] 75/75 batches, loss: 0.1288
[2025-05-07 21:00:29,325][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1288
[2025-05-07 21:00:29,542][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0602, Metrics: {'mse': 0.05983695387840271, 'rmse': 0.24461593136670945, 'r2': -0.027302265167236328}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1200Epoch 5/15: [                              ] 2/75 batches, loss: 0.0997Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1177Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1334Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1267Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1258Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1274Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1198Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1173Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1122Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1112Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1118Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1161Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1190Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1163Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1183Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1160Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1157Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1138Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1131Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1118Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1144Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1138Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1129Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1141Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1133Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1136Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1132Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1131Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1113Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1113Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1113Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1106Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1097Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1100Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1081Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1070Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1098Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1087Epoch 5/15: [================              ] 40/75 batches, loss: 0.1081Epoch 5/15: [================              ] 41/75 batches, loss: 0.1074Epoch 5/15: [================              ] 42/75 batches, loss: 0.1098Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1092Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1090Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1078Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1086Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1078Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1088Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1091Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1095Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1085Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1088Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1092Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1099Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1095Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1092Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1095Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1097Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1095Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1091Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1089Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1098Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1096Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1087Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1091Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1090Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1091Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1086Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1088Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1093Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1093Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1088Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1082Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1081Epoch 5/15: [==============================] 75/75 batches, loss: 0.1083
[2025-05-07 21:00:32,173][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1083
[2025-05-07 21:00:32,399][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0627, Metrics: {'mse': 0.06230887770652771, 'rmse': 0.2496174627435503, 'r2': -0.0697411298751831}
[2025-05-07 21:00:32,400][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0757Epoch 6/15: [                              ] 2/75 batches, loss: 0.1080Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1171Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1149Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1134Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1129Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1075Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1065Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1102Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1119Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1145Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1172Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1141Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1153Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1212Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1202Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1194Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1183Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1221Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1218Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1233Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1205Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1184Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1169Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1177Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1159Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1134Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1124Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1140Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1145Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1138Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1143Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1133Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1123Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1137Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1149Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1142Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1133Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1120Epoch 6/15: [================              ] 40/75 batches, loss: 0.1115Epoch 6/15: [================              ] 41/75 batches, loss: 0.1116Epoch 6/15: [================              ] 42/75 batches, loss: 0.1131Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1128Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1120Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1116Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1107Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1095Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1094Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1097Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1098Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1090Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1087Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1083Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1083Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1078Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1073Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1072Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1070Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1071Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1060Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1058Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1064Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1063Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1076Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1086Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1080Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1076Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1070Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1079Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1069Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1066Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1059Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1058Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1052Epoch 6/15: [==============================] 75/75 batches, loss: 0.1079
[2025-05-07 21:00:34,678][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1079
[2025-05-07 21:00:34,967][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0568, Metrics: {'mse': 0.05644630268216133, 'rmse': 0.23758430647279996, 'r2': 0.030909597873687744}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0771Epoch 7/15: [                              ] 2/75 batches, loss: 0.0795Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0777Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0780Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0922Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0955Epoch 7/15: [==                            ] 7/75 batches, loss: 0.1025Epoch 7/15: [===                           ] 8/75 batches, loss: 0.1168Epoch 7/15: [===                           ] 9/75 batches, loss: 0.1164Epoch 7/15: [====                          ] 10/75 batches, loss: 0.1180Epoch 7/15: [====                          ] 11/75 batches, loss: 0.1158Epoch 7/15: [====                          ] 12/75 batches, loss: 0.1120Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.1111Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.1088Epoch 7/15: [======                        ] 15/75 batches, loss: 0.1070Epoch 7/15: [======                        ] 16/75 batches, loss: 0.1054Epoch 7/15: [======                        ] 17/75 batches, loss: 0.1048Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.1045Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.1082Epoch 7/15: [========                      ] 20/75 batches, loss: 0.1075Epoch 7/15: [========                      ] 21/75 batches, loss: 0.1069Epoch 7/15: [========                      ] 22/75 batches, loss: 0.1076Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.1106Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.1092Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.1084Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.1086Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.1079Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.1099Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.1091Epoch 7/15: [============                  ] 30/75 batches, loss: 0.1114Epoch 7/15: [============                  ] 31/75 batches, loss: 0.1125Epoch 7/15: [============                  ] 32/75 batches, loss: 0.1113Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.1091Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.1104Epoch 7/15: [==============                ] 35/75 batches, loss: 0.1089Epoch 7/15: [==============                ] 36/75 batches, loss: 0.1083Epoch 7/15: [==============                ] 37/75 batches, loss: 0.1071Epoch 7/15: [===============               ] 38/75 batches, loss: 0.1081Epoch 7/15: [===============               ] 39/75 batches, loss: 0.1068Epoch 7/15: [================              ] 40/75 batches, loss: 0.1067Epoch 7/15: [================              ] 41/75 batches, loss: 0.1061Epoch 7/15: [================              ] 42/75 batches, loss: 0.1050Epoch 7/15: [=================             ] 43/75 batches, loss: 0.1038Epoch 7/15: [=================             ] 44/75 batches, loss: 0.1047Epoch 7/15: [==================            ] 45/75 batches, loss: 0.1048Epoch 7/15: [==================            ] 46/75 batches, loss: 0.1041Epoch 7/15: [==================            ] 47/75 batches, loss: 0.1044Epoch 7/15: [===================           ] 48/75 batches, loss: 0.1046Epoch 7/15: [===================           ] 49/75 batches, loss: 0.1039Epoch 7/15: [====================          ] 50/75 batches, loss: 0.1030Epoch 7/15: [====================          ] 51/75 batches, loss: 0.1029Epoch 7/15: [====================          ] 52/75 batches, loss: 0.1020Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.1028Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.1021Epoch 7/15: [======================        ] 55/75 batches, loss: 0.1022Epoch 7/15: [======================        ] 56/75 batches, loss: 0.1024Epoch 7/15: [======================        ] 57/75 batches, loss: 0.1026Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.1027Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.1031Epoch 7/15: [========================      ] 60/75 batches, loss: 0.1046Epoch 7/15: [========================      ] 61/75 batches, loss: 0.1038Epoch 7/15: [========================      ] 62/75 batches, loss: 0.1037Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.1032Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.1040Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.1042Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.1036Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.1033Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.1032Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.1031Epoch 7/15: [============================  ] 70/75 batches, loss: 0.1035Epoch 7/15: [============================  ] 71/75 batches, loss: 0.1031Epoch 7/15: [============================  ] 72/75 batches, loss: 0.1021Epoch 7/15: [============================= ] 73/75 batches, loss: 0.1014Epoch 7/15: [============================= ] 74/75 batches, loss: 0.1013Epoch 7/15: [==============================] 75/75 batches, loss: 0.1008
[2025-05-07 21:00:37,672][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1008
[2025-05-07 21:00:37,898][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0580, Metrics: {'mse': 0.057597171515226364, 'rmse': 0.23999410725104556, 'r2': 0.01115107536315918}
[2025-05-07 21:00:37,899][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1094Epoch 8/15: [                              ] 2/75 batches, loss: 0.1178Epoch 8/15: [=                             ] 3/75 batches, loss: 0.1110Epoch 8/15: [=                             ] 4/75 batches, loss: 0.1031Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0933Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0943Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0947Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0999Epoch 8/15: [===                           ] 9/75 batches, loss: 0.1055Epoch 8/15: [====                          ] 10/75 batches, loss: 0.1132Epoch 8/15: [====                          ] 11/75 batches, loss: 0.1078Epoch 8/15: [====                          ] 12/75 batches, loss: 0.1031Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.1017Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.1023Epoch 8/15: [======                        ] 15/75 batches, loss: 0.1059Epoch 8/15: [======                        ] 16/75 batches, loss: 0.1061Epoch 8/15: [======                        ] 17/75 batches, loss: 0.1064Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.1059Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.1081Epoch 8/15: [========                      ] 20/75 batches, loss: 0.1068Epoch 8/15: [========                      ] 21/75 batches, loss: 0.1068Epoch 8/15: [========                      ] 22/75 batches, loss: 0.1069Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.1071Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.1051Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.1045Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.1047Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.1043Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.1038Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.1035Epoch 8/15: [============                  ] 30/75 batches, loss: 0.1029Epoch 8/15: [============                  ] 31/75 batches, loss: 0.1024Epoch 8/15: [============                  ] 32/75 batches, loss: 0.1029Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.1008Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.1018Epoch 8/15: [==============                ] 35/75 batches, loss: 0.1019Epoch 8/15: [==============                ] 36/75 batches, loss: 0.1010Epoch 8/15: [==============                ] 37/75 batches, loss: 0.1010Epoch 8/15: [===============               ] 38/75 batches, loss: 0.1012Epoch 8/15: [===============               ] 39/75 batches, loss: 0.1019Epoch 8/15: [================              ] 40/75 batches, loss: 0.1023Epoch 8/15: [================              ] 41/75 batches, loss: 0.1013Epoch 8/15: [================              ] 42/75 batches, loss: 0.1012Epoch 8/15: [=================             ] 43/75 batches, loss: 0.1007Epoch 8/15: [=================             ] 44/75 batches, loss: 0.1010Epoch 8/15: [==================            ] 45/75 batches, loss: 0.1002Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0993Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0985Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0987Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0989Epoch 8/15: [====================          ] 50/75 batches, loss: 0.1005Epoch 8/15: [====================          ] 51/75 batches, loss: 0.1018Epoch 8/15: [====================          ] 52/75 batches, loss: 0.1020Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.1014Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.1010Epoch 8/15: [======================        ] 55/75 batches, loss: 0.1004Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0999Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0996Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0984Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0976Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0978Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0975Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0968Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0965Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0962Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0957Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0950Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0952Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0948Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0950Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0947Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0940Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0940Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0942Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0938Epoch 8/15: [==============================] 75/75 batches, loss: 0.0931
[2025-05-07 21:00:40,173][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0931
[2025-05-07 21:00:40,405][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0660, Metrics: {'mse': 0.06553371995687485, 'rmse': 0.25599554675203795, 'r2': -0.12510645389556885}
[2025-05-07 21:00:40,406][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0912Epoch 9/15: [                              ] 2/75 batches, loss: 0.0998Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0971Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0930Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0985Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0918Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0940Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0997Epoch 9/15: [===                           ] 9/75 batches, loss: 0.1001Epoch 9/15: [====                          ] 10/75 batches, loss: 0.1029Epoch 9/15: [====                          ] 11/75 batches, loss: 0.1011Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0962Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0976Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0976Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0990Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0981Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0980Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0963Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0960Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0945Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0931Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0930Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0932Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0956Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0944Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0932Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0926Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0938Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0941Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0940Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0931Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0925Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0937Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0921Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0913Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0910Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0897Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0921Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0909Epoch 9/15: [================              ] 40/75 batches, loss: 0.0917Epoch 9/15: [================              ] 41/75 batches, loss: 0.0914Epoch 9/15: [================              ] 42/75 batches, loss: 0.0916Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0914Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0916Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0920Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0920Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0914Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0909Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0908Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0904Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0903Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0903Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0903Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0902Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0897Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0901Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0904Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0897Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0892Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0894Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0910Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0907Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0910Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0915Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0914Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0912Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0909Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0907Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0914Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0910Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0919Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0913Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0915Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0908Epoch 9/15: [==============================] 75/75 batches, loss: 0.0912
[2025-05-07 21:00:42,713][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0912
[2025-05-07 21:00:42,920][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0570, Metrics: {'mse': 0.056753408163785934, 'rmse': 0.2382297382019842, 'r2': 0.025637149810791016}
[2025-05-07 21:00:42,921][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0997Epoch 10/15: [                              ] 2/75 batches, loss: 0.0796Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0783Epoch 10/15: [=                             ] 4/75 batches, loss: 0.1000Epoch 10/15: [==                            ] 5/75 batches, loss: 0.1019Epoch 10/15: [==                            ] 6/75 batches, loss: 0.1015Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0963Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0949Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0907Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0967Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0941Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0925Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0923Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0963Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0960Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0980Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0960Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0955Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0938Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0929Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0915Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0902Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0952Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0943Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0938Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0952Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0962Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0971Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0966Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0952Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0937Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0929Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0918Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0924Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0912Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0911Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0931Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0919Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0913Epoch 10/15: [================              ] 40/75 batches, loss: 0.0908Epoch 10/15: [================              ] 41/75 batches, loss: 0.0903Epoch 10/15: [================              ] 42/75 batches, loss: 0.0903Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0905Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0899Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0889Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0888Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0885Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0895Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0902Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0898Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0896Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0888Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0881Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0884Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0878Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0878Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0881Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0879Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0874Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0872Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0871Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0871Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0867Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0866Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0872Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0876Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0870Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0872Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0883Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0881Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0881Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0880Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0874Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0874Epoch 10/15: [==============================] 75/75 batches, loss: 0.0870
[2025-05-07 21:00:45,213][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0870
[2025-05-07 21:00:45,411][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0598, Metrics: {'mse': 0.05948975682258606, 'rmse': 0.24390522098263098, 'r2': -0.021341562271118164}
[2025-05-07 21:00:45,411][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:00:45,411][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:00:45,412][src.training.lm_trainer][INFO] - Training completed in 27.74 seconds
[2025-05-07 21:00:45,412][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:00:48,309][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06430336833000183, 'rmse': 0.25358108827355763, 'r2': 0.0027730464935302734}
[2025-05-07 21:00:48,310][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05644630268216133, 'rmse': 0.23758430647279996, 'r2': 0.030909597873687744}
[2025-05-07 21:00:48,310][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10209809988737106, 'rmse': 0.31952793287500086, 'r2': -0.3875161409378052}
[2025-05-07 21:00:50,023][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ja/ja/model.pt
[2025-05-07 21:00:50,026][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▁
wandb:     best_val_mse █▆▅▃▁
wandb:      best_val_r2 ▁▃▄▆█
wandb:    best_val_rmse █▆▅▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▂▃▂▃▃▂▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▃▄▁▂▆▁▃
wandb:          val_mse █▆▅▃▄▁▂▆▁▃
wandb:           val_r2 ▁▃▄▆▅█▇▃█▆
wandb:         val_rmse █▆▅▃▄▁▂▆▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05676
wandb:     best_val_mse 0.05645
wandb:      best_val_r2 0.03091
wandb:    best_val_rmse 0.23758
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.1021
wandb:    final_test_r2 -0.38752
wandb:  final_test_rmse 0.31953
wandb:  final_train_mse 0.0643
wandb:   final_train_r2 0.00277
wandb: final_train_rmse 0.25358
wandb:    final_val_mse 0.05645
wandb:     final_val_r2 0.03091
wandb:   final_val_rmse 0.23758
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08696
wandb:       train_time 27.74287
wandb:         val_loss 0.0598
wandb:          val_mse 0.05949
wandb:           val_r2 -0.02134
wandb:         val_rmse 0.24391
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210000-qx0414ii
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210000-qx0414ii/logs
Experiment probe_layer2_avg_subordinate_chain_len_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:01:10,526][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ja
experiment_name: probe_layer2_avg_subordinate_chain_len_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:01:10,526][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:01:10,527][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 21:01:10,527][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:01:10,527][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:01:10,531][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:01:10,531][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 21:01:10,531][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:01:13,130][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:01:15,375][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:01:15,375][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:01:15,531][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 21:01:15,594][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:27:11 2025).
[2025-05-07 21:01:15,808][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:01:15,817][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:01:15,818][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:01:15,819][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:01:15,872][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:01:15,928][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:01:15,979][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:01:15,980][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:01:15,980][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:01:15,982][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:01:16,058][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:01:16,165][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:01:16,183][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:01:16,185][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:01:16,185][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:01:16,187][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:01:16,187][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:01:16,187][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:01:16,187][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:01:16,187][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:01:16,187][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:01:16,188][src.data.datasets][INFO] -   Mean: 0.1797, Std: 0.2539
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:01:16,188][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:01:16,188][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5000
[2025-05-07 21:01:16,189][src.data.datasets][INFO] -   Mean: 0.1848, Std: 0.2413
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 21:01:16,189][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:01:16,189][src.data.datasets][INFO] -   Mean: 0.3168, Std: 0.2713
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:01:16,189][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:01:16,190][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:01:16,190][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:01:16,190][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:01:16,190][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 21:01:16,190][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:01:22,977][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:01:22,978][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:01:22,978][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:01:22,978][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:01:22,981][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:01:22,981][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:01:22,981][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:01:22,981][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:01:22,982][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:01:22,982][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:01:22,983][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5525Epoch 1/15: [                              ] 2/75 batches, loss: 0.5827Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4959Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4951Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4702Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4574Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4409Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4815Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4558Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4320Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4300Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4294Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4117Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4193Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4155Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4231Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4215Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4458Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4414Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4438Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4365Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4354Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4209Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4113Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4086Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4005Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3985Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3911Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3911Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3859Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3794Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3822Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3816Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3810Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3801Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3808Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3775Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3735Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3738Epoch 1/15: [================              ] 40/75 batches, loss: 0.3681Epoch 1/15: [================              ] 41/75 batches, loss: 0.3636Epoch 1/15: [================              ] 42/75 batches, loss: 0.3626Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3633Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3642Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3638Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3608Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3593Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3566Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3556Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3531Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3554Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3585Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3548Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3593Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3602Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3558Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3526Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3506Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3508Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3505Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3476Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3461Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3465Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3476Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3482Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3449Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3441Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3413Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3405Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3376Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3364Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3370Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3347Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3341Epoch 1/15: [==============================] 75/75 batches, loss: 0.3302
[2025-05-07 21:01:29,141][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3302
[2025-05-07 21:01:29,346][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0740, Metrics: {'mse': 0.07360310852527618, 'rmse': 0.2712989283526129, 'r2': -0.2636446952819824}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2852Epoch 2/15: [                              ] 2/75 batches, loss: 0.3182Epoch 2/15: [=                             ] 3/75 batches, loss: 0.3063Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2963Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2598Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2369Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2255Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2233Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2322Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2263Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2176Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2148Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2171Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2109Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2155Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2165Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2147Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2098Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2088Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2050Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2013Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2072Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2043Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2037Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1993Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2040Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2042Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2051Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2039Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2037Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2022Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2055Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2056Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2060Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2049Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2028Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2054Epoch 2/15: [===============               ] 38/75 batches, loss: 0.2030Epoch 2/15: [===============               ] 39/75 batches, loss: 0.2020Epoch 2/15: [================              ] 40/75 batches, loss: 0.2024Epoch 2/15: [================              ] 41/75 batches, loss: 0.2013Epoch 2/15: [================              ] 42/75 batches, loss: 0.2026Epoch 2/15: [=================             ] 43/75 batches, loss: 0.2007Epoch 2/15: [=================             ] 44/75 batches, loss: 0.2072Epoch 2/15: [==================            ] 45/75 batches, loss: 0.2063Epoch 2/15: [==================            ] 46/75 batches, loss: 0.2035Epoch 2/15: [==================            ] 47/75 batches, loss: 0.2037Epoch 2/15: [===================           ] 48/75 batches, loss: 0.2025Epoch 2/15: [===================           ] 49/75 batches, loss: 0.2022Epoch 2/15: [====================          ] 50/75 batches, loss: 0.2012Epoch 2/15: [====================          ] 51/75 batches, loss: 0.2008Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1998Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1990Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1974Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1954Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1962Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1949Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1934Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1918Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1926Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1909Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1923Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1918Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1910Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1902Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1911Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1933Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1935Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1926Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1921Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1917Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1911Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1900Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1890Epoch 2/15: [==============================] 75/75 batches, loss: 0.1887
[2025-05-07 21:01:32,022][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1887
[2025-05-07 21:01:32,220][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0632, Metrics: {'mse': 0.06277842074632645, 'rmse': 0.25055622272521283, 'r2': -0.07780241966247559}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2033Epoch 3/15: [                              ] 2/75 batches, loss: 0.1601Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1548Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1610Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1772Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1797Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1702Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1720Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1664Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1635Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1689Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1604Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1576Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1521Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1527Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1529Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1522Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1518Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1520Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1564Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1591Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1577Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1587Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1554Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1513Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1519Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1549Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1546Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1543Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1528Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1520Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1603Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1655Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1639Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1644Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1637Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1628Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1616Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1604Epoch 3/15: [================              ] 40/75 batches, loss: 0.1607Epoch 3/15: [================              ] 41/75 batches, loss: 0.1610Epoch 3/15: [================              ] 42/75 batches, loss: 0.1608Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1598Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1600Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1606Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1600Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1590Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1585Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1583Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1569Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1573Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1564Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1545Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1528Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1519Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1504Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1497Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1502Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1492Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1477Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1475Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1473Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1467Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1454Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1461Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1460Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1454Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1448Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1449Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1440Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1440Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1439Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1445Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1435Epoch 3/15: [==============================] 75/75 batches, loss: 0.1433
[2025-05-07 21:01:34,917][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1433
[2025-05-07 21:01:35,118][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0581, Metrics: {'mse': 0.05777579918503761, 'rmse': 0.24036596927401685, 'r2': 0.008084356784820557}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1903Epoch 4/15: [                              ] 2/75 batches, loss: 0.1831Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1601Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1694Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1629Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1495Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1409Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1330Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1308Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1280Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1353Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1291Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1300Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1263Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1245Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1303Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1317Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1327Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1322Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1309Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1306Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1282Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1276Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1273Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1251Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1242Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1246Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1270Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1263Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1242Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1231Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1216Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1218Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1204Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1208Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1209Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1199Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1210Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1199Epoch 4/15: [================              ] 40/75 batches, loss: 0.1208Epoch 4/15: [================              ] 41/75 batches, loss: 0.1209Epoch 4/15: [================              ] 42/75 batches, loss: 0.1204Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1201Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1195Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1196Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1211Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1210Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1208Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1203Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1197Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1185Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1184Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1190Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1187Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1189Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1190Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1183Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1192Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1190Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1183Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1181Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1185Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1188Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1191Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1191Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1193Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1191Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1188Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1190Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1182Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1184Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1189Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1187Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1204Epoch 4/15: [==============================] 75/75 batches, loss: 0.1202
[2025-05-07 21:01:37,765][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1202
[2025-05-07 21:01:37,988][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0657, Metrics: {'mse': 0.06522435694932938, 'rmse': 0.25539059683028537, 'r2': -0.11979520320892334}
[2025-05-07 21:01:37,988][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0862Epoch 5/15: [                              ] 2/75 batches, loss: 0.1035Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1051Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1114Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1133Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1386Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1426Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1306Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1373Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1378Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1379Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1339Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1312Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1327Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1289Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1294Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1279Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1270Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1246Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1232Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1220Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1231Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1244Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1229Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1242Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1243Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1243Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1237Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1230Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1233Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1230Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1214Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1205Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1220Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1220Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1228Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1230Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1222Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1211Epoch 5/15: [================              ] 40/75 batches, loss: 0.1206Epoch 5/15: [================              ] 41/75 batches, loss: 0.1204Epoch 5/15: [================              ] 42/75 batches, loss: 0.1250Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1241Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1226Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1225Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1231Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1228Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1235Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1236Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1219Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1209Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1204Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1196Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1189Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1186Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1199Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1198Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1190Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1200Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1197Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1204Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1216Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1221Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1216Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1214Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1215Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1214Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1202Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1191Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1192Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1193Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1192Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1191Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1190Epoch 5/15: [==============================] 75/75 batches, loss: 0.1183
[2025-05-07 21:01:40,263][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1183
[2025-05-07 21:01:40,482][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0570, Metrics: {'mse': 0.056604959070682526, 'rmse': 0.2379179671035429, 'r2': 0.028185725212097168}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1177Epoch 6/15: [                              ] 2/75 batches, loss: 0.0951Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0896Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0965Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1047Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1067Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1131Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1160Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1130Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1111Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1123Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1102Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1068Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1072Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1062Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1049Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1078Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1080Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1041Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1034Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1025Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1010Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1000Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1025Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1045Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1050Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1064Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1078Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1096Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1105Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1104Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1100Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1086Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1088Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1088Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1091Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1091Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1082Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1087Epoch 6/15: [================              ] 40/75 batches, loss: 0.1095Epoch 6/15: [================              ] 41/75 batches, loss: 0.1105Epoch 6/15: [================              ] 42/75 batches, loss: 0.1110Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1108Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1096Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1098Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1103Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1089Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1092Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1095Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1094Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1105Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1118Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1118Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1110Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1111Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1111Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1102Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1092Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1084Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1078Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1081Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1079Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1071Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1066Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1063Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1053Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1058Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1052Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1055Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1054Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1049Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1039Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1043Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1038Epoch 6/15: [==============================] 75/75 batches, loss: 0.1035
[2025-05-07 21:01:43,165][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1035
[2025-05-07 21:01:43,362][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0560, Metrics: {'mse': 0.05558716878294945, 'rmse': 0.2357693126404483, 'r2': 0.045659542083740234}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0504Epoch 7/15: [                              ] 2/75 batches, loss: 0.0784Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0915Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0840Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0879Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0917Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0857Epoch 7/15: [===                           ] 8/75 batches, loss: 0.1035Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0968Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0987Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0967Epoch 7/15: [====                          ] 12/75 batches, loss: 0.1000Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.1023Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.1046Epoch 7/15: [======                        ] 15/75 batches, loss: 0.1027Epoch 7/15: [======                        ] 16/75 batches, loss: 0.1029Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0999Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.1045Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.1033Epoch 7/15: [========                      ] 20/75 batches, loss: 0.1030Epoch 7/15: [========                      ] 21/75 batches, loss: 0.1013Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0999Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0970Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0966Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0951Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0947Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0967Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0976Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0972Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0998Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0993Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0980Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0975Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0985Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0991Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0991Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0985Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0981Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0981Epoch 7/15: [================              ] 40/75 batches, loss: 0.0978Epoch 7/15: [================              ] 41/75 batches, loss: 0.0968Epoch 7/15: [================              ] 42/75 batches, loss: 0.0963Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0956Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0954Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0948Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0943Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0947Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0946Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0941Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0958Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0950Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0947Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0970Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0967Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0962Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0956Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0956Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0961Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0960Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0955Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0956Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0970Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0969Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0966Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0964Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0970Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0975Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0975Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0969Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0976Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0972Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0968Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0965Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0964Epoch 7/15: [==============================] 75/75 batches, loss: 0.0965
[2025-05-07 21:01:46,069][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0965
[2025-05-07 21:01:46,282][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0557, Metrics: {'mse': 0.05532970651984215, 'rmse': 0.235222674331881, 'r2': 0.05007976293563843}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0803Epoch 8/15: [                              ] 2/75 batches, loss: 0.0997Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0974Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0929Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0961Epoch 8/15: [==                            ] 6/75 batches, loss: 0.1000Epoch 8/15: [==                            ] 7/75 batches, loss: 0.1012Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0976Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0939Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0932Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0917Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0922Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0901Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0947Epoch 8/15: [======                        ] 15/75 batches, loss: 0.1015Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0988Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0981Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0982Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.1009Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0983Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0965Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0966Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0958Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0959Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0938Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0924Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0933Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0946Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0929Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0952Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0972Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0981Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0983Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.1000Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0998Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0993Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0989Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0981Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0976Epoch 8/15: [================              ] 40/75 batches, loss: 0.0970Epoch 8/15: [================              ] 41/75 batches, loss: 0.0972Epoch 8/15: [================              ] 42/75 batches, loss: 0.0965Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0963Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0963Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0963Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0961Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0958Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0965Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0956Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0956Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0970Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0966Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0977Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0985Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0981Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0979Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0974Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0977Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0975Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0967Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0961Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0962Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0956Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0950Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0942Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0939Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0952Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0946Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0947Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0944Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0945Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0941Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0941Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0942Epoch 8/15: [==============================] 75/75 batches, loss: 0.0935
[2025-05-07 21:01:48,973][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0935
[2025-05-07 21:01:49,199][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0575, Metrics: {'mse': 0.05705520883202553, 'rmse': 0.23886232191793147, 'r2': 0.02045571804046631}
[2025-05-07 21:01:49,199][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0790Epoch 9/15: [                              ] 2/75 batches, loss: 0.0571Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0759Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0776Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0934Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0891Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0903Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0875Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0877Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0842Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0835Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0833Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0846Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0829Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0833Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0818Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0810Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0798Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0818Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0802Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0794Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0796Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0804Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0816Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0818Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0828Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0826Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0808Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0824Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0824Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0816Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0848Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0857Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0844Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0850Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0845Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0851Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0854Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0850Epoch 9/15: [================              ] 40/75 batches, loss: 0.0867Epoch 9/15: [================              ] 41/75 batches, loss: 0.0862Epoch 9/15: [================              ] 42/75 batches, loss: 0.0885Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0880Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0877Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0870Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0875Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0874Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0879Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0876Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0873Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0876Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0871Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0873Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0875Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0875Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0875Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0872Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0875Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0874Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0880Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0884Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0875Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0874Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0871Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0876Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0874Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0870Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0870Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0864Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0865Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0864Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0861Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0859Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0853Epoch 9/15: [==============================] 75/75 batches, loss: 0.0849
[2025-05-07 21:01:51,497][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0849
[2025-05-07 21:01:51,718][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0570, Metrics: {'mse': 0.056605540215969086, 'rmse': 0.23791918841482518, 'r2': 0.028175771236419678}
[2025-05-07 21:01:51,719][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0706Epoch 10/15: [                              ] 2/75 batches, loss: 0.0697Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0733Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0781Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0799Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0800Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0837Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0920Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0897Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0942Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0906Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0928Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0915Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0899Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0914Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0916Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0937Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0944Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0934Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0961Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0948Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0955Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0935Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0915Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0915Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0917Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0917Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0911Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0910Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0889Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0903Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0897Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0911Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0906Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0895Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0906Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0912Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0925Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0922Epoch 10/15: [================              ] 40/75 batches, loss: 0.0914Epoch 10/15: [================              ] 41/75 batches, loss: 0.0911Epoch 10/15: [================              ] 42/75 batches, loss: 0.0910Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0907Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0906Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0904Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0901Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0906Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0917Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0916Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0920Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0912Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0910Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0906Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0912Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0904Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0898Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0892Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0889Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0879Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0876Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0874Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0880Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0877Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0878Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0881Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0885Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0885Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0888Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0890Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0893Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0889Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0890Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0887Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0887Epoch 10/15: [==============================] 75/75 batches, loss: 0.0880
[2025-05-07 21:01:54,014][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0880
[2025-05-07 21:01:54,236][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0594, Metrics: {'mse': 0.059101007878780365, 'rmse': 0.243106988543687, 'r2': -0.014667272567749023}
[2025-05-07 21:01:54,236][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0864Epoch 11/15: [                              ] 2/75 batches, loss: 0.1032Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0883Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0878Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0860Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0856Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0800Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0859Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0862Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0831Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0846Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0881Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0853Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0846Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0842Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0830Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0829Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0820Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0814Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0813Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0814Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0811Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0800Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0801Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0810Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0809Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0806Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0806Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0797Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0807Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0811Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0806Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0800Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0792Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0794Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0798Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0795Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0800Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0799Epoch 11/15: [================              ] 40/75 batches, loss: 0.0796Epoch 11/15: [================              ] 41/75 batches, loss: 0.0799Epoch 11/15: [================              ] 42/75 batches, loss: 0.0807Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0806Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0800Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0800Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0817Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0809Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0810Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0821Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0833Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0829Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0825Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0826Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0821Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0823Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0826Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0820Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0822Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0827Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0828Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0831Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0829Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0829Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0825Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0825Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0824Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0824Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0820Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0834Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0839Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0839Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0849Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0847Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0855Epoch 11/15: [==============================] 75/75 batches, loss: 0.0859
[2025-05-07 21:01:56,542][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0859
[2025-05-07 21:01:56,759][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0573, Metrics: {'mse': 0.05698418989777565, 'rmse': 0.23871361481443754, 'r2': 0.021674931049346924}
[2025-05-07 21:01:56,760][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:01:56,760][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 21:01:56,760][src.training.lm_trainer][INFO] - Training completed in 30.68 seconds
[2025-05-07 21:01:56,760][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:01:59,620][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06506919115781784, 'rmse': 0.25508663461227804, 'r2': -0.00910341739654541}
[2025-05-07 21:01:59,620][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05532970651984215, 'rmse': 0.235222674331881, 'r2': 0.05007976293563843}
[2025-05-07 21:01:59,620][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.1066325232386589, 'rmse': 0.326546356952055, 'r2': -0.4491391181945801}
[2025-05-07 21:02:01,274][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ja/ja/model.pt
[2025-05-07 21:02:01,275][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▂▁▁▁
wandb:      best_val_r2 ▁▅▇███
wandb:    best_val_rmse █▄▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▂▄▄▄▄▄▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▅▁▁▁▂▁▂▂
wandb:          val_mse █▄▂▅▁▁▁▂▁▂▂
wandb:           val_r2 ▁▅▇▄███▇█▇▇
wandb:         val_rmse █▄▂▅▂▁▁▂▂▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05572
wandb:     best_val_mse 0.05533
wandb:      best_val_r2 0.05008
wandb:    best_val_rmse 0.23522
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.10663
wandb:    final_test_r2 -0.44914
wandb:  final_test_rmse 0.32655
wandb:  final_train_mse 0.06507
wandb:   final_train_r2 -0.0091
wandb: final_train_rmse 0.25509
wandb:    final_val_mse 0.05533
wandb:     final_val_r2 0.05008
wandb:   final_val_rmse 0.23522
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08594
wandb:       train_time 30.68022
wandb:         val_loss 0.05726
wandb:          val_mse 0.05698
wandb:           val_r2 0.02167
wandb:         val_rmse 0.23871
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210110-6s9wnhyg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210110-6s9wnhyg/logs
Experiment probe_layer2_avg_subordinate_chain_len_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:02:21,870][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ja
experiment_name: probe_layer2_avg_verb_edges_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:02:21,870][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:02:21,870][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:02:21,870][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:02:21,870][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:02:21,875][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:02:21,875][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:02:21,875][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:02:24,384][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:02:26,643][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:02:26,643][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:02:26,805][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 21:02:26,864][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:29:02 2025).
[2025-05-07 21:02:27,036][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:02:27,044][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:02:27,044][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:02:27,046][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:02:27,074][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:02:27,136][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:02:27,176][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:02:27,176][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:02:27,177][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:02:27,178][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:02:27,229][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:02:27,305][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:02:27,320][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:02:27,321][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:02:27,321][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:02:27,322][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:02:27,322][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:02:27,322][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:02:27,323][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:02:27,323][src.data.datasets][INFO] -   Mean: 0.2629, Std: 0.2549
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:02:27,323][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:02:27,324][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:02:27,324][src.data.datasets][INFO] -   Mean: 0.3093, Std: 0.2933
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Sample label: 0.800000011920929
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:02:27,324][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:02:27,325][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:02:27,325][src.data.datasets][INFO] -   Mean: 0.3726, Std: 0.3091
[2025-05-07 21:02:27,325][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:02:27,325][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:02:27,325][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:02:27,325][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:02:27,325][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:02:27,325][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 21:02:27,326][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:02:33,593][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:02:33,594][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:02:33,594][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:02:33,594][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:02:33,597][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:02:33,598][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:02:33,598][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:02:33,598][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:02:33,598][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:02:33,599][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:02:33,599][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4985Epoch 1/15: [                              ] 2/75 batches, loss: 0.4889Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4573Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4410Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3996Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4004Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3914Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4147Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3925Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3782Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3690Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3801Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3665Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3944Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3938Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4052Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4101Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4287Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4243Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4307Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4254Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4259Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4126Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4047Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4066Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4018Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3973Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3934Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3944Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3903Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3847Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3829Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3819Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3805Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3780Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3832Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3780Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3747Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3734Epoch 1/15: [================              ] 40/75 batches, loss: 0.3677Epoch 1/15: [================              ] 41/75 batches, loss: 0.3643Epoch 1/15: [================              ] 42/75 batches, loss: 0.3647Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3610Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3607Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3603Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3557Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3510Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3463Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3433Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3396Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3389Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3439Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3406Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3452Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3452Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3410Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3384Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3370Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3355Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3343Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3337Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3333Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3353Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3367Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3360Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3329Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3332Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3322Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3318Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3292Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3278Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3273Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3251Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3233Epoch 1/15: [==============================] 75/75 batches, loss: 0.3212
[2025-05-07 21:02:39,909][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3212
[2025-05-07 21:02:40,107][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1160, Metrics: {'mse': 0.11738847941160202, 'rmse': 0.3426200219070713, 'r2': -0.3646094799041748}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2611Epoch 2/15: [                              ] 2/75 batches, loss: 0.2719Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2660Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2737Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2636Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2514Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2741Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2587Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2786Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2978Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2794Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2788Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2777Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2720Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2675Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2671Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2629Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2584Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2530Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2514Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2489Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2490Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2472Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2434Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2371Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2373Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2371Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2368Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2335Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2315Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2299Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2293Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2292Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2280Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2234Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2212Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2236Epoch 2/15: [===============               ] 38/75 batches, loss: 0.2216Epoch 2/15: [===============               ] 39/75 batches, loss: 0.2192Epoch 2/15: [================              ] 40/75 batches, loss: 0.2176Epoch 2/15: [================              ] 41/75 batches, loss: 0.2154Epoch 2/15: [================              ] 42/75 batches, loss: 0.2160Epoch 2/15: [=================             ] 43/75 batches, loss: 0.2132Epoch 2/15: [=================             ] 44/75 batches, loss: 0.2162Epoch 2/15: [==================            ] 45/75 batches, loss: 0.2141Epoch 2/15: [==================            ] 46/75 batches, loss: 0.2120Epoch 2/15: [==================            ] 47/75 batches, loss: 0.2119Epoch 2/15: [===================           ] 48/75 batches, loss: 0.2093Epoch 2/15: [===================           ] 49/75 batches, loss: 0.2091Epoch 2/15: [====================          ] 50/75 batches, loss: 0.2083Epoch 2/15: [====================          ] 51/75 batches, loss: 0.2075Epoch 2/15: [====================          ] 52/75 batches, loss: 0.2072Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.2059Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.2056Epoch 2/15: [======================        ] 55/75 batches, loss: 0.2037Epoch 2/15: [======================        ] 56/75 batches, loss: 0.2039Epoch 2/15: [======================        ] 57/75 batches, loss: 0.2032Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.2027Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.2016Epoch 2/15: [========================      ] 60/75 batches, loss: 0.2012Epoch 2/15: [========================      ] 61/75 batches, loss: 0.2010Epoch 2/15: [========================      ] 62/75 batches, loss: 0.2008Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1999Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1989Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1973Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1959Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1975Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1978Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1967Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1958Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1951Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1945Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1940Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1928Epoch 2/15: [==============================] 75/75 batches, loss: 0.1939
[2025-05-07 21:02:42,786][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1939
[2025-05-07 21:02:43,010][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1155, Metrics: {'mse': 0.1166447252035141, 'rmse': 0.3415329050084546, 'r2': -0.3559635877609253}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2496Epoch 3/15: [                              ] 2/75 batches, loss: 0.2057Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1789Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1740Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1730Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1897Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1737Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1695Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1787Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1692Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1669Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1629Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1684Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1608Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1582Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1586Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1551Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1582Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1555Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1563Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1578Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1572Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1549Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1533Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1502Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1488Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1513Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1517Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1522Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1499Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1505Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1565Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1562Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1569Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1563Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1573Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1597Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1584Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1567Epoch 3/15: [================              ] 40/75 batches, loss: 0.1549Epoch 3/15: [================              ] 41/75 batches, loss: 0.1553Epoch 3/15: [================              ] 42/75 batches, loss: 0.1544Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1542Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1535Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1528Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1519Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1528Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1512Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1503Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1505Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1492Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1496Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1488Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1473Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1471Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1468Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1458Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1469Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1470Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1460Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1454Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1451Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1448Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1443Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1470Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1463Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1451Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1463Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1468Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1468Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1458Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1454Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1461Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1455Epoch 3/15: [==============================] 75/75 batches, loss: 0.1466
[2025-05-07 21:02:45,707][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1466
[2025-05-07 21:02:45,942][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1002, Metrics: {'mse': 0.10127918422222137, 'rmse': 0.31824390681083176, 'r2': -0.1773432493209839}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1310Epoch 4/15: [                              ] 2/75 batches, loss: 0.1084Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1455Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1594Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1508Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1512Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1557Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1608Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1530Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1464Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1511Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1446Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1404Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1379Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1331Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1365Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1363Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1371Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1382Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1373Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1338Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1346Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1347Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1379Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1355Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1352Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1376Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1353Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1340Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1326Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1316Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1317Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1303Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1289Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1278Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1286Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1273Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1259Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1252Epoch 4/15: [================              ] 40/75 batches, loss: 0.1265Epoch 4/15: [================              ] 41/75 batches, loss: 0.1251Epoch 4/15: [================              ] 42/75 batches, loss: 0.1253Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1254Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1248Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1240Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1234Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1222Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1252Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1233Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1225Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1222Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1215Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1218Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1214Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1209Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1205Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1200Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1202Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1194Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1186Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1184Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1175Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1176Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1169Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1177Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1179Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1179Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1186Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1195Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1190Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1184Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1181Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1183Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1205Epoch 4/15: [==============================] 75/75 batches, loss: 0.1211
[2025-05-07 21:02:48,569][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1211
[2025-05-07 21:02:48,791][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1018, Metrics: {'mse': 0.10276996344327927, 'rmse': 0.32057754669233973, 'r2': -0.19467318058013916}
[2025-05-07 21:02:48,792][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1279Epoch 5/15: [                              ] 2/75 batches, loss: 0.1275Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1044Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1154Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1225Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1332Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1365Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1298Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1351Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1340Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1366Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1345Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1337Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1304Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1259Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1231Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1193Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1174Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1157Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1130Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1120Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1146Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1187Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1173Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1166Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1140Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1144Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1166Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1194Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1170Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1166Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1164Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1160Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1164Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1194Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1183Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1189Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1206Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1205Epoch 5/15: [================              ] 40/75 batches, loss: 0.1188Epoch 5/15: [================              ] 41/75 batches, loss: 0.1187Epoch 5/15: [================              ] 42/75 batches, loss: 0.1225Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1223Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1210Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1193Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1191Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1186Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1178Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1170Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1173Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1165Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1163Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1173Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1175Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1173Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1170Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1168Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1166Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1166Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1158Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1168Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1179Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1180Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1177Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1186Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1187Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1185Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1184Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1184Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1185Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1182Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1180Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1173Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1174Epoch 5/15: [==============================] 75/75 batches, loss: 0.1180
[2025-05-07 21:02:51,060][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1180
[2025-05-07 21:02:51,269][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0945, Metrics: {'mse': 0.09565434604883194, 'rmse': 0.3092803680300965, 'r2': -0.11195600032806396}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0924Epoch 6/15: [                              ] 2/75 batches, loss: 0.1054Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1085Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0990Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1098Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1047Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1119Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1106Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1111Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1126Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1114Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1110Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1094Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1095Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1093Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1104Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1133Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1151Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1120Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1111Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1099Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1082Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1078Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1087Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1108Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1092Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1077Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1085Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1079Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1089Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1081Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1069Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1053Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1037Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1027Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1030Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1022Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1031Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1032Epoch 6/15: [================              ] 40/75 batches, loss: 0.1052Epoch 6/15: [================              ] 41/75 batches, loss: 0.1066Epoch 6/15: [================              ] 42/75 batches, loss: 0.1071Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1067Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1071Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1063Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1063Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1059Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1057Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1058Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1090Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1088Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1080Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1093Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1087Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1082Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1084Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1079Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1091Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1088Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1087Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1089Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1088Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1090Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1082Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1080Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1077Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1078Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1074Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1072Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1065Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1057Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1053Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1056Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1050Epoch 6/15: [==============================] 75/75 batches, loss: 0.1065
[2025-05-07 21:02:53,902][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1065
[2025-05-07 21:02:54,112][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0887, Metrics: {'mse': 0.08991368114948273, 'rmse': 0.29985610073747493, 'r2': -0.04522228240966797}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0913Epoch 7/15: [                              ] 2/75 batches, loss: 0.0944Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0930Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0944Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0996Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0965Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0956Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0918Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0926Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0920Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0918Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0887Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0881Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0899Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0881Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0841Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0836Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0835Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0851Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0853Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0846Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0837Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0852Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0859Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0908Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0915Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0924Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0927Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0939Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0946Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0937Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0933Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0920Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0925Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0932Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0954Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0947Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0947Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0945Epoch 7/15: [================              ] 40/75 batches, loss: 0.0955Epoch 7/15: [================              ] 41/75 batches, loss: 0.0961Epoch 7/15: [================              ] 42/75 batches, loss: 0.0959Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0964Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0971Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0971Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0963Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0960Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0956Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0956Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0971Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0968Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0962Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0963Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0966Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0966Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0965Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0962Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0960Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0962Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0967Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0969Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0964Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0960Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0960Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0959Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0953Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0944Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0946Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0939Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0937Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0932Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0931Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0929Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0924Epoch 7/15: [==============================] 75/75 batches, loss: 0.0918
[2025-05-07 21:02:56,830][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0918
[2025-05-07 21:02:57,062][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1008, Metrics: {'mse': 0.10186924785375595, 'rmse': 0.3191696223855835, 'r2': -0.1842026710510254}
[2025-05-07 21:02:57,063][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0687Epoch 8/15: [                              ] 2/75 batches, loss: 0.0801Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0736Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0920Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0936Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0941Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0912Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0908Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0862Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0954Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0919Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0894Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0884Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0901Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0922Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0913Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0885Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0893Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0917Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0917Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0905Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0892Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0897Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0905Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0917Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0915Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0923Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0946Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0937Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0931Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0941Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0945Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0939Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0939Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0928Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0921Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0912Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0914Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0905Epoch 8/15: [================              ] 40/75 batches, loss: 0.0895Epoch 8/15: [================              ] 41/75 batches, loss: 0.0889Epoch 8/15: [================              ] 42/75 batches, loss: 0.0888Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0889Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0902Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0918Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0917Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0912Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0916Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0926Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0927Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0923Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0915Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0923Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0920Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0919Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0912Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0909Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0908Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0902Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0899Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0898Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0895Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0890Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0892Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0890Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0887Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0888Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0890Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0887Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0887Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0897Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0900Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0901Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0902Epoch 8/15: [==============================] 75/75 batches, loss: 0.0899
[2025-05-07 21:02:59,392][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0899
[2025-05-07 21:02:59,614][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0941, Metrics: {'mse': 0.09528745710849762, 'rmse': 0.3086866649346836, 'r2': -0.10769104957580566}
[2025-05-07 21:02:59,615][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.1162Epoch 9/15: [                              ] 2/75 batches, loss: 0.0967Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0894Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0900Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0970Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0927Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0880Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0828Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0813Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0818Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0873Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0863Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0885Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0890Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0902Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0910Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0915Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0898Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0872Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0892Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0907Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0902Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0898Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0897Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0882Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0868Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0860Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0847Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0863Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0870Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0867Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0856Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0865Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0857Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0854Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0846Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0853Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0867Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0860Epoch 9/15: [================              ] 40/75 batches, loss: 0.0857Epoch 9/15: [================              ] 41/75 batches, loss: 0.0848Epoch 9/15: [================              ] 42/75 batches, loss: 0.0858Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0865Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0857Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0855Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0866Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0862Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0865Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0866Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0871Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0873Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0871Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0872Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0886Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0888Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0884Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0880Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0877Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0877Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0879Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0880Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0878Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0879Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0873Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0874Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0866Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0873Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0874Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0877Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0872Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0868Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0868Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0872Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0872Epoch 9/15: [==============================] 75/75 batches, loss: 0.0870
[2025-05-07 21:03:01,903][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0870
[2025-05-07 21:03:02,134][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0922, Metrics: {'mse': 0.09348999708890915, 'rmse': 0.30576134008227585, 'r2': -0.08679604530334473}
[2025-05-07 21:03:02,134][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0637Epoch 10/15: [                              ] 2/75 batches, loss: 0.0690Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0880Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0869Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0927Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0937Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0869Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0892Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0886Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0911Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0911Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0904Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0900Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0912Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0908Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0900Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0919Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0932Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0929Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0942Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0952Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0935Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0922Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0920Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0924Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0914Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0907Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0913Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0921Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0933Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0934Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0935Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0945Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0935Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0929Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0936Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0940Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0933Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0922Epoch 10/15: [================              ] 40/75 batches, loss: 0.0909Epoch 10/15: [================              ] 41/75 batches, loss: 0.0923Epoch 10/15: [================              ] 42/75 batches, loss: 0.0927Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0936Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0936Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0944Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0945Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0944Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0952Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0946Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0947Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0940Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0936Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0935Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0945Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0942Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0944Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0941Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0943Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0936Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0929Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0928Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0925Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0925Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0921Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0915Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0911Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0924Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0923Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0922Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0918Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0921Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0921Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0917Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0914Epoch 10/15: [==============================] 75/75 batches, loss: 0.0908
[2025-05-07 21:03:04,434][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0908
[2025-05-07 21:03:04,658][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0919, Metrics: {'mse': 0.09317058324813843, 'rmse': 0.3052385677599383, 'r2': -0.08308291435241699}
[2025-05-07 21:03:04,659][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:03:04,659][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:03:04,659][src.training.lm_trainer][INFO] - Training completed in 28.00 seconds
[2025-05-07 21:03:04,659][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:03:07,518][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06510943174362183, 'rmse': 0.255165498732924, 'r2': -0.0022284984588623047}
[2025-05-07 21:03:07,519][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.08991368114948273, 'rmse': 0.29985610073747493, 'r2': -0.04522228240966797}
[2025-05-07 21:03:07,519][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12132477760314941, 'rmse': 0.34831706476018287, 'r2': -0.2700597047805786}
[2025-05-07 21:03:09,174][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ja/ja/model.pt
[2025-05-07 21:03:09,176][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▄▂▁
wandb:     best_val_mse ██▄▂▁
wandb:      best_val_r2 ▁▁▅▇█
wandb:    best_val_rmse ██▄▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▃▃▃▄▃▃▄
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▄▄▂▁▄▂▂▂
wandb:          val_mse ██▄▄▂▁▄▂▂▂
wandb:           val_r2 ▁▁▅▅▇█▅▇▇▇
wandb:         val_rmse ██▄▄▃▁▄▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08871
wandb:     best_val_mse 0.08991
wandb:      best_val_r2 -0.04522
wandb:    best_val_rmse 0.29986
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.12132
wandb:    final_test_r2 -0.27006
wandb:  final_test_rmse 0.34832
wandb:  final_train_mse 0.06511
wandb:   final_train_r2 -0.00223
wandb: final_train_rmse 0.25517
wandb:    final_val_mse 0.08991
wandb:     final_val_r2 -0.04522
wandb:   final_val_rmse 0.29986
wandb:    learning_rate 0.0001
wandb:       train_loss 0.09079
wandb:       train_time 27.99558
wandb:         val_loss 0.09189
wandb:          val_mse 0.09317
wandb:           val_r2 -0.08308
wandb:         val_rmse 0.30524
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210221-7q9tjpzc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210221-7q9tjpzc/logs
Experiment probe_layer2_avg_verb_edges_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:03:30,239][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ja
experiment_name: probe_layer2_avg_verb_edges_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:03:30,239][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:03:30,239][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:03:30,239][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:03:30,239][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:03:30,244][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:03:30,244][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:03:30,244][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:03:32,741][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:03:35,048][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:03:35,048][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:03:35,235][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 21:03:35,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:30:51 2025).
[2025-05-07 21:03:35,537][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:03:35,546][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:03:35,546][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:03:35,549][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:03:35,657][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:03:35,728][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:03:35,767][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:03:35,768][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:03:35,768][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:03:35,770][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:03:35,844][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:03:35,938][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:03:35,970][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:03:35,971][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:03:35,971][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:03:35,973][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:03:35,973][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:03:35,973][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:03:35,974][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:03:35,974][src.data.datasets][INFO] -   Mean: 0.2629, Std: 0.2549
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:03:35,974][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:03:35,975][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:03:35,975][src.data.datasets][INFO] -   Mean: 0.3093, Std: 0.2933
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Sample label: 0.800000011920929
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:03:35,975][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:03:35,976][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:03:35,976][src.data.datasets][INFO] -   Mean: 0.3726, Std: 0.3091
[2025-05-07 21:03:35,976][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:03:35,976][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:03:35,976][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:03:35,976][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:03:35,976][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:03:35,976][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 21:03:35,977][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:03:44,088][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:03:44,089][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:03:44,089][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:03:44,089][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:03:44,092][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:03:44,093][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:03:44,093][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:03:44,093][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:03:44,093][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:03:44,094][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:03:44,094][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6645Epoch 1/15: [                              ] 2/75 batches, loss: 0.5682Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5310Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5208Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5181Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4860Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4840Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4873Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4615Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4364Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4333Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4500Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4337Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4356Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4305Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4432Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4445Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4631Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4501Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4468Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4420Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4403Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4282Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4202Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4190Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4088Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4030Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3960Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3946Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3881Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3823Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3789Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3776Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3755Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3756Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3783Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3739Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3687Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3674Epoch 1/15: [================              ] 40/75 batches, loss: 0.3628Epoch 1/15: [================              ] 41/75 batches, loss: 0.3585Epoch 1/15: [================              ] 42/75 batches, loss: 0.3582Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3581Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3628Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3604Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3568Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3543Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3525Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3504Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3492Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3484Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3522Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3490Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3479Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3478Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3435Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3401Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3385Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3381Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3356Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3342Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3341Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3333Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3328Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3327Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3306Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3314Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3279Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3264Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3244Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3247Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3256Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3238Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3240Epoch 1/15: [==============================] 75/75 batches, loss: 0.3213
[2025-05-07 21:03:50,670][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3213
[2025-05-07 21:03:50,863][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1155, Metrics: {'mse': 0.1167788952589035, 'rmse': 0.34172927187892976, 'r2': -0.35752320289611816}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2084Epoch 2/15: [                              ] 2/75 batches, loss: 0.1957Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2025Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2359Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2204Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2090Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2223Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2212Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2320Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2410Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2362Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2299Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2394Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2329Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2299Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2249Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2206Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2165Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2133Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2063Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2032Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2064Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2057Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2049Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2007Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2022Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2046Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2027Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2038Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2025Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2026Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2050Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2031Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2016Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1994Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1969Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1991Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1977Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1980Epoch 2/15: [================              ] 40/75 batches, loss: 0.1954Epoch 2/15: [================              ] 41/75 batches, loss: 0.1936Epoch 2/15: [================              ] 42/75 batches, loss: 0.1959Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1958Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1973Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1946Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1936Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1920Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1909Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1893Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1889Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1891Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1889Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1889Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1882Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1868Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1873Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1859Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1843Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1849Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1848Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1853Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1848Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1842Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1833Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1820Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1812Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1819Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1835Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1830Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1838Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1829Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1827Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1814Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1820Epoch 2/15: [==============================] 75/75 batches, loss: 0.1818
[2025-05-07 21:03:53,593][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1818
[2025-05-07 21:03:53,808][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1140, Metrics: {'mse': 0.11505945771932602, 'rmse': 0.3392041534523509, 'r2': -0.3375352621078491}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1180Epoch 3/15: [                              ] 2/75 batches, loss: 0.1401Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1453Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1278Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1643Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1795Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1748Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1673Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1616Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1540Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1626Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1620Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1617Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1574Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1556Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1551Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1504Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1501Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1484Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1521Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1578Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1578Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1564Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1568Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1521Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1559Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1586Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1593Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1610Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1578Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1570Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1613Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1628Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1634Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1652Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1631Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1627Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1622Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1614Epoch 3/15: [================              ] 40/75 batches, loss: 0.1599Epoch 3/15: [================              ] 41/75 batches, loss: 0.1588Epoch 3/15: [================              ] 42/75 batches, loss: 0.1575Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1558Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1545Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1543Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1522Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1526Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1516Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1509Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1518Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1515Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1527Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1521Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1514Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1509Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1503Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1501Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1506Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1504Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1491Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1492Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1480Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1474Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1469Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1462Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1456Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1450Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1449Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1449Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1452Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1443Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1447Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1452Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1444Epoch 3/15: [==============================] 75/75 batches, loss: 0.1440
[2025-05-07 21:03:56,488][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1440
[2025-05-07 21:03:56,699][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1075, Metrics: {'mse': 0.10876281559467316, 'rmse': 0.32979207933889676, 'r2': -0.26433849334716797}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1058Epoch 4/15: [                              ] 2/75 batches, loss: 0.1253Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1523Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1507Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1422Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1367Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1312Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1236Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1210Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1222Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1243Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1226Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1244Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1221Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1210Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1255Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1230Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1219Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1238Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1278Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1260Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1270Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1248Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1267Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1249Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1248Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1250Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1248Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1262Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1285Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1278Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1272Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1266Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1283Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1278Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1271Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1273Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1270Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1265Epoch 4/15: [================              ] 40/75 batches, loss: 0.1266Epoch 4/15: [================              ] 41/75 batches, loss: 0.1256Epoch 4/15: [================              ] 42/75 batches, loss: 0.1269Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1279Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1276Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1278Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1269Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1278Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1289Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1286Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1274Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1266Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1265Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1272Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1266Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1262Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1268Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1261Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1265Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1274Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1276Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1298Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1294Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1299Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1302Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1304Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1299Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1290Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1288Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1291Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1291Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1290Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1289Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1288Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1290Epoch 4/15: [==============================] 75/75 batches, loss: 0.1289
[2025-05-07 21:03:59,347][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1289
[2025-05-07 21:03:59,567][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1067, Metrics: {'mse': 0.10787639766931534, 'rmse': 0.32844542570922697, 'r2': -0.25403404235839844}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1251Epoch 5/15: [                              ] 2/75 batches, loss: 0.1018Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1023Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1159Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1114Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1069Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1044Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1013Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1032Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1029Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1072Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1129Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1173Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1173Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1180Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1187Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1174Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1189Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1190Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1194Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1183Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1191Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1192Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1197Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1236Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1210Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1203Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1191Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1194Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1186Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1187Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1184Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1175Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1163Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1149Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1131Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1122Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1113Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1110Epoch 5/15: [================              ] 40/75 batches, loss: 0.1112Epoch 5/15: [================              ] 41/75 batches, loss: 0.1107Epoch 5/15: [================              ] 42/75 batches, loss: 0.1125Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1117Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1114Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1115Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1110Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1108Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1118Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1113Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1114Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1100Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1101Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1100Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1119Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1128Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1125Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1124Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1122Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1131Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1127Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1142Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1166Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1163Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1153Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1161Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1160Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1156Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1153Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1155Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1161Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1166Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1159Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1159Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1163Epoch 5/15: [==============================] 75/75 batches, loss: 0.1165
[2025-05-07 21:04:02,188][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1165
[2025-05-07 21:04:02,391][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1009, Metrics: {'mse': 0.10203299671411514, 'rmse': 0.31942604263603047, 'r2': -0.18610608577728271}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1720Epoch 6/15: [                              ] 2/75 batches, loss: 0.1795Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1550Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1547Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1568Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1423Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1400Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1414Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1396Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1330Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1254Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1271Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1227Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1213Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1228Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1219Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1252Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1240Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1235Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1211Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1207Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1186Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1200Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1180Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1178Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1161Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1163Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1147Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1191Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1192Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1184Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1184Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1172Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1162Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1162Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1170Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1161Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1148Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1148Epoch 6/15: [================              ] 40/75 batches, loss: 0.1162Epoch 6/15: [================              ] 41/75 batches, loss: 0.1169Epoch 6/15: [================              ] 42/75 batches, loss: 0.1168Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1154Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1154Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1163Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1153Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1147Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1152Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1147Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1161Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1161Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1155Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1151Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1147Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1157Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1161Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1161Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1151Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1138Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1135Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1130Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1130Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1122Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1118Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1117Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1112Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1101Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1112Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1115Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1107Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1105Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1099Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1098Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1090Epoch 6/15: [==============================] 75/75 batches, loss: 0.1084
[2025-05-07 21:04:05,030][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1084
[2025-05-07 21:04:05,257][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0923, Metrics: {'mse': 0.09354721009731293, 'rmse': 0.30585488405012096, 'r2': -0.08746111392974854}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0706Epoch 7/15: [                              ] 2/75 batches, loss: 0.0684Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0773Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0802Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0936Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0911Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0994Epoch 7/15: [===                           ] 8/75 batches, loss: 0.1035Epoch 7/15: [===                           ] 9/75 batches, loss: 0.1001Epoch 7/15: [====                          ] 10/75 batches, loss: 0.1041Epoch 7/15: [====                          ] 11/75 batches, loss: 0.1012Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0997Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.1026Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.1015Epoch 7/15: [======                        ] 15/75 batches, loss: 0.1005Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0991Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0986Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0961Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0975Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0975Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0957Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0946Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0947Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0960Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0950Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0949Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0938Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0966Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0968Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0976Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0973Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0963Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0957Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.1007Epoch 7/15: [==============                ] 35/75 batches, loss: 0.1002Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0990Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0984Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0980Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0983Epoch 7/15: [================              ] 40/75 batches, loss: 0.1001Epoch 7/15: [================              ] 41/75 batches, loss: 0.0985Epoch 7/15: [================              ] 42/75 batches, loss: 0.0976Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0973Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0966Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0971Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0969Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0965Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0972Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0976Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0974Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0972Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0964Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0964Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0959Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0963Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0962Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0963Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0960Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0960Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0962Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0964Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0965Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0962Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0960Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0960Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0959Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0956Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0959Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0956Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0959Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0952Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0949Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0946Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0949Epoch 7/15: [==============================] 75/75 batches, loss: 0.0946
[2025-05-07 21:04:07,960][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0946
[2025-05-07 21:04:08,174][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0951, Metrics: {'mse': 0.09631235897541046, 'rmse': 0.3103423254656227, 'r2': -0.1196051836013794}
[2025-05-07 21:04:08,175][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1072Epoch 8/15: [                              ] 2/75 batches, loss: 0.0895Epoch 8/15: [=                             ] 3/75 batches, loss: 0.1013Epoch 8/15: [=                             ] 4/75 batches, loss: 0.1003Epoch 8/15: [==                            ] 5/75 batches, loss: 0.1040Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0987Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0990Epoch 8/15: [===                           ] 8/75 batches, loss: 0.1035Epoch 8/15: [===                           ] 9/75 batches, loss: 0.1040Epoch 8/15: [====                          ] 10/75 batches, loss: 0.1038Epoch 8/15: [====                          ] 11/75 batches, loss: 0.1016Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0977Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0958Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.1016Epoch 8/15: [======                        ] 15/75 batches, loss: 0.1053Epoch 8/15: [======                        ] 16/75 batches, loss: 0.1034Epoch 8/15: [======                        ] 17/75 batches, loss: 0.1050Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.1074Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.1066Epoch 8/15: [========                      ] 20/75 batches, loss: 0.1056Epoch 8/15: [========                      ] 21/75 batches, loss: 0.1057Epoch 8/15: [========                      ] 22/75 batches, loss: 0.1062Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.1060Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.1059Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.1033Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.1032Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.1046Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.1052Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.1042Epoch 8/15: [============                  ] 30/75 batches, loss: 0.1039Epoch 8/15: [============                  ] 31/75 batches, loss: 0.1038Epoch 8/15: [============                  ] 32/75 batches, loss: 0.1026Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.1020Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.1014Epoch 8/15: [==============                ] 35/75 batches, loss: 0.1003Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0992Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0999Epoch 8/15: [===============               ] 38/75 batches, loss: 0.1006Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0996Epoch 8/15: [================              ] 40/75 batches, loss: 0.0996Epoch 8/15: [================              ] 41/75 batches, loss: 0.0988Epoch 8/15: [================              ] 42/75 batches, loss: 0.0985Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0980Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0992Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0996Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0987Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0980Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0980Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0973Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0980Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0973Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0970Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0965Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0964Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0957Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0957Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0958Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0964Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0955Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0955Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0957Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0951Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0950Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0945Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0946Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0944Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0942Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0938Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0936Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0931Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0931Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0930Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0924Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0923Epoch 8/15: [==============================] 75/75 batches, loss: 0.0912
[2025-05-07 21:04:10,483][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0912
[2025-05-07 21:04:10,702][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1198, Metrics: {'mse': 0.12073596566915512, 'rmse': 0.3474708126866991, 'r2': -0.4035230875015259}
[2025-05-07 21:04:10,703][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0880Epoch 9/15: [                              ] 2/75 batches, loss: 0.0989Epoch 9/15: [=                             ] 3/75 batches, loss: 0.1008Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0968Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0976Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0958Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0916Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0943Epoch 9/15: [===                           ] 9/75 batches, loss: 0.1018Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0970Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0943Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0925Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0932Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0927Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0900Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0900Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0884Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0874Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0882Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0879Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0868Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0863Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0869Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0900Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0903Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0899Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0909Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0900Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0905Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0885Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0884Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0891Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0904Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0895Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0880Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0868Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0874Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0866Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0861Epoch 9/15: [================              ] 40/75 batches, loss: 0.0861Epoch 9/15: [================              ] 41/75 batches, loss: 0.0856Epoch 9/15: [================              ] 42/75 batches, loss: 0.0873Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0874Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0878Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0875Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0878Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0881Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0880Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0874Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0872Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0866Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0856Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0853Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0850Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0849Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0863Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0864Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0870Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0868Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0872Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0882Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0880Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0885Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0889Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0897Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0895Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0892Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0900Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0903Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0904Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0908Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0902Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0902Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0900Epoch 9/15: [==============================] 75/75 batches, loss: 0.0901
[2025-05-07 21:04:12,977][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0901
[2025-05-07 21:04:13,193][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0970, Metrics: {'mse': 0.09821586310863495, 'rmse': 0.31339410190467043, 'r2': -0.14173293113708496}
[2025-05-07 21:04:13,194][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0799Epoch 10/15: [                              ] 2/75 batches, loss: 0.0834Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0826Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0955Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0892Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0844Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0837Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0936Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0912Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0929Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0926Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0927Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0918Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0944Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0944Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0940Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0916Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0915Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0905Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0907Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0912Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0901Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0902Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0925Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0932Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0923Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0917Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0900Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0909Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0902Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0897Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0892Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0888Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0887Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0888Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0880Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0915Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0913Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0906Epoch 10/15: [================              ] 40/75 batches, loss: 0.0909Epoch 10/15: [================              ] 41/75 batches, loss: 0.0912Epoch 10/15: [================              ] 42/75 batches, loss: 0.0915Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0912Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0913Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0902Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0910Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0905Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0916Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0915Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0919Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0916Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0914Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0908Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0905Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0900Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0902Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0910Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0908Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0907Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0904Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0901Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0906Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0904Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0906Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0911Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0912Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0903Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0908Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0909Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0912Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0912Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0913Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0921Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0921Epoch 10/15: [==============================] 75/75 batches, loss: 0.0918
[2025-05-07 21:04:15,495][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0918
[2025-05-07 21:04:15,722][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0978, Metrics: {'mse': 0.09889203310012817, 'rmse': 0.3144710369813541, 'r2': -0.14959335327148438}
[2025-05-07 21:04:15,722][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:04:15,722][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:04:15,723][src.training.lm_trainer][INFO] - Training completed in 28.31 seconds
[2025-05-07 21:04:15,723][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:04:18,584][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06565717607736588, 'rmse': 0.2562365627254742, 'r2': -0.01065981388092041}
[2025-05-07 21:04:18,585][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.09354721009731293, 'rmse': 0.30585488405012096, 'r2': -0.08746111392974854}
[2025-05-07 21:04:18,585][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12122289091348648, 'rmse': 0.3481707783738987, 'r2': -0.2689931392669678}
[2025-05-07 21:04:20,225][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ja/ja/model.pt
[2025-05-07 21:04:20,226][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▆▅▄▁
wandb:     best_val_mse █▇▆▅▄▁
wandb:      best_val_r2 ▁▂▃▄▅█
wandb:    best_val_rmse ██▆▅▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▂▂▃▄▄▁▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇▇▅▅▃▁▂█▂▂
wandb:          val_mse ▇▇▅▅▃▁▂█▂▂
wandb:           val_r2 ▂▂▄▄▆█▇▁▇▇
wandb:         val_rmse ▇▇▅▅▃▁▂█▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.09232
wandb:     best_val_mse 0.09355
wandb:      best_val_r2 -0.08746
wandb:    best_val_rmse 0.30585
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.12122
wandb:    final_test_r2 -0.26899
wandb:  final_test_rmse 0.34817
wandb:  final_train_mse 0.06566
wandb:   final_train_r2 -0.01066
wandb: final_train_rmse 0.25624
wandb:    final_val_mse 0.09355
wandb:     final_val_r2 -0.08746
wandb:   final_val_rmse 0.30585
wandb:    learning_rate 0.0001
wandb:       train_loss 0.09184
wandb:       train_time 28.30764
wandb:         val_loss 0.09777
wandb:          val_mse 0.09889
wandb:           val_r2 -0.14959
wandb:         val_rmse 0.31447
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210330-wyr2nkyw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210330-wyr2nkyw/logs
Experiment probe_layer2_avg_verb_edges_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_avg_verb_edges_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_verb_edges_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_verb_edges"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:04:42,550][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ja
experiment_name: probe_layer2_avg_verb_edges_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:04:42,550][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:04:42,551][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:04:42,551][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:04:42,551][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:04:42,555][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:04:42,555][__main__][INFO] - Using submetric: avg_verb_edges
[2025-05-07 21:04:42,555][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:04:45,682][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:04:47,987][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:04:47,988][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:04:48,242][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 21:04:48,319][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:32:16 2025).
[2025-05-07 21:04:48,550][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:04:48,560][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:04:48,560][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:04:48,562][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:04:48,629][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:04:48,687][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:04:48,714][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:04:48,716][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:04:48,720][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:04:48,722][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:04:48,774][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:04:48,840][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:04:48,891][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:04:48,893][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:04:48,894][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:04:48,895][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:04:48,896][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:04:48,896][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:04:48,896][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:04:48,896][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:04:48,896][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:04:48,896][src.data.datasets][INFO] -   Mean: 0.2629, Std: 0.2549
[2025-05-07 21:04:48,896][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:04:48,897][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:04:48,897][src.data.datasets][INFO] -   Mean: 0.3093, Std: 0.2933
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:04:48,897][src.data.datasets][INFO] - Sample label: 0.800000011920929
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-05-07 21:04:48,898][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:04:48,898][src.data.datasets][INFO] -   Mean: 0.3726, Std: 0.3091
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:04:48,898][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:04:48,899][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:04:48,899][__main__][INFO] - Using model type: lm_probe for submetric avg_verb_edges
[2025-05-07 21:04:48,899][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:04:56,291][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:04:56,292][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:04:56,292][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:04:56,293][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:04:56,295][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:04:56,296][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:04:56,296][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:04:56,296][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:04:56,296][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:04:56,297][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:04:56,297][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5549Epoch 1/15: [                              ] 2/75 batches, loss: 0.5890Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5523Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5238Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4881Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4467Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4476Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4706Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4567Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4298Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4294Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4344Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4190Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4401Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4426Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4579Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4574Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4724Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4668Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4630Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4540Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4518Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4376Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4254Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4210Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4142Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4160Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4128Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4114Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4059Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3989Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3973Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3984Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3988Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3949Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4008Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3967Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3930Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3899Epoch 1/15: [================              ] 40/75 batches, loss: 0.3839Epoch 1/15: [================              ] 41/75 batches, loss: 0.3813Epoch 1/15: [================              ] 42/75 batches, loss: 0.3803Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3826Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3862Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3821Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3768Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3714Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3697Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3688Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3658Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3653Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3725Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3690Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3705Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3697Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3653Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3617Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3614Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3599Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3581Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3556Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3524Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3541Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3529Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3517Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3485Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3477Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3455Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3440Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3416Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3400Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3415Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3389Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3388Epoch 1/15: [==============================] 75/75 batches, loss: 0.3353
[2025-05-07 21:05:03,246][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3353
[2025-05-07 21:05:03,425][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1264, Metrics: {'mse': 0.1276616007089615, 'rmse': 0.3572976360248714, 'r2': -0.4840317964553833}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2587Epoch 2/15: [                              ] 2/75 batches, loss: 0.2791Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2359Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2493Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2225Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2075Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2080Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2090Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2204Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2156Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2109Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2109Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2088Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2065Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2103Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2088Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2066Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2012Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2029Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2012Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2018Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2011Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2010Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1989Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1955Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2022Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2048Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2028Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2030Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2049Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2030Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2086Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2070Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2052Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2040Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2026Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2026Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1993Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1969Epoch 2/15: [================              ] 40/75 batches, loss: 0.1968Epoch 2/15: [================              ] 41/75 batches, loss: 0.1942Epoch 2/15: [================              ] 42/75 batches, loss: 0.1962Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1952Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1986Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1971Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1953Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1934Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1922Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1933Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1919Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1910Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1903Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1918Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1910Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1900Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1918Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1911Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1906Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1893Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1910Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1903Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1912Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1913Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1915Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1910Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1902Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1901Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1904Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1893Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1887Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1889Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1884Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1871Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1872Epoch 2/15: [==============================] 75/75 batches, loss: 0.1869
[2025-05-07 21:05:06,092][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1869
[2025-05-07 21:05:06,304][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1177, Metrics: {'mse': 0.11881870031356812, 'rmse': 0.3447008852810914, 'r2': -0.38123536109924316}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.3193Epoch 3/15: [                              ] 2/75 batches, loss: 0.2308Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1892Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1716Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1711Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1751Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1724Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1759Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1770Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1665Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1686Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1630Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1650Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1609Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1602Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1585Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1556Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1530Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1532Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1524Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1567Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1560Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1608Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1565Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1526Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1530Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1569Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1580Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1572Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1540Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1549Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1641Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1655Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1649Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1643Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1652Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1663Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1656Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1644Epoch 3/15: [================              ] 40/75 batches, loss: 0.1626Epoch 3/15: [================              ] 41/75 batches, loss: 0.1640Epoch 3/15: [================              ] 42/75 batches, loss: 0.1634Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1623Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1628Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1624Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1629Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1613Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1596Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1601Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1611Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1604Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1603Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1599Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1587Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1579Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1574Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1560Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1569Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1566Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1548Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1542Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1540Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1540Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1531Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1528Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1516Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1512Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1509Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1504Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1504Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1502Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1512Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1518Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1508Epoch 3/15: [==============================] 75/75 batches, loss: 0.1504
[2025-05-07 21:05:08,978][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1504
[2025-05-07 21:05:09,193][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1092, Metrics: {'mse': 0.11028366535902023, 'rmse': 0.33208984531150637, 'r2': -0.28201794624328613}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.2854Epoch 4/15: [                              ] 2/75 batches, loss: 0.2178Epoch 4/15: [=                             ] 3/75 batches, loss: 0.2175Epoch 4/15: [=                             ] 4/75 batches, loss: 0.2031Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1820Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1680Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1521Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1463Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1500Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1479Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1503Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1474Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1456Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1457Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1410Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1402Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1398Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1360Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1342Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1358Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1361Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1353Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1353Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1341Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1335Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1336Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1316Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1308Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1310Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1295Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1300Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1304Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1308Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1296Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1279Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1263Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1261Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1266Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1249Epoch 4/15: [================              ] 40/75 batches, loss: 0.1240Epoch 4/15: [================              ] 41/75 batches, loss: 0.1232Epoch 4/15: [================              ] 42/75 batches, loss: 0.1212Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1221Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1215Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1212Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1215Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1216Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1215Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1212Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1199Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1191Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1183Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1182Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1177Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1176Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1175Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1172Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1183Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1185Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1183Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1190Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1196Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1195Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1189Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1194Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1197Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1196Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1194Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1194Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1189Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1181Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1191Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1188Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1187Epoch 4/15: [==============================] 75/75 batches, loss: 0.1192
[2025-05-07 21:05:11,832][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1192
[2025-05-07 21:05:12,044][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0994, Metrics: {'mse': 0.10061322897672653, 'rmse': 0.31719588423673867, 'r2': -0.16960179805755615}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1104Epoch 5/15: [                              ] 2/75 batches, loss: 0.1006Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0925Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0912Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1038Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1158Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1115Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1083Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1138Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1175Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1213Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1235Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1249Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1243Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1236Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1256Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1246Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1278Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1247Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1221Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1178Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1216Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1232Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1218Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1214Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1232Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1202Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1188Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1181Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1194Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1199Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1184Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1173Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1166Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1183Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1195Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1189Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1195Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1180Epoch 5/15: [================              ] 40/75 batches, loss: 0.1179Epoch 5/15: [================              ] 41/75 batches, loss: 0.1184Epoch 5/15: [================              ] 42/75 batches, loss: 0.1205Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1193Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1186Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1174Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1162Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1161Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1169Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1156Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1150Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1143Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1141Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1136Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1137Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1139Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1139Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1133Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1137Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1144Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1138Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1149Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1164Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1155Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1153Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1151Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1144Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1147Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1136Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1130Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1132Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1131Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1123Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1123Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1126Epoch 5/15: [==============================] 75/75 batches, loss: 0.1122
[2025-05-07 21:05:14,688][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1122
[2025-05-07 21:05:14,907][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1143, Metrics: {'mse': 0.11526409536600113, 'rmse': 0.33950566323111775, 'r2': -0.33991408348083496}
[2025-05-07 21:05:14,907][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1471Epoch 6/15: [                              ] 2/75 batches, loss: 0.1163Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1138Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1080Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1277Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1277Epoch 6/15: [==                            ] 7/75 batches, loss: 0.1180Epoch 6/15: [===                           ] 8/75 batches, loss: 0.1188Epoch 6/15: [===                           ] 9/75 batches, loss: 0.1139Epoch 6/15: [====                          ] 10/75 batches, loss: 0.1094Epoch 6/15: [====                          ] 11/75 batches, loss: 0.1076Epoch 6/15: [====                          ] 12/75 batches, loss: 0.1129Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.1101Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.1119Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1116Epoch 6/15: [======                        ] 16/75 batches, loss: 0.1138Epoch 6/15: [======                        ] 17/75 batches, loss: 0.1122Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1116Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.1082Epoch 6/15: [========                      ] 20/75 batches, loss: 0.1061Epoch 6/15: [========                      ] 21/75 batches, loss: 0.1058Epoch 6/15: [========                      ] 22/75 batches, loss: 0.1061Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.1054Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.1058Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.1064Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1062Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1060Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1060Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1070Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1069Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1074Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1085Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1073Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1092Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1097Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1092Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1091Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1092Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1089Epoch 6/15: [================              ] 40/75 batches, loss: 0.1099Epoch 6/15: [================              ] 41/75 batches, loss: 0.1086Epoch 6/15: [================              ] 42/75 batches, loss: 0.1092Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1085Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1085Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1081Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1081Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1091Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1092Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1096Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1101Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1110Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1100Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1102Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1099Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1089Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1104Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1097Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1106Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1108Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1106Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1110Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1110Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1107Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1104Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1097Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1088Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1089Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1085Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1089Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1083Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1082Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1081Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1076Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1068Epoch 6/15: [==============================] 75/75 batches, loss: 0.1067
[2025-05-07 21:05:17,191][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1067
[2025-05-07 21:05:17,406][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1000, Metrics: {'mse': 0.10108860582113266, 'rmse': 0.3179443439049241, 'r2': -0.17512786388397217}
[2025-05-07 21:05:17,406][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0373Epoch 7/15: [                              ] 2/75 batches, loss: 0.0584Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0770Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0802Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0741Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0838Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0829Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0887Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0868Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0910Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0912Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0891Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0898Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0907Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0886Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0909Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0920Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0923Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0939Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0933Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0938Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0929Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0925Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0914Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0917Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0925Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0919Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0921Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0930Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0946Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0961Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0957Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0954Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0972Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0973Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0972Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0973Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0988Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0990Epoch 7/15: [================              ] 40/75 batches, loss: 0.1000Epoch 7/15: [================              ] 41/75 batches, loss: 0.1000Epoch 7/15: [================              ] 42/75 batches, loss: 0.0990Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0984Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0993Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0996Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0993Epoch 7/15: [==================            ] 47/75 batches, loss: 0.1011Epoch 7/15: [===================           ] 48/75 batches, loss: 0.1014Epoch 7/15: [===================           ] 49/75 batches, loss: 0.1008Epoch 7/15: [====================          ] 50/75 batches, loss: 0.1016Epoch 7/15: [====================          ] 51/75 batches, loss: 0.1016Epoch 7/15: [====================          ] 52/75 batches, loss: 0.1009Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.1015Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.1002Epoch 7/15: [======================        ] 55/75 batches, loss: 0.1006Epoch 7/15: [======================        ] 56/75 batches, loss: 0.1012Epoch 7/15: [======================        ] 57/75 batches, loss: 0.1009Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.1005Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.1014Epoch 7/15: [========================      ] 60/75 batches, loss: 0.1019Epoch 7/15: [========================      ] 61/75 batches, loss: 0.1017Epoch 7/15: [========================      ] 62/75 batches, loss: 0.1022Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.1019Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.1012Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.1015Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.1016Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.1021Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.1017Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.1013Epoch 7/15: [============================  ] 70/75 batches, loss: 0.1021Epoch 7/15: [============================  ] 71/75 batches, loss: 0.1021Epoch 7/15: [============================  ] 72/75 batches, loss: 0.1016Epoch 7/15: [============================= ] 73/75 batches, loss: 0.1013Epoch 7/15: [============================= ] 74/75 batches, loss: 0.1015Epoch 7/15: [==============================] 75/75 batches, loss: 0.1015
[2025-05-07 21:05:19,681][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1015
[2025-05-07 21:05:19,882][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0900, Metrics: {'mse': 0.09123293310403824, 'rmse': 0.302047898691645, 'r2': -0.060558319091796875}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0744Epoch 8/15: [                              ] 2/75 batches, loss: 0.0672Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0661Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0878Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0911Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0845Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0875Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0851Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0915Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0899Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0923Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0942Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0950Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0962Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0982Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0956Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0923Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0948Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0959Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0932Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0940Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0933Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0943Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0934Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0924Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0940Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0960Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0988Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0981Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0986Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0976Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0981Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0963Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0965Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0962Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0955Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0951Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0944Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0938Epoch 8/15: [================              ] 40/75 batches, loss: 0.0929Epoch 8/15: [================              ] 41/75 batches, loss: 0.0927Epoch 8/15: [================              ] 42/75 batches, loss: 0.0926Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0915Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0925Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0924Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0921Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0916Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0917Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0910Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0909Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0904Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0899Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0895Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0892Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0896Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0896Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0892Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0888Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0891Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0886Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0894Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0892Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0897Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0905Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0902Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0901Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0909Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0908Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0908Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0905Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0904Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0904Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0905Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0905Epoch 8/15: [==============================] 75/75 batches, loss: 0.0895
[2025-05-07 21:05:22,584][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0895
[2025-05-07 21:05:22,789][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1010, Metrics: {'mse': 0.10215543955564499, 'rmse': 0.3196176458765144, 'r2': -0.18752944469451904}
[2025-05-07 21:05:22,790][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.1122Epoch 9/15: [                              ] 2/75 batches, loss: 0.0822Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0848Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0871Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0795Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0821Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0854Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0849Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0810Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0820Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0825Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0840Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0858Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0935Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0938Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0928Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0909Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0900Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0889Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0908Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0917Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0930Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0912Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0916Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0927Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0944Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0936Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0920Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0930Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0939Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0939Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0946Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0944Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0928Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0925Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0923Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0918Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0921Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0927Epoch 9/15: [================              ] 40/75 batches, loss: 0.0916Epoch 9/15: [================              ] 41/75 batches, loss: 0.0918Epoch 9/15: [================              ] 42/75 batches, loss: 0.0922Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0927Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0929Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0927Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0920Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0913Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0906Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0906Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0901Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0906Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0910Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0912Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0908Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0910Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0911Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0912Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0911Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0914Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0921Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0921Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0919Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0917Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0922Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0920Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0919Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0920Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0918Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0919Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0917Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0917Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0912Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0905Epoch 9/15: [==============================] 75/75 batches, loss: 0.0905
[2025-05-07 21:05:25,095][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0905
[2025-05-07 21:05:25,317][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0958, Metrics: {'mse': 0.09706883877515793, 'rmse': 0.3115587244407672, 'r2': -0.12839913368225098}
[2025-05-07 21:05:25,317][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.1007Epoch 10/15: [                              ] 2/75 batches, loss: 0.0827Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0902Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0951Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0898Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0857Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0839Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0880Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0892Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0841Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0850Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0905Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0905Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0883Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0880Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0883Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0895Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0908Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0893Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0898Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0890Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0872Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0865Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0868Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0865Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0866Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0866Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0855Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0852Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0846Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0859Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0849Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0864Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0857Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0857Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0855Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0853Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0851Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0851Epoch 10/15: [================              ] 40/75 batches, loss: 0.0852Epoch 10/15: [================              ] 41/75 batches, loss: 0.0867Epoch 10/15: [================              ] 42/75 batches, loss: 0.0866Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0858Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0857Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0856Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0855Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0860Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0865Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0865Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0879Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0876Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0878Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0876Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0877Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0883Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0883Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0895Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0897Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0896Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0890Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0894Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0899Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0898Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0900Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0898Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0908Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0904Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0901Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0910Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0909Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0906Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0907Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0904Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0903Epoch 10/15: [==============================] 75/75 batches, loss: 0.0895
[2025-05-07 21:05:27,609][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0895
[2025-05-07 21:05:27,817][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0999, Metrics: {'mse': 0.10113637149333954, 'rmse': 0.31801945143864946, 'r2': -0.1756831407546997}
[2025-05-07 21:05:27,818][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0465Epoch 11/15: [                              ] 2/75 batches, loss: 0.0647Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0644Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0603Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0672Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0723Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0700Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0681Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0695Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0668Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0725Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0725Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0732Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0796Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0873Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0866Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0883Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0875Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0871Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0866Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0888Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0878Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0882Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0901Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0895Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0896Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0888Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0889Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0876Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0885Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0897Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0887Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0884Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0874Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0868Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0878Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0877Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0878Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0872Epoch 11/15: [================              ] 40/75 batches, loss: 0.0867Epoch 11/15: [================              ] 41/75 batches, loss: 0.0863Epoch 11/15: [================              ] 42/75 batches, loss: 0.0858Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0875Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0883Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0878Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0877Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0883Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0882Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0888Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0886Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0881Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0873Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0872Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0867Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0866Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0863Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0857Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0854Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0857Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0860Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0855Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0850Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0843Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0838Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0840Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0836Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0833Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0831Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0831Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0833Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0829Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0836Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0835Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0842Epoch 11/15: [==============================] 75/75 batches, loss: 0.0839
[2025-05-07 21:05:30,123][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0839
[2025-05-07 21:05:30,354][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1010, Metrics: {'mse': 0.10211551934480667, 'rmse': 0.3195551898261186, 'r2': -0.18706536293029785}
[2025-05-07 21:05:30,354][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:05:30,355][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 21:05:30,355][src.training.lm_trainer][INFO] - Training completed in 31.28 seconds
[2025-05-07 21:05:30,355][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:05:33,182][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06472983211278915, 'rmse': 0.25442058115016786, 'r2': 0.003614664077758789}
[2025-05-07 21:05:33,182][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.09123293310403824, 'rmse': 0.302047898691645, 'r2': -0.060558319091796875}
[2025-05-07 21:05:33,182][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.11486241221427917, 'rmse': 0.3389135763203935, 'r2': -0.20240998268127441}
[2025-05-07 21:05:34,836][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ja/ja/model.pt
[2025-05-07 21:05:34,837][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▃▁
wandb:     best_val_mse █▆▅▃▁
wandb:      best_val_r2 ▁▃▄▆█
wandb:    best_val_rmse █▆▅▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▄▂▄▄▄▄▄
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▃▆▃▁▃▂▃▃
wandb:          val_mse █▆▅▃▆▃▁▃▂▃▃
wandb:           val_r2 ▁▃▄▆▃▆█▆▇▆▆
wandb:         val_rmse █▆▅▃▆▃▁▃▂▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08998
wandb:     best_val_mse 0.09123
wandb:      best_val_r2 -0.06056
wandb:    best_val_rmse 0.30205
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.11486
wandb:    final_test_r2 -0.20241
wandb:  final_test_rmse 0.33891
wandb:  final_train_mse 0.06473
wandb:   final_train_r2 0.00361
wandb: final_train_rmse 0.25442
wandb:    final_val_mse 0.09123
wandb:     final_val_r2 -0.06056
wandb:   final_val_rmse 0.30205
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08392
wandb:       train_time 31.27692
wandb:         val_loss 0.10097
wandb:          val_mse 0.10212
wandb:           val_r2 -0.18707
wandb:         val_rmse 0.31956
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210442-mym83ryo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210442-mym83ryo/logs
Experiment probe_layer2_avg_verb_edges_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_verb_edges/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:05:57,383][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ja
experiment_name: probe_layer2_lexical_density_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:05:57,383][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:05:57,383][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:05:57,383][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:05:57,383][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:05:57,387][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:05:57,387][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:05:57,388][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:06:00,683][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:06:03,058][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:06:03,058][src.data.datasets][INFO] - Loading 'control_lexical_density_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:06:03,167][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 21:06:03,220][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:47:48 2025).
[2025-05-07 21:06:03,435][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:06:03,450][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:06:03,452][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:06:03,454][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:06:03,545][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:06:03,605][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:06:03,655][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:06:03,656][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:06:03,656][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:06:03,658][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:06:03,711][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:06:03,795][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:06:03,810][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:06:03,812][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:06:03,813][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:06:03,814][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:06:03,815][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:06:03,815][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:06:03,815][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:06:03,815][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:06:03,815][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:06:03,816][src.data.datasets][INFO] -   Mean: 0.4139, Std: 0.1614
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Sample label: 0.35100001096725464
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:06:03,816][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:06:03,816][src.data.datasets][INFO] -   Min: 0.0670, Max: 0.7920
[2025-05-07 21:06:03,817][src.data.datasets][INFO] -   Mean: 0.4093, Std: 0.1643
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:06:03,817][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:06:03,817][src.data.datasets][INFO] -   Mean: 0.5256, Std: 0.2289
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:06:03,817][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:06:03,818][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:06:03,818][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:06:03,818][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:06:03,818][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 21:06:03,818][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:06:09,642][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:06:09,643][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:06:09,643][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:06:09,643][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:06:09,646][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:06:09,647][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:06:09,647][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:06:09,647][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:06:09,647][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:06:09,648][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:06:09,648][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4775Epoch 1/15: [                              ] 2/75 batches, loss: 0.5113Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4956Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5129Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5215Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4743Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4573Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4668Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4506Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4256Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4117Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4146Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4006Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4106Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4098Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4249Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4172Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4306Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4166Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4153Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4081Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4080Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3960Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3870Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3842Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3798Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3772Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3694Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3637Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3581Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3527Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3461Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3440Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3437Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3424Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3448Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3416Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3368Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3357Epoch 1/15: [================              ] 40/75 batches, loss: 0.3318Epoch 1/15: [================              ] 41/75 batches, loss: 0.3285Epoch 1/15: [================              ] 42/75 batches, loss: 0.3288Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3288Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3338Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3315Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3277Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3245Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3211Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3187Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3166Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3161Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3191Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3157Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3151Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3128Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3089Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3063Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3053Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3028Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3013Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2996Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2974Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2972Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2963Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2951Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2929Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2914Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2891Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2870Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2848Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2846Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2860Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2842Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2834Epoch 1/15: [==============================] 75/75 batches, loss: 0.2808
[2025-05-07 21:06:15,127][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2808
[2025-05-07 21:06:15,316][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0380, Metrics: {'mse': 0.037671349942684174, 'rmse': 0.19409108671622244, 'r2': -0.39528417587280273}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1888Epoch 2/15: [                              ] 2/75 batches, loss: 0.1725Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1744Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1860Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1760Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1592Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1723Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1727Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1762Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1782Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1752Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1738Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1689Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1631Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1640Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1625Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1586Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1589Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1549Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1509Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1487Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1474Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1450Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1431Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1392Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1440Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1470Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1454Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1478Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1475Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1477Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1493Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1475Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1450Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1466Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1446Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1461Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1455Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1447Epoch 2/15: [================              ] 40/75 batches, loss: 0.1438Epoch 2/15: [================              ] 41/75 batches, loss: 0.1432Epoch 2/15: [================              ] 42/75 batches, loss: 0.1455Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1446Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1487Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1481Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1469Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1458Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1459Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1449Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1437Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1431Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1422Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1412Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1414Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1411Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1424Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1409Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1394Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1384Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1383Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1380Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1393Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1393Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1387Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1381Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1371Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1385Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1382Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1374Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1380Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1376Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1374Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1363Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1355Epoch 2/15: [==============================] 75/75 batches, loss: 0.1347
[2025-05-07 21:06:18,012][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1347
[2025-05-07 21:06:18,262][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0338, Metrics: {'mse': 0.0335785374045372, 'rmse': 0.18324447441747652, 'r2': -0.24369311332702637}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1212Epoch 3/15: [                              ] 2/75 batches, loss: 0.1089Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1122Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1054Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1112Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1097Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1083Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1117Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1131Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1104Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1136Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1106Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1112Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1081Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1079Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1084Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1056Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1023Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1012Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1033Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1075Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1072Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1070Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1055Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1032Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1036Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1063Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1069Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1056Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1050Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1056Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1109Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1116Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1107Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1092Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1081Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1092Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1085Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1082Epoch 3/15: [================              ] 40/75 batches, loss: 0.1065Epoch 3/15: [================              ] 41/75 batches, loss: 0.1065Epoch 3/15: [================              ] 42/75 batches, loss: 0.1079Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1075Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1071Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1068Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1062Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1055Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1055Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1053Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1058Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1058Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1054Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1054Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1050Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1045Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1037Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1036Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1045Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1040Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1028Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1029Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1021Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1016Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1010Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1006Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1001Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0995Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0991Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0987Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0982Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0978Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0984Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0986Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0983Epoch 3/15: [==============================] 75/75 batches, loss: 0.0977
[2025-05-07 21:06:21,030][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0977
[2025-05-07 21:06:21,273][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0321, Metrics: {'mse': 0.031868819147348404, 'rmse': 0.17851840002461483, 'r2': -0.18036794662475586}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1102Epoch 4/15: [                              ] 2/75 batches, loss: 0.1127Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1194Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1157Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0978Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1033Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0937Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0897Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0908Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0924Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0956Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0935Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0952Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0937Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0920Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0941Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0946Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0924Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0928Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0926Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0905Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0900Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0907Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0906Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0909Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0892Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0882Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0887Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0877Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0886Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0891Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0884Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0884Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0886Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0890Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0881Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0881Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0880Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0870Epoch 4/15: [================              ] 40/75 batches, loss: 0.0862Epoch 4/15: [================              ] 41/75 batches, loss: 0.0853Epoch 4/15: [================              ] 42/75 batches, loss: 0.0850Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0854Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0847Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0849Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0857Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0856Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0857Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0860Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0856Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0858Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0854Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0858Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0854Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0845Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0852Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0852Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0859Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0861Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0858Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0864Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0858Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0857Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0859Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0857Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0855Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0854Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0849Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0856Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0848Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0840Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0838Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0838Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0836Epoch 4/15: [==============================] 75/75 batches, loss: 0.0834
[2025-05-07 21:06:23,923][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0834
[2025-05-07 21:06:24,142][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0323, Metrics: {'mse': 0.03208509460091591, 'rmse': 0.17912312692926033, 'r2': -0.1883784532546997}
[2025-05-07 21:06:24,143][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0428Epoch 5/15: [                              ] 2/75 batches, loss: 0.0413Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0531Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0545Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0563Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0594Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0583Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0545Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0574Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0540Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0596Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0610Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0633Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0626Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0634Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0640Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0658Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0662Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0658Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0656Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0657Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0682Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0681Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0686Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0708Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0702Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0708Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0693Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0697Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0698Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0700Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0696Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0694Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0697Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0683Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0696Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0701Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0702Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0702Epoch 5/15: [================              ] 40/75 batches, loss: 0.0700Epoch 5/15: [================              ] 41/75 batches, loss: 0.0697Epoch 5/15: [================              ] 42/75 batches, loss: 0.0697Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0687Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0680Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0682Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0694Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0689Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0691Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0691Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0694Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0695Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0692Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0687Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0685Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0688Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0685Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0680Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0673Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0681Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0684Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0691Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0695Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0688Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0685Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0686Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0686Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0681Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0682Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0684Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0684Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0681Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0685Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0684Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0684Epoch 5/15: [==============================] 75/75 batches, loss: 0.0691
[2025-05-07 21:06:26,435][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0691
[2025-05-07 21:06:26,646][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0311, Metrics: {'mse': 0.03086775727570057, 'rmse': 0.17569222315088556, 'r2': -0.14329040050506592}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0608Epoch 6/15: [                              ] 2/75 batches, loss: 0.0739Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0709Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0683Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0788Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0748Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0734Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0737Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0705Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0702Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0696Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0746Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0711Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0723Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0710Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0725Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0734Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0716Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0713Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0722Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0735Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0718Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0713Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0707Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0709Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0705Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0701Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0697Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0712Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0709Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0709Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0726Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0717Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0718Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0717Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0731Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0723Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0726Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0722Epoch 6/15: [================              ] 40/75 batches, loss: 0.0718Epoch 6/15: [================              ] 41/75 batches, loss: 0.0711Epoch 6/15: [================              ] 42/75 batches, loss: 0.0702Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0697Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0697Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0693Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0688Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0681Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0685Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0682Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0677Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0678Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0674Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0674Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0666Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0664Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0659Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0658Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0654Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0656Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0653Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0654Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0651Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0650Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0650Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0646Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0643Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0638Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0633Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0629Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0629Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0625Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0626Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0623Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0620Epoch 6/15: [==============================] 75/75 batches, loss: 0.0621
[2025-05-07 21:06:29,305][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0621
[2025-05-07 21:06:29,520][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0291, Metrics: {'mse': 0.02886887826025486, 'rmse': 0.16990844081520748, 'r2': -0.06925511360168457}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0449Epoch 7/15: [                              ] 2/75 batches, loss: 0.0560Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0619Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0577Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0516Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0520Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0605Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0692Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0677Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0686Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0681Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0649Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0659Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0654Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0650Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0658Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0664Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0666Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0662Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0661Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0651Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0644Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0649Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0649Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0645Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0636Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0635Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0628Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0620Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0623Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0625Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0624Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0619Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0632Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0627Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0619Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0617Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0622Epoch 7/15: [================              ] 40/75 batches, loss: 0.0622Epoch 7/15: [================              ] 41/75 batches, loss: 0.0615Epoch 7/15: [================              ] 42/75 batches, loss: 0.0605Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0605Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0606Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0603Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0598Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0604Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0605Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0598Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0592Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0595Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0590Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0593Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0590Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0588Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0583Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0585Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0586Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0584Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0584Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0579Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0578Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0574Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0575Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0570Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0573Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0579Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0575Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0575Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0581Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0579Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0575Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0570Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0568Epoch 7/15: [==============================] 75/75 batches, loss: 0.0568
[2025-05-07 21:06:32,246][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0568
[2025-05-07 21:06:32,469][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0292, Metrics: {'mse': 0.028980256989598274, 'rmse': 0.1702358863154249, 'r2': -0.0733804702758789}
[2025-05-07 21:06:32,470][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0552Epoch 8/15: [                              ] 2/75 batches, loss: 0.0539Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0547Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0518Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0475Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0471Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0488Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0482Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0510Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0494Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0509Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0495Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0472Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0487Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0483Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0481Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0503Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0510Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0512Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0513Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0505Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0504Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0516Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0513Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0510Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0516Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0522Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0515Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0531Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0523Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0527Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0519Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0516Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0522Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0516Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0514Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0511Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0513Epoch 8/15: [================              ] 40/75 batches, loss: 0.0509Epoch 8/15: [================              ] 41/75 batches, loss: 0.0504Epoch 8/15: [================              ] 42/75 batches, loss: 0.0508Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0504Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0498Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0495Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0496Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0494Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0492Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0491Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0492Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0492Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0492Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0489Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0490Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0485Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0490Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0488Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0489Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0488Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0485Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0487Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0484Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0484Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0479Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0481Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0482Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0487Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0483Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0481Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0481Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0479Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0478Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0477Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0477Epoch 8/15: [==============================] 75/75 batches, loss: 0.0477
[2025-05-07 21:06:34,769][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0477
[2025-05-07 21:06:35,031][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0328, Metrics: {'mse': 0.032576680183410645, 'rmse': 0.1804901110404962, 'r2': -0.20658600330352783}
[2025-05-07 21:06:35,032][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0561Epoch 9/15: [                              ] 2/75 batches, loss: 0.0430Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0517Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0540Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0480Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0526Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0494Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0486Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0523Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0530Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0555Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0556Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0531Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0521Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0522Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0532Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0543Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0530Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0533Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0525Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0515Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0517Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0515Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0515Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0512Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0506Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0497Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0500Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0501Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0494Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0492Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0494Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0498Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0504Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0510Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0501Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0501Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0498Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0501Epoch 9/15: [================              ] 40/75 batches, loss: 0.0497Epoch 9/15: [================              ] 41/75 batches, loss: 0.0492Epoch 9/15: [================              ] 42/75 batches, loss: 0.0487Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0489Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0489Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0488Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0492Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0485Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0482Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0486Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0481Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0480Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0478Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0472Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0469Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0463Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0465Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0463Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0460Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0460Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0458Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0457Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0454Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0461Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0466Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0464Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0470Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0470Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0476Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0481Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0477Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0480Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0478Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0478Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0476Epoch 9/15: [==============================] 75/75 batches, loss: 0.0476
[2025-05-07 21:06:37,335][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0476
[2025-05-07 21:06:37,562][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0323, Metrics: {'mse': 0.03204413130879402, 'rmse': 0.17900874645892034, 'r2': -0.18686127662658691}
[2025-05-07 21:06:37,563][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0440Epoch 10/15: [                              ] 2/75 batches, loss: 0.0412Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0436Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0384Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0347Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0341Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0342Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0369Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0358Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0415Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0411Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0431Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0419Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0447Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0446Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0451Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0463Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0464Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0453Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0443Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0446Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0454Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0452Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0461Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0459Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0462Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0464Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0460Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0455Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0453Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0449Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0447Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0441Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0449Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0444Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0443Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0441Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0442Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0446Epoch 10/15: [================              ] 40/75 batches, loss: 0.0450Epoch 10/15: [================              ] 41/75 batches, loss: 0.0450Epoch 10/15: [================              ] 42/75 batches, loss: 0.0450Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0447Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0450Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0446Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0447Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0447Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0448Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0448Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0447Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0447Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0451Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0448Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0444Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0445Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0450Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0449Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0453Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0451Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0450Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0450Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0457Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0460Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0458Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0457Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0453Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0450Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0451Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0458Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0457Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0456Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0457Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0456Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0453Epoch 10/15: [==============================] 75/75 batches, loss: 0.0450
[2025-05-07 21:06:39,918][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0450
[2025-05-07 21:06:40,144][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0341, Metrics: {'mse': 0.03386642783880234, 'rmse': 0.18402833433686872, 'r2': -0.2543560266494751}
[2025-05-07 21:06:40,145][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:06:40,145][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:06:40,145][src.training.lm_trainer][INFO] - Training completed in 28.00 seconds
[2025-05-07 21:06:40,145][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:06:43,107][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026971280574798584, 'rmse': 0.1642293535723702, 'r2': -0.0357661247253418}
[2025-05-07 21:06:43,108][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02886887826025486, 'rmse': 0.16990844081520748, 'r2': -0.06925511360168457}
[2025-05-07 21:06:43,108][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0765053853392601, 'rmse': 0.27659606891505184, 'r2': -0.46001553535461426}
[2025-05-07 21:06:44,814][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ja/ja/model.pt
[2025-05-07 21:06:44,815][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▁
wandb:     best_val_mse █▅▃▃▁
wandb:      best_val_r2 ▁▄▆▆█
wandb:    best_val_rmse █▅▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▃▃▄▄▃▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▃▃▁▁▄▃▅
wandb:          val_mse █▅▃▄▃▁▁▄▄▅
wandb:           val_r2 ▁▄▆▅▆██▅▅▄
wandb:         val_rmse █▅▃▄▃▁▁▄▄▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02911
wandb:     best_val_mse 0.02887
wandb:      best_val_r2 -0.06926
wandb:    best_val_rmse 0.16991
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.07651
wandb:    final_test_r2 -0.46002
wandb:  final_test_rmse 0.2766
wandb:  final_train_mse 0.02697
wandb:   final_train_r2 -0.03577
wandb: final_train_rmse 0.16423
wandb:    final_val_mse 0.02887
wandb:     final_val_r2 -0.06926
wandb:   final_val_rmse 0.16991
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04503
wandb:       train_time 28.00074
wandb:         val_loss 0.03411
wandb:          val_mse 0.03387
wandb:           val_r2 -0.25436
wandb:         val_rmse 0.18403
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210557-xzqwamzy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210557-xzqwamzy/logs
Experiment probe_layer2_lexical_density_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:07:14,571][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ja
experiment_name: probe_layer2_lexical_density_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:07:14,571][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:07:14,571][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:07:14,571][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:07:14,571][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:07:14,575][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:07:14,575][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:07:14,575][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:07:17,445][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:07:19,749][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:07:19,750][src.data.datasets][INFO] - Loading 'control_lexical_density_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:07:19,934][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 21:07:20,006][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:48:37 2025).
[2025-05-07 21:07:20,277][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:07:20,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:07:20,292][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:07:20,295][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:07:20,336][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:07:20,405][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:07:20,420][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:07:20,421][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:07:20,421][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:07:20,423][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:07:20,475][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:07:20,527][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:07:20,558][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:07:20,560][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:07:20,560][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:07:20,562][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:07:20,562][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:07:20,562][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:07:20,562][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:07:20,562][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:07:20,562][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:07:20,563][src.data.datasets][INFO] -   Mean: 0.4139, Std: 0.1614
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Sample label: 0.22100000083446503
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:07:20,563][src.data.datasets][INFO] -   Min: 0.0670, Max: 0.7920
[2025-05-07 21:07:20,563][src.data.datasets][INFO] -   Mean: 0.4093, Std: 0.1643
[2025-05-07 21:07:20,563][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:07:20,564][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:07:20,564][src.data.datasets][INFO] -   Mean: 0.5256, Std: 0.2289
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:07:20,564][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:07:20,565][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:07:20,565][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 21:07:20,565][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:07:26,444][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:07:26,445][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:07:26,445][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:07:26,445][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:07:26,448][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:07:26,449][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:07:26,449][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:07:26,449][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:07:26,449][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:07:26,450][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:07:26,450][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4267Epoch 1/15: [                              ] 2/75 batches, loss: 0.4543Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4621Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4792Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4582Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4063Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4024Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4224Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4067Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3859Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3780Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3928Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3768Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3945Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3879Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3992Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3967Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4028Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3949Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3967Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3889Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3929Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3815Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3738Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3685Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3649Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3606Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3549Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3516Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3477Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3412Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3376Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3387Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3401Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3366Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3410Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3380Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3343Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3331Epoch 1/15: [================              ] 40/75 batches, loss: 0.3279Epoch 1/15: [================              ] 41/75 batches, loss: 0.3242Epoch 1/15: [================              ] 42/75 batches, loss: 0.3248Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3238Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3257Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3251Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3203Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3173Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3138Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3104Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3074Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3076Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3117Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3076Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3086Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3090Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3050Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3015Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3010Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2988Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2973Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2951Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2934Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2936Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2931Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2925Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2897Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2877Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2849Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2831Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2806Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2793Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2803Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2775Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2759Epoch 1/15: [==============================] 75/75 batches, loss: 0.2735
[2025-05-07 21:07:32,129][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2735
[2025-05-07 21:07:32,324][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0398, Metrics: {'mse': 0.03931741416454315, 'rmse': 0.19828619257160382, 'r2': -0.45625150203704834}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2235Epoch 2/15: [                              ] 2/75 batches, loss: 0.2130Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1958Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1901Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1748Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1611Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1682Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1689Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1751Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1796Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1709Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1735Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1675Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1624Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1666Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1666Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1620Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1618Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1589Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1564Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1538Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1534Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1520Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1495Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1460Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1508Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1524Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1525Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1543Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1535Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1538Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1552Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1528Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1525Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1539Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1550Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1557Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1551Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1537Epoch 2/15: [================              ] 40/75 batches, loss: 0.1539Epoch 2/15: [================              ] 41/75 batches, loss: 0.1529Epoch 2/15: [================              ] 42/75 batches, loss: 0.1544Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1535Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1553Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1538Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1525Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1516Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1503Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1510Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1500Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1494Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1484Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1477Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1477Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1469Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1475Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1463Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1456Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1440Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1442Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1439Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1449Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1443Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1433Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1426Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1416Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1425Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1420Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1418Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1411Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1411Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1411Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1397Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1390Epoch 2/15: [==============================] 75/75 batches, loss: 0.1392
[2025-05-07 21:07:35,010][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1392
[2025-05-07 21:07:35,220][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0349, Metrics: {'mse': 0.03474823385477066, 'rmse': 0.18640878159242033, 'r2': -0.28701674938201904}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1507Epoch 3/15: [                              ] 2/75 batches, loss: 0.1332Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1066Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1020Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1074Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1067Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1021Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1112Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1133Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1126Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1128Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1104Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1122Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1085Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1086Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1072Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1047Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1025Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1015Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1028Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1075Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1066Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1089Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1067Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1052Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1058Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1084Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1073Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1048Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1035Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1040Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1086Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1069Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1068Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1050Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1051Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1060Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1060Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1047Epoch 3/15: [================              ] 40/75 batches, loss: 0.1033Epoch 3/15: [================              ] 41/75 batches, loss: 0.1038Epoch 3/15: [================              ] 42/75 batches, loss: 0.1062Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1054Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1063Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1062Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1058Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1050Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1044Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1044Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1042Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1048Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1041Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1041Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1033Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1030Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1027Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1020Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1027Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1018Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1010Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1007Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1004Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1001Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0996Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0997Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0994Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0990Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0991Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0994Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0991Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0992Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0992Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0997Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0995Epoch 3/15: [==============================] 75/75 batches, loss: 0.0986
[2025-05-07 21:07:37,927][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0986
[2025-05-07 21:07:38,145][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0304, Metrics: {'mse': 0.03015800751745701, 'rmse': 0.17366061014938594, 'r2': -0.11700236797332764}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1070Epoch 4/15: [                              ] 2/75 batches, loss: 0.0993Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1021Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1077Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0923Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0949Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0946Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0962Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0970Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0918Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0995Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0953Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0924Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0918Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0879Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0899Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0900Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0892Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0906Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0911Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0883Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0899Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0911Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0919Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0899Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0889Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0882Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0874Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0866Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0870Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0873Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0870Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0873Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0866Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0866Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0853Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0855Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0854Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0856Epoch 4/15: [================              ] 40/75 batches, loss: 0.0867Epoch 4/15: [================              ] 41/75 batches, loss: 0.0864Epoch 4/15: [================              ] 42/75 batches, loss: 0.0865Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0873Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0865Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0855Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0866Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0854Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0857Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0854Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0843Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0845Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0840Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0845Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0838Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0830Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0831Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0828Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0836Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0832Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0831Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0825Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0823Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0823Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0822Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0824Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0820Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0831Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0826Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0827Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0824Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0821Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0820Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0820Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0821Epoch 4/15: [==============================] 75/75 batches, loss: 0.0816
[2025-05-07 21:07:40,797][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0816
[2025-05-07 21:07:41,018][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0314, Metrics: {'mse': 0.031140010803937912, 'rmse': 0.17646532465030604, 'r2': -0.15337419509887695}
[2025-05-07 21:07:41,018][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0566Epoch 5/15: [                              ] 2/75 batches, loss: 0.0589Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0601Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0652Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0667Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0726Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0732Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0738Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0707Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0687Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0814Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0798Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0797Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0782Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0765Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0749Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0759Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0748Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0738Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0712Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0711Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0733Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0738Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0734Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0743Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0739Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0752Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0748Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0763Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0754Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0754Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0742Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0745Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0748Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0743Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0746Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0748Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0738Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0730Epoch 5/15: [================              ] 40/75 batches, loss: 0.0722Epoch 5/15: [================              ] 41/75 batches, loss: 0.0718Epoch 5/15: [================              ] 42/75 batches, loss: 0.0718Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0715Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0713Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0709Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0715Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0712Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0710Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0715Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0714Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0709Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0707Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0706Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0704Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0701Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0709Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0707Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0710Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0719Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0719Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0721Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0719Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0719Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0718Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0724Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0719Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0717Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0716Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0714Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0713Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0710Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0710Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0707Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0705Epoch 5/15: [==============================] 75/75 batches, loss: 0.0719
[2025-05-07 21:07:43,305][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0719
[2025-05-07 21:07:43,521][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0386, Metrics: {'mse': 0.03834857791662216, 'rmse': 0.19582792935794976, 'r2': -0.4203674793243408}
[2025-05-07 21:07:43,522][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0244Epoch 6/15: [                              ] 2/75 batches, loss: 0.0550Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0464Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0486Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0646Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0701Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0687Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0688Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0677Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0656Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0648Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0650Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0622Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0627Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0609Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0604Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0617Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0604Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0592Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0584Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0600Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0599Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0597Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0606Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0621Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0628Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0619Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0652Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0649Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0650Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0642Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0639Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0644Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0649Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0642Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0648Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0639Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0645Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0638Epoch 6/15: [================              ] 40/75 batches, loss: 0.0639Epoch 6/15: [================              ] 41/75 batches, loss: 0.0637Epoch 6/15: [================              ] 42/75 batches, loss: 0.0627Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0622Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0622Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0620Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0619Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0623Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0619Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0621Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0624Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0615Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0613Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0608Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0603Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0606Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0602Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0601Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0600Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0596Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0600Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0601Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0603Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0600Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0598Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0599Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0600Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0600Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0595Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0594Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0596Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0596Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0596Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0592Epoch 6/15: [==============================] 75/75 batches, loss: 0.0589
[2025-05-07 21:07:45,812][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0589
[2025-05-07 21:07:46,017][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0293, Metrics: {'mse': 0.029083270579576492, 'rmse': 0.1705381792431727, 'r2': -0.07719588279724121}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0340Epoch 7/15: [                              ] 2/75 batches, loss: 0.0614Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0646Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0626Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0580Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0543Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0536Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0587Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0576Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0570Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0546Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0531Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0556Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0550Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0526Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0514Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0501Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0520Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0518Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0520Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0515Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0526Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0536Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0527Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0534Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0532Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0549Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0548Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0554Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0549Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0543Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0544Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0561Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0563Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0560Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0561Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0558Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0560Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0571Epoch 7/15: [================              ] 40/75 batches, loss: 0.0566Epoch 7/15: [================              ] 41/75 batches, loss: 0.0563Epoch 7/15: [================              ] 42/75 batches, loss: 0.0560Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0556Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0552Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0548Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0545Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0543Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0539Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0539Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0538Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0538Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0536Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0533Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0535Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0533Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0532Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0532Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0533Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0530Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0536Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0532Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0527Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0528Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0527Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0525Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0529Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0530Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0527Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0525Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0525Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0523Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0521Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0521Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0521Epoch 7/15: [==============================] 75/75 batches, loss: 0.0519
[2025-05-07 21:07:48,731][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0519
[2025-05-07 21:07:48,956][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0307, Metrics: {'mse': 0.03040662594139576, 'rmse': 0.1743749578964702, 'r2': -0.12621080875396729}
[2025-05-07 21:07:48,957][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0505Epoch 8/15: [                              ] 2/75 batches, loss: 0.0494Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0464Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0436Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0416Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0448Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0431Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0442Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0409Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0385Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0431Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0443Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0435Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0438Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0432Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0426Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0421Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0417Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0428Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0428Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0430Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0441Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0440Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0450Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0451Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0443Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0456Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0451Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0447Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0450Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0446Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0448Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0443Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0457Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0461Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0463Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0459Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0460Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0458Epoch 8/15: [================              ] 40/75 batches, loss: 0.0451Epoch 8/15: [================              ] 41/75 batches, loss: 0.0451Epoch 8/15: [================              ] 42/75 batches, loss: 0.0456Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0459Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0463Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0464Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0467Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0465Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0465Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0470Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0469Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0470Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0471Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0472Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0470Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0466Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0471Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0476Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0475Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0475Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0474Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0475Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0473Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0476Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0477Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0480Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0478Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0480Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0480Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0478Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0479Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0481Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0488Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0488Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0489Epoch 8/15: [==============================] 75/75 batches, loss: 0.0491
[2025-05-07 21:07:51,280][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0491
[2025-05-07 21:07:51,489][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0321, Metrics: {'mse': 0.03181688115000725, 'rmse': 0.17837287111555739, 'r2': -0.1784442663192749}
[2025-05-07 21:07:51,490][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0419Epoch 9/15: [                              ] 2/75 batches, loss: 0.0412Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0394Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0411Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0389Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0474Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0472Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0442Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0436Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0456Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0438Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0439Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0449Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0471Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0469Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0483Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0486Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0494Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0482Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0484Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0494Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0493Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0492Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0496Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0497Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0491Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0489Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0501Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0507Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0503Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0508Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0504Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0508Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0508Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0508Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0508Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0523Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0524Epoch 9/15: [================              ] 40/75 batches, loss: 0.0518Epoch 9/15: [================              ] 41/75 batches, loss: 0.0511Epoch 9/15: [================              ] 42/75 batches, loss: 0.0503Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0501Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0499Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0495Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0493Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0495Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0499Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0493Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0496Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0492Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0491Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0486Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0497Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0496Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0490Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0487Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0484Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0483Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0485Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0480Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0479Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0477Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0475Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0473Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0474Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0476Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0472Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0470Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0466Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0466Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0467Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0465Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0465Epoch 9/15: [==============================] 75/75 batches, loss: 0.0461
[2025-05-07 21:07:53,771][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0461
[2025-05-07 21:07:53,989][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0329, Metrics: {'mse': 0.03267456218600273, 'rmse': 0.18076106379970974, 'r2': -0.2102113962173462}
[2025-05-07 21:07:53,990][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0441Epoch 10/15: [                              ] 2/75 batches, loss: 0.0393Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0478Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0433Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0411Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0378Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0413Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0406Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0433Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0445Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0457Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0489Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0482Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0476Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0460Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0449Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0450Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0454Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0455Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0466Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0466Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0470Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0470Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0465Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0467Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0471Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0469Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0467Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0464Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0463Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0470Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0466Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0466Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0472Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0475Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0470Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0476Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0477Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0470Epoch 10/15: [================              ] 40/75 batches, loss: 0.0470Epoch 10/15: [================              ] 41/75 batches, loss: 0.0468Epoch 10/15: [================              ] 42/75 batches, loss: 0.0466Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0464Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0463Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0462Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0459Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0465Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0464Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0459Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0459Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0459Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0458Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0454Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0452Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0453Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0459Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0455Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0457Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0456Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0453Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0451Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0451Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0454Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0450Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0450Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0449Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0449Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0446Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0450Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0451Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0451Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0450Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0449Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0446Epoch 10/15: [==============================] 75/75 batches, loss: 0.0444
[2025-05-07 21:07:56,283][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0444
[2025-05-07 21:07:56,506][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0346, Metrics: {'mse': 0.03435588255524635, 'rmse': 0.18535339909277723, 'r2': -0.27248477935791016}
[2025-05-07 21:07:56,507][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:07:56,507][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:07:56,507][src.training.lm_trainer][INFO] - Training completed in 27.53 seconds
[2025-05-07 21:07:56,507][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:07:59,395][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026878222823143005, 'rmse': 0.16394579233131604, 'r2': -0.03219246864318848}
[2025-05-07 21:07:59,396][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.029083270579576492, 'rmse': 0.1705381792431727, 'r2': -0.07719588279724121}
[2025-05-07 21:07:59,396][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07520636171102524, 'rmse': 0.2742377831572908, 'r2': -0.435225248336792}
[2025-05-07 21:08:01,084][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ja/ja/model.pt
[2025-05-07 21:08:01,085][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▁
wandb:     best_val_mse █▅▂▁
wandb:      best_val_r2 ▁▄▇█
wandb:    best_val_rmse █▅▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▄▁▄▄▃▃
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▂▇▁▂▃▃▅
wandb:          val_mse █▅▂▂▇▁▂▃▃▅
wandb:           val_r2 ▁▄▇▇▂█▇▆▆▄
wandb:         val_rmse █▅▂▂▇▁▂▃▄▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0293
wandb:     best_val_mse 0.02908
wandb:      best_val_r2 -0.0772
wandb:    best_val_rmse 0.17054
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.07521
wandb:    final_test_r2 -0.43523
wandb:  final_test_rmse 0.27424
wandb:  final_train_mse 0.02688
wandb:   final_train_r2 -0.03219
wandb: final_train_rmse 0.16395
wandb:    final_val_mse 0.02908
wandb:     final_val_r2 -0.0772
wandb:   final_val_rmse 0.17054
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04441
wandb:       train_time 27.5302
wandb:         val_loss 0.03463
wandb:          val_mse 0.03436
wandb:           val_r2 -0.27248
wandb:         val_rmse 0.18535
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210714-et8sj6qa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210714-et8sj6qa/logs
Experiment probe_layer2_lexical_density_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_lexical_density_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_lexical_density_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=lexical_density"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:08:24,054][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ja
experiment_name: probe_layer2_lexical_density_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:08:24,054][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:08:24,054][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:08:24,054][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:08:24,054][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:08:24,059][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:08:24,059][__main__][INFO] - Using submetric: lexical_density
[2025-05-07 21:08:24,059][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:08:28,306][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:08:30,664][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:08:30,664][src.data.datasets][INFO] - Loading 'control_lexical_density_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:08:30,829][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 21:08:30,914][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_lexical_density_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_lexical_density_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:49:28 2025).
[2025-05-07 21:08:31,147][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:08:31,156][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:08:31,156][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:08:31,158][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:08:31,193][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:08:31,229][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:08:31,247][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:08:31,248][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:08:31,249][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:08:31,250][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:08:31,330][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:08:31,415][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:08:31,439][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:08:31,440][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:08:31,440][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:08:31,443][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:08:31,444][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:08:31,444][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:08:31,444][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:08:31,444][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:08:31,444][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:08:31,444][src.data.datasets][INFO] -   Mean: 0.4139, Std: 0.1614
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Sample label: 0.5860000252723694
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:08:31,445][src.data.datasets][INFO] -   Min: 0.0670, Max: 0.7920
[2025-05-07 21:08:31,445][src.data.datasets][INFO] -   Mean: 0.4093, Std: 0.1643
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:08:31,445][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-05-07 21:08:31,446][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:08:31,446][src.data.datasets][INFO] -   Mean: 0.5256, Std: 0.2289
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:08:31,446][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:08:31,447][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:08:31,447][__main__][INFO] - Using model type: lm_probe for submetric lexical_density
[2025-05-07 21:08:31,447][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:08:38,982][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:08:38,983][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:08:38,984][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:08:38,984][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:08:38,987][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:08:38,987][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:08:38,987][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:08:38,987][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:08:38,987][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:08:38,988][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:08:38,988][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4142Epoch 1/15: [                              ] 2/75 batches, loss: 0.4656Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4390Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4766Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4807Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4397Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4387Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4434Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4212Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4021Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3903Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4091Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3981Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4085Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4061Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4148Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4051Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4124Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4032Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4018Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3935Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3924Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3824Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3732Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3700Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3655Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3621Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3554Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3511Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3470Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3432Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3374Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3353Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3361Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3335Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3347Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3302Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3252Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3246Epoch 1/15: [================              ] 40/75 batches, loss: 0.3200Epoch 1/15: [================              ] 41/75 batches, loss: 0.3164Epoch 1/15: [================              ] 42/75 batches, loss: 0.3167Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3153Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3190Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3181Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3140Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3114Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3093Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3064Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3038Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3045Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3091Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3057Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3062Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3055Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3024Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2990Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2977Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2953Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2942Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2929Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2920Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2921Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2927Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2931Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2909Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2887Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2859Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2835Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2811Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2807Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2820Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2797Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2788Epoch 1/15: [==============================] 75/75 batches, loss: 0.2770
[2025-05-07 21:08:44,781][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2770
[2025-05-07 21:08:44,970][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0402, Metrics: {'mse': 0.039800893515348434, 'rmse': 0.1995016128139029, 'r2': -0.4741588830947876}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1171Epoch 2/15: [                              ] 2/75 batches, loss: 0.1280Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1408Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1640Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1699Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1583Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1816Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1682Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1717Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1734Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1671Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1684Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1712Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1665Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1701Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1681Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1653Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1656Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1624Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1584Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1556Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1554Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1521Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1510Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1471Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1528Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1523Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1533Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1559Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1544Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1535Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1550Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1545Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1513Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1525Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1513Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1531Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1530Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1531Epoch 2/15: [================              ] 40/75 batches, loss: 0.1538Epoch 2/15: [================              ] 41/75 batches, loss: 0.1528Epoch 2/15: [================              ] 42/75 batches, loss: 0.1535Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1527Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1549Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1533Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1519Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1501Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1496Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1483Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1474Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1478Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1473Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1464Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1470Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1460Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1462Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1450Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1439Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1434Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1427Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1428Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1434Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1424Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1420Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1408Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1402Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1406Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1397Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1398Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1403Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1398Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1390Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1379Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1373Epoch 2/15: [==============================] 75/75 batches, loss: 0.1380
[2025-05-07 21:08:47,648][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1380
[2025-05-07 21:08:47,865][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0326, Metrics: {'mse': 0.03238843008875847, 'rmse': 0.17996785848800465, 'r2': -0.1996135711669922}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0927Epoch 3/15: [                              ] 2/75 batches, loss: 0.1075Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1047Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0984Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1128Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1071Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1076Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1170Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1154Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1124Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1206Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1174Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1143Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1100Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1082Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1081Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1054Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1024Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1015Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1037Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1054Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1027Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1016Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1001Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0977Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0977Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1008Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1000Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1005Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0991Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0984Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1018Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1016Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1026Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1020Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1018Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1020Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1009Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0998Epoch 3/15: [================              ] 40/75 batches, loss: 0.0987Epoch 3/15: [================              ] 41/75 batches, loss: 0.0986Epoch 3/15: [================              ] 42/75 batches, loss: 0.0989Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0992Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0996Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1003Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1008Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1009Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0998Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0995Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0997Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0993Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0992Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0985Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0979Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0977Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0979Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0976Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0981Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0979Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0967Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0964Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0953Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0950Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0947Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0956Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0954Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0946Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0948Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0947Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0946Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0942Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0944Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0944Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0946Epoch 3/15: [==============================] 75/75 batches, loss: 0.0946
[2025-05-07 21:08:50,579][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0946
[2025-05-07 21:08:50,808][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0313, Metrics: {'mse': 0.031154751777648926, 'rmse': 0.1765070870465232, 'r2': -0.15392017364501953}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0923Epoch 4/15: [                              ] 2/75 batches, loss: 0.0855Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0982Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1146Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0974Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0964Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0970Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0961Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0901Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0866Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0931Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0920Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0919Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0899Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0891Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0907Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0899Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0906Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0932Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0913Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0896Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0899Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0902Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0915Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0918Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0899Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0914Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0906Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0895Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0891Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0895Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0895Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0898Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0907Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0904Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0904Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0898Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0897Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0896Epoch 4/15: [================              ] 40/75 batches, loss: 0.0900Epoch 4/15: [================              ] 41/75 batches, loss: 0.0895Epoch 4/15: [================              ] 42/75 batches, loss: 0.0891Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0886Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0879Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0871Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0863Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0859Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0870Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0862Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0858Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0852Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0848Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0851Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0847Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0837Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0839Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0833Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0838Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0843Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0838Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0836Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0833Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0832Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0833Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0828Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0830Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0839Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0835Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0836Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0837Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0837Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0836Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0833Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0836Epoch 4/15: [==============================] 75/75 batches, loss: 0.0831
[2025-05-07 21:08:53,466][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0831
[2025-05-07 21:08:53,688][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0354, Metrics: {'mse': 0.035196635872125626, 'rmse': 0.1876076647478072, 'r2': -0.3036247491836548}
[2025-05-07 21:08:53,689][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0471Epoch 5/15: [                              ] 2/75 batches, loss: 0.0707Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0729Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0769Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0718Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0690Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0682Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0660Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0718Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0701Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0762Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0733Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0735Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0717Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0697Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0703Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0708Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0700Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0682Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0668Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0680Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0676Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0682Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0701Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0696Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0697Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0690Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0700Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0688Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0711Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0710Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0701Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0698Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0692Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0687Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0685Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0684Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0689Epoch 5/15: [================              ] 40/75 batches, loss: 0.0687Epoch 5/15: [================              ] 41/75 batches, loss: 0.0689Epoch 5/15: [================              ] 42/75 batches, loss: 0.0692Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0697Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0693Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0697Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0701Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0696Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0690Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0689Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0687Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0700Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0701Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0698Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0697Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0694Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0695Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0690Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0691Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0703Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0701Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0707Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0710Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0709Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0706Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0716Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0720Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0720Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0732Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0735Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0734Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0727Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0726Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0723Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0723Epoch 5/15: [==============================] 75/75 batches, loss: 0.0730
[2025-05-07 21:08:55,976][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0730
[2025-05-07 21:08:56,191][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0300, Metrics: {'mse': 0.029706649482250214, 'rmse': 0.17235617042116658, 'r2': -0.10028481483459473}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0417Epoch 6/15: [                              ] 2/75 batches, loss: 0.0565Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0582Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0629Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0629Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0682Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0697Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0671Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0676Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0699Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0669Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0709Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0691Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0684Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0669Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0671Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0680Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0666Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0656Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0663Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0667Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0642Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0647Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0645Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0637Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0639Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0646Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0645Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0649Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0647Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0639Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0633Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0628Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0631Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0629Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0634Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0637Epoch 6/15: [================              ] 40/75 batches, loss: 0.0642Epoch 6/15: [================              ] 41/75 batches, loss: 0.0650Epoch 6/15: [================              ] 42/75 batches, loss: 0.0646Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0638Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0642Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0644Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0637Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0640Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0655Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0649Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0639Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0643Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0637Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0633Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0628Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0625Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0621Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0627Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0622Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0624Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0622Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0624Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0621Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0621Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0622Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0618Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0618Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0615Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0611Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0609Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0607Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0608Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0606Epoch 6/15: [==============================] 75/75 batches, loss: 0.0607
[2025-05-07 21:08:58,851][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0607
[2025-05-07 21:08:59,067][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0303, Metrics: {'mse': 0.03005192242562771, 'rmse': 0.17335490309082033, 'r2': -0.11307322978973389}
[2025-05-07 21:08:59,067][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0492Epoch 7/15: [                              ] 2/75 batches, loss: 0.0482Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0581Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0585Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0591Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0572Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0621Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0640Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0673Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0676Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0659Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0645Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0645Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0622Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0605Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0597Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0574Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0576Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0572Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0580Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0571Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0556Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0565Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0577Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0587Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0580Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0582Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0579Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0581Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0581Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0586Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0575Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0581Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0591Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0589Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0598Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0593Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0584Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0596Epoch 7/15: [================              ] 40/75 batches, loss: 0.0596Epoch 7/15: [================              ] 41/75 batches, loss: 0.0591Epoch 7/15: [================              ] 42/75 batches, loss: 0.0585Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0583Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0579Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0583Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0577Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0576Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0573Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0568Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0562Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0556Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0551Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0550Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0553Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0547Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0545Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0539Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0536Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0533Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0535Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0536Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0531Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0531Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0530Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0528Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0528Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0527Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0526Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0524Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0528Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0526Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0525Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0522Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0523Epoch 7/15: [==============================] 75/75 batches, loss: 0.0519
[2025-05-07 21:09:01,362][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0519
[2025-05-07 21:09:01,574][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0342, Metrics: {'mse': 0.03400320187211037, 'rmse': 0.1843995712362433, 'r2': -0.25942206382751465}
[2025-05-07 21:09:01,575][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0563Epoch 8/15: [                              ] 2/75 batches, loss: 0.0475Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0530Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0537Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0508Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0535Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0534Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0522Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0522Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0526Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0525Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0516Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0508Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0507Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0512Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0536Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0532Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0542Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0548Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0545Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0549Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0562Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0563Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0571Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0556Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0563Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0563Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0563Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0568Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0558Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0554Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0554Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0566Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0559Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0563Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0557Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0563Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0562Epoch 8/15: [================              ] 40/75 batches, loss: 0.0557Epoch 8/15: [================              ] 41/75 batches, loss: 0.0551Epoch 8/15: [================              ] 42/75 batches, loss: 0.0553Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0547Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0553Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0551Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0550Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0544Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0541Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0540Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0538Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0533Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0535Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0541Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0541Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0539Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0543Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0544Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0548Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0543Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0541Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0539Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0537Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0533Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0530Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0529Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0528Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0526Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0525Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0527Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0524Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0523Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0522Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0521Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0523Epoch 8/15: [==============================] 75/75 batches, loss: 0.0522
[2025-05-07 21:09:03,876][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0522
[2025-05-07 21:09:04,073][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0302, Metrics: {'mse': 0.0299073439091444, 'rmse': 0.17293739881571135, 'r2': -0.10771822929382324}
[2025-05-07 21:09:04,073][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0614Epoch 9/15: [                              ] 2/75 batches, loss: 0.0631Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0638Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0600Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0565Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0534Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0562Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0521Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0537Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0528Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0514Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0513Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0513Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0493Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0497Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0492Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0504Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0493Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0490Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0487Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0478Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0473Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0476Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0483Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0481Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0476Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0469Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0466Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0468Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0467Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0469Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0463Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0470Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0473Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0474Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0469Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0475Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0481Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0483Epoch 9/15: [================              ] 40/75 batches, loss: 0.0490Epoch 9/15: [================              ] 41/75 batches, loss: 0.0488Epoch 9/15: [================              ] 42/75 batches, loss: 0.0491Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0489Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0485Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0480Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0477Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0479Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0477Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0476Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0476Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0482Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0483Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0482Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0485Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0489Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0486Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0486Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0481Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0481Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0480Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0479Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0480Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0485Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0482Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0483Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0484Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0480Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0479Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0480Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0479Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0480Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0481Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0483Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0480Epoch 9/15: [==============================] 75/75 batches, loss: 0.0479
[2025-05-07 21:09:06,362][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0479
[2025-05-07 21:09:06,584][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0339, Metrics: {'mse': 0.033638134598731995, 'rmse': 0.18340701894620062, 'r2': -0.24590051174163818}
[2025-05-07 21:09:06,585][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:09:06,585][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 21:09:06,585][src.training.lm_trainer][INFO] - Training completed in 25.05 seconds
[2025-05-07 21:09:06,585][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:09:09,421][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026531238108873367, 'rmse': 0.16288412479082598, 'r2': -0.01886725425720215}
[2025-05-07 21:09:09,422][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.029706649482250214, 'rmse': 0.17235617042116658, 'r2': -0.10028481483459473}
[2025-05-07 21:09:09,422][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07962191104888916, 'rmse': 0.28217354774834785, 'r2': -0.5194907188415527}
[2025-05-07 21:09:11,244][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ja/ja/model.pt
[2025-05-07 21:09:11,246][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▁
wandb:     best_val_mse █▃▂▁
wandb:      best_val_r2 ▁▆▇█
wandb:    best_val_rmse █▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▃▄▄▃▄
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▅▁▁▄▁▄
wandb:          val_mse █▃▂▅▁▁▄▁▄
wandb:           val_r2 ▁▆▇▄██▅█▅
wandb:         val_rmse █▃▂▅▁▁▄▁▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02998
wandb:     best_val_mse 0.02971
wandb:      best_val_r2 -0.10028
wandb:    best_val_rmse 0.17236
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.07962
wandb:    final_test_r2 -0.51949
wandb:  final_test_rmse 0.28217
wandb:  final_train_mse 0.02653
wandb:   final_train_r2 -0.01887
wandb: final_train_rmse 0.16288
wandb:    final_val_mse 0.02971
wandb:     final_val_r2 -0.10028
wandb:   final_val_rmse 0.17236
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04785
wandb:       train_time 25.04857
wandb:         val_loss 0.03391
wandb:          val_mse 0.03364
wandb:           val_r2 -0.2459
wandb:         val_rmse 0.18341
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210824-51jw5n50
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210824-51jw5n50/logs
Experiment probe_layer2_lexical_density_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/lexical_density/control3/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:09:31,383][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ja
experiment_name: probe_layer2_n_tokens_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:09:31,383][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:09:31,383][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:09:31,383][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:09:31,383][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:09:31,387][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:09:31,387][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:09:31,388][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:09:33,883][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:09:36,351][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:09:36,351][src.data.datasets][INFO] - Loading 'control_n_tokens_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:09:36,530][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 21:09:36,592][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:50:23 2025).
[2025-05-07 21:09:36,800][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:09:36,808][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:09:36,809][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:09:36,815][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:09:36,863][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:09:36,941][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:09:36,969][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:09:36,970][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:09:36,970][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:09:36,971][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:09:37,010][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:09:37,068][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:09:37,097][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:09:37,098][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:09:37,098][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:09:37,100][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:09:37,101][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:09:37,101][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:09:37,101][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:09:37,102][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9500
[2025-05-07 21:09:37,102][src.data.datasets][INFO] -   Mean: 0.2931, Std: 0.1442
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Sample label: 0.3479999899864197
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:09:37,102][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:09:37,103][src.data.datasets][INFO] -   Min: 0.0500, Max: 0.5430
[2025-05-07 21:09:37,103][src.data.datasets][INFO] -   Mean: 0.2933, Std: 0.1414
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Sample label: 0.27900001406669617
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:09:37,103][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:09:37,103][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:09:37,104][src.data.datasets][INFO] -   Mean: 0.3277, Std: 0.2443
[2025-05-07 21:09:37,104][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:09:37,104][src.data.datasets][INFO] - Sample label: 0.17100000381469727
[2025-05-07 21:09:37,104][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:09:37,104][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:09:37,104][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:09:37,104][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 21:09:37,105][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:09:43,269][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:09:43,270][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:09:43,270][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:09:43,270][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:09:43,273][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:09:43,273][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:09:43,273][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:09:43,274][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:09:43,274][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:09:43,274][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:09:43,275][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5010Epoch 1/15: [                              ] 2/75 batches, loss: 0.5077Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4974Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4850Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4529Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4177Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4068Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4159Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3935Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3800Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3685Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3811Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3645Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3765Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3731Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3903Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3837Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3972Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3901Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3927Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3835Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3865Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3739Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3634Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3610Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3578Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3526Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3475Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3468Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3408Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3353Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3314Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3323Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3311Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3291Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3310Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3278Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3238Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3248Epoch 1/15: [================              ] 40/75 batches, loss: 0.3200Epoch 1/15: [================              ] 41/75 batches, loss: 0.3160Epoch 1/15: [================              ] 42/75 batches, loss: 0.3166Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3155Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3169Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3153Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3113Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3073Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3036Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3012Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2988Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2979Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3012Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2971Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2982Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2982Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2951Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2919Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2911Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2889Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2880Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2870Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2862Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2859Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2856Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2846Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2828Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2823Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2799Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2782Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2757Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2747Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2742Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2724Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2716Epoch 1/15: [==============================] 75/75 batches, loss: 0.2693
[2025-05-07 21:09:48,979][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2693
[2025-05-07 21:09:49,186][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0307, Metrics: {'mse': 0.030694616958498955, 'rmse': 0.17519879268562027, 'r2': -0.5362133979797363}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1497Epoch 2/15: [                              ] 2/75 batches, loss: 0.1888Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2025Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2182Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2021Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1853Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1933Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1861Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2013Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2019Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1921Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1917Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1902Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1845Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1847Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1837Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1808Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1804Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1774Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1725Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1699Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1696Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1661Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1638Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1594Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1653Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1652Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1654Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1659Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1642Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1627Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1643Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1632Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1618Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1608Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1587Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1595Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1575Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1556Epoch 2/15: [================              ] 40/75 batches, loss: 0.1546Epoch 2/15: [================              ] 41/75 batches, loss: 0.1536Epoch 2/15: [================              ] 42/75 batches, loss: 0.1542Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1530Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1546Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1544Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1527Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1517Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1503Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1500Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1490Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1488Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1482Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1476Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1475Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1462Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1472Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1457Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1447Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1436Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1427Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1433Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1438Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1433Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1420Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1405Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1393Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1416Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1415Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1407Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1408Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1404Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1401Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1391Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1385Epoch 2/15: [==============================] 75/75 batches, loss: 0.1389
[2025-05-07 21:09:51,857][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1389
[2025-05-07 21:09:52,048][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0277, Metrics: {'mse': 0.027467211708426476, 'rmse': 0.1657323496135455, 'r2': -0.37468719482421875}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1232Epoch 3/15: [                              ] 2/75 batches, loss: 0.1177Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1024Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0930Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0905Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0976Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0965Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1033Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1042Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1003Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1001Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0984Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0997Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0951Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0953Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0948Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0909Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0911Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0918Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0944Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0996Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0974Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1000Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0987Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0974Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0982Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1018Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1027Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1011Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1005Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1006Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1049Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1062Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1063Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1051Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1052Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1077Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1062Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1054Epoch 3/15: [================              ] 40/75 batches, loss: 0.1039Epoch 3/15: [================              ] 41/75 batches, loss: 0.1036Epoch 3/15: [================              ] 42/75 batches, loss: 0.1041Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1035Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1025Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1022Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1010Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1002Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0992Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0986Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0986Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0981Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0977Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0969Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0963Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0955Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0950Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0946Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0961Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0959Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0952Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0954Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0951Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0945Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0940Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0947Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0941Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0934Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0934Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0933Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0928Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0922Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0926Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0932Epoch 3/15: [==============================] 75/75 batches, loss: 0.0932
[2025-05-07 21:09:54,912][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0932
[2025-05-07 21:09:55,129][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0225, Metrics: {'mse': 0.022340111434459686, 'rmse': 0.1494660879077916, 'r2': -0.11808454990386963}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1023Epoch 4/15: [                              ] 2/75 batches, loss: 0.0886Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1015Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0987Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0864Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0825Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0868Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0851Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0836Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0802Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0854Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0849Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0836Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0814Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0784Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0792Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0788Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0809Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0806Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0785Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0782Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0776Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0795Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0790Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0776Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0786Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0776Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0775Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0774Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0775Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0775Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0768Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0757Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0759Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0751Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0750Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0748Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0750Epoch 4/15: [================              ] 40/75 batches, loss: 0.0752Epoch 4/15: [================              ] 41/75 batches, loss: 0.0749Epoch 4/15: [================              ] 42/75 batches, loss: 0.0752Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0766Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0760Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0755Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0763Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0758Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0760Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0751Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0746Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0742Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0740Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0746Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0741Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0734Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0746Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0741Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0749Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0745Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0741Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0740Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0738Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0740Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0739Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0738Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0735Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0738Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0741Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0744Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0745Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0742Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0741Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0737Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0746Epoch 4/15: [==============================] 75/75 batches, loss: 0.0744
[2025-05-07 21:09:57,781][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0744
[2025-05-07 21:09:58,053][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0222, Metrics: {'mse': 0.02198098786175251, 'rmse': 0.14825986598453578, 'r2': -0.10011112689971924}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0744Epoch 5/15: [                              ] 2/75 batches, loss: 0.0802Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0636Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0638Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0662Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0698Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0691Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0653Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0658Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0670Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0754Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0741Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0712Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0708Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0705Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0704Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0698Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0707Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0695Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0680Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0664Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0673Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0692Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0693Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0709Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0705Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0707Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0696Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0698Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0687Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0683Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0674Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0671Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0669Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0675Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0664Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0660Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0658Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0651Epoch 5/15: [================              ] 40/75 batches, loss: 0.0642Epoch 5/15: [================              ] 41/75 batches, loss: 0.0644Epoch 5/15: [================              ] 42/75 batches, loss: 0.0657Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0648Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0640Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0635Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0641Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0641Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0640Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0641Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0639Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0639Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0643Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0646Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0648Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0646Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0641Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0639Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0644Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0639Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0645Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0655Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0653Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0648Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0656Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0655Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0653Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0654Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0654Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0658Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0652Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0648Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0646Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0648Epoch 5/15: [==============================] 75/75 batches, loss: 0.0655
[2025-05-07 21:10:00,719][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0655
[2025-05-07 21:10:01,023][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0199, Metrics: {'mse': 0.019800737500190735, 'rmse': 0.140715093363117, 'r2': 0.009006798267364502}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0339Epoch 6/15: [                              ] 2/75 batches, loss: 0.0486Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0455Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0507Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0603Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0642Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0660Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0623Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0621Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0635Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0610Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0620Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0594Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0603Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0583Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0587Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0599Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0590Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0589Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0582Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0588Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0574Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0559Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0559Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0571Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0580Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0573Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0574Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0570Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0573Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0568Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0573Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0561Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0557Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0552Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0562Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0563Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0565Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0572Epoch 6/15: [================              ] 40/75 batches, loss: 0.0574Epoch 6/15: [================              ] 41/75 batches, loss: 0.0580Epoch 6/15: [================              ] 42/75 batches, loss: 0.0581Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0576Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0583Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0580Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0576Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0573Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0575Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0577Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0585Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0580Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0571Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0575Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0570Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0566Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0564Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0563Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0563Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0569Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0565Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0566Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0563Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0561Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0559Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0561Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0560Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0557Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0559Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0559Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0556Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0554Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0553Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0551Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0548Epoch 6/15: [==============================] 75/75 batches, loss: 0.0555
[2025-05-07 21:10:03,764][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0555
[2025-05-07 21:10:04,155][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0190, Metrics: {'mse': 0.01899595744907856, 'rmse': 0.13782582286740958, 'r2': 0.04928463697433472}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0353Epoch 7/15: [                              ] 2/75 batches, loss: 0.0379Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0358Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0365Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0416Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0436Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0435Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0476Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0526Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0562Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0548Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0528Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0539Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0535Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0522Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0512Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0505Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0512Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0517Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0524Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0515Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0501Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0504Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0505Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0511Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0515Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0522Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0517Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0527Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0538Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0536Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0531Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0530Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0534Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0531Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0533Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0532Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0531Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0533Epoch 7/15: [================              ] 40/75 batches, loss: 0.0533Epoch 7/15: [================              ] 41/75 batches, loss: 0.0528Epoch 7/15: [================              ] 42/75 batches, loss: 0.0524Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0522Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0521Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0519Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0513Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0511Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0508Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0505Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0508Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0505Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0502Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0499Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0496Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0496Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0500Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0502Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0500Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0502Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0504Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0501Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0501Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0500Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0496Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0499Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0497Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0497Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0495Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0492Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0492Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0493Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0491Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0491Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0490Epoch 7/15: [==============================] 75/75 batches, loss: 0.0489
[2025-05-07 21:10:06,980][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0489
[2025-05-07 21:10:07,208][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0194, Metrics: {'mse': 0.019272835925221443, 'rmse': 0.1388266398254364, 'r2': 0.0354272723197937}
[2025-05-07 21:10:07,209][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0457Epoch 8/15: [                              ] 2/75 batches, loss: 0.0515Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0522Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0511Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0507Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0488Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0480Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0475Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0467Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0482Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0476Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0490Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0481Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0511Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0496Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0491Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0489Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0494Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0490Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0488Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0497Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0507Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0503Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0500Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0493Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0501Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0497Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0498Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0496Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0491Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0490Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0489Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0499Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0495Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0489Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0487Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0484Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0483Epoch 8/15: [================              ] 40/75 batches, loss: 0.0480Epoch 8/15: [================              ] 41/75 batches, loss: 0.0481Epoch 8/15: [================              ] 42/75 batches, loss: 0.0481Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0478Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0478Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0477Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0475Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0469Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0473Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0470Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0473Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0468Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0463Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0460Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0460Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0456Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0455Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0455Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0455Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0453Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0453Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0454Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0453Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0453Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0452Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0449Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0452Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0451Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0449Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0447Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0445Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0448Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0449Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0454Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0455Epoch 8/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-07 21:10:09,532][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0452
[2025-05-07 21:10:09,772][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0211, Metrics: {'mse': 0.020891224965453148, 'rmse': 0.1445379706701777, 'r2': -0.0455702543258667}
[2025-05-07 21:10:09,773][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0357Epoch 9/15: [                              ] 2/75 batches, loss: 0.0473Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0381Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0378Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0476Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0429Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0405Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0390Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0389Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0400Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0433Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0442Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0457Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0464Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0471Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0462Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0465Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0460Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0462Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0464Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0459Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0454Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0450Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0453Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0444Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0437Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0432Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0434Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0440Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0440Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0434Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0439Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0444Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0440Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0439Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0431Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0432Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0439Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0441Epoch 9/15: [================              ] 40/75 batches, loss: 0.0440Epoch 9/15: [================              ] 41/75 batches, loss: 0.0437Epoch 9/15: [================              ] 42/75 batches, loss: 0.0439Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0441Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0438Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0433Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0435Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0432Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0434Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0430Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0432Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0428Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0427Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0428Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0431Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0429Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0426Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0423Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0421Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0421Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0419Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0418Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0414Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0417Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0418Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0423Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0424Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0421Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0420Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0423Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0421Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0420Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0422Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 9/15: [==============================] 75/75 batches, loss: 0.0419
[2025-05-07 21:10:12,060][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0419
[2025-05-07 21:10:12,279][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0214, Metrics: {'mse': 0.021189766004681587, 'rmse': 0.1455670498590996, 'r2': -0.06051170825958252}
[2025-05-07 21:10:12,280][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0538Epoch 10/15: [                              ] 2/75 batches, loss: 0.0470Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0487Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0422Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0429Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0409Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0409Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0385Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0378Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0353Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0382Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0392Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0407Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0414Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0418Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0413Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0419Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0419Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0415Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0423Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0420Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0418Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0420Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0418Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0415Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0408Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0409Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0405Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0413Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0408Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0408Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0411Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0409Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0409Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0407Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0410Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0407Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0406Epoch 10/15: [================              ] 40/75 batches, loss: 0.0408Epoch 10/15: [================              ] 41/75 batches, loss: 0.0408Epoch 10/15: [================              ] 42/75 batches, loss: 0.0411Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0413Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0418Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0420Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0419Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0415Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0417Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0415Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0415Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0413Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0416Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0413Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0414Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0414Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0416Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0419Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0417Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0414Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0410Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0408Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0412Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0413Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0411Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0416Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0415Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0416Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0416Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0419Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0418Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0415Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0414Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0423Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 10/15: [==============================] 75/75 batches, loss: 0.0418
[2025-05-07 21:10:14,603][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0418
[2025-05-07 21:10:14,843][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0218, Metrics: {'mse': 0.021585363894701004, 'rmse': 0.14691958308782735, 'r2': -0.08031070232391357}
[2025-05-07 21:10:14,844][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:10:14,844][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:10:14,844][src.training.lm_trainer][INFO] - Training completed in 28.86 seconds
[2025-05-07 21:10:14,844][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:10:17,757][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.020801730453968048, 'rmse': 0.1442280501635103, 'r2': -0.00098419189453125}
[2025-05-07 21:10:17,758][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.01899595744907856, 'rmse': 0.13782582286740958, 'r2': 0.04928463697433472}
[2025-05-07 21:10:17,758][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06470626592636108, 'rmse': 0.2543742634905526, 'r2': -0.08461201190948486}
[2025-05-07 21:10:19,756][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ja/ja/model.pt
[2025-05-07 21:10:19,757][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▃▂▁
wandb:     best_val_mse █▆▃▃▁▁
wandb:      best_val_r2 ▁▃▆▆██
wandb:    best_val_rmse █▆▃▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▄▅▅▅▅▅
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▃▂▁▁▂▂▃
wandb:          val_mse █▆▃▃▁▁▁▂▂▃
wandb:           val_r2 ▁▃▆▆███▇▇▆
wandb:         val_rmse █▆▃▃▂▁▁▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01901
wandb:     best_val_mse 0.019
wandb:      best_val_r2 0.04928
wandb:    best_val_rmse 0.13783
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.06471
wandb:    final_test_r2 -0.08461
wandb:  final_test_rmse 0.25437
wandb:  final_train_mse 0.0208
wandb:   final_train_r2 -0.00098
wandb: final_train_rmse 0.14423
wandb:    final_val_mse 0.019
wandb:     final_val_r2 0.04928
wandb:   final_val_rmse 0.13783
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0418
wandb:       train_time 28.86448
wandb:         val_loss 0.02176
wandb:          val_mse 0.02159
wandb:           val_r2 -0.08031
wandb:         val_rmse 0.14692
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210931-6afkwc7q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_210931-6afkwc7q/logs
Experiment probe_layer2_n_tokens_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:10:40,182][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ja
experiment_name: probe_layer2_n_tokens_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:10:40,183][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:10:40,183][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:10:40,183][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:10:40,183][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:10:40,187][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:10:40,187][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:10:40,187][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:10:42,511][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:10:44,747][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:10:44,747][src.data.datasets][INFO] - Loading 'control_n_tokens_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:10:44,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 21:10:44,934][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:51:35 2025).
[2025-05-07 21:10:45,117][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:10:45,126][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:10:45,126][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:10:45,128][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:10:45,160][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:10:45,216][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:10:45,233][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:10:45,234][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:10:45,234][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:10:45,236][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:10:45,292][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:10:45,364][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:10:45,389][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:10:45,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:10:45,390][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:10:45,391][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:10:45,392][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:10:45,392][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:10:45,392][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:10:45,392][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:10:45,392][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9500
[2025-05-07 21:10:45,393][src.data.datasets][INFO] -   Mean: 0.2931, Std: 0.1442
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Sample label: 0.3479999899864197
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:10:45,393][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:10:45,393][src.data.datasets][INFO] -   Min: 0.0500, Max: 0.5430
[2025-05-07 21:10:45,394][src.data.datasets][INFO] -   Mean: 0.2933, Std: 0.1414
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Sample label: 0.27900001406669617
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:10:45,394][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:10:45,394][src.data.datasets][INFO] -   Mean: 0.3277, Std: 0.2443
[2025-05-07 21:10:45,394][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:10:45,395][src.data.datasets][INFO] - Sample label: 0.17100000381469727
[2025-05-07 21:10:45,395][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:10:45,395][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:10:45,395][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:10:45,395][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 21:10:45,395][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:10:51,303][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:10:51,305][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:10:51,305][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:10:51,305][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:10:51,308][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:10:51,309][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:10:51,309][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:10:51,309][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:10:51,309][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:10:51,310][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:10:51,310][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4292Epoch 1/15: [                              ] 2/75 batches, loss: 0.3870Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4025Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4039Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3909Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3604Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3590Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3895Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3674Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3563Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3472Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3612Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3463Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3571Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3585Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3703Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3702Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3843Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3738Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3734Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3662Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3666Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3545Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3460Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3442Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3393Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3347Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3292Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3285Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3230Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3189Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3154Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3142Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3147Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3151Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3170Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3125Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3097Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3091Epoch 1/15: [================              ] 40/75 batches, loss: 0.3042Epoch 1/15: [================              ] 41/75 batches, loss: 0.3012Epoch 1/15: [================              ] 42/75 batches, loss: 0.3016Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3011Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3047Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3037Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3003Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2970Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2951Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2943Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2916Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2926Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2964Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2926Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2925Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2918Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2880Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2848Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2832Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2813Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2805Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2790Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2781Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2786Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2776Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2775Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2756Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2754Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2732Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2717Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2703Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2696Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2705Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2682Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2678Epoch 1/15: [==============================] 75/75 batches, loss: 0.2653
[2025-05-07 21:10:56,999][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2653
[2025-05-07 21:10:57,202][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0304, Metrics: {'mse': 0.030372804030776024, 'rmse': 0.174277950500848, 'r2': -0.5201071500778198}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1905Epoch 2/15: [                              ] 2/75 batches, loss: 0.1848Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1806Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2087Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1931Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1768Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1907Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1877Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1967Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1994Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1899Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1853Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1849Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1803Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1802Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1809Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1768Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1732Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1685Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1640Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1602Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1594Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1575Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1563Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1538Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1557Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1576Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1556Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1558Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1548Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1539Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1548Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1523Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1514Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1506Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1481Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1486Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1467Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1454Epoch 2/15: [================              ] 40/75 batches, loss: 0.1450Epoch 2/15: [================              ] 41/75 batches, loss: 0.1435Epoch 2/15: [================              ] 42/75 batches, loss: 0.1448Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1444Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1494Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1469Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1451Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1436Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1430Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1423Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1422Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1425Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1418Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1413Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1421Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1411Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1422Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1409Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1394Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1384Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1381Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1373Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1382Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1381Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1376Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1364Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1358Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1365Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1368Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1362Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1371Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1364Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1361Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1350Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1347Epoch 2/15: [==============================] 75/75 batches, loss: 0.1348
[2025-05-07 21:10:59,886][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1348
[2025-05-07 21:11:00,091][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0261, Metrics: {'mse': 0.025899337604641914, 'rmse': 0.16093271141890922, 'r2': -0.29621779918670654}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1195Epoch 3/15: [                              ] 2/75 batches, loss: 0.1062Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0960Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0952Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1091Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1101Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1070Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1102Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1101Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1051Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1062Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1073Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1079Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1039Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1023Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1011Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0973Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0989Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0976Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0989Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1030Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1019Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1016Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0997Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0971Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0965Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0994Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1014Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1003Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0986Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0992Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1041Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1041Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1037Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1034Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1028Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1034Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1032Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1017Epoch 3/15: [================              ] 40/75 batches, loss: 0.1004Epoch 3/15: [================              ] 41/75 batches, loss: 0.0999Epoch 3/15: [================              ] 42/75 batches, loss: 0.0995Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0991Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0994Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0992Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0990Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0987Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0978Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0971Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0970Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0967Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0973Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0968Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0958Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0951Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0948Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0946Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0956Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0951Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0939Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0934Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0927Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0920Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0919Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0920Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0913Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0907Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0903Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0908Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0908Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0902Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0905Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0904Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0904Epoch 3/15: [==============================] 75/75 batches, loss: 0.0903
[2025-05-07 21:11:02,845][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0903
[2025-05-07 21:11:03,067][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0251, Metrics: {'mse': 0.024968305602669716, 'rmse': 0.15801362473745648, 'r2': -0.24962127208709717}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0976Epoch 4/15: [                              ] 2/75 batches, loss: 0.0926Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1102Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1149Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0997Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0923Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0877Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0842Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0828Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0799Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0839Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0813Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0813Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0807Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0791Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0813Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0806Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0796Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0816Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0838Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0820Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0820Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0819Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0823Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0813Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0806Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0809Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0803Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0811Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0809Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0802Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0796Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0792Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0790Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0793Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0783Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0778Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0775Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0777Epoch 4/15: [================              ] 40/75 batches, loss: 0.0782Epoch 4/15: [================              ] 41/75 batches, loss: 0.0774Epoch 4/15: [================              ] 42/75 batches, loss: 0.0775Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0784Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0773Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0768Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0770Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0770Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0781Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0776Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0764Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0756Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0750Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0763Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0759Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0753Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0764Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0759Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0769Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0765Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0765Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0767Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0763Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0773Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0774Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0770Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0769Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0771Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0768Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0772Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0772Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0769Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0767Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0769Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0778Epoch 4/15: [==============================] 75/75 batches, loss: 0.0772
[2025-05-07 21:11:05,686][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0772
[2025-05-07 21:11:05,952][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0243, Metrics: {'mse': 0.024116454645991325, 'rmse': 0.1552947347658359, 'r2': -0.20698750019073486}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0804Epoch 5/15: [                              ] 2/75 batches, loss: 0.0762Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0748Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0733Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0740Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0735Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0762Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0741Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0769Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0730Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0745Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0737Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0776Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0746Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0734Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0715Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0699Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0702Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0702Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0689Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0678Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0700Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0699Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0698Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0703Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0686Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0691Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0684Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0685Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0674Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0681Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0673Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0666Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0659Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0657Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0654Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0650Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0651Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0652Epoch 5/15: [================              ] 40/75 batches, loss: 0.0648Epoch 5/15: [================              ] 41/75 batches, loss: 0.0643Epoch 5/15: [================              ] 42/75 batches, loss: 0.0651Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0647Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0635Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0645Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0631Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0634Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0632Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0633Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0636Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0636Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0631Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0627Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0635Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0632Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0639Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0645Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0643Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0638Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0641Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0642Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0646Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0643Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0641Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0646Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0643Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0640Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0636Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0634Epoch 5/15: [==============================] 75/75 batches, loss: 0.0643
[2025-05-07 21:11:08,660][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0643
[2025-05-07 21:11:08,881][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0261, Metrics: {'mse': 0.0258353091776371, 'rmse': 0.16073365913098941, 'r2': -0.293013334274292}
[2025-05-07 21:11:08,882][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0719Epoch 6/15: [                              ] 2/75 batches, loss: 0.0779Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0678Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0716Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0752Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0669Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0628Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0647Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0645Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0630Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0625Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0620Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0609Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0613Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0619Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0616Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0621Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0603Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0601Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0603Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0607Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0599Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0588Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0594Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0603Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0603Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0591Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0592Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0604Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0603Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0590Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0597Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0595Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0594Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0593Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0601Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0596Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0596Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0589Epoch 6/15: [================              ] 40/75 batches, loss: 0.0593Epoch 6/15: [================              ] 41/75 batches, loss: 0.0598Epoch 6/15: [================              ] 42/75 batches, loss: 0.0601Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0592Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0589Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0583Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0580Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0582Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0583Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0586Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0589Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0586Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0580Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0580Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0576Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0573Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0577Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0579Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0582Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0580Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0576Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0574Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0580Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0580Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0582Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0581Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0581Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0580Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0579Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0579Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0577Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0576Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0574Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0573Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0569Epoch 6/15: [==============================] 75/75 batches, loss: 0.0572
[2025-05-07 21:11:11,194][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0572
[2025-05-07 21:11:11,405][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0193, Metrics: {'mse': 0.019245576113462448, 'rmse': 0.13872842575861102, 'r2': 0.03679168224334717}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0522Epoch 7/15: [                              ] 2/75 batches, loss: 0.0526Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0509Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0484Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0555Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0525Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0533Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0556Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0552Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0601Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0574Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0562Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0570Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0554Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0559Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0581Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0572Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0574Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0573Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0574Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0574Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0567Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0563Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0571Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0570Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0562Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0558Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0550Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0555Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0556Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0564Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0560Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0555Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0573Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0570Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0562Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0556Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0562Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0568Epoch 7/15: [================              ] 40/75 batches, loss: 0.0568Epoch 7/15: [================              ] 41/75 batches, loss: 0.0564Epoch 7/15: [================              ] 42/75 batches, loss: 0.0557Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0554Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0554Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0554Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0552Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0557Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0558Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0553Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0549Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0546Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0540Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0543Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0537Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0533Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0532Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0533Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0534Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0538Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0540Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0535Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0533Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0531Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0526Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0525Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0525Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0522Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0520Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0518Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0523Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0520Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0518Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0517Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0518Epoch 7/15: [==============================] 75/75 batches, loss: 0.0518
[2025-05-07 21:11:14,167][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0518
[2025-05-07 21:11:14,394][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0214, Metrics: {'mse': 0.021171649917960167, 'rmse': 0.14550481063511325, 'r2': -0.05960512161254883}
[2025-05-07 21:11:14,395][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0460Epoch 8/15: [                              ] 2/75 batches, loss: 0.0579Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0498Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0457Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0406Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0429Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0427Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0432Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0439Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0451Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0445Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0437Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0437Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0450Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0468Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0457Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0463Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0471Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0474Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0479Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0475Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0483Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0483Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0477Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0477Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0476Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0477Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0482Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0482Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0486Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0485Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0491Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0484Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0491Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0487Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0482Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0484Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0480Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0482Epoch 8/15: [================              ] 40/75 batches, loss: 0.0478Epoch 8/15: [================              ] 41/75 batches, loss: 0.0471Epoch 8/15: [================              ] 42/75 batches, loss: 0.0473Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0474Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0481Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0485Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0481Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0480Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0479Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0479Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0479Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0476Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0476Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0471Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0473Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0470Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0467Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0466Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0463Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0462Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0460Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0460Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0455Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0452Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0449Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0446Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0446Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0448Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0445Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0449Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0452Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0448Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0447Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0445Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0445Epoch 8/15: [==============================] 75/75 batches, loss: 0.0446
[2025-05-07 21:11:16,692][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0446
[2025-05-07 21:11:16,940][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0268, Metrics: {'mse': 0.026539379730820656, 'rmse': 0.1629091149408794, 'r2': -0.3282508850097656}
[2025-05-07 21:11:16,941][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0289Epoch 9/15: [                              ] 2/75 batches, loss: 0.0465Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0465Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0454Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0458Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0427Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0415Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0415Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0411Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0414Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0413Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0402Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0420Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0433Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0439Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0433Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0418Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0409Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0407Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0416Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0415Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0415Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0419Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0420Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0421Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0414Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0405Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0401Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0411Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0412Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0409Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0424Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0426Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0416Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0413Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0412Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0411Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0424Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0419Epoch 9/15: [================              ] 40/75 batches, loss: 0.0425Epoch 9/15: [================              ] 41/75 batches, loss: 0.0422Epoch 9/15: [================              ] 42/75 batches, loss: 0.0423Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0425Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0426Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0423Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0424Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0424Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0420Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0424Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0424Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0424Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0420Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0418Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0418Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0413Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0415Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0415Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0414Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0416Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0418Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0419Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0417Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0416Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0420Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0420Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0421Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0418Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0418Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0420Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0417Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0420Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0418Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0419Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0417Epoch 9/15: [==============================] 75/75 batches, loss: 0.0422
[2025-05-07 21:11:19,236][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0422
[2025-05-07 21:11:19,460][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0220, Metrics: {'mse': 0.021818023175001144, 'rmse': 0.14770925216451794, 'r2': -0.09195494651794434}
[2025-05-07 21:11:19,461][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0233Epoch 10/15: [                              ] 2/75 batches, loss: 0.0279Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0292Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0381Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0342Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0321Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0308Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0319Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0346Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0340Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0373Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0365Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0366Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0367Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0365Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0371Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0381Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0379Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0383Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0380Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0371Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0391Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0398Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0403Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0408Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0409Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0408Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0406Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0400Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0393Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0398Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0397Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0403Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0399Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0396Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0398Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0394Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0394Epoch 10/15: [================              ] 40/75 batches, loss: 0.0396Epoch 10/15: [================              ] 41/75 batches, loss: 0.0397Epoch 10/15: [================              ] 42/75 batches, loss: 0.0394Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0395Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0392Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0389Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0388Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0388Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0394Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0394Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0393Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0392Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0389Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0390Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0392Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0391Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0394Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0399Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0400Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0399Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0398Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0401Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0402Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0404Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0410Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0414Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0413Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0412Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0413Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0416Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0418Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0417Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0418Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0414Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0417Epoch 10/15: [==============================] 75/75 batches, loss: 0.0415
[2025-05-07 21:11:21,779][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0415
[2025-05-07 21:11:21,991][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0238, Metrics: {'mse': 0.023598212748765945, 'rmse': 0.1536170978399408, 'r2': -0.18105041980743408}
[2025-05-07 21:11:21,992][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:11:21,992][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 21:11:21,992][src.training.lm_trainer][INFO] - Training completed in 28.12 seconds
[2025-05-07 21:11:21,993][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:11:24,996][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021617185324430466, 'rmse': 0.14702783860354632, 'r2': -0.04022407531738281}
[2025-05-07 21:11:24,997][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.019245576113462448, 'rmse': 0.13872842575861102, 'r2': 0.03679168224334717}
[2025-05-07 21:11:24,997][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06694036722183228, 'rmse': 0.25872836570780616, 'r2': -0.12206029891967773}
[2025-05-07 21:11:26,996][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ja/ja/model.pt
[2025-05-07 21:11:26,998][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▄▁
wandb:     best_val_mse █▅▅▄▁
wandb:      best_val_r2 ▁▄▄▅█
wandb:    best_val_rmse █▅▅▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▃▄▃▅▅▃▄
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▅▄▅▁▂▆▃▄
wandb:          val_mse █▅▅▄▅▁▂▆▃▄
wandb:           val_r2 ▁▄▄▅▄█▇▃▆▅
wandb:         val_rmse █▅▅▄▅▁▂▆▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01934
wandb:     best_val_mse 0.01925
wandb:      best_val_r2 0.03679
wandb:    best_val_rmse 0.13873
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.06694
wandb:    final_test_r2 -0.12206
wandb:  final_test_rmse 0.25873
wandb:  final_train_mse 0.02162
wandb:   final_train_r2 -0.04022
wandb: final_train_rmse 0.14703
wandb:    final_val_mse 0.01925
wandb:     final_val_r2 0.03679
wandb:   final_val_rmse 0.13873
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04153
wandb:       train_time 28.12226
wandb:         val_loss 0.02381
wandb:          val_mse 0.0236
wandb:           val_r2 -0.18105
wandb:         val_rmse 0.15362
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_211040-g2xyttu1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_211040-g2xyttu1/logs
Experiment probe_layer2_n_tokens_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_n_tokens_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_n_tokens_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=n_tokens"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 21:11:50,540][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ja
experiment_name: probe_layer2_n_tokens_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 21:11:50,540][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 21:11:50,540][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:11:50,540][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 21:11:50,540][__main__][INFO] - Determined Task Type: regression
[2025-05-07 21:11:50,545][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 21:11:50,545][__main__][INFO] - Using submetric: n_tokens
[2025-05-07 21:11:50,545][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 21:11:53,338][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 21:11:55,604][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 21:11:55,604][src.data.datasets][INFO] - Loading 'control_n_tokens_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:11:55,752][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 21:11:55,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_n_tokens_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_n_tokens_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr 12 15:52:51 2025).
[2025-05-07 21:11:56,154][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 21:11:56,162][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:11:56,163][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 21:11:56,164][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:11:56,243][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:11:56,334][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:11:56,401][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 21:11:56,403][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:11:56,403][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 21:11:56,404][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 21:11:56,522][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:11:56,644][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 21:11:56,673][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 21:11:56,674][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 21:11:56,675][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 21:11:56,676][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 21:11:56,676][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:11:56,677][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9500
[2025-05-07 21:11:56,677][src.data.datasets][INFO] -   Mean: 0.2931, Std: 0.1442
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 21:11:56,677][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:11:56,678][src.data.datasets][INFO] -   Min: 0.0500, Max: 0.5430
[2025-05-07 21:11:56,678][src.data.datasets][INFO] -   Mean: 0.2933, Std: 0.1414
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Sample label: 0.27900001406669617
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-05-07 21:11:56,678][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-05-07 21:11:56,679][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 21:11:56,679][src.data.datasets][INFO] -   Mean: 0.3277, Std: 0.2443
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Sample label: 0.17100000381469727
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 21:11:56,679][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 21:11:56,680][__main__][INFO] - Using model type: lm_probe for submetric n_tokens
[2025-05-07 21:11:56,680][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 21:12:02,865][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 21:12:02,866][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 21:12:02,866][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 21:12:02,866][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 21:12:02,869][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 21:12:02,869][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 21:12:02,870][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 21:12:02,870][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 21:12:02,870][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 21:12:02,871][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 21:12:02,871][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4771Epoch 1/15: [                              ] 2/75 batches, loss: 0.4825Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4587Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4538Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4339Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4042Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3952Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4069Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3908Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3694Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3615Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3687Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3552Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3647Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3654Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3774Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3757Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3923Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3851Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3832Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3792Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3803Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3677Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3592Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3559Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3519Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3498Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3458Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3426Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3371Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3313Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3306Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3306Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3305Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3286Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3326Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3296Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3253Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3239Epoch 1/15: [================              ] 40/75 batches, loss: 0.3189Epoch 1/15: [================              ] 41/75 batches, loss: 0.3145Epoch 1/15: [================              ] 42/75 batches, loss: 0.3150Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3159Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3189Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3173Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3128Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3094Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3074Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3060Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3034Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3051Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3102Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3065Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3078Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3081Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3037Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3002Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2985Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2968Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2956Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2941Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2924Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2925Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2927Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2922Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2897Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2884Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2854Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2838Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2812Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2804Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2817Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2797Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2794Epoch 1/15: [==============================] 75/75 batches, loss: 0.2767
[2025-05-07 21:12:08,857][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2767
[2025-05-07 21:12:09,041][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0309, Metrics: {'mse': 0.030832454562187195, 'rmse': 0.17559172691840352, 'r2': -0.5431119203567505}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1556Epoch 2/15: [                              ] 2/75 batches, loss: 0.1853Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1780Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1992Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1819Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1664Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1765Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1757Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1805Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1781Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1709Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1707Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1676Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1617Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1660Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1672Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1635Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1629Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1621Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1590Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1564Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1579Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1549Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1545Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1502Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1552Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1556Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1555Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1551Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1540Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1537Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1569Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1554Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1543Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1539Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1523Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1546Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1531Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1525Epoch 2/15: [================              ] 40/75 batches, loss: 0.1518Epoch 2/15: [================              ] 41/75 batches, loss: 0.1506Epoch 2/15: [================              ] 42/75 batches, loss: 0.1519Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1506Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1535Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1518Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1495Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1483Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1474Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1465Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1456Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1453Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1444Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1446Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1438Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1427Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1435Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1423Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1410Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1398Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1406Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1401Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1409Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1401Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1394Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1383Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1377Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1383Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1379Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1373Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1375Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1369Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1369Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1356Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1350Epoch 2/15: [==============================] 75/75 batches, loss: 0.1349
[2025-05-07 21:12:11,759][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1349
[2025-05-07 21:12:11,980][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0286, Metrics: {'mse': 0.028376325964927673, 'rmse': 0.16845274104308208, 'r2': -0.4201868772506714}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1497Epoch 3/15: [                              ] 2/75 batches, loss: 0.1304Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1170Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1118Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1101Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1166Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1105Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1122Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1138Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1099Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1123Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1082Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1084Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1039Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1061Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1049Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1004Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0984Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0976Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1001Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1059Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1053Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1061Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1037Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1012Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1006Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1038Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1038Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1031Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1011Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1014Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1069Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1078Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1079Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1079Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1072Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1078Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1059Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1058Epoch 3/15: [================              ] 40/75 batches, loss: 0.1052Epoch 3/15: [================              ] 41/75 batches, loss: 0.1049Epoch 3/15: [================              ] 42/75 batches, loss: 0.1050Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1042Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1042Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1046Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1038Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1030Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1024Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1022Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1014Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1022Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1022Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1017Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1011Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1001Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0998Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0996Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1002Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0999Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0985Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0980Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0977Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0975Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0967Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0971Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0961Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0953Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0956Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0959Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0955Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0947Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0951Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0957Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0952Epoch 3/15: [==============================] 75/75 batches, loss: 0.0945
[2025-05-07 21:12:14,727][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0945
[2025-05-07 21:12:14,947][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0248, Metrics: {'mse': 0.024642763659358025, 'rmse': 0.15698013778614806, 'r2': -0.23332834243774414}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0882Epoch 4/15: [                              ] 2/75 batches, loss: 0.0810Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0914Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0959Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0827Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0794Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0763Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0724Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0777Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0775Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0826Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0816Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0814Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0799Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0776Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0785Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0798Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0787Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0792Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0782Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0774Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0779Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0787Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0777Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0768Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0775Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0766Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0761Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0750Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0762Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0758Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0758Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0748Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0747Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0751Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0744Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0746Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0737Epoch 4/15: [================              ] 40/75 batches, loss: 0.0744Epoch 4/15: [================              ] 41/75 batches, loss: 0.0744Epoch 4/15: [================              ] 42/75 batches, loss: 0.0737Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0741Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0735Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0726Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0731Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0734Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0735Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0733Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0725Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0722Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0720Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0719Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0712Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0710Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0708Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0705Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0714Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0717Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0713Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0714Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0717Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0720Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0718Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0719Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0722Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0722Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0719Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0722Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0720Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0718Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0716Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0715Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0719Epoch 4/15: [==============================] 75/75 batches, loss: 0.0717
[2025-05-07 21:12:17,595][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0717
[2025-05-07 21:12:17,821][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0239, Metrics: {'mse': 0.023645175620913506, 'rmse': 0.15376987878291867, 'r2': -0.18340086936950684}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0348Epoch 5/15: [                              ] 2/75 batches, loss: 0.0453Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0635Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0629Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0609Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0702Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0730Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0684Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0743Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0729Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0763Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0765Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0746Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0727Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0716Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0715Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0701Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0714Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0697Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0686Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0673Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0696Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0697Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0695Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0698Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0701Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0709Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0693Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0688Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0703Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0712Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0703Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0696Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0701Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0698Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0694Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0690Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0690Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0686Epoch 5/15: [================              ] 40/75 batches, loss: 0.0683Epoch 5/15: [================              ] 41/75 batches, loss: 0.0687Epoch 5/15: [================              ] 42/75 batches, loss: 0.0701Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0696Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0685Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0683Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0687Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0685Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0688Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0687Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0680Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0675Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0679Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0678Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0676Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0673Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0678Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0673Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0673Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0681Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0678Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0687Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0694Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0688Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0683Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0689Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0689Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0687Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0685Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0683Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0686Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0690Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0688Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0683Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0685Epoch 5/15: [==============================] 75/75 batches, loss: 0.0685
[2025-05-07 21:12:20,464][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0685
[2025-05-07 21:12:20,730][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0217, Metrics: {'mse': 0.021487003192305565, 'rmse': 0.1465844575400324, 'r2': -0.07538807392120361}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0658Epoch 6/15: [                              ] 2/75 batches, loss: 0.0633Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0582Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0540Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0645Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0608Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0610Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0608Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0590Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0564Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0560Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0553Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0556Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0553Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0551Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0556Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0551Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0539Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0530Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0524Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0515Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0517Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0537Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0545Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0539Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0536Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0546Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0555Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0564Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0554Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0561Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0555Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0557Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0568Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0571Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0564Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0567Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0568Epoch 6/15: [================              ] 40/75 batches, loss: 0.0572Epoch 6/15: [================              ] 41/75 batches, loss: 0.0572Epoch 6/15: [================              ] 42/75 batches, loss: 0.0574Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0569Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0567Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0570Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0569Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0569Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0574Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0575Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0579Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0583Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0578Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0584Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0579Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0579Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0581Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0579Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0578Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0577Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0574Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0577Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0577Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0576Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0573Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0571Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0568Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0564Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0560Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0558Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0557Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0555Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0552Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0550Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0546Epoch 6/15: [==============================] 75/75 batches, loss: 0.0545
[2025-05-07 21:12:23,412][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0545
[2025-05-07 21:12:23,694][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0207, Metrics: {'mse': 0.020524106919765472, 'rmse': 0.1432623709135287, 'r2': -0.027196645736694336}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0261Epoch 7/15: [                              ] 2/75 batches, loss: 0.0388Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0417Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0382Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0404Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0422Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0447Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0482Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0466Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0481Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0475Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0469Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0478Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0482Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0482Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0491Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0483Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0492Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0487Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0483Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0477Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0471Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0465Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0461Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0468Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0474Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0465Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0471Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0478Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0482Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0475Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0481Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0496Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0489Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0494Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0494Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0494Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0499Epoch 7/15: [================              ] 40/75 batches, loss: 0.0495Epoch 7/15: [================              ] 41/75 batches, loss: 0.0489Epoch 7/15: [================              ] 42/75 batches, loss: 0.0488Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0482Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0478Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0478Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0477Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0486Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0485Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0485Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0492Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0489Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0484Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0493Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0491Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0497Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0496Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0494Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0496Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0496Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0495Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0491Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0487Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0487Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0485Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0486Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0488Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0492Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0491Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0489Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0495Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0495Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0492Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0490Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0490Epoch 7/15: [==============================] 75/75 batches, loss: 0.0489
[2025-05-07 21:12:26,443][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0489
[2025-05-07 21:12:26,671][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0204, Metrics: {'mse': 0.020221183076500893, 'rmse': 0.14220120631169375, 'r2': -0.012035846710205078}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0401Epoch 8/15: [                              ] 2/75 batches, loss: 0.0459Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0425Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0428Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0405Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0426Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0445Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0439Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0459Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0456Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0460Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0459Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0462Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0458Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0459Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0449Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0469Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0471Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0494Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0488Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0483Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0495Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0506Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0504Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0495Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0481Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0500Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0504Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0499Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0507Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0500Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0505Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0500Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0498Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0498Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0495Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0489Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0487Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0492Epoch 8/15: [================              ] 40/75 batches, loss: 0.0487Epoch 8/15: [================              ] 41/75 batches, loss: 0.0485Epoch 8/15: [================              ] 42/75 batches, loss: 0.0486Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0485Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0485Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0487Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0483Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0487Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0489Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0484Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0487Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0486Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0483Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0481Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0481Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0480Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0477Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0479Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0479Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0480Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0477Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0476Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0473Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0470Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0468Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0465Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0461Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0466Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0462Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0463Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0461Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0464Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0462Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0461Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0461Epoch 8/15: [==============================] 75/75 batches, loss: 0.0460
[2025-05-07 21:12:29,328][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0460
[2025-05-07 21:12:29,665][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0260, Metrics: {'mse': 0.0256811510771513, 'rmse': 0.16025339646057832, 'r2': -0.2852979898452759}
[2025-05-07 21:12:29,666][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0659Epoch 9/15: [                              ] 2/75 batches, loss: 0.0516Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0452Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0457Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0424Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0404Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0424Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0419Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0423Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0427Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0476Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0461Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0455Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0447Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0438Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0421Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0415Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0410Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0423Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0419Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0409Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0419Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0421Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0419Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0423Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0417Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0414Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0410Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0416Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0413Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0404Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0414Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0422Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0419Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0420Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0420Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0423Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0422Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0426Epoch 9/15: [================              ] 40/75 batches, loss: 0.0426Epoch 9/15: [================              ] 41/75 batches, loss: 0.0423Epoch 9/15: [================              ] 42/75 batches, loss: 0.0421Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0419Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0416Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0413Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0416Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0416Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0415Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0411Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0416Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0416Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0414Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0411Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0415Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0414Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0415Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0411Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0411Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0415Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0414Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0416Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0413Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0413Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0411Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0412Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0413Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0411Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0409Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0408Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0405Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0404Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0402Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0400Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0398Epoch 9/15: [==============================] 75/75 batches, loss: 0.0400
[2025-05-07 21:12:32,007][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0400
[2025-05-07 21:12:32,252][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0250, Metrics: {'mse': 0.024707965552806854, 'rmse': 0.1571876762116129, 'r2': -0.23659169673919678}
[2025-05-07 21:12:32,253][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0461Epoch 10/15: [                              ] 2/75 batches, loss: 0.0402Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0452Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0460Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0418Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0411Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0405Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0430Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0450Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0449Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0442Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0444Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0434Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0439Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0440Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0441Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0441Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0452Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0440Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0441Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0438Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0434Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0425Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0424Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0433Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0433Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0428Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0428Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0424Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0418Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0414Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0412Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0413Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0413Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0410Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0410Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0410Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0405Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0403Epoch 10/15: [================              ] 40/75 batches, loss: 0.0403Epoch 10/15: [================              ] 41/75 batches, loss: 0.0402Epoch 10/15: [================              ] 42/75 batches, loss: 0.0404Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0402Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0401Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0405Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0403Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0405Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0409Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0406Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0409Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0403Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0402Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0400Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0399Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0398Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0397Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0397Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0398Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0397Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0395Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0393Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0401Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0402Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0403Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0404Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0403Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0403Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0406Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0410Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0411Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0410Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0412Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0415Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0413Epoch 10/15: [==============================] 75/75 batches, loss: 0.0415
[2025-05-07 21:12:34,585][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0415
[2025-05-07 21:12:34,796][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0199, Metrics: {'mse': 0.019850345328450203, 'rmse': 0.14089125355553553, 'r2': 0.00652390718460083}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0213Epoch 11/15: [                              ] 2/75 batches, loss: 0.0312Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0272Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0301Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0277Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0309Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0299Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0349Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0351Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0348Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0363Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0368Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0378Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0375Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0405Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0394Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0400Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0398Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0399Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0402Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0399Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0393Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0387Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0386Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0387Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0385Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0377Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0383Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0386Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0391Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0389Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0388Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0389Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0386Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0384Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0382Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0379Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0377Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0374Epoch 11/15: [================              ] 40/75 batches, loss: 0.0370Epoch 11/15: [================              ] 41/75 batches, loss: 0.0367Epoch 11/15: [================              ] 42/75 batches, loss: 0.0378Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0380Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0376Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0371Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0370Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0367Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0365Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0368Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0368Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0365Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0360Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0360Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0357Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0358Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0356Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0353Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0354Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0354Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0359Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0358Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0355Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0354Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0353Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0355Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0353Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0352Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0356Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0355Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0361Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0362Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0370Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0370Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0371Epoch 11/15: [==============================] 75/75 batches, loss: 0.0369
[2025-05-07 21:12:37,466][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0369
[2025-05-07 21:12:37,684][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0195, Metrics: {'mse': 0.019396554678678513, 'rmse': 0.1392715142399138, 'r2': 0.029235482215881348}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0388Epoch 12/15: [                              ] 2/75 batches, loss: 0.0316Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0382Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0365Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0357Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0342Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0328Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0305Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0341Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0339Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0334Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0348Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0348Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0352Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0337Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0341Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0345Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0341Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0344Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0348Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0348Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0341Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0335Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0330Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0326Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0323Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0331Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0331Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0340Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0350Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0356Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0360Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0364Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0366Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0368Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0364Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0360Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0359Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0358Epoch 12/15: [================              ] 40/75 batches, loss: 0.0366Epoch 12/15: [================              ] 41/75 batches, loss: 0.0370Epoch 12/15: [================              ] 42/75 batches, loss: 0.0370Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0373Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0371Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0381Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0381Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0381Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0377Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0376Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0374Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0376Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0377Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0383Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0385Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0383Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0383Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0381Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0377Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0375Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0377Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0377Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0376Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0374Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0373Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0373Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0377Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0378Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0375Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0375Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0376Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0374Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0373Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0378Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0375Epoch 12/15: [==============================] 75/75 batches, loss: 0.0377
[2025-05-07 21:12:40,376][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0377
[2025-05-07 21:12:40,583][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0323, Metrics: {'mse': 0.031917158514261246, 'rmse': 0.17865373915555544, 'r2': -0.5973995923995972}
[2025-05-07 21:12:40,584][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0494Epoch 13/15: [                              ] 2/75 batches, loss: 0.0465Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0508Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0449Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0389Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0425Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0389Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0390Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0385Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0384Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0375Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0368Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0359Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0373Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0373Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0379Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0374Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0379Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0381Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0376Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0394Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0391Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0382Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0386Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0385Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0381Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0374Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0374Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0370Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0368Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0368Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0362Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0361Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0363Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0360Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0362Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0363Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0361Epoch 13/15: [================              ] 40/75 batches, loss: 0.0360Epoch 13/15: [================              ] 41/75 batches, loss: 0.0361Epoch 13/15: [================              ] 42/75 batches, loss: 0.0360Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0358Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0357Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0354Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0351Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0352Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0347Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0351Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0349Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0346Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0344Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0343Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0344Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0347Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0345Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0344Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0341Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0342Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0340Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0340Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0339Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0336Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0335Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0337Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0334Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0335Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0334Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0335Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0338Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0339Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0338Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0339Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0342Epoch 13/15: [==============================] 75/75 batches, loss: 0.0346
[2025-05-07 21:12:42,896][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0346
[2025-05-07 21:12:43,109][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0200, Metrics: {'mse': 0.01990811713039875, 'rmse': 0.14109612726931506, 'r2': 0.0036326050758361816}
[2025-05-07 21:12:43,110][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0175Epoch 14/15: [                              ] 2/75 batches, loss: 0.0166Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0250Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0241Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0269Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0258Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0260Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0249Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0266Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0290Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0291Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0298Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0296Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0310Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0296Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0288Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0303Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0295Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0306Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0311Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0308Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0306Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0318Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0313Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0313Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0313Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0313Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0322Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0323Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0319Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0317Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0315Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0315Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0318Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0315Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0319Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0317Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0318Epoch 14/15: [================              ] 40/75 batches, loss: 0.0322Epoch 14/15: [================              ] 41/75 batches, loss: 0.0324Epoch 14/15: [================              ] 42/75 batches, loss: 0.0330Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0331Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0336Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0338Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0340Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0338Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0337Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0337Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0339Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0338Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0336Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0335Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0337Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0337Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0338Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0340Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0339Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0341Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0337Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0338Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0336Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0337Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0338Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0337Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0338Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0336Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0336Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0337Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0338Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0339Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0339Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0340Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0338Epoch 14/15: [==============================] 75/75 batches, loss: 0.0339
[2025-05-07 21:12:45,409][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0339
[2025-05-07 21:12:45,661][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0202, Metrics: {'mse': 0.020006921142339706, 'rmse': 0.1414458240540869, 'r2': -0.0013123750686645508}
[2025-05-07 21:12:45,662][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0367Epoch 15/15: [                              ] 2/75 batches, loss: 0.0308Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0308Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0325Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0339Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0357Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0337Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0317Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0320Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0331Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0330Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0329Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0343Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0337Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0326Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0327Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0329Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0334Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0327Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0318Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0324Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0323Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0316Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0318Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0319Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0317Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0324Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0325Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0321Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0323Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0325Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0321Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0320Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0319Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0318Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0323Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0321Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0319Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0320Epoch 15/15: [================              ] 40/75 batches, loss: 0.0319Epoch 15/15: [================              ] 41/75 batches, loss: 0.0317Epoch 15/15: [================              ] 42/75 batches, loss: 0.0316Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0317Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0316Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0313Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0311Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0316Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0318Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0318Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0325Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0323Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0326Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0324Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0325Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0323Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0326Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0325Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0323Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0322Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0326Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0324Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0324Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0324Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0322Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0322Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0322Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0321Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0321Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0321Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0320Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0321Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0320Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0318Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0318Epoch 15/15: [==============================] 75/75 batches, loss: 0.0320
[2025-05-07 21:12:47,946][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0320
[2025-05-07 21:12:48,214][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0229, Metrics: {'mse': 0.02269401028752327, 'rmse': 0.1506453128627747, 'r2': -0.13579654693603516}
[2025-05-07 21:12:48,215][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 21:12:48,215][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 21:12:48,215][src.training.lm_trainer][INFO] - Training completed in 42.46 seconds
[2025-05-07 21:12:48,215][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 21:12:51,099][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.020891526713967323, 'rmse': 0.14453901450462198, 'r2': -0.005305290222167969}
[2025-05-07 21:12:51,100][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.019396554678678513, 'rmse': 0.1392715142399138, 'r2': 0.029235482215881348}
[2025-05-07 21:12:51,100][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06525465846061707, 'rmse': 0.2554499138003712, 'r2': -0.0938042402267456}
[2025-05-07 21:12:52,980][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ja/ja/model.pt
[2025-05-07 21:12:52,982][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▄▂▂▂▁▁
wandb:     best_val_mse █▆▄▄▂▂▂▁▁
wandb:      best_val_r2 ▁▃▅▅▇▇▇██
wandb:    best_val_rmse █▇▄▄▂▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▂▄▄▅▅▅▃▄▅▅▁▅▅
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇▆▄▃▂▂▂▅▄▁▁█▁▁▃
wandb:          val_mse ▇▆▄▃▂▂▁▅▄▁▁█▁▁▃
wandb:           val_r2 ▂▃▅▆▇▇█▄▅██▁██▆
wandb:         val_rmse ▇▆▄▄▂▂▂▅▄▁▁█▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01946
wandb:     best_val_mse 0.0194
wandb:      best_val_r2 0.02924
wandb:    best_val_rmse 0.13927
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.06525
wandb:    final_test_r2 -0.0938
wandb:  final_test_rmse 0.25545
wandb:  final_train_mse 0.02089
wandb:   final_train_r2 -0.00531
wandb: final_train_rmse 0.14454
wandb:    final_val_mse 0.0194
wandb:     final_val_r2 0.02924
wandb:   final_val_rmse 0.13927
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03198
wandb:       train_time 42.46363
wandb:         val_loss 0.02293
wandb:          val_mse 0.02269
wandb:           val_r2 -0.1358
wandb:         val_rmse 0.15065
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_211150-iwv167vh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_211150-iwv167vh/logs
Experiment probe_layer2_n_tokens_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/n_tokens/control3/layer2/ja/ja/results.json for layer 2
Some experiments failed. See /scratch/leuven/371/vsc37132/makeup_probes_output/failed_experiments.log for details.
Failed experiments (2):
probe_layer2_n_tokens,_fi
probe_layer2_avg_verb_edges,_fi
==============================================
probing experiments completed!
 planned experiments: 28
 completed: 112
