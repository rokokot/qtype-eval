SLURM_JOB_ID: 64437392
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Fri May  2 10:59:00 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 10:59:13,989][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 10:59:13,989][__main__][INFO] - Normalized task: question_type
[2025-05-02 10:59:13,990][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 10:59:13,990][__main__][INFO] - Determined Task Type: classification
[2025-05-02 10:59:13,994][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 10:59:13,994][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 10:59:15,927][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 10:59:18,196][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 10:59:18,196][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 10:59:18,273][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,311][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,406][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 10:59:18,414][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 10:59:18,414][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 10:59:18,415][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 10:59:18,433][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,474][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 10:59:18,475][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 10:59:18,475][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 10:59:18,476][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 10:59:18,492][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,520][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 10:59:18,533][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 10:59:18,534][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 10:59:18,535][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 10:59:18,535][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 10:59:18,536][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 10:59:18,537][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 10:59:18,537][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 10:59:18,537][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 10:59:18,538][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 10:59:18,538][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 10:59:18,538][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 10:59:18,539][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 10:59:18,539][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 10:59:18,539][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 10:59:18,540][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 10:59:18,540][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 10:59:22,476][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 10:59:22,477][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 10:59:22,477][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 10:59:22,477][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 10:59:22,483][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 10:59:22,483][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 10:59:22,483][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 10:59:22,483][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 10:59:22,483][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 10:59:22,484][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 10:59:22,484][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7447Epoch 1/15: [                              ] 2/63 batches, loss: 0.7093Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7162Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7297Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7345Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7321Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7265Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7153Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7040Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7001Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7017Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.6977Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6965Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7014Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6998Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6984Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6998Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6965Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6950Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6964Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6981Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6974Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6985Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6973Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6982Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6967Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6970Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6963Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6965Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6956Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6960Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6957Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6952Epoch 1/15: [================              ] 34/63 batches, loss: 0.6953Epoch 1/15: [================              ] 35/63 batches, loss: 0.6951Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6942Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6933Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6935Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6914Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6911Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6921Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6915Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6913Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6909Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6911Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6914Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6917Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6911Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6912Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6908Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6907Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6902Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6897Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6894Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6886Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6885Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6883Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6885Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6882Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6873Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6871Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6868Epoch 1/15: [==============================] 63/63 batches, loss: 0.6854
[2025-05-02 10:59:27,057][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6854
[2025-05-02 10:59:27,258][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6716Epoch 2/15: [                              ] 2/63 batches, loss: 0.6766Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6869Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6837Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6771Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6793Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6823Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6865Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6900Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6906Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6871Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6844Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6844Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6833Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6833Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6833Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6830Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6830Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6833Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6820Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6818Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6819Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6818Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6821Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6819Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6827Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6828Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6815Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6814Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6825Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6828Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6831Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6825Epoch 2/15: [================              ] 34/63 batches, loss: 0.6823Epoch 2/15: [================              ] 35/63 batches, loss: 0.6820Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6819Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6817Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6814Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6815Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6813Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6811Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6807Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6811Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6810Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6805Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6806Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6801Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6808Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6807Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6805Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6804Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6800Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6803Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6799Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6801Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6782Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6772Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6755Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6757Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6756Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6757Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6746Epoch 2/15: [==============================] 63/63 batches, loss: 0.6749
[2025-05-02 10:59:29,575][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6749
[2025-05-02 10:59:29,780][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6916, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6456Epoch 3/15: [                              ] 2/63 batches, loss: 0.6548Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6420Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6486Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6572Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6635Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6641Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6703Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6689Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6676Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6675Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6724Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6718Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6700Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6688Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6693Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6707Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6716Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6709Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6696Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6694Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6699Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6684Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6673Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6676Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6690Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6706Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6718Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6728Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6729Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6732Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6722Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6731Epoch 3/15: [================              ] 34/63 batches, loss: 0.6734Epoch 3/15: [================              ] 35/63 batches, loss: 0.6715Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6727Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6721Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6701Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6693Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6688Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6694Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6685Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6689Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6679Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6668Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6664Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6657Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6652Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6640Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6643Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6651Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6634Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6627Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6609Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6615Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6604Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6604Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6605Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6597Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6606Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6602Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6593Epoch 3/15: [==============================] 63/63 batches, loss: 0.6596
[2025-05-02 10:59:32,117][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6596
[2025-05-02 10:59:32,334][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6476, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8205128205128205, 'precision': 0.8421052631578947, 'recall': 0.8}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6284Epoch 4/15: [                              ] 2/63 batches, loss: 0.6251Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6258Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6221Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6330Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6275Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6285Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6345Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6223Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6311Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6387Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6425Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6426Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6406Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6444Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6452Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6466Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6451Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6454Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6419Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6402Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6401Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6416Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6425Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6423Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6441Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6418Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6431Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6440Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6433Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6423Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6431Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6426Epoch 4/15: [================              ] 34/63 batches, loss: 0.6419Epoch 4/15: [================              ] 35/63 batches, loss: 0.6424Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6400Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6393Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6396Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6389Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6375Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6371Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6365Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6356Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6357Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6348Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6339Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6326Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6312Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6310Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6298Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6305Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6308Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6321Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6314Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6309Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6306Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6322Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6303Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6305Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6297Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6292Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6288Epoch 4/15: [==============================] 63/63 batches, loss: 0.6264
[2025-05-02 10:59:34,624][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6264
[2025-05-02 10:59:34,847][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6048, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6384Epoch 5/15: [                              ] 2/63 batches, loss: 0.5774Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6015Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6089Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6270Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6211Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6176Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6095Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6109Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6061Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6042Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5997Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5996Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5963Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5915Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5991Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5980Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5989Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6022Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6055Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6022Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5989Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5959Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5977Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5947Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5948Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5942Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5916Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5958Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5967Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5946Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5924Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5936Epoch 5/15: [================              ] 34/63 batches, loss: 0.5959Epoch 5/15: [================              ] 35/63 batches, loss: 0.5961Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5947Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5933Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5918Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5914Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5918Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5922Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5938Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5937Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5928Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5929Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5925Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5923Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5934Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5932Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5944Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5946Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5945Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5944Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5940Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5946Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5936Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5935Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5936Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5936Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5937Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5936Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5933Epoch 5/15: [==============================] 63/63 batches, loss: 0.5925
[2025-05-02 10:59:37,132][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5925
[2025-05-02 10:59:37,337][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5950, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5682Epoch 6/15: [                              ] 2/63 batches, loss: 0.5739Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5859Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5813Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5767Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5774Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5536Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5641Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5650Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5695Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5665Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5724Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5755Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5739Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5748Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5773Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5770Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5790Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5793Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5796Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5803Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5832Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5814Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5834Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5873Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5876Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5883Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5872Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5864Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5840Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5855Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5861Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5844Epoch 6/15: [================              ] 34/63 batches, loss: 0.5814Epoch 6/15: [================              ] 35/63 batches, loss: 0.5793Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5791Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5808Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5792Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5809Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5815Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5816Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5796Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5805Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5788Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5792Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5789Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5793Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5800Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5819Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5830Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5815Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5810Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5796Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5817Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5823Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5836Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5822Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5839Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5835Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5822Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5817Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5823Epoch 6/15: [==============================] 63/63 batches, loss: 0.5806
[2025-05-02 10:59:39,641][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5806
[2025-05-02 10:59:39,870][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5986, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043, 'precision': 0.7692307692307693, 'recall': 1.0}
[2025-05-02 10:59:39,870][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5111Epoch 7/15: [                              ] 2/63 batches, loss: 0.5228Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5691Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5678Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5804Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5796Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5763Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5729Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5726Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5633Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5573Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5625Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5623Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5529Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5585Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5586Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5606Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5587Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5588Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5567Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5565Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5599Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5591Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5604Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5602Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5586Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5595Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5621Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5640Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5677Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5659Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5651Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5645Epoch 7/15: [================              ] 34/63 batches, loss: 0.5653Epoch 7/15: [================              ] 35/63 batches, loss: 0.5683Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5671Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5661Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5671Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5662Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5680Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5680Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5665Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5675Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5667Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5660Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5655Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5677Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5661Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5667Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5648Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5629Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5653Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5649Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5662Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5672Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5664Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5651Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5648Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5638Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5633Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5646Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5648Epoch 7/15: [==============================] 63/63 batches, loss: 0.5633
[2025-05-02 10:59:41,813][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5633
[2025-05-02 10:59:42,039][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6032, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043, 'precision': 0.7692307692307693, 'recall': 1.0}
[2025-05-02 10:59:42,040][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6476Epoch 8/15: [                              ] 2/63 batches, loss: 0.5920Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5595Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5285Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5508Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5702Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5706Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5651Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5684Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5654Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5651Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5653Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5642Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5646Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5655Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5687Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5726Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5741Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5756Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5721Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5735Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5699Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5716Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5711Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5704Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5694Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5682Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5688Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5692Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5676Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5675Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5694Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5674Epoch 8/15: [================              ] 34/63 batches, loss: 0.5673Epoch 8/15: [================              ] 35/63 batches, loss: 0.5669Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5645Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5640Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5640Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5653Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5642Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5660Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5675Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5672Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5680Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5683Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5689Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5689Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5672Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5666Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5666Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5677Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5686Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5681Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5682Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5672Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5675Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5672Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5685Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5680Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5671Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5670Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5655Epoch 8/15: [==============================] 63/63 batches, loss: 0.5659
[2025-05-02 10:59:43,958][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5659
[2025-05-02 10:59:44,154][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5953, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043, 'precision': 0.7692307692307693, 'recall': 1.0}
[2025-05-02 10:59:44,154][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 10:59:44,155][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 10:59:44,155][src.training.lm_trainer][INFO] - Training completed in 20.01 seconds
[2025-05-02 10:59:44,155][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 10:59:46,636][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.985929648241206, 'f1': 0.9860557768924303, 'precision': 0.9763313609467456, 'recall': 0.9959758551307847}
[2025-05-02 10:59:46,636][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 10:59:46,637][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5454545454545454, 'f1': 0.5569620253164557, 'precision': 0.38596491228070173, 'recall': 1.0}
[2025-05-02 10:59:48,318][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/ar/model.pt
[2025-05-02 10:59:48,320][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▇██
wandb:           best_val_f1 ▁▁▇██
wandb:         best_val_loss ██▅▂▁
wandb:    best_val_precision ▁▁███
wandb:       best_val_recall ▁▁▇██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▂▃▃▃▃
wandb:            train_loss █▇▇▅▃▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▇██▇▇▇
wandb:                val_f1 ▁▁▇█████
wandb:              val_loss ██▅▂▁▁▂▁
wandb:         val_precision ▁▁███▇▇▇
wandb:            val_recall ▁▁▇█████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.59501
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.54545
wandb:         final_test_f1 0.55696
wandb:  final_test_precision 0.38596
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.98593
wandb:        final_train_f1 0.98606
wandb: final_train_precision 0.97633
wandb:    final_train_recall 0.99598
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.56594
wandb:            train_time 20.00909
wandb:          val_accuracy 0.86364
wandb:                val_f1 0.86957
wandb:              val_loss 0.59531
wandb:         val_precision 0.76923
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_105914-rtowu7bi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_105914-rtowu7bi/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/ar/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 10:59:58,209][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 10:59:58,209][__main__][INFO] - Normalized task: complexity
[2025-05-02 10:59:58,209][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 10:59:58,209][__main__][INFO] - Determined Task Type: regression
[2025-05-02 10:59:58,214][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 10:59:58,214][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 10:59:59,523][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:00:01,768][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:00:01,768][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:00:01,823][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:01,878][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:01,999][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:00:02,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:00:02,007][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:00:02,009][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:00:02,053][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:02,108][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:02,126][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:00:02,127][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:00:02,128][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:00:02,129][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:00:02,172][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:02,224][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:00:02,242][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:00:02,243][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:00:02,243][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:00:02,245][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:00:02,245][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:00:02,245][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:00:02,245][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:00:02,245][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:00:02,245][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:00:02,246][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:00:02,246][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:00:02,246][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:00:02,246][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:00:02,247][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:00:02,247][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:00:02,247][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:00:02,248][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:00:02,248][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:00:02,248][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:00:02,248][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:00:02,248][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:00:06,672][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:00:06,673][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:00:06,673][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 11:00:06,674][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:00:06,678][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:00:06,678][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:00:06,678][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:00:06,678][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:00:06,678][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:00:06,679][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:00:06,679][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.3664Epoch 1/15: [                              ] 2/63 batches, loss: 0.4012Epoch 1/15: [=                             ] 3/63 batches, loss: 0.3585Epoch 1/15: [=                             ] 4/63 batches, loss: 0.3295Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3115Epoch 1/15: [==                            ] 6/63 batches, loss: 0.3124Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3131Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2962Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2855Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2774Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2674Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2529Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2377Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2370Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2289Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2250Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2182Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2145Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2105Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2099Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2055Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2029Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2012Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.1988Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.1975Epoch 1/15: [============                  ] 26/63 batches, loss: 0.1935Epoch 1/15: [============                  ] 27/63 batches, loss: 0.1907Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.1890Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.1881Epoch 1/15: [==============                ] 30/63 batches, loss: 0.1850Epoch 1/15: [==============                ] 31/63 batches, loss: 0.1829Epoch 1/15: [===============               ] 32/63 batches, loss: 0.1824Epoch 1/15: [===============               ] 33/63 batches, loss: 0.1793Epoch 1/15: [================              ] 34/63 batches, loss: 0.1776Epoch 1/15: [================              ] 35/63 batches, loss: 0.1741Epoch 1/15: [=================             ] 36/63 batches, loss: 0.1704Epoch 1/15: [=================             ] 37/63 batches, loss: 0.1707Epoch 1/15: [==================            ] 38/63 batches, loss: 0.1695Epoch 1/15: [==================            ] 39/63 batches, loss: 0.1723Epoch 1/15: [===================           ] 40/63 batches, loss: 0.1714Epoch 1/15: [===================           ] 41/63 batches, loss: 0.1756Epoch 1/15: [====================          ] 42/63 batches, loss: 0.1758Epoch 1/15: [====================          ] 43/63 batches, loss: 0.1746Epoch 1/15: [====================          ] 44/63 batches, loss: 0.1748Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.1732Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.1735Epoch 1/15: [======================        ] 47/63 batches, loss: 0.1723Epoch 1/15: [======================        ] 48/63 batches, loss: 0.1727Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.1735Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.1718Epoch 1/15: [========================      ] 51/63 batches, loss: 0.1708Epoch 1/15: [========================      ] 52/63 batches, loss: 0.1701Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.1687Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.1700Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.1700Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.1690Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.1701Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.1719Epoch 1/15: [============================  ] 59/63 batches, loss: 0.1705Epoch 1/15: [============================  ] 60/63 batches, loss: 0.1689Epoch 1/15: [============================= ] 61/63 batches, loss: 0.1688Epoch 1/15: [============================= ] 62/63 batches, loss: 0.1683Epoch 1/15: [==============================] 63/63 batches, loss: 0.1692
[2025-05-02 11:00:11,298][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1692
[2025-05-02 11:00:11,504][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0600, Metrics: {'mse': 0.06143387407064438, 'rmse': 0.2478585767542539, 'r2': 0.05309969186782837}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.0828Epoch 2/15: [                              ] 2/63 batches, loss: 0.1404Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1168Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1243Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1289Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1639Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1499Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1418Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1347Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1299Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1380Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1370Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1417Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1448Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1471Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1438Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1399Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1367Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1366Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1351Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1333Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1320Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1297Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1285Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1278Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1285Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1311Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1321Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1345Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1342Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1333Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1321Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1323Epoch 2/15: [================              ] 34/63 batches, loss: 0.1310Epoch 2/15: [================              ] 35/63 batches, loss: 0.1309Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1294Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1322Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1338Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1355Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1348Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1353Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1355Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1340Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1324Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1329Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1341Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1340Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1336Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1330Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1333Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1331Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1319Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1322Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1313Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1312Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1302Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1301Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1309Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1302Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1300Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1295Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1287Epoch 2/15: [==============================] 63/63 batches, loss: 0.1280
[2025-05-02 11:00:13,841][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1280
[2025-05-02 11:00:14,054][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0549, Metrics: {'mse': 0.05618899688124657, 'rmse': 0.2370421837590233, 'r2': 0.1339406967163086}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1072Epoch 3/15: [                              ] 2/63 batches, loss: 0.1489Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1314Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1235Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1137Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1182Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1272Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1245Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1221Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1283Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1315Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1281Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1325Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1327Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1304Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1284Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1267Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1249Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1225Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1209Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1222Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1214Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1258Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1275Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1265Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1292Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1264Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1243Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1234Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1270Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1280Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1287Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1289Epoch 3/15: [================              ] 34/63 batches, loss: 0.1333Epoch 3/15: [================              ] 35/63 batches, loss: 0.1318Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1324Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1313Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1326Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1325Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1311Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1298Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1310Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1308Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1305Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1309Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1297Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1283Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1276Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1274Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1281Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1277Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1290Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1285Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1275Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1277Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1273Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1267Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1274Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1278Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1275Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1261Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1262Epoch 3/15: [==============================] 63/63 batches, loss: 0.1314
[2025-05-02 11:00:16,392][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1314
[2025-05-02 11:00:16,624][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0524, Metrics: {'mse': 0.05350296571850777, 'rmse': 0.23130708099517353, 'r2': 0.17534136772155762}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0743Epoch 4/15: [                              ] 2/63 batches, loss: 0.1129Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1117Epoch 4/15: [=                             ] 4/63 batches, loss: 0.0954Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1181Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1143Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1381Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1318Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1252Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1316Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1293Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1233Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1224Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1211Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1208Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1193Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1217Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1231Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1311Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1284Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1249Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1274Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1275Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1293Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1271Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1266Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1278Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1280Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1284Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1290Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1306Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1286Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1272Epoch 4/15: [================              ] 34/63 batches, loss: 0.1264Epoch 4/15: [================              ] 35/63 batches, loss: 0.1246Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1236Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1234Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1217Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1195Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1198Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1188Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1186Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1178Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1173Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1193Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1181Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1190Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1180Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1178Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1180Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1178Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1176Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1168Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1161Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1154Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1147Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1137Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1144Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1144Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1142Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1141Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1136Epoch 4/15: [==============================] 63/63 batches, loss: 0.1123
[2025-05-02 11:00:18,915][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1123
[2025-05-02 11:00:19,131][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0503, Metrics: {'mse': 0.05132020637392998, 'rmse': 0.22653963532664648, 'r2': 0.20898497104644775}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.0806Epoch 5/15: [                              ] 2/63 batches, loss: 0.0765Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1133Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0955Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0910Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0840Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0911Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1032Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1009Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1000Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1037Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1104Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1083Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1064Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1048Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1049Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1115Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1111Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1118Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1104Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1111Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.1088Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.1090Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1090Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1097Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1090Epoch 5/15: [============                  ] 27/63 batches, loss: 0.1073Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1055Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1040Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1063Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1053Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1067Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1072Epoch 5/15: [================              ] 34/63 batches, loss: 0.1078Epoch 5/15: [================              ] 35/63 batches, loss: 0.1069Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1077Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1090Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1077Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1067Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1056Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1043Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1033Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1044Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1037Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1032Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1031Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1027Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1021Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1027Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1029Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1027Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1042Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.1037Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.1031Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.1031Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.1031Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.1023Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.1036Epoch 5/15: [============================  ] 59/63 batches, loss: 0.1051Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1040Epoch 5/15: [============================= ] 61/63 batches, loss: 0.1038Epoch 5/15: [============================= ] 62/63 batches, loss: 0.1039Epoch 5/15: [==============================] 63/63 batches, loss: 0.1041
[2025-05-02 11:00:21,403][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1041
[2025-05-02 11:00:21,628][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0597, Metrics: {'mse': 0.0607525110244751, 'rmse': 0.24648024469412372, 'r2': 0.06360173225402832}
[2025-05-02 11:00:21,628][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0985Epoch 6/15: [                              ] 2/63 batches, loss: 0.0950Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1058Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1115Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1054Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1122Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1076Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1026Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1013Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1042Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1005Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1016Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1012Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0974Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0993Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0984Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1026Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1005Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1037Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1033Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1051Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1046Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1024Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1008Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1016Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1000Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0987Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1002Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0982Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1014Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1028Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1022Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1021Epoch 6/15: [================              ] 34/63 batches, loss: 0.1019Epoch 6/15: [================              ] 35/63 batches, loss: 0.1027Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1024Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1027Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1027Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1021Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1021Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1019Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1016Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1014Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1007Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1030Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1041Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1045Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1034Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1032Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1030Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1043Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1036Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1029Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1033Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1023Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1038Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1035Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1028Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1030Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1050Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1044Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1049Epoch 6/15: [==============================] 63/63 batches, loss: 0.1039
[2025-05-02 11:00:23,558][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1039
[2025-05-02 11:00:23,783][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0557, Metrics: {'mse': 0.05660886690020561, 'rmse': 0.23792617951836575, 'r2': 0.12746912240982056}
[2025-05-02 11:00:23,783][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0778Epoch 7/15: [                              ] 2/63 batches, loss: 0.0801Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0677Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0774Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0745Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0792Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0796Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0820Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0824Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0867Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0884Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0904Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0924Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0903Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0925Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0897Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0923Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0911Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0931Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0940Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0932Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0920Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0911Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0904Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0908Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0909Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0911Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0914Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0914Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0939Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0943Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0945Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0958Epoch 7/15: [================              ] 34/63 batches, loss: 0.0956Epoch 7/15: [================              ] 35/63 batches, loss: 0.0940Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0941Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0946Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0956Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0964Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0960Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0956Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0960Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0952Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0950Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0942Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0941Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0947Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0946Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0939Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0935Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0943Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0941Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0943Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0932Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0922Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0921Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0919Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0918Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0919Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0911Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0906Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0898Epoch 7/15: [==============================] 63/63 batches, loss: 0.0905
[2025-05-02 11:00:25,702][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0905
[2025-05-02 11:00:25,905][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0488, Metrics: {'mse': 0.04958159103989601, 'rmse': 0.22266924134216654, 'r2': 0.23578274250030518}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0749Epoch 8/15: [                              ] 2/63 batches, loss: 0.0629Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0558Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0687Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0678Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0697Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0664Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0744Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0723Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0762Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0748Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0722Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0748Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0775Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0820Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0841Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0830Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0839Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0847Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0848Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0866Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0887Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0872Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0877Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0876Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0879Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0865Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0870Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0866Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0918Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0904Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0905Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0904Epoch 8/15: [================              ] 34/63 batches, loss: 0.0911Epoch 8/15: [================              ] 35/63 batches, loss: 0.0911Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0911Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0899Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0920Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0916Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0922Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0915Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0911Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0918Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0914Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0908Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0897Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0893Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0886Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0895Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0890Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0897Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0892Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0895Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0893Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0885Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0884Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0874Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0874Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0871Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0868Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0872Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0869Epoch 8/15: [==============================] 63/63 batches, loss: 0.0858
[2025-05-02 11:00:28,245][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0858
[2025-05-02 11:00:28,470][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0512, Metrics: {'mse': 0.05196236073970795, 'rmse': 0.22795254054234174, 'r2': 0.1990872025489807}
[2025-05-02 11:00:28,471][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1239Epoch 9/15: [                              ] 2/63 batches, loss: 0.0994Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0802Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0791Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0734Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0676Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0693Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0678Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0701Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0696Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0723Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0694Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0659Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0667Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0650Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0672Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0671Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0700Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0681Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0692Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0691Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0694Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0700Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0699Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0691Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0688Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0686Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0706Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0703Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0741Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0755Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0759Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0769Epoch 9/15: [================              ] 34/63 batches, loss: 0.0766Epoch 9/15: [================              ] 35/63 batches, loss: 0.0767Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0761Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0760Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0774Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0780Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0788Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0798Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0791Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0799Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0790Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0788Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0779Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0773Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0772Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0770Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0772Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0776Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0775Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0778Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0780Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0780Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0773Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0779Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0788Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0791Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0796Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0800Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0804Epoch 9/15: [==============================] 63/63 batches, loss: 0.0837
[2025-05-02 11:00:30,423][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0837
[2025-05-02 11:00:30,621][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0449, Metrics: {'mse': 0.04557396471500397, 'rmse': 0.2134805956404562, 'r2': 0.2975535988807678}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0758Epoch 10/15: [                              ] 2/63 batches, loss: 0.0847Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0833Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0891Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0843Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0863Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0807Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0853Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0894Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0938Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0882Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0889Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0906Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0870Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0857Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0881Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0872Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0857Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0843Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0913Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0941Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0942Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0947Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0973Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0974Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0960Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0945Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0930Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0925Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0911Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0931Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0920Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0907Epoch 10/15: [================              ] 34/63 batches, loss: 0.0910Epoch 10/15: [================              ] 35/63 batches, loss: 0.0915Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0913Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0910Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0896Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0887Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0897Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0896Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0890Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0884Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0885Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0877Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0869Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0866Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0865Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0859Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0850Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0849Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0861Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0863Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0868Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0859Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0858Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0855Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0852Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0846Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0845Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0848Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0845Epoch 10/15: [==============================] 63/63 batches, loss: 0.0834
[2025-05-02 11:00:32,937][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0834
[2025-05-02 11:00:33,133][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0464, Metrics: {'mse': 0.04702764376997948, 'rmse': 0.21685858011611964, 'r2': 0.27514761686325073}
[2025-05-02 11:00:33,134][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0679Epoch 11/15: [                              ] 2/63 batches, loss: 0.0579Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1061Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0975Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0949Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1066Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1015Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0953Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0892Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0928Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0921Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0890Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0851Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0859Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0843Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0858Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0853Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0857Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0851Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0848Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0835Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0811Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0800Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0817Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0804Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0829Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0831Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0825Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0859Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0866Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0846Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0839Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0840Epoch 11/15: [================              ] 34/63 batches, loss: 0.0828Epoch 11/15: [================              ] 35/63 batches, loss: 0.0827Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0823Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0835Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0833Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0823Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0832Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0831Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0842Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0828Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0820Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0831Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0823Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0810Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0798Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0793Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0802Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0811Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0803Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0806Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0802Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0796Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0800Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0799Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0804Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0808Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0810Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0799Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0791Epoch 11/15: [==============================] 63/63 batches, loss: 0.0791
[2025-05-02 11:00:35,066][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0791
[2025-05-02 11:00:35,292][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0500, Metrics: {'mse': 0.05051272734999657, 'rmse': 0.2247503667405163, 'r2': 0.22143089771270752}
[2025-05-02 11:00:35,293][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1043Epoch 12/15: [                              ] 2/63 batches, loss: 0.0722Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0712Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0773Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0701Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0713Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0744Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0755Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0729Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0726Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0711Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0780Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0766Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0768Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0802Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0785Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0801Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0783Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0775Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0820Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0812Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0818Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0805Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0794Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0800Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0801Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0785Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0788Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0787Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0809Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0797Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0787Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0788Epoch 12/15: [================              ] 34/63 batches, loss: 0.0778Epoch 12/15: [================              ] 35/63 batches, loss: 0.0783Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0782Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0783Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0775Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0777Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0773Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0788Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0793Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0805Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0811Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0806Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0801Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0794Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0793Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0788Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0781Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0776Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0772Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0766Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0773Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0771Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0780Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0774Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0775Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0771Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0765Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0763Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0759Epoch 12/15: [==============================] 63/63 batches, loss: 0.0754
[2025-05-02 11:00:37,225][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0754
[2025-05-02 11:00:37,426][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0521, Metrics: {'mse': 0.05256446450948715, 'rmse': 0.22926941468387613, 'r2': 0.18980681896209717}
[2025-05-02 11:00:37,427][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0491Epoch 13/15: [                              ] 2/63 batches, loss: 0.0696Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0645Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0583Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0549Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0572Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0592Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0585Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0563Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0556Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0559Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0570Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0576Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0550Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0537Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0547Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0532Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0524Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0529Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0518Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0546Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0550Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0557Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0555Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0562Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0581Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0599Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0596Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0601Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0597Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0603Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0599Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0595Epoch 13/15: [================              ] 34/63 batches, loss: 0.0599Epoch 13/15: [================              ] 35/63 batches, loss: 0.0603Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0599Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0612Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0607Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0612Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0609Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0611Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0618Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0618Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0616Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0616Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0614Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0616Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0641Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0636Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0633Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0647Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0639Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0643Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0636Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0645Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0643Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0658Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0653Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0653Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0655Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0652Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0650Epoch 13/15: [==============================] 63/63 batches, loss: 0.0642
[2025-05-02 11:00:39,346][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0642
[2025-05-02 11:00:39,579][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0671, Metrics: {'mse': 0.0676683560013771, 'rmse': 0.26013142063460365, 'r2': -0.04299437999725342}
[2025-05-02 11:00:39,580][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:00:39,580][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-02 11:00:39,580][src.training.lm_trainer][INFO] - Training completed in 31.03 seconds
[2025-05-02 11:00:39,580][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:00:42,084][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02067917212843895, 'rmse': 0.1438025456257258, 'r2': 0.3263534903526306}
[2025-05-02 11:00:42,085][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04557396471500397, 'rmse': 0.2134805956404562, 'r2': 0.2975535988807678}
[2025-05-02 11:00:42,085][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05160701647400856, 'rmse': 0.22717177745927983, 'r2': 0.11031800508499146}
[2025-05-02 11:00:43,751][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/ar/model.pt
[2025-05-02 11:00:43,752][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▃▁
wandb:     best_val_mse █▆▄▄▃▁
wandb:      best_val_r2 ▁▃▅▅▆█
wandb:    best_val_rmse █▆▅▄▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▃▁▂▃▃▄▄▃▃
wandb:       train_loss █▅▅▄▄▄▃▂▂▂▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▆▄▃▃▆▄▂▃▁▁▃▃█
wandb:          val_mse ▆▄▄▃▆▄▂▃▁▁▃▃█
wandb:           val_r2 ▃▅▅▆▃▅▇▆██▆▆▁
wandb:         val_rmse ▆▅▄▃▆▅▂▃▁▂▃▃█
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04495
wandb:     best_val_mse 0.04557
wandb:      best_val_r2 0.29755
wandb:    best_val_rmse 0.21348
wandb: early_stop_epoch 13
wandb:            epoch 13
wandb:   final_test_mse 0.05161
wandb:    final_test_r2 0.11032
wandb:  final_test_rmse 0.22717
wandb:  final_train_mse 0.02068
wandb:   final_train_r2 0.32635
wandb: final_train_rmse 0.1438
wandb:    final_val_mse 0.04557
wandb:     final_val_r2 0.29755
wandb:   final_val_rmse 0.21348
wandb:    learning_rate 2e-05
wandb:       train_loss 0.06424
wandb:       train_time 31.02532
wandb:         val_loss 0.06714
wandb:          val_mse 0.06767
wandb:           val_r2 -0.04299
wandb:         val_rmse 0.26013
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_105958-59uexy1k
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_105958-59uexy1k/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/ar/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:00:56,307][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:00:56,307][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:00:56,307][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:00:56,307][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:00:56,311][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-02 11:00:56,311][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:00:58,050][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:01:00,326][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:01:00,326][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:01:00,392][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,419][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,566][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-02 11:01:00,573][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:01:00,573][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-02 11:01:00,575][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:01:00,594][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,624][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,640][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-02 11:01:00,641][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:01:00,641][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-02 11:01:00,642][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:01:00,662][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,691][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:01:00,704][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-02 11:01:00,705][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:01:00,705][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-02 11:01:00,706][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:01:00,707][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-02 11:01:00,707][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:01:00,707][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:01:00,708][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-02 11:01:00,708][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:01:00,708][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:01:00,709][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-02 11:01:00,709][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-02 11:01:00,709][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-02 11:01:00,709][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:01:00,709][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-02 11:01:00,709][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:01:00,709][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:01:00,709][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:01:00,710][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:01:04,788][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:01:04,789][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:01:04,789][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 11:01:04,789][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:01:04,795][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:01:04,796][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:01:04,796][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:01:04,796][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:01:04,796][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-02 11:01:04,797][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:01:04,797][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.8484Epoch 1/15: [=                             ] 2/60 batches, loss: 0.7353Epoch 1/15: [=                             ] 3/60 batches, loss: 0.7436Epoch 1/15: [==                            ] 4/60 batches, loss: 0.7431Epoch 1/15: [==                            ] 5/60 batches, loss: 0.7371Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7306Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7282Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7275Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7224Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7170Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7156Epoch 1/15: [======                        ] 12/60 batches, loss: 0.7133Epoch 1/15: [======                        ] 13/60 batches, loss: 0.7102Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.7081Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.7070Epoch 1/15: [========                      ] 16/60 batches, loss: 0.7071Epoch 1/15: [========                      ] 17/60 batches, loss: 0.7067Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.7056Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.7054Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.7047Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.7032Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.7027Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.7028Epoch 1/15: [============                  ] 24/60 batches, loss: 0.7016Epoch 1/15: [============                  ] 25/60 batches, loss: 0.7008Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.7002Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.7000Epoch 1/15: [==============                ] 28/60 batches, loss: 0.6993Epoch 1/15: [==============                ] 29/60 batches, loss: 0.6988Epoch 1/15: [===============               ] 30/60 batches, loss: 0.6986Epoch 1/15: [===============               ] 31/60 batches, loss: 0.6972Epoch 1/15: [================              ] 32/60 batches, loss: 0.6969Epoch 1/15: [================              ] 33/60 batches, loss: 0.6970Epoch 1/15: [=================             ] 34/60 batches, loss: 0.6970Epoch 1/15: [=================             ] 35/60 batches, loss: 0.6966Epoch 1/15: [==================            ] 36/60 batches, loss: 0.6965Epoch 1/15: [==================            ] 37/60 batches, loss: 0.6974Epoch 1/15: [===================           ] 38/60 batches, loss: 0.6973Epoch 1/15: [===================           ] 39/60 batches, loss: 0.6978Epoch 1/15: [====================          ] 40/60 batches, loss: 0.6975Epoch 1/15: [====================          ] 41/60 batches, loss: 0.6979Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.6971Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.6975Epoch 1/15: [======================        ] 44/60 batches, loss: 0.6972Epoch 1/15: [======================        ] 45/60 batches, loss: 0.6972Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.6963Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.6965Epoch 1/15: [========================      ] 48/60 batches, loss: 0.6963Epoch 1/15: [========================      ] 49/60 batches, loss: 0.6958Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.6954Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.6951Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.6950Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.6950Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.6945Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.6939Epoch 1/15: [============================  ] 56/60 batches, loss: 0.6929Epoch 1/15: [============================  ] 57/60 batches, loss: 0.6933Epoch 1/15: [============================= ] 58/60 batches, loss: 0.6934Epoch 1/15: [============================= ] 59/60 batches, loss: 0.6936Epoch 1/15: [==============================] 60/60 batches, loss: 0.6936
[2025-05-02 11:01:09,333][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6936
[2025-05-02 11:01:09,574][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6923, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.6708Epoch 2/15: [=                             ] 2/60 batches, loss: 0.6850Epoch 2/15: [=                             ] 3/60 batches, loss: 0.6934Epoch 2/15: [==                            ] 4/60 batches, loss: 0.6944Epoch 2/15: [==                            ] 5/60 batches, loss: 0.6943Epoch 2/15: [===                           ] 6/60 batches, loss: 0.6964Epoch 2/15: [===                           ] 7/60 batches, loss: 0.6959Epoch 2/15: [====                          ] 8/60 batches, loss: 0.6935Epoch 2/15: [====                          ] 9/60 batches, loss: 0.6885Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.6903Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.6887Epoch 2/15: [======                        ] 12/60 batches, loss: 0.6867Epoch 2/15: [======                        ] 13/60 batches, loss: 0.6835Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.6806Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.6811Epoch 2/15: [========                      ] 16/60 batches, loss: 0.6797Epoch 2/15: [========                      ] 17/60 batches, loss: 0.6810Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.6792Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.6797Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.6794Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.6787Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.6793Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.6798Epoch 2/15: [============                  ] 24/60 batches, loss: 0.6778Epoch 2/15: [============                  ] 25/60 batches, loss: 0.6777Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.6770Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.6781Epoch 2/15: [==============                ] 28/60 batches, loss: 0.6786Epoch 2/15: [==============                ] 29/60 batches, loss: 0.6782Epoch 2/15: [===============               ] 30/60 batches, loss: 0.6773Epoch 2/15: [===============               ] 31/60 batches, loss: 0.6769Epoch 2/15: [================              ] 32/60 batches, loss: 0.6774Epoch 2/15: [================              ] 33/60 batches, loss: 0.6786Epoch 2/15: [=================             ] 34/60 batches, loss: 0.6790Epoch 2/15: [=================             ] 35/60 batches, loss: 0.6801Epoch 2/15: [==================            ] 36/60 batches, loss: 0.6806Epoch 2/15: [==================            ] 37/60 batches, loss: 0.6807Epoch 2/15: [===================           ] 38/60 batches, loss: 0.6812Epoch 2/15: [===================           ] 39/60 batches, loss: 0.6813Epoch 2/15: [====================          ] 40/60 batches, loss: 0.6806Epoch 2/15: [====================          ] 41/60 batches, loss: 0.6799Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.6803Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.6790Epoch 2/15: [======================        ] 44/60 batches, loss: 0.6790Epoch 2/15: [======================        ] 45/60 batches, loss: 0.6774Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.6771Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.6766Epoch 2/15: [========================      ] 48/60 batches, loss: 0.6768Epoch 2/15: [========================      ] 49/60 batches, loss: 0.6767Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.6764Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.6760Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.6759Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.6754Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.6748Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.6756Epoch 2/15: [============================  ] 56/60 batches, loss: 0.6765Epoch 2/15: [============================  ] 57/60 batches, loss: 0.6765Epoch 2/15: [============================= ] 58/60 batches, loss: 0.6761Epoch 2/15: [============================= ] 59/60 batches, loss: 0.6764Epoch 2/15: [==============================] 60/60 batches, loss: 0.6770
[2025-05-02 11:01:11,858][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6770
[2025-05-02 11:01:12,115][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6904, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.6685Epoch 3/15: [=                             ] 2/60 batches, loss: 0.6809Epoch 3/15: [=                             ] 3/60 batches, loss: 0.6705Epoch 3/15: [==                            ] 4/60 batches, loss: 0.6732Epoch 3/15: [==                            ] 5/60 batches, loss: 0.6672Epoch 3/15: [===                           ] 6/60 batches, loss: 0.6688Epoch 3/15: [===                           ] 7/60 batches, loss: 0.6687Epoch 3/15: [====                          ] 8/60 batches, loss: 0.6723Epoch 3/15: [====                          ] 9/60 batches, loss: 0.6744Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.6722Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.6732Epoch 3/15: [======                        ] 12/60 batches, loss: 0.6764Epoch 3/15: [======                        ] 13/60 batches, loss: 0.6781Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.6757Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.6745Epoch 3/15: [========                      ] 16/60 batches, loss: 0.6742Epoch 3/15: [========                      ] 17/60 batches, loss: 0.6740Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.6752Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.6747Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.6748Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.6748Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.6750Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.6730Epoch 3/15: [============                  ] 24/60 batches, loss: 0.6736Epoch 3/15: [============                  ] 25/60 batches, loss: 0.6734Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.6740Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.6745Epoch 3/15: [==============                ] 28/60 batches, loss: 0.6747Epoch 3/15: [==============                ] 29/60 batches, loss: 0.6746Epoch 3/15: [===============               ] 30/60 batches, loss: 0.6745Epoch 3/15: [===============               ] 31/60 batches, loss: 0.6753Epoch 3/15: [================              ] 32/60 batches, loss: 0.6743Epoch 3/15: [================              ] 33/60 batches, loss: 0.6740Epoch 3/15: [=================             ] 34/60 batches, loss: 0.6741Epoch 3/15: [=================             ] 35/60 batches, loss: 0.6759Epoch 3/15: [==================            ] 36/60 batches, loss: 0.6759Epoch 3/15: [==================            ] 37/60 batches, loss: 0.6764Epoch 3/15: [===================           ] 38/60 batches, loss: 0.6759Epoch 3/15: [===================           ] 39/60 batches, loss: 0.6753Epoch 3/15: [====================          ] 40/60 batches, loss: 0.6765Epoch 3/15: [====================          ] 41/60 batches, loss: 0.6760Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.6760Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.6749Epoch 3/15: [======================        ] 44/60 batches, loss: 0.6753Epoch 3/15: [======================        ] 45/60 batches, loss: 0.6755Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.6750Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.6748Epoch 3/15: [========================      ] 48/60 batches, loss: 0.6750Epoch 3/15: [========================      ] 49/60 batches, loss: 0.6748Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.6750Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.6755Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.6760Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.6759Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.6758Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.6754Epoch 3/15: [============================  ] 56/60 batches, loss: 0.6745Epoch 3/15: [============================  ] 57/60 batches, loss: 0.6743Epoch 3/15: [============================= ] 58/60 batches, loss: 0.6746Epoch 3/15: [============================= ] 59/60 batches, loss: 0.6749Epoch 3/15: [==============================] 60/60 batches, loss: 0.6753
[2025-05-02 11:01:14,363][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6753
[2025-05-02 11:01:14,635][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6785, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.6281Epoch 4/15: [=                             ] 2/60 batches, loss: 0.6277Epoch 4/15: [=                             ] 3/60 batches, loss: 0.6486Epoch 4/15: [==                            ] 4/60 batches, loss: 0.6466Epoch 4/15: [==                            ] 5/60 batches, loss: 0.6589Epoch 4/15: [===                           ] 6/60 batches, loss: 0.6631Epoch 4/15: [===                           ] 7/60 batches, loss: 0.6641Epoch 4/15: [====                          ] 8/60 batches, loss: 0.6644Epoch 4/15: [====                          ] 9/60 batches, loss: 0.6632Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.6631Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.6683Epoch 4/15: [======                        ] 12/60 batches, loss: 0.6699Epoch 4/15: [======                        ] 13/60 batches, loss: 0.6710Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.6674Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.6660Epoch 4/15: [========                      ] 16/60 batches, loss: 0.6676Epoch 4/15: [========                      ] 17/60 batches, loss: 0.6657Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.6644Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.6607Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.6584Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.6604Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.6596Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.6606Epoch 4/15: [============                  ] 24/60 batches, loss: 0.6635Epoch 4/15: [============                  ] 25/60 batches, loss: 0.6624Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.6645Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.6631Epoch 4/15: [==============                ] 28/60 batches, loss: 0.6645Epoch 4/15: [==============                ] 29/60 batches, loss: 0.6633Epoch 4/15: [===============               ] 30/60 batches, loss: 0.6644Epoch 4/15: [===============               ] 31/60 batches, loss: 0.6639Epoch 4/15: [================              ] 32/60 batches, loss: 0.6632Epoch 4/15: [================              ] 33/60 batches, loss: 0.6603Epoch 4/15: [=================             ] 34/60 batches, loss: 0.6599Epoch 4/15: [=================             ] 35/60 batches, loss: 0.6600Epoch 4/15: [==================            ] 36/60 batches, loss: 0.6624Epoch 4/15: [==================            ] 37/60 batches, loss: 0.6633Epoch 4/15: [===================           ] 38/60 batches, loss: 0.6631Epoch 4/15: [===================           ] 39/60 batches, loss: 0.6649Epoch 4/15: [====================          ] 40/60 batches, loss: 0.6659Epoch 4/15: [====================          ] 41/60 batches, loss: 0.6647Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.6645Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.6632Epoch 4/15: [======================        ] 44/60 batches, loss: 0.6625Epoch 4/15: [======================        ] 45/60 batches, loss: 0.6623Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.6623Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.6617Epoch 4/15: [========================      ] 48/60 batches, loss: 0.6612Epoch 4/15: [========================      ] 49/60 batches, loss: 0.6611Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.6609Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.6605Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.6600Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.6600Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.6598Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.6588Epoch 4/15: [============================  ] 56/60 batches, loss: 0.6588Epoch 4/15: [============================  ] 57/60 batches, loss: 0.6577Epoch 4/15: [============================= ] 58/60 batches, loss: 0.6583Epoch 4/15: [============================= ] 59/60 batches, loss: 0.6592Epoch 4/15: [==============================] 60/60 batches, loss: 0.6572
[2025-05-02 11:01:16,866][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6572
[2025-05-02 11:01:17,137][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6548, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7733333333333333, 'precision': 0.7435897435897436, 'recall': 0.8055555555555556}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.6585Epoch 5/15: [=                             ] 2/60 batches, loss: 0.6339Epoch 5/15: [=                             ] 3/60 batches, loss: 0.6507Epoch 5/15: [==                            ] 4/60 batches, loss: 0.6315Epoch 5/15: [==                            ] 5/60 batches, loss: 0.6410Epoch 5/15: [===                           ] 6/60 batches, loss: 0.6341Epoch 5/15: [===                           ] 7/60 batches, loss: 0.6285Epoch 5/15: [====                          ] 8/60 batches, loss: 0.6361Epoch 5/15: [====                          ] 9/60 batches, loss: 0.6320Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.6266Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.6295Epoch 5/15: [======                        ] 12/60 batches, loss: 0.6282Epoch 5/15: [======                        ] 13/60 batches, loss: 0.6383Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.6362Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.6356Epoch 5/15: [========                      ] 16/60 batches, loss: 0.6379Epoch 5/15: [========                      ] 17/60 batches, loss: 0.6384Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.6422Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.6422Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.6451Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.6460Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.6430Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.6427Epoch 5/15: [============                  ] 24/60 batches, loss: 0.6424Epoch 5/15: [============                  ] 25/60 batches, loss: 0.6425Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.6400Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.6416Epoch 5/15: [==============                ] 28/60 batches, loss: 0.6417Epoch 5/15: [==============                ] 29/60 batches, loss: 0.6417Epoch 5/15: [===============               ] 30/60 batches, loss: 0.6424Epoch 5/15: [===============               ] 31/60 batches, loss: 0.6417Epoch 5/15: [================              ] 32/60 batches, loss: 0.6429Epoch 5/15: [================              ] 33/60 batches, loss: 0.6424Epoch 5/15: [=================             ] 34/60 batches, loss: 0.6412Epoch 5/15: [=================             ] 35/60 batches, loss: 0.6405Epoch 5/15: [==================            ] 36/60 batches, loss: 0.6401Epoch 5/15: [==================            ] 37/60 batches, loss: 0.6414Epoch 5/15: [===================           ] 38/60 batches, loss: 0.6417Epoch 5/15: [===================           ] 39/60 batches, loss: 0.6417Epoch 5/15: [====================          ] 40/60 batches, loss: 0.6414Epoch 5/15: [====================          ] 41/60 batches, loss: 0.6411Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.6422Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.6424Epoch 5/15: [======================        ] 44/60 batches, loss: 0.6419Epoch 5/15: [======================        ] 45/60 batches, loss: 0.6396Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.6396Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.6377Epoch 5/15: [========================      ] 48/60 batches, loss: 0.6364Epoch 5/15: [========================      ] 49/60 batches, loss: 0.6343Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.6348Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.6362Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.6364Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.6353Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.6362Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.6359Epoch 5/15: [============================  ] 56/60 batches, loss: 0.6352Epoch 5/15: [============================  ] 57/60 batches, loss: 0.6355Epoch 5/15: [============================= ] 58/60 batches, loss: 0.6330Epoch 5/15: [============================= ] 59/60 batches, loss: 0.6334Epoch 5/15: [==============================] 60/60 batches, loss: 0.6320
[2025-05-02 11:01:19,367][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6320
[2025-05-02 11:01:19,638][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6628, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7951807228915663, 'precision': 0.7021276595744681, 'recall': 0.9166666666666666}
[2025-05-02 11:01:19,639][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.6297Epoch 6/15: [=                             ] 2/60 batches, loss: 0.6097Epoch 6/15: [=                             ] 3/60 batches, loss: 0.6088Epoch 6/15: [==                            ] 4/60 batches, loss: 0.6238Epoch 6/15: [==                            ] 5/60 batches, loss: 0.6211Epoch 6/15: [===                           ] 6/60 batches, loss: 0.6139Epoch 6/15: [===                           ] 7/60 batches, loss: 0.6077Epoch 6/15: [====                          ] 8/60 batches, loss: 0.6034Epoch 6/15: [====                          ] 9/60 batches, loss: 0.6079Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.6052Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.6085Epoch 6/15: [======                        ] 12/60 batches, loss: 0.6176Epoch 6/15: [======                        ] 13/60 batches, loss: 0.6130Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.6125Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.6166Epoch 6/15: [========                      ] 16/60 batches, loss: 0.6144Epoch 6/15: [========                      ] 17/60 batches, loss: 0.6108Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.6140Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.6182Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.6182Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.6177Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.6175Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.6190Epoch 6/15: [============                  ] 24/60 batches, loss: 0.6208Epoch 6/15: [============                  ] 25/60 batches, loss: 0.6219Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.6228Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.6215Epoch 6/15: [==============                ] 28/60 batches, loss: 0.6217Epoch 6/15: [==============                ] 29/60 batches, loss: 0.6238Epoch 6/15: [===============               ] 30/60 batches, loss: 0.6235Epoch 6/15: [===============               ] 31/60 batches, loss: 0.6230Epoch 6/15: [================              ] 32/60 batches, loss: 0.6236Epoch 6/15: [================              ] 33/60 batches, loss: 0.6215Epoch 6/15: [=================             ] 34/60 batches, loss: 0.6192Epoch 6/15: [=================             ] 35/60 batches, loss: 0.6188Epoch 6/15: [==================            ] 36/60 batches, loss: 0.6180Epoch 6/15: [==================            ] 37/60 batches, loss: 0.6199Epoch 6/15: [===================           ] 38/60 batches, loss: 0.6185Epoch 6/15: [===================           ] 39/60 batches, loss: 0.6204Epoch 6/15: [====================          ] 40/60 batches, loss: 0.6199Epoch 6/15: [====================          ] 41/60 batches, loss: 0.6198Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.6218Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.6226Epoch 6/15: [======================        ] 44/60 batches, loss: 0.6219Epoch 6/15: [======================        ] 45/60 batches, loss: 0.6198Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.6183Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.6175Epoch 6/15: [========================      ] 48/60 batches, loss: 0.6187Epoch 6/15: [========================      ] 49/60 batches, loss: 0.6179Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.6179Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.6178Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.6166Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.6169Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.6179Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.6176Epoch 6/15: [============================  ] 56/60 batches, loss: 0.6178Epoch 6/15: [============================  ] 57/60 batches, loss: 0.6181Epoch 6/15: [============================= ] 58/60 batches, loss: 0.6184Epoch 6/15: [============================= ] 59/60 batches, loss: 0.6182Epoch 6/15: [==============================] 60/60 batches, loss: 0.6176
[2025-05-02 11:01:21,495][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6176
[2025-05-02 11:01:21,765][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6363, Metrics: {'accuracy': 0.75, 'f1': 0.775, 'precision': 0.7045454545454546, 'recall': 0.8611111111111112}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.5868Epoch 7/15: [=                             ] 2/60 batches, loss: 0.5927Epoch 7/15: [=                             ] 3/60 batches, loss: 0.5874Epoch 7/15: [==                            ] 4/60 batches, loss: 0.5966Epoch 7/15: [==                            ] 5/60 batches, loss: 0.6054Epoch 7/15: [===                           ] 6/60 batches, loss: 0.5999Epoch 7/15: [===                           ] 7/60 batches, loss: 0.6024Epoch 7/15: [====                          ] 8/60 batches, loss: 0.6083Epoch 7/15: [====                          ] 9/60 batches, loss: 0.6106Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.6103Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.6103Epoch 7/15: [======                        ] 12/60 batches, loss: 0.6104Epoch 7/15: [======                        ] 13/60 batches, loss: 0.6100Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.6038Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.6047Epoch 7/15: [========                      ] 16/60 batches, loss: 0.5964Epoch 7/15: [========                      ] 17/60 batches, loss: 0.5986Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.5962Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.5911Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.5932Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.5961Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.5987Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.5949Epoch 7/15: [============                  ] 24/60 batches, loss: 0.5949Epoch 7/15: [============                  ] 25/60 batches, loss: 0.5950Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.5908Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.5922Epoch 7/15: [==============                ] 28/60 batches, loss: 0.5913Epoch 7/15: [==============                ] 29/60 batches, loss: 0.5908Epoch 7/15: [===============               ] 30/60 batches, loss: 0.5937Epoch 7/15: [===============               ] 31/60 batches, loss: 0.5961Epoch 7/15: [================              ] 32/60 batches, loss: 0.5953Epoch 7/15: [================              ] 33/60 batches, loss: 0.5940Epoch 7/15: [=================             ] 34/60 batches, loss: 0.5949Epoch 7/15: [=================             ] 35/60 batches, loss: 0.5959Epoch 7/15: [==================            ] 36/60 batches, loss: 0.5944Epoch 7/15: [==================            ] 37/60 batches, loss: 0.5951Epoch 7/15: [===================           ] 38/60 batches, loss: 0.5954Epoch 7/15: [===================           ] 39/60 batches, loss: 0.5943Epoch 7/15: [====================          ] 40/60 batches, loss: 0.5933Epoch 7/15: [====================          ] 41/60 batches, loss: 0.5944Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.5948Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.5929Epoch 7/15: [======================        ] 44/60 batches, loss: 0.5925Epoch 7/15: [======================        ] 45/60 batches, loss: 0.5920Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.5917Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.5913Epoch 7/15: [========================      ] 48/60 batches, loss: 0.5917Epoch 7/15: [========================      ] 49/60 batches, loss: 0.5915Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.5917Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.5927Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.5931Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.5929Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.5926Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.5924Epoch 7/15: [============================  ] 56/60 batches, loss: 0.5931Epoch 7/15: [============================  ] 57/60 batches, loss: 0.5939Epoch 7/15: [============================= ] 58/60 batches, loss: 0.5940Epoch 7/15: [============================= ] 59/60 batches, loss: 0.5942Epoch 7/15: [==============================] 60/60 batches, loss: 0.5930
[2025-05-02 11:01:24,009][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5930
[2025-05-02 11:01:24,275][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6351, Metrics: {'accuracy': 0.75, 'f1': 0.775, 'precision': 0.7045454545454546, 'recall': 0.8611111111111112}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.5661Epoch 8/15: [=                             ] 2/60 batches, loss: 0.5882Epoch 8/15: [=                             ] 3/60 batches, loss: 0.5897Epoch 8/15: [==                            ] 4/60 batches, loss: 0.5898Epoch 8/15: [==                            ] 5/60 batches, loss: 0.6007Epoch 8/15: [===                           ] 6/60 batches, loss: 0.6012Epoch 8/15: [===                           ] 7/60 batches, loss: 0.5899Epoch 8/15: [====                          ] 8/60 batches, loss: 0.5990Epoch 8/15: [====                          ] 9/60 batches, loss: 0.6032Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.5984Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.5981Epoch 8/15: [======                        ] 12/60 batches, loss: 0.5970Epoch 8/15: [======                        ] 13/60 batches, loss: 0.5970Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.5946Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.5950Epoch 8/15: [========                      ] 16/60 batches, loss: 0.5892Epoch 8/15: [========                      ] 17/60 batches, loss: 0.5852Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.5885Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.5831Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.5857Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.5842Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.5848Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.5880Epoch 8/15: [============                  ] 24/60 batches, loss: 0.5904Epoch 8/15: [============                  ] 25/60 batches, loss: 0.5903Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.5885Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.5867Epoch 8/15: [==============                ] 28/60 batches, loss: 0.5905Epoch 8/15: [==============                ] 29/60 batches, loss: 0.5906Epoch 8/15: [===============               ] 30/60 batches, loss: 0.5904Epoch 8/15: [===============               ] 31/60 batches, loss: 0.5907Epoch 8/15: [================              ] 32/60 batches, loss: 0.5914Epoch 8/15: [================              ] 33/60 batches, loss: 0.5917Epoch 8/15: [=================             ] 34/60 batches, loss: 0.5906Epoch 8/15: [=================             ] 35/60 batches, loss: 0.5905Epoch 8/15: [==================            ] 36/60 batches, loss: 0.5904Epoch 8/15: [==================            ] 37/60 batches, loss: 0.5911Epoch 8/15: [===================           ] 38/60 batches, loss: 0.5894Epoch 8/15: [===================           ] 39/60 batches, loss: 0.5900Epoch 8/15: [====================          ] 40/60 batches, loss: 0.5898Epoch 8/15: [====================          ] 41/60 batches, loss: 0.5895Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.5888Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.5889Epoch 8/15: [======================        ] 44/60 batches, loss: 0.5882Epoch 8/15: [======================        ] 45/60 batches, loss: 0.5886Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.5905Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.5900Epoch 8/15: [========================      ] 48/60 batches, loss: 0.5899Epoch 8/15: [========================      ] 49/60 batches, loss: 0.5905Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.5914Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.5920Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.5904Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.5898Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.5897Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.5910Epoch 8/15: [============================  ] 56/60 batches, loss: 0.5910Epoch 8/15: [============================  ] 57/60 batches, loss: 0.5912Epoch 8/15: [============================= ] 58/60 batches, loss: 0.5922Epoch 8/15: [============================= ] 59/60 batches, loss: 0.5908Epoch 8/15: [==============================] 60/60 batches, loss: 0.5901
[2025-05-02 11:01:26,594][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5901
[2025-05-02 11:01:26,872][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6307, Metrics: {'accuracy': 0.75, 'f1': 0.7692307692307693, 'precision': 0.7142857142857143, 'recall': 0.8333333333333334}
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.5863Epoch 9/15: [=                             ] 2/60 batches, loss: 0.5710Epoch 9/15: [=                             ] 3/60 batches, loss: 0.5870Epoch 9/15: [==                            ] 4/60 batches, loss: 0.5915Epoch 9/15: [==                            ] 5/60 batches, loss: 0.5807Epoch 9/15: [===                           ] 6/60 batches, loss: 0.5744Epoch 9/15: [===                           ] 7/60 batches, loss: 0.5765Epoch 9/15: [====                          ] 8/60 batches, loss: 0.5725Epoch 9/15: [====                          ] 9/60 batches, loss: 0.5655Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.5642Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.5691Epoch 9/15: [======                        ] 12/60 batches, loss: 0.5745Epoch 9/15: [======                        ] 13/60 batches, loss: 0.5791Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.5767Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.5785Epoch 9/15: [========                      ] 16/60 batches, loss: 0.5835Epoch 9/15: [========                      ] 17/60 batches, loss: 0.5843Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.5865Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.5862Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.5874Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.5920Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.5906Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.5941Epoch 9/15: [============                  ] 24/60 batches, loss: 0.5943Epoch 9/15: [============                  ] 25/60 batches, loss: 0.5954Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.5975Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.5953Epoch 9/15: [==============                ] 28/60 batches, loss: 0.5964Epoch 9/15: [==============                ] 29/60 batches, loss: 0.5951Epoch 9/15: [===============               ] 30/60 batches, loss: 0.5931Epoch 9/15: [===============               ] 31/60 batches, loss: 0.5914Epoch 9/15: [================              ] 32/60 batches, loss: 0.5924Epoch 9/15: [================              ] 33/60 batches, loss: 0.5891Epoch 9/15: [=================             ] 34/60 batches, loss: 0.5896Epoch 9/15: [=================             ] 35/60 batches, loss: 0.5902Epoch 9/15: [==================            ] 36/60 batches, loss: 0.5889Epoch 9/15: [==================            ] 37/60 batches, loss: 0.5877Epoch 9/15: [===================           ] 38/60 batches, loss: 0.5858Epoch 9/15: [===================           ] 39/60 batches, loss: 0.5824Epoch 9/15: [====================          ] 40/60 batches, loss: 0.5818Epoch 9/15: [====================          ] 41/60 batches, loss: 0.5830Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.5829Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.5816Epoch 9/15: [======================        ] 44/60 batches, loss: 0.5824Epoch 9/15: [======================        ] 45/60 batches, loss: 0.5824Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.5822Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.5814Epoch 9/15: [========================      ] 48/60 batches, loss: 0.5820Epoch 9/15: [========================      ] 49/60 batches, loss: 0.5831Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.5820Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.5832Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.5824Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.5813Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.5804Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.5800Epoch 9/15: [============================  ] 56/60 batches, loss: 0.5794Epoch 9/15: [============================  ] 57/60 batches, loss: 0.5795Epoch 9/15: [============================= ] 58/60 batches, loss: 0.5797Epoch 9/15: [============================= ] 59/60 batches, loss: 0.5781Epoch 9/15: [==============================] 60/60 batches, loss: 0.5801
[2025-05-02 11:01:29,105][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5801
[2025-05-02 11:01:29,389][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6413, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7901234567901234, 'precision': 0.7111111111111111, 'recall': 0.8888888888888888}
[2025-05-02 11:01:29,389][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.6015Epoch 10/15: [=                             ] 2/60 batches, loss: 0.5528Epoch 10/15: [=                             ] 3/60 batches, loss: 0.5505Epoch 10/15: [==                            ] 4/60 batches, loss: 0.5466Epoch 10/15: [==                            ] 5/60 batches, loss: 0.5450Epoch 10/15: [===                           ] 6/60 batches, loss: 0.5522Epoch 10/15: [===                           ] 7/60 batches, loss: 0.5540Epoch 10/15: [====                          ] 8/60 batches, loss: 0.5616Epoch 10/15: [====                          ] 9/60 batches, loss: 0.5598Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.5675Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.5659Epoch 10/15: [======                        ] 12/60 batches, loss: 0.5661Epoch 10/15: [======                        ] 13/60 batches, loss: 0.5675Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.5706Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.5709Epoch 10/15: [========                      ] 16/60 batches, loss: 0.5762Epoch 10/15: [========                      ] 17/60 batches, loss: 0.5767Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.5781Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.5786Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.5783Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.5784Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.5788Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.5762Epoch 10/15: [============                  ] 24/60 batches, loss: 0.5761Epoch 10/15: [============                  ] 25/60 batches, loss: 0.5767Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.5763Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.5798Epoch 10/15: [==============                ] 28/60 batches, loss: 0.5798Epoch 10/15: [==============                ] 29/60 batches, loss: 0.5782Epoch 10/15: [===============               ] 30/60 batches, loss: 0.5750Epoch 10/15: [===============               ] 31/60 batches, loss: 0.5722Epoch 10/15: [================              ] 32/60 batches, loss: 0.5716Epoch 10/15: [================              ] 33/60 batches, loss: 0.5720Epoch 10/15: [=================             ] 34/60 batches, loss: 0.5729Epoch 10/15: [=================             ] 35/60 batches, loss: 0.5745Epoch 10/15: [==================            ] 36/60 batches, loss: 0.5746Epoch 10/15: [==================            ] 37/60 batches, loss: 0.5734Epoch 10/15: [===================           ] 38/60 batches, loss: 0.5726Epoch 10/15: [===================           ] 39/60 batches, loss: 0.5707Epoch 10/15: [====================          ] 40/60 batches, loss: 0.5702Epoch 10/15: [====================          ] 41/60 batches, loss: 0.5695Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.5690Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.5699Epoch 10/15: [======================        ] 44/60 batches, loss: 0.5696Epoch 10/15: [======================        ] 45/60 batches, loss: 0.5695Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.5698Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.5703Epoch 10/15: [========================      ] 48/60 batches, loss: 0.5728Epoch 10/15: [========================      ] 49/60 batches, loss: 0.5744Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.5753Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.5755Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.5750Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.5747Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.5737Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.5741Epoch 10/15: [============================  ] 56/60 batches, loss: 0.5748Epoch 10/15: [============================  ] 57/60 batches, loss: 0.5742Epoch 10/15: [============================= ] 58/60 batches, loss: 0.5742Epoch 10/15: [============================= ] 59/60 batches, loss: 0.5737Epoch 10/15: [==============================] 60/60 batches, loss: 0.5757
[2025-05-02 11:01:31,259][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5757
[2025-05-02 11:01:31,543][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6256, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7792207792207793, 'precision': 0.7317073170731707, 'recall': 0.8333333333333334}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.5064Epoch 11/15: [=                             ] 2/60 batches, loss: 0.5924Epoch 11/15: [=                             ] 3/60 batches, loss: 0.5921Epoch 11/15: [==                            ] 4/60 batches, loss: 0.5772Epoch 11/15: [==                            ] 5/60 batches, loss: 0.5784Epoch 11/15: [===                           ] 6/60 batches, loss: 0.5708Epoch 11/15: [===                           ] 7/60 batches, loss: 0.5660Epoch 11/15: [====                          ] 8/60 batches, loss: 0.5634Epoch 11/15: [====                          ] 9/60 batches, loss: 0.5549Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.5562Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.5559Epoch 11/15: [======                        ] 12/60 batches, loss: 0.5587Epoch 11/15: [======                        ] 13/60 batches, loss: 0.5643Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.5603Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.5639Epoch 11/15: [========                      ] 16/60 batches, loss: 0.5684Epoch 11/15: [========                      ] 17/60 batches, loss: 0.5708Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.5713Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.5792Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.5834Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.5787Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.5789Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.5768Epoch 11/15: [============                  ] 24/60 batches, loss: 0.5764Epoch 11/15: [============                  ] 25/60 batches, loss: 0.5753Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.5768Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.5756Epoch 11/15: [==============                ] 28/60 batches, loss: 0.5766Epoch 11/15: [==============                ] 29/60 batches, loss: 0.5777Epoch 11/15: [===============               ] 30/60 batches, loss: 0.5746Epoch 11/15: [===============               ] 31/60 batches, loss: 0.5741Epoch 11/15: [================              ] 32/60 batches, loss: 0.5749Epoch 11/15: [================              ] 33/60 batches, loss: 0.5724Epoch 11/15: [=================             ] 34/60 batches, loss: 0.5726Epoch 11/15: [=================             ] 35/60 batches, loss: 0.5706Epoch 11/15: [==================            ] 36/60 batches, loss: 0.5690Epoch 11/15: [==================            ] 37/60 batches, loss: 0.5686Epoch 11/15: [===================           ] 38/60 batches, loss: 0.5704Epoch 11/15: [===================           ] 39/60 batches, loss: 0.5701Epoch 11/15: [====================          ] 40/60 batches, loss: 0.5695Epoch 11/15: [====================          ] 41/60 batches, loss: 0.5708Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.5724Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.5728Epoch 11/15: [======================        ] 44/60 batches, loss: 0.5732Epoch 11/15: [======================        ] 45/60 batches, loss: 0.5710Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.5707Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.5686Epoch 11/15: [========================      ] 48/60 batches, loss: 0.5682Epoch 11/15: [========================      ] 49/60 batches, loss: 0.5688Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.5684Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.5678Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.5673Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.5705Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.5713Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.5725Epoch 11/15: [============================  ] 56/60 batches, loss: 0.5731Epoch 11/15: [============================  ] 57/60 batches, loss: 0.5723Epoch 11/15: [============================= ] 58/60 batches, loss: 0.5735Epoch 11/15: [============================= ] 59/60 batches, loss: 0.5728Epoch 11/15: [==============================] 60/60 batches, loss: 0.5720
[2025-05-02 11:01:33,797][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5720
[2025-05-02 11:01:34,078][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6185, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7894736842105263, 'precision': 0.75, 'recall': 0.8333333333333334}
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.4817Epoch 12/15: [=                             ] 2/60 batches, loss: 0.5116Epoch 12/15: [=                             ] 3/60 batches, loss: 0.5234Epoch 12/15: [==                            ] 4/60 batches, loss: 0.5282Epoch 12/15: [==                            ] 5/60 batches, loss: 0.5317Epoch 12/15: [===                           ] 6/60 batches, loss: 0.5362Epoch 12/15: [===                           ] 7/60 batches, loss: 0.5464Epoch 12/15: [====                          ] 8/60 batches, loss: 0.5532Epoch 12/15: [====                          ] 9/60 batches, loss: 0.5500Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.5470Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.5598Epoch 12/15: [======                        ] 12/60 batches, loss: 0.5615Epoch 12/15: [======                        ] 13/60 batches, loss: 0.5587Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.5612Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.5626Epoch 12/15: [========                      ] 16/60 batches, loss: 0.5651Epoch 12/15: [========                      ] 17/60 batches, loss: 0.5623Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.5589Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.5606Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.5617Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.5636Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.5642Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.5603Epoch 12/15: [============                  ] 24/60 batches, loss: 0.5621Epoch 12/15: [============                  ] 25/60 batches, loss: 0.5614Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.5615Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.5628Epoch 12/15: [==============                ] 28/60 batches, loss: 0.5610Epoch 12/15: [==============                ] 29/60 batches, loss: 0.5615Epoch 12/15: [===============               ] 30/60 batches, loss: 0.5615Epoch 12/15: [===============               ] 31/60 batches, loss: 0.5624Epoch 12/15: [================              ] 32/60 batches, loss: 0.5622Epoch 12/15: [================              ] 33/60 batches, loss: 0.5599Epoch 12/15: [=================             ] 34/60 batches, loss: 0.5585Epoch 12/15: [=================             ] 35/60 batches, loss: 0.5594Epoch 12/15: [==================            ] 36/60 batches, loss: 0.5601Epoch 12/15: [==================            ] 37/60 batches, loss: 0.5585Epoch 12/15: [===================           ] 38/60 batches, loss: 0.5584Epoch 12/15: [===================           ] 39/60 batches, loss: 0.5605Epoch 12/15: [====================          ] 40/60 batches, loss: 0.5638Epoch 12/15: [====================          ] 41/60 batches, loss: 0.5645Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.5683Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.5671Epoch 12/15: [======================        ] 44/60 batches, loss: 0.5651Epoch 12/15: [======================        ] 45/60 batches, loss: 0.5649Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.5646Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.5642Epoch 12/15: [========================      ] 48/60 batches, loss: 0.5645Epoch 12/15: [========================      ] 49/60 batches, loss: 0.5657Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.5649Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.5647Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.5645Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.5646Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.5653Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.5643Epoch 12/15: [============================  ] 56/60 batches, loss: 0.5652Epoch 12/15: [============================  ] 57/60 batches, loss: 0.5653Epoch 12/15: [============================= ] 58/60 batches, loss: 0.5658Epoch 12/15: [============================= ] 59/60 batches, loss: 0.5673Epoch 12/15: [==============================] 60/60 batches, loss: 0.5667
[2025-05-02 11:01:36,352][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5667
[2025-05-02 11:01:36,632][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6228, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7792207792207793, 'precision': 0.7317073170731707, 'recall': 0.8333333333333334}
[2025-05-02 11:01:36,633][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.5252Epoch 13/15: [=                             ] 2/60 batches, loss: 0.5212Epoch 13/15: [=                             ] 3/60 batches, loss: 0.5298Epoch 13/15: [==                            ] 4/60 batches, loss: 0.5339Epoch 13/15: [==                            ] 5/60 batches, loss: 0.5335Epoch 13/15: [===                           ] 6/60 batches, loss: 0.5290Epoch 13/15: [===                           ] 7/60 batches, loss: 0.5325Epoch 13/15: [====                          ] 8/60 batches, loss: 0.5342Epoch 13/15: [====                          ] 9/60 batches, loss: 0.5399Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.5494Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.5584Epoch 13/15: [======                        ] 12/60 batches, loss: 0.5605Epoch 13/15: [======                        ] 13/60 batches, loss: 0.5583Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.5546Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.5522Epoch 13/15: [========                      ] 16/60 batches, loss: 0.5507Epoch 13/15: [========                      ] 17/60 batches, loss: 0.5537Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.5507Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.5544Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.5545Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.5518Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.5568Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.5514Epoch 13/15: [============                  ] 24/60 batches, loss: 0.5577Epoch 13/15: [============                  ] 25/60 batches, loss: 0.5613Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.5603Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.5620Epoch 13/15: [==============                ] 28/60 batches, loss: 0.5612Epoch 13/15: [==============                ] 29/60 batches, loss: 0.5665Epoch 13/15: [===============               ] 30/60 batches, loss: 0.5666Epoch 13/15: [===============               ] 31/60 batches, loss: 0.5647Epoch 13/15: [================              ] 32/60 batches, loss: 0.5654Epoch 13/15: [================              ] 33/60 batches, loss: 0.5644Epoch 13/15: [=================             ] 34/60 batches, loss: 0.5657Epoch 13/15: [=================             ] 35/60 batches, loss: 0.5681Epoch 13/15: [==================            ] 36/60 batches, loss: 0.5665Epoch 13/15: [==================            ] 37/60 batches, loss: 0.5678Epoch 13/15: [===================           ] 38/60 batches, loss: 0.5682Epoch 13/15: [===================           ] 39/60 batches, loss: 0.5684Epoch 13/15: [====================          ] 40/60 batches, loss: 0.5671Epoch 13/15: [====================          ] 41/60 batches, loss: 0.5676Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.5680Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.5671Epoch 13/15: [======================        ] 44/60 batches, loss: 0.5677Epoch 13/15: [======================        ] 45/60 batches, loss: 0.5688Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.5682Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.5678Epoch 13/15: [========================      ] 48/60 batches, loss: 0.5685Epoch 13/15: [========================      ] 49/60 batches, loss: 0.5680Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.5683Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.5671Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.5653Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.5642Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.5646Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.5647Epoch 13/15: [============================  ] 56/60 batches, loss: 0.5640Epoch 13/15: [============================  ] 57/60 batches, loss: 0.5647Epoch 13/15: [============================= ] 58/60 batches, loss: 0.5650Epoch 13/15: [============================= ] 59/60 batches, loss: 0.5634Epoch 13/15: [==============================] 60/60 batches, loss: 0.5624
[2025-05-02 11:01:38,506][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5624
[2025-05-02 11:01:38,793][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.6308, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.7848101265822784, 'precision': 0.7209302325581395, 'recall': 0.8611111111111112}
[2025-05-02 11:01:38,793][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.6754Epoch 14/15: [=                             ] 2/60 batches, loss: 0.6308Epoch 14/15: [=                             ] 3/60 batches, loss: 0.6268Epoch 14/15: [==                            ] 4/60 batches, loss: 0.6106Epoch 14/15: [==                            ] 5/60 batches, loss: 0.5963Epoch 14/15: [===                           ] 6/60 batches, loss: 0.6100Epoch 14/15: [===                           ] 7/60 batches, loss: 0.6010Epoch 14/15: [====                          ] 8/60 batches, loss: 0.6057Epoch 14/15: [====                          ] 9/60 batches, loss: 0.6034Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.6013Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.5983Epoch 14/15: [======                        ] 12/60 batches, loss: 0.5942Epoch 14/15: [======                        ] 13/60 batches, loss: 0.5913Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.5888Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.5845Epoch 14/15: [========                      ] 16/60 batches, loss: 0.5883Epoch 14/15: [========                      ] 17/60 batches, loss: 0.5827Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.5880Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.5896Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.5859Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.5869Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.5856Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.5824Epoch 14/15: [============                  ] 24/60 batches, loss: 0.5819Epoch 14/15: [============                  ] 25/60 batches, loss: 0.5815Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.5826Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.5791Epoch 14/15: [==============                ] 28/60 batches, loss: 0.5816Epoch 14/15: [==============                ] 29/60 batches, loss: 0.5806Epoch 14/15: [===============               ] 30/60 batches, loss: 0.5789Epoch 14/15: [===============               ] 31/60 batches, loss: 0.5771Epoch 14/15: [================              ] 32/60 batches, loss: 0.5770Epoch 14/15: [================              ] 33/60 batches, loss: 0.5787Epoch 14/15: [=================             ] 34/60 batches, loss: 0.5778Epoch 14/15: [=================             ] 35/60 batches, loss: 0.5793Epoch 14/15: [==================            ] 36/60 batches, loss: 0.5774Epoch 14/15: [==================            ] 37/60 batches, loss: 0.5771Epoch 14/15: [===================           ] 38/60 batches, loss: 0.5742Epoch 14/15: [===================           ] 39/60 batches, loss: 0.5728Epoch 14/15: [====================          ] 40/60 batches, loss: 0.5725Epoch 14/15: [====================          ] 41/60 batches, loss: 0.5712Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.5714Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.5707Epoch 14/15: [======================        ] 44/60 batches, loss: 0.5717Epoch 14/15: [======================        ] 45/60 batches, loss: 0.5716Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.5713Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.5712Epoch 14/15: [========================      ] 48/60 batches, loss: 0.5723Epoch 14/15: [========================      ] 49/60 batches, loss: 0.5737Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.5725Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.5717Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.5689Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.5703Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.5711Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.5710Epoch 14/15: [============================  ] 56/60 batches, loss: 0.5706Epoch 14/15: [============================  ] 57/60 batches, loss: 0.5701Epoch 14/15: [============================= ] 58/60 batches, loss: 0.5710Epoch 14/15: [============================= ] 59/60 batches, loss: 0.5713Epoch 14/15: [==============================] 60/60 batches, loss: 0.5696
[2025-05-02 11:01:40,665][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5696
[2025-05-02 11:01:40,936][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.6355, Metrics: {'accuracy': 0.75, 'f1': 0.775, 'precision': 0.7045454545454546, 'recall': 0.8611111111111112}
[2025-05-02 11:01:40,937][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:01:40,937][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 14
[2025-05-02 11:01:40,937][src.training.lm_trainer][INFO] - Training completed in 34.40 seconds
[2025-05-02 11:01:40,937][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:01:43,494][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9622641509433962, 'f1': 0.9612068965517241, 'precision': 0.9469214437367304, 'recall': 0.975929978118162}
[2025-05-02 11:01:43,495][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7894736842105263, 'precision': 0.75, 'recall': 0.8333333333333334}
[2025-05-02 11:01:43,495][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6090909090909091, 'f1': 0.6260869565217392, 'precision': 0.6, 'recall': 0.6545454545454545}
[2025-05-02 11:01:45,177][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/id/model.pt
[2025-05-02 11:01:45,179][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁█▇▇▇██
wandb:           best_val_f1 ▁▁▁██████
wandb:         best_val_loss ██▇▄▃▃▂▂▁
wandb:    best_val_precision ▁▁▁██████
wandb:       best_val_recall ▁▁▁██████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▂▂▂▂▂▂▂▂▂▂
wandb:            train_loss █▇▇▆▅▄▃▂▂▂▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁██▇▇▇█████▇
wandb:                val_f1 ▁▁▁███████████
wandb:              val_loss ██▇▄▅▃▃▂▃▂▁▁▂▃
wandb:         val_precision ▁▁▁███████████
wandb:            val_recall ▁▁▁▇███▇█▇▇▇██
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.77778
wandb:           best_val_f1 0.78947
wandb:         best_val_loss 0.61849
wandb:    best_val_precision 0.75
wandb:       best_val_recall 0.83333
wandb:      early_stop_epoch 14
wandb:                 epoch 14
wandb:   final_test_accuracy 0.60909
wandb:         final_test_f1 0.62609
wandb:  final_test_precision 0.6
wandb:     final_test_recall 0.65455
wandb:  final_train_accuracy 0.96226
wandb:        final_train_f1 0.96121
wandb: final_train_precision 0.94692
wandb:    final_train_recall 0.97593
wandb:    final_val_accuracy 0.77778
wandb:          final_val_f1 0.78947
wandb:   final_val_precision 0.75
wandb:      final_val_recall 0.83333
wandb:         learning_rate 0.0001
wandb:            train_loss 0.56963
wandb:            train_time 34.39926
wandb:          val_accuracy 0.75
wandb:                val_f1 0.775
wandb:              val_loss 0.63554
wandb:         val_precision 0.70455
wandb:            val_recall 0.86111
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110056-67kk2vbm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110056-67kk2vbm/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/id/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:01:57,538][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:01:57,538][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:01:57,539][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:01:57,539][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:01:57,543][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-02 11:01:57,543][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:01:59,163][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:02:01,366][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:02:01,367][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:02:01,431][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,460][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,557][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-02 11:02:01,564][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:02:01,564][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-02 11:02:01,565][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:02:01,587][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,634][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-02 11:02:01,635][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:02:01,635][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-02 11:02:01,637][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:02:01,668][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,709][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:02:01,723][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-02 11:02:01,724][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:02:01,724][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-02 11:02:01,725][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-02 11:02:01,726][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:02:01,726][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:02:01,727][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:02:01,727][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:02:01,727][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:02:01,728][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:02:01,728][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:02:01,728][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:02:01,729][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:02:01,729][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-02 11:02:01,729][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-02 11:02:01,729][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-02 11:02:01,729][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-02 11:02:01,729][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:02:01,729][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:02:01,729][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:02:01,730][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:02:06,135][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:02:06,136][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:02:06,136][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 11:02:06,136][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:02:06,140][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:02:06,141][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:02:06,141][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:02:06,141][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:02:06,141][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-02 11:02:06,142][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:02:06,142][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5718Epoch 1/15: [=                             ] 2/60 batches, loss: 0.5538Epoch 1/15: [=                             ] 3/60 batches, loss: 0.5592Epoch 1/15: [==                            ] 4/60 batches, loss: 0.5008Epoch 1/15: [==                            ] 5/60 batches, loss: 0.4933Epoch 1/15: [===                           ] 6/60 batches, loss: 0.4381Epoch 1/15: [===                           ] 7/60 batches, loss: 0.4132Epoch 1/15: [====                          ] 8/60 batches, loss: 0.4067Epoch 1/15: [====                          ] 9/60 batches, loss: 0.4121Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.4214Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.3999Epoch 1/15: [======                        ] 12/60 batches, loss: 0.3864Epoch 1/15: [======                        ] 13/60 batches, loss: 0.3627Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.3615Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.3460Epoch 1/15: [========                      ] 16/60 batches, loss: 0.3389Epoch 1/15: [========                      ] 17/60 batches, loss: 0.3266Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.3196Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.3125Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.3167Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.3071Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.2992Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.2965Epoch 1/15: [============                  ] 24/60 batches, loss: 0.2947Epoch 1/15: [============                  ] 25/60 batches, loss: 0.2919Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.2856Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.2776Epoch 1/15: [==============                ] 28/60 batches, loss: 0.2749Epoch 1/15: [==============                ] 29/60 batches, loss: 0.2695Epoch 1/15: [===============               ] 30/60 batches, loss: 0.2652Epoch 1/15: [===============               ] 31/60 batches, loss: 0.2637Epoch 1/15: [================              ] 32/60 batches, loss: 0.2591Epoch 1/15: [================              ] 33/60 batches, loss: 0.2552Epoch 1/15: [=================             ] 34/60 batches, loss: 0.2544Epoch 1/15: [=================             ] 35/60 batches, loss: 0.2497Epoch 1/15: [==================            ] 36/60 batches, loss: 0.2444Epoch 1/15: [==================            ] 37/60 batches, loss: 0.2413Epoch 1/15: [===================           ] 38/60 batches, loss: 0.2379Epoch 1/15: [===================           ] 39/60 batches, loss: 0.2340Epoch 1/15: [====================          ] 40/60 batches, loss: 0.2318Epoch 1/15: [====================          ] 41/60 batches, loss: 0.2310Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.2290Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.2267Epoch 1/15: [======================        ] 44/60 batches, loss: 0.2249Epoch 1/15: [======================        ] 45/60 batches, loss: 0.2213Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.2215Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.2194Epoch 1/15: [========================      ] 48/60 batches, loss: 0.2170Epoch 1/15: [========================      ] 49/60 batches, loss: 0.2161Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.2133Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.2110Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.2095Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.2074Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.2074Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.2071Epoch 1/15: [============================  ] 56/60 batches, loss: 0.2078Epoch 1/15: [============================  ] 57/60 batches, loss: 0.2065Epoch 1/15: [============================= ] 58/60 batches, loss: 0.2059Epoch 1/15: [============================= ] 59/60 batches, loss: 0.2044Epoch 1/15: [==============================] 60/60 batches, loss: 0.2021
[2025-05-02 11:02:10,497][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2021
[2025-05-02 11:02:10,752][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0355, Metrics: {'mse': 0.03640417382121086, 'rmse': 0.19079877835355985, 'r2': 0.1292775273323059}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2202Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1632Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2067Epoch 2/15: [==                            ] 4/60 batches, loss: 0.1876Epoch 2/15: [==                            ] 5/60 batches, loss: 0.1826Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1778Epoch 2/15: [===                           ] 7/60 batches, loss: 0.1705Epoch 2/15: [====                          ] 8/60 batches, loss: 0.1617Epoch 2/15: [====                          ] 9/60 batches, loss: 0.1526Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.1520Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.1463Epoch 2/15: [======                        ] 12/60 batches, loss: 0.1477Epoch 2/15: [======                        ] 13/60 batches, loss: 0.1473Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.1506Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.1603Epoch 2/15: [========                      ] 16/60 batches, loss: 0.1609Epoch 2/15: [========                      ] 17/60 batches, loss: 0.1579Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.1540Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.1554Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.1587Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.1584Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.1604Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.1579Epoch 2/15: [============                  ] 24/60 batches, loss: 0.1582Epoch 2/15: [============                  ] 25/60 batches, loss: 0.1551Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.1556Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.1546Epoch 2/15: [==============                ] 28/60 batches, loss: 0.1516Epoch 2/15: [==============                ] 29/60 batches, loss: 0.1488Epoch 2/15: [===============               ] 30/60 batches, loss: 0.1508Epoch 2/15: [===============               ] 31/60 batches, loss: 0.1500Epoch 2/15: [================              ] 32/60 batches, loss: 0.1508Epoch 2/15: [================              ] 33/60 batches, loss: 0.1483Epoch 2/15: [=================             ] 34/60 batches, loss: 0.1525Epoch 2/15: [=================             ] 35/60 batches, loss: 0.1509Epoch 2/15: [==================            ] 36/60 batches, loss: 0.1519Epoch 2/15: [==================            ] 37/60 batches, loss: 0.1502Epoch 2/15: [===================           ] 38/60 batches, loss: 0.1523Epoch 2/15: [===================           ] 39/60 batches, loss: 0.1550Epoch 2/15: [====================          ] 40/60 batches, loss: 0.1551Epoch 2/15: [====================          ] 41/60 batches, loss: 0.1603Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.1609Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.1605Epoch 2/15: [======================        ] 44/60 batches, loss: 0.1597Epoch 2/15: [======================        ] 45/60 batches, loss: 0.1596Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.1587Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1591Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1589Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1571Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1555Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1584Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1589Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1579Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1568Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1569Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1562Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1551Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1551Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1556Epoch 2/15: [==============================] 60/60 batches, loss: 0.1578
[2025-05-02 11:02:13,024][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1578
[2025-05-02 11:02:13,281][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0337, Metrics: {'mse': 0.03454771265387535, 'rmse': 0.1858701499807738, 'r2': 0.17368072271347046}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.1263Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1944Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1761Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1846Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1707Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1631Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1496Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1427Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1348Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1302Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1259Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1347Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1351Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1412Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1381Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1375Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1331Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1354Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1340Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1340Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1357Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1357Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1356Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1364Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1346Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1363Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1336Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1365Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1352Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1338Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1368Epoch 3/15: [================              ] 32/60 batches, loss: 0.1373Epoch 3/15: [================              ] 33/60 batches, loss: 0.1353Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1377Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1382Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1384Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1368Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1364Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1382Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1372Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1370Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1377Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1354Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1351Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1345Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1345Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1351Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1343Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1340Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1331Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1330Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1340Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1325Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1321Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1317Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1307Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1340Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1334Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1342Epoch 3/15: [==============================] 60/60 batches, loss: 0.1350
[2025-05-02 11:02:15,563][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1350
[2025-05-02 11:02:15,825][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0327, Metrics: {'mse': 0.03334478661417961, 'rmse': 0.18260554924256714, 'r2': 0.20245260000228882}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.0646Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0934Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1086Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1188Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1163Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1202Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1238Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1195Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1173Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1150Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1149Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1144Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1165Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1140Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1125Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1087Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1081Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1092Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1102Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1086Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1089Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1071Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1101Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1172Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1164Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1173Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1202Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1215Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1210Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1218Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1216Epoch 4/15: [================              ] 32/60 batches, loss: 0.1207Epoch 4/15: [================              ] 33/60 batches, loss: 0.1214Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1216Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1239Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1264Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1253Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1268Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1306Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1302Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1298Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1292Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1280Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1299Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1302Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1297Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1301Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1284Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1290Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1277Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1277Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1263Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1255Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1250Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1237Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1227Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1215Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1215Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1224Epoch 4/15: [==============================] 60/60 batches, loss: 0.1220
[2025-05-02 11:02:18,039][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1220
[2025-05-02 11:02:18,309][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0317, Metrics: {'mse': 0.032238319516181946, 'rmse': 0.17955032585930314, 'r2': 0.22891724109649658}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.0864Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1029Epoch 5/15: [=                             ] 3/60 batches, loss: 0.0822Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1213Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1288Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1387Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1410Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1377Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1310Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1282Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1321Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1301Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1278Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1224Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1194Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1186Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1165Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1168Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1186Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1159Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1157Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1164Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1201Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1227Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1209Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1246Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1256Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1256Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1251Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1243Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1226Epoch 5/15: [================              ] 32/60 batches, loss: 0.1216Epoch 5/15: [================              ] 33/60 batches, loss: 0.1221Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1231Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1226Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1208Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1197Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1199Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1201Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1200Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1188Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1181Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1170Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1180Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1186Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1176Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1166Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1183Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1173Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1173Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1167Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1169Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1187Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1206Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1207Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1199Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1197Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1193Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1193Epoch 5/15: [==============================] 60/60 batches, loss: 0.1184
[2025-05-02 11:02:20,528][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1184
[2025-05-02 11:02:20,807][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0295, Metrics: {'mse': 0.02996096946299076, 'rmse': 0.17309237263088967, 'r2': 0.28338736295700073}
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1253Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1177Epoch 6/15: [=                             ] 3/60 batches, loss: 0.1022Epoch 6/15: [==                            ] 4/60 batches, loss: 0.1044Epoch 6/15: [==                            ] 5/60 batches, loss: 0.1240Epoch 6/15: [===                           ] 6/60 batches, loss: 0.1139Epoch 6/15: [===                           ] 7/60 batches, loss: 0.1082Epoch 6/15: [====                          ] 8/60 batches, loss: 0.1117Epoch 6/15: [====                          ] 9/60 batches, loss: 0.1173Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.1239Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.1228Epoch 6/15: [======                        ] 12/60 batches, loss: 0.1197Epoch 6/15: [======                        ] 13/60 batches, loss: 0.1200Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.1218Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.1217Epoch 6/15: [========                      ] 16/60 batches, loss: 0.1184Epoch 6/15: [========                      ] 17/60 batches, loss: 0.1200Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.1230Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.1276Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.1284Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.1298Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.1280Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.1248Epoch 6/15: [============                  ] 24/60 batches, loss: 0.1240Epoch 6/15: [============                  ] 25/60 batches, loss: 0.1215Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.1218Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.1204Epoch 6/15: [==============                ] 28/60 batches, loss: 0.1185Epoch 6/15: [==============                ] 29/60 batches, loss: 0.1166Epoch 6/15: [===============               ] 30/60 batches, loss: 0.1161Epoch 6/15: [===============               ] 31/60 batches, loss: 0.1153Epoch 6/15: [================              ] 32/60 batches, loss: 0.1145Epoch 6/15: [================              ] 33/60 batches, loss: 0.1135Epoch 6/15: [=================             ] 34/60 batches, loss: 0.1121Epoch 6/15: [=================             ] 35/60 batches, loss: 0.1151Epoch 6/15: [==================            ] 36/60 batches, loss: 0.1169Epoch 6/15: [==================            ] 37/60 batches, loss: 0.1172Epoch 6/15: [===================           ] 38/60 batches, loss: 0.1166Epoch 6/15: [===================           ] 39/60 batches, loss: 0.1162Epoch 6/15: [====================          ] 40/60 batches, loss: 0.1169Epoch 6/15: [====================          ] 41/60 batches, loss: 0.1169Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.1170Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.1192Epoch 6/15: [======================        ] 44/60 batches, loss: 0.1194Epoch 6/15: [======================        ] 45/60 batches, loss: 0.1194Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.1179Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.1183Epoch 6/15: [========================      ] 48/60 batches, loss: 0.1182Epoch 6/15: [========================      ] 49/60 batches, loss: 0.1184Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.1198Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.1206Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.1200Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.1198Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.1192Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.1203Epoch 6/15: [============================  ] 56/60 batches, loss: 0.1195Epoch 6/15: [============================  ] 57/60 batches, loss: 0.1205Epoch 6/15: [============================= ] 58/60 batches, loss: 0.1198Epoch 6/15: [============================= ] 59/60 batches, loss: 0.1198Epoch 6/15: [==============================] 60/60 batches, loss: 0.1242
[2025-05-02 11:02:23,055][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1242
[2025-05-02 11:02:23,317][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0299, Metrics: {'mse': 0.03021775372326374, 'rmse': 0.17383254506352872, 'r2': 0.27724558115005493}
[2025-05-02 11:02:23,318][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0692Epoch 7/15: [=                             ] 2/60 batches, loss: 0.1092Epoch 7/15: [=                             ] 3/60 batches, loss: 0.1213Epoch 7/15: [==                            ] 4/60 batches, loss: 0.1116Epoch 7/15: [==                            ] 5/60 batches, loss: 0.1132Epoch 7/15: [===                           ] 6/60 batches, loss: 0.1160Epoch 7/15: [===                           ] 7/60 batches, loss: 0.1116Epoch 7/15: [====                          ] 8/60 batches, loss: 0.1187Epoch 7/15: [====                          ] 9/60 batches, loss: 0.1099Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.1156Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.1156Epoch 7/15: [======                        ] 12/60 batches, loss: 0.1131Epoch 7/15: [======                        ] 13/60 batches, loss: 0.1144Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.1172Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.1130Epoch 7/15: [========                      ] 16/60 batches, loss: 0.1108Epoch 7/15: [========                      ] 17/60 batches, loss: 0.1121Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.1092Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.1080Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.1075Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.1051Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.1064Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.1050Epoch 7/15: [============                  ] 24/60 batches, loss: 0.1031Epoch 7/15: [============                  ] 25/60 batches, loss: 0.1033Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.1030Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.1075Epoch 7/15: [==============                ] 28/60 batches, loss: 0.1067Epoch 7/15: [==============                ] 29/60 batches, loss: 0.1069Epoch 7/15: [===============               ] 30/60 batches, loss: 0.1051Epoch 7/15: [===============               ] 31/60 batches, loss: 0.1049Epoch 7/15: [================              ] 32/60 batches, loss: 0.1038Epoch 7/15: [================              ] 33/60 batches, loss: 0.1029Epoch 7/15: [=================             ] 34/60 batches, loss: 0.1029Epoch 7/15: [=================             ] 35/60 batches, loss: 0.1043Epoch 7/15: [==================            ] 36/60 batches, loss: 0.1030Epoch 7/15: [==================            ] 37/60 batches, loss: 0.1036Epoch 7/15: [===================           ] 38/60 batches, loss: 0.1069Epoch 7/15: [===================           ] 39/60 batches, loss: 0.1078Epoch 7/15: [====================          ] 40/60 batches, loss: 0.1087Epoch 7/15: [====================          ] 41/60 batches, loss: 0.1078Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.1081Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.1088Epoch 7/15: [======================        ] 44/60 batches, loss: 0.1095Epoch 7/15: [======================        ] 45/60 batches, loss: 0.1094Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.1087Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.1092Epoch 7/15: [========================      ] 48/60 batches, loss: 0.1086Epoch 7/15: [========================      ] 49/60 batches, loss: 0.1096Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.1095Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.1089Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.1089Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.1098Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.1103Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.1111Epoch 7/15: [============================  ] 56/60 batches, loss: 0.1098Epoch 7/15: [============================  ] 57/60 batches, loss: 0.1122Epoch 7/15: [============================= ] 58/60 batches, loss: 0.1112Epoch 7/15: [============================= ] 59/60 batches, loss: 0.1101Epoch 7/15: [==============================] 60/60 batches, loss: 0.1105
[2025-05-02 11:02:25,182][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1105
[2025-05-02 11:02:25,441][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0375, Metrics: {'mse': 0.03745259344577789, 'rmse': 0.19352672540447197, 'r2': 0.10420125722885132}
[2025-05-02 11:02:25,442][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1261Epoch 8/15: [=                             ] 2/60 batches, loss: 0.1216Epoch 8/15: [=                             ] 3/60 batches, loss: 0.1186Epoch 8/15: [==                            ] 4/60 batches, loss: 0.1000Epoch 8/15: [==                            ] 5/60 batches, loss: 0.1005Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0922Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0897Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0901Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0923Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0908Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0929Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0957Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0955Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.1042Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.1031Epoch 8/15: [========                      ] 16/60 batches, loss: 0.1075Epoch 8/15: [========                      ] 17/60 batches, loss: 0.1063Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.1041Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.1023Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0996Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.1020Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.1021Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.1004Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0986Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0983Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0966Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0967Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0955Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0957Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0977Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0982Epoch 8/15: [================              ] 32/60 batches, loss: 0.0988Epoch 8/15: [================              ] 33/60 batches, loss: 0.0981Epoch 8/15: [=================             ] 34/60 batches, loss: 0.1000Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0994Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0993Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0990Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0999Epoch 8/15: [===================           ] 39/60 batches, loss: 0.1017Epoch 8/15: [====================          ] 40/60 batches, loss: 0.1020Epoch 8/15: [====================          ] 41/60 batches, loss: 0.1015Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.1034Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.1026Epoch 8/15: [======================        ] 44/60 batches, loss: 0.1024Epoch 8/15: [======================        ] 45/60 batches, loss: 0.1016Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.1004Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0998Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0991Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0983Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0976Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0966Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0965Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0965Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0963Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0955Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0949Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0955Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0956Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0950Epoch 8/15: [==============================] 60/60 batches, loss: 0.0958
[2025-05-02 11:02:27,295][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0958
[2025-05-02 11:02:27,558][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0321, Metrics: {'mse': 0.032080505043268204, 'rmse': 0.1791103152899581, 'r2': 0.2326919436454773}
[2025-05-02 11:02:27,559][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1053Epoch 9/15: [=                             ] 2/60 batches, loss: 0.0890Epoch 9/15: [=                             ] 3/60 batches, loss: 0.0892Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0841Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0784Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0847Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0879Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0901Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0880Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0867Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0823Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0822Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0816Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0818Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0781Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0813Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0809Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0784Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0817Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0810Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0815Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0817Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0806Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0803Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0794Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0786Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0786Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0785Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0797Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0787Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0781Epoch 9/15: [================              ] 32/60 batches, loss: 0.0774Epoch 9/15: [================              ] 33/60 batches, loss: 0.0761Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0761Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0761Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0774Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0785Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0787Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0782Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0773Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0772Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0784Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0786Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0792Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0789Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0796Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0800Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0801Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0797Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0810Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0810Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0807Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0804Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0800Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0794Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0793Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0818Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0819Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0818Epoch 9/15: [==============================] 60/60 batches, loss: 0.0810
[2025-05-02 11:02:29,417][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0810
[2025-05-02 11:02:29,685][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0290, Metrics: {'mse': 0.029017547145485878, 'rmse': 0.17034537606135916, 'r2': 0.30595237016677856}
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0937Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0795Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0811Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0759Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0711Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0742Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0817Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0855Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0866Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0974Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0973Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0936Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0918Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0904Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0911Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0967Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0945Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0927Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0917Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0898Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0893Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0895Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0884Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0876Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0903Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0889Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0884Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0872Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0882Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0884Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0905Epoch 10/15: [================              ] 32/60 batches, loss: 0.0907Epoch 10/15: [================              ] 33/60 batches, loss: 0.0917Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0911Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0903Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0896Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0887Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0889Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0894Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0885Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0873Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0895Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0897Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0889Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0891Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0878Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0870Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0881Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0886Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0899Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0893Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0886Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0895Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0890Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0889Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0897Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0896Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0892Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0886Epoch 10/15: [==============================] 60/60 batches, loss: 0.0892
[2025-05-02 11:02:32,004][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0892
[2025-05-02 11:02:32,275][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0308, Metrics: {'mse': 0.03066818229854107, 'rmse': 0.1751233345346675, 'r2': 0.2664721608161926}
[2025-05-02 11:02:32,275][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.1290Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0920Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0717Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0762Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0971Epoch 11/15: [===                           ] 6/60 batches, loss: 0.1041Epoch 11/15: [===                           ] 7/60 batches, loss: 0.1000Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0926Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0910Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0864Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0868Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0903Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0906Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0888Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0892Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0866Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0873Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0873Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0856Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0858Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0856Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0867Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0863Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0856Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0851Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0845Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0854Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0884Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0889Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0922Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0915Epoch 11/15: [================              ] 32/60 batches, loss: 0.0897Epoch 11/15: [================              ] 33/60 batches, loss: 0.0895Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0879Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0881Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0872Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0878Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0872Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0864Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0861Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0861Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0861Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0851Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0860Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0862Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0878Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0870Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0866Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0856Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0852Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0852Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0860Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0864Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0861Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0857Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0857Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0866Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0869Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0870Epoch 11/15: [==============================] 60/60 batches, loss: 0.0894
[2025-05-02 11:02:34,146][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0894
[2025-05-02 11:02:34,412][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0346, Metrics: {'mse': 0.03426222503185272, 'rmse': 0.18510058085228345, 'r2': 0.18050915002822876}
[2025-05-02 11:02:34,413][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.1290Epoch 12/15: [=                             ] 2/60 batches, loss: 0.1315Epoch 12/15: [=                             ] 3/60 batches, loss: 0.1125Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0954Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0865Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0762Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0825Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0810Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0803Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0856Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0916Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0950Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0934Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0952Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0917Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0911Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0906Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0907Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0937Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0929Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0919Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0902Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0910Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0905Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0894Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0920Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0920Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0912Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0909Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0908Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0890Epoch 12/15: [================              ] 32/60 batches, loss: 0.0884Epoch 12/15: [================              ] 33/60 batches, loss: 0.0880Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0860Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0864Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0864Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0855Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0844Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0842Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0841Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0831Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0827Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0822Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0809Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0809Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0803Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0814Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0820Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0816Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0810Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0805Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0810Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0807Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0807Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0807Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0811Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0800Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0800Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0794Epoch 12/15: [==============================] 60/60 batches, loss: 0.0788
[2025-05-02 11:02:36,284][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0788
[2025-05-02 11:02:36,559][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0271, Metrics: {'mse': 0.02700924687087536, 'rmse': 0.16434490217489364, 'r2': 0.35398727655410767}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0352Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0573Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0779Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0742Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0743Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0670Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0698Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0673Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0716Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0752Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0719Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0734Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0740Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0736Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0758Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0770Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0795Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0806Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0805Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0783Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0800Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0801Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0817Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0822Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0808Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0797Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0781Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0776Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0785Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0765Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0766Epoch 13/15: [================              ] 32/60 batches, loss: 0.0768Epoch 13/15: [================              ] 33/60 batches, loss: 0.0766Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0785Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0785Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0776Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0778Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0773Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0760Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0751Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0741Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0740Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0733Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0741Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0736Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0738Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0733Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0735Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0729Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0729Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0732Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0734Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0737Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0743Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0741Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0746Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0744Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0749Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0769Epoch 13/15: [==============================] 60/60 batches, loss: 0.0766
[2025-05-02 11:02:38,823][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0766
[2025-05-02 11:02:39,109][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0268, Metrics: {'mse': 0.02664864994585514, 'rmse': 0.1632441421486699, 'r2': 0.36261212825775146}
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.1747Epoch 14/15: [=                             ] 2/60 batches, loss: 0.1074Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0931Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0850Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0789Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0786Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0721Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0769Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0834Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0810Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0833Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0825Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0802Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0808Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0835Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0895Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0944Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0924Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0909Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0935Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0909Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0910Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0909Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0883Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0870Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0862Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0849Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0874Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0886Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0880Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0874Epoch 14/15: [================              ] 32/60 batches, loss: 0.0888Epoch 14/15: [================              ] 33/60 batches, loss: 0.0877Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0880Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0864Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0863Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0860Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0851Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0834Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0833Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0839Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0845Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0843Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0840Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0838Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0835Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0833Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0836Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0827Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0834Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0826Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0819Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0814Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0811Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0813Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0812Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0813Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0808Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0804Epoch 14/15: [==============================] 60/60 batches, loss: 0.0799
[2025-05-02 11:02:41,587][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0799
[2025-05-02 11:02:41,872][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0311, Metrics: {'mse': 0.03071201592683792, 'rmse': 0.17524844058318442, 'r2': 0.2654237151145935}
[2025-05-02 11:02:41,873][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0744Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0674Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0789Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0833Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0777Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0759Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0722Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0712Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0702Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0668Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0674Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0685Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0674Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0699Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0709Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0712Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0702Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0682Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0672Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0721Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0712Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0729Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0729Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0720Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0727Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0739Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0725Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0725Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0738Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0730Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0731Epoch 15/15: [================              ] 32/60 batches, loss: 0.0730Epoch 15/15: [================              ] 33/60 batches, loss: 0.0732Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0739Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0731Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0721Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0722Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0728Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0723Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0718Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0717Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0713Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0719Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0721Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0716Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0719Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0726Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0732Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0727Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0733Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0729Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0731Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0728Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0720Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0732Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0723Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0724Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0719Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0721Epoch 15/15: [==============================] 60/60 batches, loss: 0.0726
[2025-05-02 11:02:43,768][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0726
[2025-05-02 11:02:44,045][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0293, Metrics: {'mse': 0.02886202000081539, 'rmse': 0.16988825739531085, 'r2': 0.3096722960472107}
[2025-05-02 11:02:44,046][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-02 11:02:44,046][src.training.lm_trainer][INFO] - Training completed in 36.17 seconds
[2025-05-02 11:02:44,046][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:02:46,603][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.026282478123903275, 'rmse': 0.1621187161431501, 'r2': 0.27570009231567383}
[2025-05-02 11:02:46,604][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02664864994585514, 'rmse': 0.1632441421486699, 'r2': 0.36261212825775146}
[2025-05-02 11:02:46,604][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.045807886868715286, 'rmse': 0.21402777125577718, 'r2': -0.12345528602600098}
[2025-05-02 11:02:48,311][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/id/model.pt
[2025-05-02 11:02:48,313][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▃▃▁▁
wandb:     best_val_mse █▇▆▅▃▃▁▁
wandb:      best_val_r2 ▁▂▃▄▆▆██
wandb:    best_val_rmse █▇▆▅▄▃▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▂▃▃▄▄▁▃▄▃▂▄▄▃
wandb:       train_loss █▆▄▄▃▄▃▂▁▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇▆▅▄▃▃█▄▂▄▆▁▁▄▃
wandb:          val_mse ▇▆▅▅▃▃█▅▃▄▆▁▁▄▂
wandb:           val_r2 ▂▃▄▄▆▆▁▄▆▅▃██▅▇
wandb:         val_rmse ▇▆▅▅▃▃█▅▃▄▆▁▁▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02682
wandb:     best_val_mse 0.02665
wandb:      best_val_r2 0.36261
wandb:    best_val_rmse 0.16324
wandb:            epoch 15
wandb:   final_test_mse 0.04581
wandb:    final_test_r2 -0.12346
wandb:  final_test_rmse 0.21403
wandb:  final_train_mse 0.02628
wandb:   final_train_r2 0.2757
wandb: final_train_rmse 0.16212
wandb:    final_val_mse 0.02665
wandb:     final_val_r2 0.36261
wandb:   final_val_rmse 0.16324
wandb:    learning_rate 2e-05
wandb:       train_loss 0.07256
wandb:       train_time 36.1742
wandb:         val_loss 0.0293
wandb:          val_mse 0.02886
wandb:           val_r2 0.30967
wandb:         val_rmse 0.16989
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110157-o4z3964g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110157-o4z3964g/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/id/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:03:01,636][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:03:01,636][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:03:01,636][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:03:01,637][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:03:01,735][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-02 11:03:01,736][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:03:04,491][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:03:06,765][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:03:06,765][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:03:06,953][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,060][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,237][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-02 11:03:07,245][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:03:07,246][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-02 11:03:07,248][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:03:07,292][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,329][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,345][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-02 11:03:07,346][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:03:07,346][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-02 11:03:07,347][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:03:07,372][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,412][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:03:07,427][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-02 11:03:07,428][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:03:07,428][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-02 11:03:07,429][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:03:07,430][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-02 11:03:07,430][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-02 11:03:07,430][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:03:07,431][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-02 11:03:07,431][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:03:07,431][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:03:07,432][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-02 11:03:07,432][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:03:07,432][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:03:07,433][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:03:07,433][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:03:12,849][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:03:12,851][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:03:12,851][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 11:03:12,851][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:03:12,856][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:03:12,857][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:03:12,857][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:03:12,857][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:03:12,857][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-02 11:03:12,858][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:03:12,858][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7746Epoch 1/15: [                              ] 2/75 batches, loss: 0.7357Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7334Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7493Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7348Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7252Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7194Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7176Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7111Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7118Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7084Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7085Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7091Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7067Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7047Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7064Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7037Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7029Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7012Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6977Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6987Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6998Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6988Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6971Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6950Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6934Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6925Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6906Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6890Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6902Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6908Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6900Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6886Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6885Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6909Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6898Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6921Epoch 1/15: [================              ] 40/75 batches, loss: 0.6921Epoch 1/15: [================              ] 41/75 batches, loss: 0.6928Epoch 1/15: [================              ] 42/75 batches, loss: 0.6928Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6913Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6919Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6918Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6911Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6903Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6899Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6900Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6906Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6912Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6907Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6916Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6903Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6906Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6913Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6911Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6905Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6903Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6898Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6895Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6897Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6896Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6899Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6896Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6894Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6891Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6891Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6894Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6888Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6883Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6886Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6886Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6889Epoch 1/15: [==============================] 75/75 batches, loss: 0.6884
[2025-05-02 11:03:19,506][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6884
[2025-05-02 11:03:19,706][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6829, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.7041Epoch 2/15: [                              ] 2/75 batches, loss: 0.6714Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6537Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6572Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6557Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6619Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6656Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6661Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6639Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6638Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6647Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6675Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6679Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6699Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6690Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6676Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6687Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6672Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6663Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6648Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6639Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6626Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6627Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6634Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6634Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6621Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6633Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6612Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6584Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6575Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6572Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6575Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6561Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6554Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6549Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6570Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6594Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6595Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6590Epoch 2/15: [================              ] 40/75 batches, loss: 0.6612Epoch 2/15: [================              ] 41/75 batches, loss: 0.6604Epoch 2/15: [================              ] 42/75 batches, loss: 0.6610Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6617Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6607Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6608Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6608Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6595Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6592Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6596Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6592Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6592Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6593Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6585Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6578Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6579Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6571Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6580Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6576Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6577Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6580Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6566Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6566Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6568Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6559Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6560Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6561Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6568Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6563Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6563Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6571Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6562Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6551Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6553Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6553Epoch 2/15: [==============================] 75/75 batches, loss: 0.6547
[2025-05-02 11:03:22,393][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6547
[2025-05-02 11:03:22,613][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5993, Metrics: {'accuracy': 0.8913043478260869, 'f1': 0.8837209302325582, 'precision': 1.0, 'recall': 0.7916666666666666}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6331Epoch 3/15: [                              ] 2/75 batches, loss: 0.6399Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6401Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6402Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6441Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6469Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6493Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6406Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6454Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6433Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6417Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6423Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6387Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6383Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6403Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6418Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6416Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6407Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6389Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6355Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6354Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6338Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6299Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6332Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6316Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6301Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6316Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6319Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6296Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6257Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6217Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6230Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6237Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6242Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6204Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6202Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6215Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6198Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6222Epoch 3/15: [================              ] 40/75 batches, loss: 0.6217Epoch 3/15: [================              ] 41/75 batches, loss: 0.6208Epoch 3/15: [================              ] 42/75 batches, loss: 0.6212Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6209Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6208Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6209Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6217Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6211Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6219Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6214Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6215Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6199Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6184Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6181Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6182Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6188Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6180Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6194Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6181Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6173Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6171Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6156Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6150Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6147Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6137Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6128Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6131Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6123Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6118Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6104Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6103Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6100Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6102Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6111Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6105Epoch 3/15: [==============================] 75/75 batches, loss: 0.6110
[2025-05-02 11:03:25,315][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6110
[2025-05-02 11:03:25,535][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5532, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6885Epoch 4/15: [                              ] 2/75 batches, loss: 0.6451Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6296Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6092Epoch 4/15: [==                            ] 5/75 batches, loss: 0.5929Epoch 4/15: [==                            ] 6/75 batches, loss: 0.5949Epoch 4/15: [==                            ] 7/75 batches, loss: 0.5964Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6032Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6090Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6092Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6062Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6100Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6085Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6064Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6107Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6074Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6060Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6066Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6086Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6118Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6109Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6138Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6108Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6119Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6108Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6118Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6087Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6083Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6076Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6068Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6038Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6025Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6011Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6013Epoch 4/15: [==============                ] 35/75 batches, loss: 0.5965Epoch 4/15: [==============                ] 36/75 batches, loss: 0.5957Epoch 4/15: [==============                ] 37/75 batches, loss: 0.5955Epoch 4/15: [===============               ] 38/75 batches, loss: 0.5952Epoch 4/15: [===============               ] 39/75 batches, loss: 0.5964Epoch 4/15: [================              ] 40/75 batches, loss: 0.5968Epoch 4/15: [================              ] 41/75 batches, loss: 0.5983Epoch 4/15: [================              ] 42/75 batches, loss: 0.6006Epoch 4/15: [=================             ] 43/75 batches, loss: 0.5994Epoch 4/15: [=================             ] 44/75 batches, loss: 0.5977Epoch 4/15: [==================            ] 45/75 batches, loss: 0.5984Epoch 4/15: [==================            ] 46/75 batches, loss: 0.5982Epoch 4/15: [==================            ] 47/75 batches, loss: 0.5975Epoch 4/15: [===================           ] 48/75 batches, loss: 0.5971Epoch 4/15: [===================           ] 49/75 batches, loss: 0.5967Epoch 4/15: [====================          ] 50/75 batches, loss: 0.5957Epoch 4/15: [====================          ] 51/75 batches, loss: 0.5947Epoch 4/15: [====================          ] 52/75 batches, loss: 0.5938Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.5937Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.5938Epoch 4/15: [======================        ] 55/75 batches, loss: 0.5924Epoch 4/15: [======================        ] 56/75 batches, loss: 0.5923Epoch 4/15: [======================        ] 57/75 batches, loss: 0.5921Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.5909Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.5912Epoch 4/15: [========================      ] 60/75 batches, loss: 0.5913Epoch 4/15: [========================      ] 61/75 batches, loss: 0.5915Epoch 4/15: [========================      ] 62/75 batches, loss: 0.5910Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.5915Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.5922Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.5930Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.5938Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.5937Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.5943Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.5938Epoch 4/15: [============================  ] 70/75 batches, loss: 0.5933Epoch 4/15: [============================  ] 71/75 batches, loss: 0.5940Epoch 4/15: [============================  ] 72/75 batches, loss: 0.5929Epoch 4/15: [============================= ] 73/75 batches, loss: 0.5931Epoch 4/15: [============================= ] 74/75 batches, loss: 0.5929Epoch 4/15: [==============================] 75/75 batches, loss: 0.5930
[2025-05-02 11:03:28,168][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5930
[2025-05-02 11:03:28,382][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5220, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.5172Epoch 5/15: [                              ] 2/75 batches, loss: 0.5355Epoch 5/15: [=                             ] 3/75 batches, loss: 0.5573Epoch 5/15: [=                             ] 4/75 batches, loss: 0.5635Epoch 5/15: [==                            ] 5/75 batches, loss: 0.5453Epoch 5/15: [==                            ] 6/75 batches, loss: 0.5604Epoch 5/15: [==                            ] 7/75 batches, loss: 0.5714Epoch 5/15: [===                           ] 8/75 batches, loss: 0.5563Epoch 5/15: [===                           ] 9/75 batches, loss: 0.5585Epoch 5/15: [====                          ] 10/75 batches, loss: 0.5661Epoch 5/15: [====                          ] 11/75 batches, loss: 0.5682Epoch 5/15: [====                          ] 12/75 batches, loss: 0.5597Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.5623Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.5726Epoch 5/15: [======                        ] 15/75 batches, loss: 0.5717Epoch 5/15: [======                        ] 16/75 batches, loss: 0.5729Epoch 5/15: [======                        ] 17/75 batches, loss: 0.5733Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.5773Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.5784Epoch 5/15: [========                      ] 20/75 batches, loss: 0.5762Epoch 5/15: [========                      ] 21/75 batches, loss: 0.5784Epoch 5/15: [========                      ] 22/75 batches, loss: 0.5757Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.5756Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.5733Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.5735Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.5721Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.5769Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.5759Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.5759Epoch 5/15: [============                  ] 30/75 batches, loss: 0.5738Epoch 5/15: [============                  ] 31/75 batches, loss: 0.5768Epoch 5/15: [============                  ] 32/75 batches, loss: 0.5785Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.5805Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.5805Epoch 5/15: [==============                ] 35/75 batches, loss: 0.5787Epoch 5/15: [==============                ] 36/75 batches, loss: 0.5773Epoch 5/15: [==============                ] 37/75 batches, loss: 0.5761Epoch 5/15: [===============               ] 38/75 batches, loss: 0.5770Epoch 5/15: [===============               ] 39/75 batches, loss: 0.5769Epoch 5/15: [================              ] 40/75 batches, loss: 0.5776Epoch 5/15: [================              ] 41/75 batches, loss: 0.5774Epoch 5/15: [================              ] 42/75 batches, loss: 0.5779Epoch 5/15: [=================             ] 43/75 batches, loss: 0.5755Epoch 5/15: [=================             ] 44/75 batches, loss: 0.5756Epoch 5/15: [==================            ] 45/75 batches, loss: 0.5765Epoch 5/15: [==================            ] 46/75 batches, loss: 0.5762Epoch 5/15: [==================            ] 47/75 batches, loss: 0.5756Epoch 5/15: [===================           ] 48/75 batches, loss: 0.5759Epoch 5/15: [===================           ] 49/75 batches, loss: 0.5747Epoch 5/15: [====================          ] 50/75 batches, loss: 0.5737Epoch 5/15: [====================          ] 51/75 batches, loss: 0.5746Epoch 5/15: [====================          ] 52/75 batches, loss: 0.5746Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.5746Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.5743Epoch 5/15: [======================        ] 55/75 batches, loss: 0.5723Epoch 5/15: [======================        ] 56/75 batches, loss: 0.5722Epoch 5/15: [======================        ] 57/75 batches, loss: 0.5722Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.5730Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.5735Epoch 5/15: [========================      ] 60/75 batches, loss: 0.5738Epoch 5/15: [========================      ] 61/75 batches, loss: 0.5735Epoch 5/15: [========================      ] 62/75 batches, loss: 0.5736Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.5736Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.5746Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.5744Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.5739Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.5747Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.5739Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.5750Epoch 5/15: [============================  ] 70/75 batches, loss: 0.5738Epoch 5/15: [============================  ] 71/75 batches, loss: 0.5736Epoch 5/15: [============================  ] 72/75 batches, loss: 0.5735Epoch 5/15: [============================= ] 73/75 batches, loss: 0.5733Epoch 5/15: [============================= ] 74/75 batches, loss: 0.5739Epoch 5/15: [==============================] 75/75 batches, loss: 0.5720
[2025-05-02 11:03:31,026][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5720
[2025-05-02 11:03:31,247][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5141, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.5961Epoch 6/15: [                              ] 2/75 batches, loss: 0.5502Epoch 6/15: [=                             ] 3/75 batches, loss: 0.5552Epoch 6/15: [=                             ] 4/75 batches, loss: 0.5519Epoch 6/15: [==                            ] 5/75 batches, loss: 0.5641Epoch 6/15: [==                            ] 6/75 batches, loss: 0.5687Epoch 6/15: [==                            ] 7/75 batches, loss: 0.5561Epoch 6/15: [===                           ] 8/75 batches, loss: 0.5480Epoch 6/15: [===                           ] 9/75 batches, loss: 0.5471Epoch 6/15: [====                          ] 10/75 batches, loss: 0.5536Epoch 6/15: [====                          ] 11/75 batches, loss: 0.5515Epoch 6/15: [====                          ] 12/75 batches, loss: 0.5583Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.5641Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.5658Epoch 6/15: [======                        ] 15/75 batches, loss: 0.5659Epoch 6/15: [======                        ] 16/75 batches, loss: 0.5628Epoch 6/15: [======                        ] 17/75 batches, loss: 0.5626Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.5625Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.5604Epoch 6/15: [========                      ] 20/75 batches, loss: 0.5589Epoch 6/15: [========                      ] 21/75 batches, loss: 0.5536Epoch 6/15: [========                      ] 22/75 batches, loss: 0.5536Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.5545Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.5536Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.5557Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.5557Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.5576Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.5602Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.5589Epoch 6/15: [============                  ] 30/75 batches, loss: 0.5587Epoch 6/15: [============                  ] 31/75 batches, loss: 0.5565Epoch 6/15: [============                  ] 32/75 batches, loss: 0.5581Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.5578Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.5562Epoch 6/15: [==============                ] 35/75 batches, loss: 0.5558Epoch 6/15: [==============                ] 36/75 batches, loss: 0.5583Epoch 6/15: [==============                ] 37/75 batches, loss: 0.5566Epoch 6/15: [===============               ] 38/75 batches, loss: 0.5573Epoch 6/15: [===============               ] 39/75 batches, loss: 0.5603Epoch 6/15: [================              ] 40/75 batches, loss: 0.5622Epoch 6/15: [================              ] 41/75 batches, loss: 0.5635Epoch 6/15: [================              ] 42/75 batches, loss: 0.5631Epoch 6/15: [=================             ] 43/75 batches, loss: 0.5626Epoch 6/15: [=================             ] 44/75 batches, loss: 0.5604Epoch 6/15: [==================            ] 45/75 batches, loss: 0.5609Epoch 6/15: [==================            ] 46/75 batches, loss: 0.5608Epoch 6/15: [==================            ] 47/75 batches, loss: 0.5599Epoch 6/15: [===================           ] 48/75 batches, loss: 0.5586Epoch 6/15: [===================           ] 49/75 batches, loss: 0.5593Epoch 6/15: [====================          ] 50/75 batches, loss: 0.5587Epoch 6/15: [====================          ] 51/75 batches, loss: 0.5588Epoch 6/15: [====================          ] 52/75 batches, loss: 0.5610Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.5617Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.5623Epoch 6/15: [======================        ] 55/75 batches, loss: 0.5621Epoch 6/15: [======================        ] 56/75 batches, loss: 0.5612Epoch 6/15: [======================        ] 57/75 batches, loss: 0.5617Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.5604Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.5609Epoch 6/15: [========================      ] 60/75 batches, loss: 0.5617Epoch 6/15: [========================      ] 61/75 batches, loss: 0.5624Epoch 6/15: [========================      ] 62/75 batches, loss: 0.5623Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.5635Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.5632Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.5645Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.5652Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.5643Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.5633Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.5626Epoch 6/15: [============================  ] 70/75 batches, loss: 0.5620Epoch 6/15: [============================  ] 71/75 batches, loss: 0.5609Epoch 6/15: [============================  ] 72/75 batches, loss: 0.5616Epoch 6/15: [============================= ] 73/75 batches, loss: 0.5624Epoch 6/15: [============================= ] 74/75 batches, loss: 0.5630Epoch 6/15: [==============================] 75/75 batches, loss: 0.5648
[2025-05-02 11:03:33,924][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5648
[2025-05-02 11:03:34,133][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5183, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
[2025-05-02 11:03:34,134][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.4646Epoch 7/15: [                              ] 2/75 batches, loss: 0.5164Epoch 7/15: [=                             ] 3/75 batches, loss: 0.5180Epoch 7/15: [=                             ] 4/75 batches, loss: 0.5412Epoch 7/15: [==                            ] 5/75 batches, loss: 0.5390Epoch 7/15: [==                            ] 6/75 batches, loss: 0.5434Epoch 7/15: [==                            ] 7/75 batches, loss: 0.5620Epoch 7/15: [===                           ] 8/75 batches, loss: 0.5677Epoch 7/15: [===                           ] 9/75 batches, loss: 0.5616Epoch 7/15: [====                          ] 10/75 batches, loss: 0.5687Epoch 7/15: [====                          ] 11/75 batches, loss: 0.5713Epoch 7/15: [====                          ] 12/75 batches, loss: 0.5688Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.5677Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.5653Epoch 7/15: [======                        ] 15/75 batches, loss: 0.5626Epoch 7/15: [======                        ] 16/75 batches, loss: 0.5656Epoch 7/15: [======                        ] 17/75 batches, loss: 0.5631Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.5596Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.5609Epoch 7/15: [========                      ] 20/75 batches, loss: 0.5669Epoch 7/15: [========                      ] 21/75 batches, loss: 0.5671Epoch 7/15: [========                      ] 22/75 batches, loss: 0.5654Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.5641Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.5630Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.5595Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.5614Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.5611Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.5598Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.5606Epoch 7/15: [============                  ] 30/75 batches, loss: 0.5607Epoch 7/15: [============                  ] 31/75 batches, loss: 0.5588Epoch 7/15: [============                  ] 32/75 batches, loss: 0.5591Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.5597Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.5599Epoch 7/15: [==============                ] 35/75 batches, loss: 0.5613Epoch 7/15: [==============                ] 36/75 batches, loss: 0.5586Epoch 7/15: [==============                ] 37/75 batches, loss: 0.5576Epoch 7/15: [===============               ] 38/75 batches, loss: 0.5584Epoch 7/15: [===============               ] 39/75 batches, loss: 0.5572Epoch 7/15: [================              ] 40/75 batches, loss: 0.5589Epoch 7/15: [================              ] 41/75 batches, loss: 0.5605Epoch 7/15: [================              ] 42/75 batches, loss: 0.5590Epoch 7/15: [=================             ] 43/75 batches, loss: 0.5606Epoch 7/15: [=================             ] 44/75 batches, loss: 0.5606Epoch 7/15: [==================            ] 45/75 batches, loss: 0.5612Epoch 7/15: [==================            ] 46/75 batches, loss: 0.5601Epoch 7/15: [==================            ] 47/75 batches, loss: 0.5583Epoch 7/15: [===================           ] 48/75 batches, loss: 0.5599Epoch 7/15: [===================           ] 49/75 batches, loss: 0.5593Epoch 7/15: [====================          ] 50/75 batches, loss: 0.5598Epoch 7/15: [====================          ] 51/75 batches, loss: 0.5602Epoch 7/15: [====================          ] 52/75 batches, loss: 0.5605Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.5602Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.5583Epoch 7/15: [======================        ] 55/75 batches, loss: 0.5574Epoch 7/15: [======================        ] 56/75 batches, loss: 0.5566Epoch 7/15: [======================        ] 57/75 batches, loss: 0.5551Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.5559Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.5550Epoch 7/15: [========================      ] 60/75 batches, loss: 0.5555Epoch 7/15: [========================      ] 61/75 batches, loss: 0.5532Epoch 7/15: [========================      ] 62/75 batches, loss: 0.5535Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.5552Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.5554Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.5550Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.5556Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.5542Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.5540Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.5556Epoch 7/15: [============================  ] 70/75 batches, loss: 0.5551Epoch 7/15: [============================  ] 71/75 batches, loss: 0.5546Epoch 7/15: [============================  ] 72/75 batches, loss: 0.5540Epoch 7/15: [============================= ] 73/75 batches, loss: 0.5546Epoch 7/15: [============================= ] 74/75 batches, loss: 0.5540Epoch 7/15: [==============================] 75/75 batches, loss: 0.5530
[2025-05-02 11:03:36,413][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5530
[2025-05-02 11:03:36,620][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5099, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.5381Epoch 8/15: [                              ] 2/75 batches, loss: 0.5433Epoch 8/15: [=                             ] 3/75 batches, loss: 0.5328Epoch 8/15: [=                             ] 4/75 batches, loss: 0.5350Epoch 8/15: [==                            ] 5/75 batches, loss: 0.5425Epoch 8/15: [==                            ] 6/75 batches, loss: 0.5424Epoch 8/15: [==                            ] 7/75 batches, loss: 0.5464Epoch 8/15: [===                           ] 8/75 batches, loss: 0.5527Epoch 8/15: [===                           ] 9/75 batches, loss: 0.5512Epoch 8/15: [====                          ] 10/75 batches, loss: 0.5574Epoch 8/15: [====                          ] 11/75 batches, loss: 0.5550Epoch 8/15: [====                          ] 12/75 batches, loss: 0.5577Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.5490Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.5499Epoch 8/15: [======                        ] 15/75 batches, loss: 0.5456Epoch 8/15: [======                        ] 16/75 batches, loss: 0.5479Epoch 8/15: [======                        ] 17/75 batches, loss: 0.5468Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.5441Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.5430Epoch 8/15: [========                      ] 20/75 batches, loss: 0.5451Epoch 8/15: [========                      ] 21/75 batches, loss: 0.5414Epoch 8/15: [========                      ] 22/75 batches, loss: 0.5437Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.5436Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.5401Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.5393Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.5400Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.5365Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.5394Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.5418Epoch 8/15: [============                  ] 30/75 batches, loss: 0.5415Epoch 8/15: [============                  ] 31/75 batches, loss: 0.5390Epoch 8/15: [============                  ] 32/75 batches, loss: 0.5406Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.5413Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.5433Epoch 8/15: [==============                ] 35/75 batches, loss: 0.5426Epoch 8/15: [==============                ] 36/75 batches, loss: 0.5436Epoch 8/15: [==============                ] 37/75 batches, loss: 0.5457Epoch 8/15: [===============               ] 38/75 batches, loss: 0.5471Epoch 8/15: [===============               ] 39/75 batches, loss: 0.5488Epoch 8/15: [================              ] 40/75 batches, loss: 0.5499Epoch 8/15: [================              ] 41/75 batches, loss: 0.5470Epoch 8/15: [================              ] 42/75 batches, loss: 0.5479Epoch 8/15: [=================             ] 43/75 batches, loss: 0.5482Epoch 8/15: [=================             ] 44/75 batches, loss: 0.5477Epoch 8/15: [==================            ] 45/75 batches, loss: 0.5465Epoch 8/15: [==================            ] 46/75 batches, loss: 0.5464Epoch 8/15: [==================            ] 47/75 batches, loss: 0.5469Epoch 8/15: [===================           ] 48/75 batches, loss: 0.5469Epoch 8/15: [===================           ] 49/75 batches, loss: 0.5464Epoch 8/15: [====================          ] 50/75 batches, loss: 0.5469Epoch 8/15: [====================          ] 51/75 batches, loss: 0.5471Epoch 8/15: [====================          ] 52/75 batches, loss: 0.5471Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.5474Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.5474Epoch 8/15: [======================        ] 55/75 batches, loss: 0.5495Epoch 8/15: [======================        ] 56/75 batches, loss: 0.5478Epoch 8/15: [======================        ] 57/75 batches, loss: 0.5473Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.5480Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.5481Epoch 8/15: [========================      ] 60/75 batches, loss: 0.5474Epoch 8/15: [========================      ] 61/75 batches, loss: 0.5479Epoch 8/15: [========================      ] 62/75 batches, loss: 0.5470Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.5473Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.5462Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.5477Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.5482Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.5477Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.5489Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.5500Epoch 8/15: [============================  ] 70/75 batches, loss: 0.5507Epoch 8/15: [============================  ] 71/75 batches, loss: 0.5496Epoch 8/15: [============================  ] 72/75 batches, loss: 0.5503Epoch 8/15: [============================= ] 73/75 batches, loss: 0.5507Epoch 8/15: [============================= ] 74/75 batches, loss: 0.5507Epoch 8/15: [==============================] 75/75 batches, loss: 0.5507
[2025-05-02 11:03:39,332][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5507
[2025-05-02 11:03:39,559][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5106, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:39,559][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.5094Epoch 9/15: [                              ] 2/75 batches, loss: 0.5445Epoch 9/15: [=                             ] 3/75 batches, loss: 0.5517Epoch 9/15: [=                             ] 4/75 batches, loss: 0.5398Epoch 9/15: [==                            ] 5/75 batches, loss: 0.5375Epoch 9/15: [==                            ] 6/75 batches, loss: 0.5529Epoch 9/15: [==                            ] 7/75 batches, loss: 0.5466Epoch 9/15: [===                           ] 8/75 batches, loss: 0.5616Epoch 9/15: [===                           ] 9/75 batches, loss: 0.5589Epoch 9/15: [====                          ] 10/75 batches, loss: 0.5593Epoch 9/15: [====                          ] 11/75 batches, loss: 0.5622Epoch 9/15: [====                          ] 12/75 batches, loss: 0.5552Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.5519Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.5515Epoch 9/15: [======                        ] 15/75 batches, loss: 0.5495Epoch 9/15: [======                        ] 16/75 batches, loss: 0.5501Epoch 9/15: [======                        ] 17/75 batches, loss: 0.5551Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.5520Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.5549Epoch 9/15: [========                      ] 20/75 batches, loss: 0.5522Epoch 9/15: [========                      ] 21/75 batches, loss: 0.5516Epoch 9/15: [========                      ] 22/75 batches, loss: 0.5513Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.5508Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.5481Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.5497Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.5478Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.5489Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.5512Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.5542Epoch 9/15: [============                  ] 30/75 batches, loss: 0.5521Epoch 9/15: [============                  ] 31/75 batches, loss: 0.5486Epoch 9/15: [============                  ] 32/75 batches, loss: 0.5477Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.5494Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.5494Epoch 9/15: [==============                ] 35/75 batches, loss: 0.5463Epoch 9/15: [==============                ] 36/75 batches, loss: 0.5467Epoch 9/15: [==============                ] 37/75 batches, loss: 0.5501Epoch 9/15: [===============               ] 38/75 batches, loss: 0.5506Epoch 9/15: [===============               ] 39/75 batches, loss: 0.5499Epoch 9/15: [================              ] 40/75 batches, loss: 0.5485Epoch 9/15: [================              ] 41/75 batches, loss: 0.5460Epoch 9/15: [================              ] 42/75 batches, loss: 0.5440Epoch 9/15: [=================             ] 43/75 batches, loss: 0.5434Epoch 9/15: [=================             ] 44/75 batches, loss: 0.5458Epoch 9/15: [==================            ] 45/75 batches, loss: 0.5449Epoch 9/15: [==================            ] 46/75 batches, loss: 0.5442Epoch 9/15: [==================            ] 47/75 batches, loss: 0.5438Epoch 9/15: [===================           ] 48/75 batches, loss: 0.5452Epoch 9/15: [===================           ] 49/75 batches, loss: 0.5454Epoch 9/15: [====================          ] 50/75 batches, loss: 0.5440Epoch 9/15: [====================          ] 51/75 batches, loss: 0.5453Epoch 9/15: [====================          ] 52/75 batches, loss: 0.5455Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.5434Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.5426Epoch 9/15: [======================        ] 55/75 batches, loss: 0.5434Epoch 9/15: [======================        ] 56/75 batches, loss: 0.5445Epoch 9/15: [======================        ] 57/75 batches, loss: 0.5448Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.5429Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.5428Epoch 9/15: [========================      ] 60/75 batches, loss: 0.5433Epoch 9/15: [========================      ] 61/75 batches, loss: 0.5431Epoch 9/15: [========================      ] 62/75 batches, loss: 0.5433Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.5442Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.5443Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.5446Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.5425Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.5423Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.5414Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.5422Epoch 9/15: [============================  ] 70/75 batches, loss: 0.5426Epoch 9/15: [============================  ] 71/75 batches, loss: 0.5421Epoch 9/15: [============================  ] 72/75 batches, loss: 0.5430Epoch 9/15: [============================= ] 73/75 batches, loss: 0.5423Epoch 9/15: [============================= ] 74/75 batches, loss: 0.5423Epoch 9/15: [==============================] 75/75 batches, loss: 0.5426
[2025-05-02 11:03:41,854][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5426
[2025-05-02 11:03:42,082][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5076, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.5125Epoch 10/15: [                              ] 2/75 batches, loss: 0.5866Epoch 10/15: [=                             ] 3/75 batches, loss: 0.5680Epoch 10/15: [=                             ] 4/75 batches, loss: 0.5574Epoch 10/15: [==                            ] 5/75 batches, loss: 0.5511Epoch 10/15: [==                            ] 6/75 batches, loss: 0.5494Epoch 10/15: [==                            ] 7/75 batches, loss: 0.5499Epoch 10/15: [===                           ] 8/75 batches, loss: 0.5346Epoch 10/15: [===                           ] 9/75 batches, loss: 0.5363Epoch 10/15: [====                          ] 10/75 batches, loss: 0.5379Epoch 10/15: [====                          ] 11/75 batches, loss: 0.5358Epoch 10/15: [====                          ] 12/75 batches, loss: 0.5336Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.5346Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.5333Epoch 10/15: [======                        ] 15/75 batches, loss: 0.5303Epoch 10/15: [======                        ] 16/75 batches, loss: 0.5331Epoch 10/15: [======                        ] 17/75 batches, loss: 0.5332Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.5358Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.5382Epoch 10/15: [========                      ] 20/75 batches, loss: 0.5332Epoch 10/15: [========                      ] 21/75 batches, loss: 0.5366Epoch 10/15: [========                      ] 22/75 batches, loss: 0.5361Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.5367Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.5325Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.5341Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.5348Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.5356Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.5371Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.5382Epoch 10/15: [============                  ] 30/75 batches, loss: 0.5361Epoch 10/15: [============                  ] 31/75 batches, loss: 0.5353Epoch 10/15: [============                  ] 32/75 batches, loss: 0.5331Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.5339Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.5373Epoch 10/15: [==============                ] 35/75 batches, loss: 0.5396Epoch 10/15: [==============                ] 36/75 batches, loss: 0.5410Epoch 10/15: [==============                ] 37/75 batches, loss: 0.5417Epoch 10/15: [===============               ] 38/75 batches, loss: 0.5406Epoch 10/15: [===============               ] 39/75 batches, loss: 0.5387Epoch 10/15: [================              ] 40/75 batches, loss: 0.5381Epoch 10/15: [================              ] 41/75 batches, loss: 0.5378Epoch 10/15: [================              ] 42/75 batches, loss: 0.5374Epoch 10/15: [=================             ] 43/75 batches, loss: 0.5380Epoch 10/15: [=================             ] 44/75 batches, loss: 0.5387Epoch 10/15: [==================            ] 45/75 batches, loss: 0.5384Epoch 10/15: [==================            ] 46/75 batches, loss: 0.5394Epoch 10/15: [==================            ] 47/75 batches, loss: 0.5402Epoch 10/15: [===================           ] 48/75 batches, loss: 0.5413Epoch 10/15: [===================           ] 49/75 batches, loss: 0.5405Epoch 10/15: [====================          ] 50/75 batches, loss: 0.5389Epoch 10/15: [====================          ] 51/75 batches, loss: 0.5389Epoch 10/15: [====================          ] 52/75 batches, loss: 0.5405Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.5419Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.5420Epoch 10/15: [======================        ] 55/75 batches, loss: 0.5415Epoch 10/15: [======================        ] 56/75 batches, loss: 0.5396Epoch 10/15: [======================        ] 57/75 batches, loss: 0.5390Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.5383Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.5383Epoch 10/15: [========================      ] 60/75 batches, loss: 0.5379Epoch 10/15: [========================      ] 61/75 batches, loss: 0.5378Epoch 10/15: [========================      ] 62/75 batches, loss: 0.5378Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.5370Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.5362Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.5380Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.5384Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.5384Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.5399Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.5407Epoch 10/15: [============================  ] 70/75 batches, loss: 0.5400Epoch 10/15: [============================  ] 71/75 batches, loss: 0.5400Epoch 10/15: [============================  ] 72/75 batches, loss: 0.5401Epoch 10/15: [============================= ] 73/75 batches, loss: 0.5391Epoch 10/15: [============================= ] 74/75 batches, loss: 0.5389Epoch 10/15: [==============================] 75/75 batches, loss: 0.5395
[2025-05-02 11:03:44,733][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5395
[2025-05-02 11:03:44,960][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5086, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:44,961][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.5015Epoch 11/15: [                              ] 2/75 batches, loss: 0.4898Epoch 11/15: [=                             ] 3/75 batches, loss: 0.5100Epoch 11/15: [=                             ] 4/75 batches, loss: 0.5105Epoch 11/15: [==                            ] 5/75 batches, loss: 0.5271Epoch 11/15: [==                            ] 6/75 batches, loss: 0.5238Epoch 11/15: [==                            ] 7/75 batches, loss: 0.5302Epoch 11/15: [===                           ] 8/75 batches, loss: 0.5205Epoch 11/15: [===                           ] 9/75 batches, loss: 0.5206Epoch 11/15: [====                          ] 10/75 batches, loss: 0.5287Epoch 11/15: [====                          ] 11/75 batches, loss: 0.5373Epoch 11/15: [====                          ] 12/75 batches, loss: 0.5391Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.5385Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.5406Epoch 11/15: [======                        ] 15/75 batches, loss: 0.5364Epoch 11/15: [======                        ] 16/75 batches, loss: 0.5332Epoch 11/15: [======                        ] 17/75 batches, loss: 0.5331Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.5292Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.5278Epoch 11/15: [========                      ] 20/75 batches, loss: 0.5316Epoch 11/15: [========                      ] 21/75 batches, loss: 0.5307Epoch 11/15: [========                      ] 22/75 batches, loss: 0.5326Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.5361Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.5357Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.5332Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.5342Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.5351Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.5346Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.5369Epoch 11/15: [============                  ] 30/75 batches, loss: 0.5373Epoch 11/15: [============                  ] 31/75 batches, loss: 0.5384Epoch 11/15: [============                  ] 32/75 batches, loss: 0.5359Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.5395Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.5400Epoch 11/15: [==============                ] 35/75 batches, loss: 0.5420Epoch 11/15: [==============                ] 36/75 batches, loss: 0.5423Epoch 11/15: [==============                ] 37/75 batches, loss: 0.5409Epoch 11/15: [===============               ] 38/75 batches, loss: 0.5436Epoch 11/15: [===============               ] 39/75 batches, loss: 0.5461Epoch 11/15: [================              ] 40/75 batches, loss: 0.5429Epoch 11/15: [================              ] 41/75 batches, loss: 0.5420Epoch 11/15: [================              ] 42/75 batches, loss: 0.5414Epoch 11/15: [=================             ] 43/75 batches, loss: 0.5406Epoch 11/15: [=================             ] 44/75 batches, loss: 0.5384Epoch 11/15: [==================            ] 45/75 batches, loss: 0.5385Epoch 11/15: [==================            ] 46/75 batches, loss: 0.5387Epoch 11/15: [==================            ] 47/75 batches, loss: 0.5403Epoch 11/15: [===================           ] 48/75 batches, loss: 0.5411Epoch 11/15: [===================           ] 49/75 batches, loss: 0.5393Epoch 11/15: [====================          ] 50/75 batches, loss: 0.5376Epoch 11/15: [====================          ] 51/75 batches, loss: 0.5374Epoch 11/15: [====================          ] 52/75 batches, loss: 0.5382Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.5413Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.5391Epoch 11/15: [======================        ] 55/75 batches, loss: 0.5407Epoch 11/15: [======================        ] 56/75 batches, loss: 0.5406Epoch 11/15: [======================        ] 57/75 batches, loss: 0.5389Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.5392Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.5397Epoch 11/15: [========================      ] 60/75 batches, loss: 0.5389Epoch 11/15: [========================      ] 61/75 batches, loss: 0.5388Epoch 11/15: [========================      ] 62/75 batches, loss: 0.5405Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.5404Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.5388Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.5395Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.5394Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.5395Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.5408Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.5407Epoch 11/15: [============================  ] 70/75 batches, loss: 0.5413Epoch 11/15: [============================  ] 71/75 batches, loss: 0.5409Epoch 11/15: [============================  ] 72/75 batches, loss: 0.5410Epoch 11/15: [============================= ] 73/75 batches, loss: 0.5410Epoch 11/15: [============================= ] 74/75 batches, loss: 0.5413Epoch 11/15: [==============================] 75/75 batches, loss: 0.5406
[2025-05-02 11:03:47,256][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5406
[2025-05-02 11:03:47,486][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5085, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:47,487][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.5134Epoch 12/15: [                              ] 2/75 batches, loss: 0.5828Epoch 12/15: [=                             ] 3/75 batches, loss: 0.5802Epoch 12/15: [=                             ] 4/75 batches, loss: 0.5627Epoch 12/15: [==                            ] 5/75 batches, loss: 0.5393Epoch 12/15: [==                            ] 6/75 batches, loss: 0.5488Epoch 12/15: [==                            ] 7/75 batches, loss: 0.5470Epoch 12/15: [===                           ] 8/75 batches, loss: 0.5353Epoch 12/15: [===                           ] 9/75 batches, loss: 0.5301Epoch 12/15: [====                          ] 10/75 batches, loss: 0.5264Epoch 12/15: [====                          ] 11/75 batches, loss: 0.5292Epoch 12/15: [====                          ] 12/75 batches, loss: 0.5264Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.5304Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.5307Epoch 12/15: [======                        ] 15/75 batches, loss: 0.5288Epoch 12/15: [======                        ] 16/75 batches, loss: 0.5236Epoch 12/15: [======                        ] 17/75 batches, loss: 0.5288Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.5276Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.5334Epoch 12/15: [========                      ] 20/75 batches, loss: 0.5285Epoch 12/15: [========                      ] 21/75 batches, loss: 0.5336Epoch 12/15: [========                      ] 22/75 batches, loss: 0.5318Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.5286Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.5334Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.5334Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.5351Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.5357Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.5362Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.5382Epoch 12/15: [============                  ] 30/75 batches, loss: 0.5370Epoch 12/15: [============                  ] 31/75 batches, loss: 0.5409Epoch 12/15: [============                  ] 32/75 batches, loss: 0.5383Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.5394Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.5395Epoch 12/15: [==============                ] 35/75 batches, loss: 0.5424Epoch 12/15: [==============                ] 36/75 batches, loss: 0.5423Epoch 12/15: [==============                ] 37/75 batches, loss: 0.5446Epoch 12/15: [===============               ] 38/75 batches, loss: 0.5423Epoch 12/15: [===============               ] 39/75 batches, loss: 0.5419Epoch 12/15: [================              ] 40/75 batches, loss: 0.5433Epoch 12/15: [================              ] 41/75 batches, loss: 0.5434Epoch 12/15: [================              ] 42/75 batches, loss: 0.5426Epoch 12/15: [=================             ] 43/75 batches, loss: 0.5447Epoch 12/15: [=================             ] 44/75 batches, loss: 0.5455Epoch 12/15: [==================            ] 45/75 batches, loss: 0.5446Epoch 12/15: [==================            ] 46/75 batches, loss: 0.5445Epoch 12/15: [==================            ] 47/75 batches, loss: 0.5427Epoch 12/15: [===================           ] 48/75 batches, loss: 0.5436Epoch 12/15: [===================           ] 49/75 batches, loss: 0.5436Epoch 12/15: [====================          ] 50/75 batches, loss: 0.5433Epoch 12/15: [====================          ] 51/75 batches, loss: 0.5415Epoch 12/15: [====================          ] 52/75 batches, loss: 0.5410Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.5409Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.5401Epoch 12/15: [======================        ] 55/75 batches, loss: 0.5400Epoch 12/15: [======================        ] 56/75 batches, loss: 0.5394Epoch 12/15: [======================        ] 57/75 batches, loss: 0.5394Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.5397Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.5404Epoch 12/15: [========================      ] 60/75 batches, loss: 0.5410Epoch 12/15: [========================      ] 61/75 batches, loss: 0.5406Epoch 12/15: [========================      ] 62/75 batches, loss: 0.5397Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.5389Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.5396Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.5388Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.5372Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.5365Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.5362Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.5364Epoch 12/15: [============================  ] 70/75 batches, loss: 0.5370Epoch 12/15: [============================  ] 71/75 batches, loss: 0.5378Epoch 12/15: [============================  ] 72/75 batches, loss: 0.5388Epoch 12/15: [============================= ] 73/75 batches, loss: 0.5380Epoch 12/15: [============================= ] 74/75 batches, loss: 0.5374Epoch 12/15: [==============================] 75/75 batches, loss: 0.5377
[2025-05-02 11:03:49,797][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5377
[2025-05-02 11:03:50,009][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5052, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.5848Epoch 13/15: [                              ] 2/75 batches, loss: 0.5671Epoch 13/15: [=                             ] 3/75 batches, loss: 0.5953Epoch 13/15: [=                             ] 4/75 batches, loss: 0.5923Epoch 13/15: [==                            ] 5/75 batches, loss: 0.5614Epoch 13/15: [==                            ] 6/75 batches, loss: 0.5528Epoch 13/15: [==                            ] 7/75 batches, loss: 0.5559Epoch 13/15: [===                           ] 8/75 batches, loss: 0.5534Epoch 13/15: [===                           ] 9/75 batches, loss: 0.5498Epoch 13/15: [====                          ] 10/75 batches, loss: 0.5410Epoch 13/15: [====                          ] 11/75 batches, loss: 0.5405Epoch 13/15: [====                          ] 12/75 batches, loss: 0.5394Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.5433Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.5410Epoch 13/15: [======                        ] 15/75 batches, loss: 0.5432Epoch 13/15: [======                        ] 16/75 batches, loss: 0.5422Epoch 13/15: [======                        ] 17/75 batches, loss: 0.5420Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.5470Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.5447Epoch 13/15: [========                      ] 20/75 batches, loss: 0.5443Epoch 13/15: [========                      ] 21/75 batches, loss: 0.5419Epoch 13/15: [========                      ] 22/75 batches, loss: 0.5421Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.5403Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.5398Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.5376Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.5396Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.5349Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.5366Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.5368Epoch 13/15: [============                  ] 30/75 batches, loss: 0.5362Epoch 13/15: [============                  ] 31/75 batches, loss: 0.5377Epoch 13/15: [============                  ] 32/75 batches, loss: 0.5384Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.5403Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.5416Epoch 13/15: [==============                ] 35/75 batches, loss: 0.5419Epoch 13/15: [==============                ] 36/75 batches, loss: 0.5416Epoch 13/15: [==============                ] 37/75 batches, loss: 0.5403Epoch 13/15: [===============               ] 38/75 batches, loss: 0.5391Epoch 13/15: [===============               ] 39/75 batches, loss: 0.5380Epoch 13/15: [================              ] 40/75 batches, loss: 0.5367Epoch 13/15: [================              ] 41/75 batches, loss: 0.5367Epoch 13/15: [================              ] 42/75 batches, loss: 0.5370Epoch 13/15: [=================             ] 43/75 batches, loss: 0.5369Epoch 13/15: [=================             ] 44/75 batches, loss: 0.5358Epoch 13/15: [==================            ] 45/75 batches, loss: 0.5362Epoch 13/15: [==================            ] 46/75 batches, loss: 0.5361Epoch 13/15: [==================            ] 47/75 batches, loss: 0.5364Epoch 13/15: [===================           ] 48/75 batches, loss: 0.5363Epoch 13/15: [===================           ] 49/75 batches, loss: 0.5352Epoch 13/15: [====================          ] 50/75 batches, loss: 0.5349Epoch 13/15: [====================          ] 51/75 batches, loss: 0.5340Epoch 13/15: [====================          ] 52/75 batches, loss: 0.5332Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.5346Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.5361Epoch 13/15: [======================        ] 55/75 batches, loss: 0.5355Epoch 13/15: [======================        ] 56/75 batches, loss: 0.5351Epoch 13/15: [======================        ] 57/75 batches, loss: 0.5362Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.5338Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.5325Epoch 13/15: [========================      ] 60/75 batches, loss: 0.5334Epoch 13/15: [========================      ] 61/75 batches, loss: 0.5335Epoch 13/15: [========================      ] 62/75 batches, loss: 0.5343Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.5351Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.5360Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.5361Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.5360Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.5360Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.5371Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.5371Epoch 13/15: [============================  ] 70/75 batches, loss: 0.5371Epoch 13/15: [============================  ] 71/75 batches, loss: 0.5372Epoch 13/15: [============================  ] 72/75 batches, loss: 0.5374Epoch 13/15: [============================= ] 73/75 batches, loss: 0.5376Epoch 13/15: [============================= ] 74/75 batches, loss: 0.5366Epoch 13/15: [==============================] 75/75 batches, loss: 0.5358
[2025-05-02 11:03:52,745][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5358
[2025-05-02 11:03:52,957][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5082, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:52,958][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.4462Epoch 14/15: [                              ] 2/75 batches, loss: 0.4682Epoch 14/15: [=                             ] 3/75 batches, loss: 0.5277Epoch 14/15: [=                             ] 4/75 batches, loss: 0.5318Epoch 14/15: [==                            ] 5/75 batches, loss: 0.5436Epoch 14/15: [==                            ] 6/75 batches, loss: 0.5236Epoch 14/15: [==                            ] 7/75 batches, loss: 0.5279Epoch 14/15: [===                           ] 8/75 batches, loss: 0.5389Epoch 14/15: [===                           ] 9/75 batches, loss: 0.5420Epoch 14/15: [====                          ] 10/75 batches, loss: 0.5382Epoch 14/15: [====                          ] 11/75 batches, loss: 0.5370Epoch 14/15: [====                          ] 12/75 batches, loss: 0.5434Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.5422Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.5449Epoch 14/15: [======                        ] 15/75 batches, loss: 0.5407Epoch 14/15: [======                        ] 16/75 batches, loss: 0.5421Epoch 14/15: [======                        ] 17/75 batches, loss: 0.5455Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.5403Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.5428Epoch 14/15: [========                      ] 20/75 batches, loss: 0.5409Epoch 14/15: [========                      ] 21/75 batches, loss: 0.5358Epoch 14/15: [========                      ] 22/75 batches, loss: 0.5317Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.5310Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.5331Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.5293Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.5329Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.5321Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.5311Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.5281Epoch 14/15: [============                  ] 30/75 batches, loss: 0.5280Epoch 14/15: [============                  ] 31/75 batches, loss: 0.5271Epoch 14/15: [============                  ] 32/75 batches, loss: 0.5277Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.5282Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.5297Epoch 14/15: [==============                ] 35/75 batches, loss: 0.5319Epoch 14/15: [==============                ] 36/75 batches, loss: 0.5323Epoch 14/15: [==============                ] 37/75 batches, loss: 0.5323Epoch 14/15: [===============               ] 38/75 batches, loss: 0.5317Epoch 14/15: [===============               ] 39/75 batches, loss: 0.5323Epoch 14/15: [================              ] 40/75 batches, loss: 0.5328Epoch 14/15: [================              ] 41/75 batches, loss: 0.5330Epoch 14/15: [================              ] 42/75 batches, loss: 0.5334Epoch 14/15: [=================             ] 43/75 batches, loss: 0.5329Epoch 14/15: [=================             ] 44/75 batches, loss: 0.5331Epoch 14/15: [==================            ] 45/75 batches, loss: 0.5345Epoch 14/15: [==================            ] 46/75 batches, loss: 0.5364Epoch 14/15: [==================            ] 47/75 batches, loss: 0.5362Epoch 14/15: [===================           ] 48/75 batches, loss: 0.5354Epoch 14/15: [===================           ] 49/75 batches, loss: 0.5349Epoch 14/15: [====================          ] 50/75 batches, loss: 0.5360Epoch 14/15: [====================          ] 51/75 batches, loss: 0.5364Epoch 14/15: [====================          ] 52/75 batches, loss: 0.5376Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.5362Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.5364Epoch 14/15: [======================        ] 55/75 batches, loss: 0.5369Epoch 14/15: [======================        ] 56/75 batches, loss: 0.5388Epoch 14/15: [======================        ] 57/75 batches, loss: 0.5382Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.5379Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.5376Epoch 14/15: [========================      ] 60/75 batches, loss: 0.5379Epoch 14/15: [========================      ] 61/75 batches, loss: 0.5375Epoch 14/15: [========================      ] 62/75 batches, loss: 0.5360Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.5365Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.5352Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.5349Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.5347Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.5349Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.5348Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.5334Epoch 14/15: [============================  ] 70/75 batches, loss: 0.5331Epoch 14/15: [============================  ] 71/75 batches, loss: 0.5331Epoch 14/15: [============================  ] 72/75 batches, loss: 0.5339Epoch 14/15: [============================= ] 73/75 batches, loss: 0.5334Epoch 14/15: [============================= ] 74/75 batches, loss: 0.5322Epoch 14/15: [==============================] 75/75 batches, loss: 0.5314
[2025-05-02 11:03:55,260][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5314
[2025-05-02 11:03:55,490][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5094, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:55,491][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.4993Epoch 15/15: [                              ] 2/75 batches, loss: 0.5735Epoch 15/15: [=                             ] 3/75 batches, loss: 0.5572Epoch 15/15: [=                             ] 4/75 batches, loss: 0.5594Epoch 15/15: [==                            ] 5/75 batches, loss: 0.5643Epoch 15/15: [==                            ] 6/75 batches, loss: 0.5507Epoch 15/15: [==                            ] 7/75 batches, loss: 0.5421Epoch 15/15: [===                           ] 8/75 batches, loss: 0.5333Epoch 15/15: [===                           ] 9/75 batches, loss: 0.5253Epoch 15/15: [====                          ] 10/75 batches, loss: 0.5283Epoch 15/15: [====                          ] 11/75 batches, loss: 0.5317Epoch 15/15: [====                          ] 12/75 batches, loss: 0.5386Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.5422Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.5362Epoch 15/15: [======                        ] 15/75 batches, loss: 0.5334Epoch 15/15: [======                        ] 16/75 batches, loss: 0.5336Epoch 15/15: [======                        ] 17/75 batches, loss: 0.5352Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.5342Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.5388Epoch 15/15: [========                      ] 20/75 batches, loss: 0.5409Epoch 15/15: [========                      ] 21/75 batches, loss: 0.5415Epoch 15/15: [========                      ] 22/75 batches, loss: 0.5442Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.5449Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.5467Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.5500Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.5474Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.5490Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.5455Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.5431Epoch 15/15: [============                  ] 30/75 batches, loss: 0.5412Epoch 15/15: [============                  ] 31/75 batches, loss: 0.5401Epoch 15/15: [============                  ] 32/75 batches, loss: 0.5406Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.5421Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.5415Epoch 15/15: [==============                ] 35/75 batches, loss: 0.5401Epoch 15/15: [==============                ] 36/75 batches, loss: 0.5405Epoch 15/15: [==============                ] 37/75 batches, loss: 0.5380Epoch 15/15: [===============               ] 38/75 batches, loss: 0.5368Epoch 15/15: [===============               ] 39/75 batches, loss: 0.5360Epoch 15/15: [================              ] 40/75 batches, loss: 0.5338Epoch 15/15: [================              ] 41/75 batches, loss: 0.5342Epoch 15/15: [================              ] 42/75 batches, loss: 0.5340Epoch 15/15: [=================             ] 43/75 batches, loss: 0.5351Epoch 15/15: [=================             ] 44/75 batches, loss: 0.5350Epoch 15/15: [==================            ] 45/75 batches, loss: 0.5343Epoch 15/15: [==================            ] 46/75 batches, loss: 0.5343Epoch 15/15: [==================            ] 47/75 batches, loss: 0.5353Epoch 15/15: [===================           ] 48/75 batches, loss: 0.5367Epoch 15/15: [===================           ] 49/75 batches, loss: 0.5358Epoch 15/15: [====================          ] 50/75 batches, loss: 0.5348Epoch 15/15: [====================          ] 51/75 batches, loss: 0.5339Epoch 15/15: [====================          ] 52/75 batches, loss: 0.5329Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.5333Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.5323Epoch 15/15: [======================        ] 55/75 batches, loss: 0.5314Epoch 15/15: [======================        ] 56/75 batches, loss: 0.5311Epoch 15/15: [======================        ] 57/75 batches, loss: 0.5332Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.5334Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.5326Epoch 15/15: [========================      ] 60/75 batches, loss: 0.5318Epoch 15/15: [========================      ] 61/75 batches, loss: 0.5325Epoch 15/15: [========================      ] 62/75 batches, loss: 0.5317Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.5324Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.5327Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.5318Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.5328Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.5342Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.5346Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.5341Epoch 15/15: [============================  ] 70/75 batches, loss: 0.5337Epoch 15/15: [============================  ] 71/75 batches, loss: 0.5334Epoch 15/15: [============================  ] 72/75 batches, loss: 0.5325Epoch 15/15: [============================= ] 73/75 batches, loss: 0.5327Epoch 15/15: [============================= ] 74/75 batches, loss: 0.5323Epoch 15/15: [==============================] 75/75 batches, loss: 0.5324
[2025-05-02 11:03:57,792][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5324
[2025-05-02 11:03:57,999][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5087, Metrics: {'accuracy': 0.9565217391304348, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-02 11:03:58,000][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:03:58,000][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-02 11:03:58,000][src.training.lm_trainer][INFO] - Training completed in 41.68 seconds
[2025-05-02 11:03:58,000][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:04:00,886][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9916036943744753, 'f1': 0.9915824915824916, 'precision': 0.9949324324324325, 'recall': 0.988255033557047}
[2025-05-02 11:04:00,887][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
[2025-05-02 11:04:00,887][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7934782608695652, 'f1': 0.8429752066115702, 'precision': 0.7727272727272727, 'recall': 0.9272727272727272}
[2025-05-02 11:04:02,554][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/ja/model.pt
[2025-05-02 11:04:02,556][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▇██████
wandb:           best_val_f1 ▁▇██████
wandb:         best_val_loss █▅▃▂▁▁▁▁
wandb:    best_val_precision ▁███████
wandb:       best_val_recall ▁▇██████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▄▄▄▄▄▄▄▄▄▄▄
wandb:            train_loss █▆▅▄▃▂▂▂▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▇█████████████
wandb:                val_f1 ▁▇█████████████
wandb:              val_loss █▅▃▂▁▂▁▁▁▁▁▁▁▁▁
wandb:         val_precision ▁██████████████
wandb:            val_recall ▁▇█████████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.97826
wandb:           best_val_f1 0.97872
wandb:         best_val_loss 0.50519
wandb:    best_val_precision 1
wandb:       best_val_recall 0.95833
wandb:      early_stop_epoch 15
wandb:                 epoch 15
wandb:   final_test_accuracy 0.79348
wandb:         final_test_f1 0.84298
wandb:  final_test_precision 0.77273
wandb:     final_test_recall 0.92727
wandb:  final_train_accuracy 0.9916
wandb:        final_train_f1 0.99158
wandb: final_train_precision 0.99493
wandb:    final_train_recall 0.98826
wandb:    final_val_accuracy 0.97826
wandb:          final_val_f1 0.97872
wandb:   final_val_precision 1
wandb:      final_val_recall 0.95833
wandb:         learning_rate 0.0001
wandb:            train_loss 0.53235
wandb:            train_time 41.67724
wandb:          val_accuracy 0.95652
wandb:                val_f1 0.95652
wandb:              val_loss 0.50873
wandb:         val_precision 1
wandb:            val_recall 0.91667
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110301-3rcdcegq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110301-3rcdcegq/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/ja/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:04:16,823][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:04:16,823][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:04:16,823][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:04:16,823][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:04:16,828][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-02 11:04:16,828][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:04:18,533][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:04:20,883][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:04:20,883][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:04:20,934][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:20,965][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:21,063][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-02 11:04:21,071][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:04:21,072][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-02 11:04:21,073][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:04:21,093][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:21,124][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:21,136][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-02 11:04:21,137][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:04:21,137][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-02 11:04:21,138][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:04:21,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:21,187][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:04:21,199][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-02 11:04:21,200][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:04:21,200][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-02 11:04:21,201][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-02 11:04:21,202][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:04:21,202][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:04:21,202][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:04:21,202][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:04:21,202][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:04:21,202][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:04:21,203][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:04:21,203][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-02 11:04:21,203][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:04:21,204][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:04:21,204][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-02 11:04:21,204][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:04:21,205][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:04:21,205][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:04:21,205][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:04:25,207][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:04:25,208][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:04:25,208][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=-1, freeze_model=True
[2025-05-02 11:04:25,208][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:04:25,212][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:04:25,213][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:04:25,213][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:04:25,213][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:04:25,213][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-02 11:04:25,214][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:04:25,214][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4429Epoch 1/15: [                              ] 2/75 batches, loss: 0.4876Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4695Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4314Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4428Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4194Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4194Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3937Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3708Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3555Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3348Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3262Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3043Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.2925Epoch 1/15: [======                        ] 15/75 batches, loss: 0.2818Epoch 1/15: [======                        ] 16/75 batches, loss: 0.2812Epoch 1/15: [======                        ] 17/75 batches, loss: 0.2739Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.2748Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.2723Epoch 1/15: [========                      ] 20/75 batches, loss: 0.2641Epoch 1/15: [========                      ] 21/75 batches, loss: 0.2621Epoch 1/15: [========                      ] 22/75 batches, loss: 0.2604Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.2600Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.2550Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.2484Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.2459Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.2428Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.2391Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.2330Epoch 1/15: [============                  ] 30/75 batches, loss: 0.2269Epoch 1/15: [============                  ] 31/75 batches, loss: 0.2248Epoch 1/15: [============                  ] 32/75 batches, loss: 0.2209Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.2168Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.2196Epoch 1/15: [==============                ] 35/75 batches, loss: 0.2166Epoch 1/15: [==============                ] 36/75 batches, loss: 0.2135Epoch 1/15: [==============                ] 37/75 batches, loss: 0.2124Epoch 1/15: [===============               ] 38/75 batches, loss: 0.2112Epoch 1/15: [===============               ] 39/75 batches, loss: 0.2098Epoch 1/15: [================              ] 40/75 batches, loss: 0.2100Epoch 1/15: [================              ] 41/75 batches, loss: 0.2151Epoch 1/15: [================              ] 42/75 batches, loss: 0.2144Epoch 1/15: [=================             ] 43/75 batches, loss: 0.2133Epoch 1/15: [=================             ] 44/75 batches, loss: 0.2107Epoch 1/15: [==================            ] 45/75 batches, loss: 0.2090Epoch 1/15: [==================            ] 46/75 batches, loss: 0.2082Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2069Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2055Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2046Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2039Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2026Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2012Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.1996Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.1988Epoch 1/15: [======================        ] 55/75 batches, loss: 0.1984Epoch 1/15: [======================        ] 56/75 batches, loss: 0.1979Epoch 1/15: [======================        ] 57/75 batches, loss: 0.1968Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.1966Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.1952Epoch 1/15: [========================      ] 60/75 batches, loss: 0.1933Epoch 1/15: [========================      ] 61/75 batches, loss: 0.1961Epoch 1/15: [========================      ] 62/75 batches, loss: 0.1958Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.1954Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.1950Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.1964Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.1966Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.1979Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.1969Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.1950Epoch 1/15: [============================  ] 70/75 batches, loss: 0.1938Epoch 1/15: [============================  ] 71/75 batches, loss: 0.1939Epoch 1/15: [============================  ] 72/75 batches, loss: 0.1941Epoch 1/15: [============================= ] 73/75 batches, loss: 0.1941Epoch 1/15: [============================= ] 74/75 batches, loss: 0.1930Epoch 1/15: [==============================] 75/75 batches, loss: 0.1970
[2025-05-02 11:04:29,877][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1970
[2025-05-02 11:04:30,062][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0853, Metrics: {'mse': 0.08536369353532791, 'rmse': 0.29217065823817406, 'r2': -0.3912510871887207}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1057Epoch 2/15: [                              ] 2/75 batches, loss: 0.1343Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1572Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1405Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1528Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1507Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1464Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1415Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1372Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1410Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1404Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1388Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1391Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1349Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1319Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1357Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1326Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1300Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1283Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1299Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1335Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1320Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1302Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1286Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1283Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1296Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1295Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1271Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1282Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1265Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1276Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1293Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1296Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1300Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1281Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1284Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1292Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1277Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1311Epoch 2/15: [================              ] 40/75 batches, loss: 0.1293Epoch 2/15: [================              ] 41/75 batches, loss: 0.1284Epoch 2/15: [================              ] 42/75 batches, loss: 0.1297Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1284Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1277Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1269Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1261Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1258Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1266Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1254Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1257Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1250Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1264Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1260Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1266Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1266Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1263Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1264Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1280Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1281Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1275Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1275Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1277Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1274Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1276Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1285Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1282Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1285Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1290Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1298Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1307Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1320Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1329Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1326Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1325Epoch 2/15: [==============================] 75/75 batches, loss: 0.1322
[2025-05-02 11:04:32,754][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1322
[2025-05-02 11:04:32,962][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0617, Metrics: {'mse': 0.06185632199048996, 'rmse': 0.2487093122311466, 'r2': -0.008129715919494629}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2077Epoch 3/15: [                              ] 2/75 batches, loss: 0.1848Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1649Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1479Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1431Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1318Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1315Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1301Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1403Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1388Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1351Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1312Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1327Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1387Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1406Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1417Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1404Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1453Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1449Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1419Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1400Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1379Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1347Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1367Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1375Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1354Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1358Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1372Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1374Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1406Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1398Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1397Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1406Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1394Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1424Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1411Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1394Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1379Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1376Epoch 3/15: [================              ] 40/75 batches, loss: 0.1363Epoch 3/15: [================              ] 41/75 batches, loss: 0.1384Epoch 3/15: [================              ] 42/75 batches, loss: 0.1372Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1371Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1370Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1351Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1364Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1384Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1386Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1408Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1423Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1420Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1403Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1406Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1403Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1405Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1392Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1401Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1414Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1445Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1440Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1438Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1442Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1442Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1449Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1445Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1441Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1431Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1423Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1433Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1431Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1428Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1421Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1416Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1415Epoch 3/15: [==============================] 75/75 batches, loss: 0.1429
[2025-05-02 11:04:35,657][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1429
[2025-05-02 11:04:35,873][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0592, Metrics: {'mse': 0.059276994317770004, 'rmse': 0.24346867214853332, 'r2': 0.033907949924468994}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1792Epoch 4/15: [                              ] 2/75 batches, loss: 0.1690Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1824Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1739Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1604Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1516Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1563Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1494Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1491Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1428Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1403Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1467Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1439Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1433Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1373Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1367Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1383Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1359Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1412Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1426Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1420Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1406Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1444Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1414Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1398Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1394Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1370Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1362Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1348Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1323Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1307Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1299Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1294Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1297Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1296Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1304Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1287Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1285Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1273Epoch 4/15: [================              ] 40/75 batches, loss: 0.1320Epoch 4/15: [================              ] 41/75 batches, loss: 0.1307Epoch 4/15: [================              ] 42/75 batches, loss: 0.1298Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1297Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1305Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1302Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1308Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1326Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1308Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1311Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1302Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1301Epoch 4/15: [====================          ] 52/75 batches, loss: 0.1299Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1294Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1292Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1285Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1275Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1266Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1261Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1255Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1246Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1253Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1262Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1268Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1257Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1264Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1260Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1258Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1260Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1263Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1259Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1256Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1245Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1247Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1261Epoch 4/15: [==============================] 75/75 batches, loss: 0.1262
[2025-05-02 11:04:38,539][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1262
[2025-05-02 11:04:38,758][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0590, Metrics: {'mse': 0.05893944576382637, 'rmse': 0.24277447510771466, 'r2': 0.03940927982330322}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0752Epoch 5/15: [                              ] 2/75 batches, loss: 0.1274Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1168Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1112Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1138Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1162Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1155Epoch 5/15: [===                           ] 8/75 batches, loss: 0.1150Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1161Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1173Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1157Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1133Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1154Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1162Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1141Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1120Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1120Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1145Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1169Epoch 5/15: [========                      ] 20/75 batches, loss: 0.1182Epoch 5/15: [========                      ] 21/75 batches, loss: 0.1169Epoch 5/15: [========                      ] 22/75 batches, loss: 0.1178Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.1145Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.1122Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.1113Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.1145Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.1118Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.1108Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.1138Epoch 5/15: [============                  ] 30/75 batches, loss: 0.1151Epoch 5/15: [============                  ] 31/75 batches, loss: 0.1156Epoch 5/15: [============                  ] 32/75 batches, loss: 0.1171Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.1215Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.1202Epoch 5/15: [==============                ] 35/75 batches, loss: 0.1202Epoch 5/15: [==============                ] 36/75 batches, loss: 0.1206Epoch 5/15: [==============                ] 37/75 batches, loss: 0.1203Epoch 5/15: [===============               ] 38/75 batches, loss: 0.1189Epoch 5/15: [===============               ] 39/75 batches, loss: 0.1197Epoch 5/15: [================              ] 40/75 batches, loss: 0.1211Epoch 5/15: [================              ] 41/75 batches, loss: 0.1222Epoch 5/15: [================              ] 42/75 batches, loss: 0.1226Epoch 5/15: [=================             ] 43/75 batches, loss: 0.1213Epoch 5/15: [=================             ] 44/75 batches, loss: 0.1203Epoch 5/15: [==================            ] 45/75 batches, loss: 0.1192Epoch 5/15: [==================            ] 46/75 batches, loss: 0.1183Epoch 5/15: [==================            ] 47/75 batches, loss: 0.1184Epoch 5/15: [===================           ] 48/75 batches, loss: 0.1181Epoch 5/15: [===================           ] 49/75 batches, loss: 0.1177Epoch 5/15: [====================          ] 50/75 batches, loss: 0.1183Epoch 5/15: [====================          ] 51/75 batches, loss: 0.1193Epoch 5/15: [====================          ] 52/75 batches, loss: 0.1188Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.1175Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.1165Epoch 5/15: [======================        ] 55/75 batches, loss: 0.1178Epoch 5/15: [======================        ] 56/75 batches, loss: 0.1173Epoch 5/15: [======================        ] 57/75 batches, loss: 0.1171Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.1162Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.1185Epoch 5/15: [========================      ] 60/75 batches, loss: 0.1189Epoch 5/15: [========================      ] 61/75 batches, loss: 0.1177Epoch 5/15: [========================      ] 62/75 batches, loss: 0.1183Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.1177Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.1180Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.1176Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.1177Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.1168Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.1162Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.1155Epoch 5/15: [============================  ] 70/75 batches, loss: 0.1151Epoch 5/15: [============================  ] 71/75 batches, loss: 0.1150Epoch 5/15: [============================  ] 72/75 batches, loss: 0.1151Epoch 5/15: [============================= ] 73/75 batches, loss: 0.1144Epoch 5/15: [============================= ] 74/75 batches, loss: 0.1147Epoch 5/15: [==============================] 75/75 batches, loss: 0.1148
[2025-05-02 11:04:41,385][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1148
[2025-05-02 11:04:41,593][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0506, Metrics: {'mse': 0.05062815546989441, 'rmse': 0.22500701204605694, 'r2': 0.17486613988876343}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0963Epoch 6/15: [                              ] 2/75 batches, loss: 0.1098Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1037Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1078Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1068Epoch 6/15: [==                            ] 6/75 batches, loss: 0.1074Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0972Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0894Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0930Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0943Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0943Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0943Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0941Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0963Epoch 6/15: [======                        ] 15/75 batches, loss: 0.1005Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0974Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0970Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.1000Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0994Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0987Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0968Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0958Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0947Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0967Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0973Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.1029Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.1015Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.1019Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.1019Epoch 6/15: [============                  ] 30/75 batches, loss: 0.1040Epoch 6/15: [============                  ] 31/75 batches, loss: 0.1029Epoch 6/15: [============                  ] 32/75 batches, loss: 0.1048Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.1048Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.1058Epoch 6/15: [==============                ] 35/75 batches, loss: 0.1038Epoch 6/15: [==============                ] 36/75 batches, loss: 0.1029Epoch 6/15: [==============                ] 37/75 batches, loss: 0.1039Epoch 6/15: [===============               ] 38/75 batches, loss: 0.1031Epoch 6/15: [===============               ] 39/75 batches, loss: 0.1023Epoch 6/15: [================              ] 40/75 batches, loss: 0.1028Epoch 6/15: [================              ] 41/75 batches, loss: 0.1029Epoch 6/15: [================              ] 42/75 batches, loss: 0.1024Epoch 6/15: [=================             ] 43/75 batches, loss: 0.1035Epoch 6/15: [=================             ] 44/75 batches, loss: 0.1033Epoch 6/15: [==================            ] 45/75 batches, loss: 0.1027Epoch 6/15: [==================            ] 46/75 batches, loss: 0.1044Epoch 6/15: [==================            ] 47/75 batches, loss: 0.1050Epoch 6/15: [===================           ] 48/75 batches, loss: 0.1046Epoch 6/15: [===================           ] 49/75 batches, loss: 0.1048Epoch 6/15: [====================          ] 50/75 batches, loss: 0.1051Epoch 6/15: [====================          ] 51/75 batches, loss: 0.1049Epoch 6/15: [====================          ] 52/75 batches, loss: 0.1049Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.1047Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.1043Epoch 6/15: [======================        ] 55/75 batches, loss: 0.1033Epoch 6/15: [======================        ] 56/75 batches, loss: 0.1045Epoch 6/15: [======================        ] 57/75 batches, loss: 0.1043Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.1042Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.1050Epoch 6/15: [========================      ] 60/75 batches, loss: 0.1050Epoch 6/15: [========================      ] 61/75 batches, loss: 0.1045Epoch 6/15: [========================      ] 62/75 batches, loss: 0.1048Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.1044Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.1055Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.1065Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.1060Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.1058Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.1067Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.1066Epoch 6/15: [============================  ] 70/75 batches, loss: 0.1064Epoch 6/15: [============================  ] 71/75 batches, loss: 0.1058Epoch 6/15: [============================  ] 72/75 batches, loss: 0.1056Epoch 6/15: [============================= ] 73/75 batches, loss: 0.1052Epoch 6/15: [============================= ] 74/75 batches, loss: 0.1049Epoch 6/15: [==============================] 75/75 batches, loss: 0.1046
[2025-05-02 11:04:44,244][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1046
[2025-05-02 11:04:44,457][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0534, Metrics: {'mse': 0.053319577127695084, 'rmse': 0.23091032269626902, 'r2': 0.13100147247314453}
[2025-05-02 11:04:44,458][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.1197Epoch 7/15: [                              ] 2/75 batches, loss: 0.0956Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0770Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0924Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0841Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0858Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0872Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0828Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0807Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0879Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0945Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0989Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0987Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.1017Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0979Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0966Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0955Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0993Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0997Epoch 7/15: [========                      ] 20/75 batches, loss: 0.1002Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0995Epoch 7/15: [========                      ] 22/75 batches, loss: 0.1014Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.1008Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0998Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0989Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.1006Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.1032Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.1017Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.1011Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0998Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0994Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0991Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0984Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0993Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0984Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0983Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0970Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0988Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0977Epoch 7/15: [================              ] 40/75 batches, loss: 0.0972Epoch 7/15: [================              ] 41/75 batches, loss: 0.0972Epoch 7/15: [================              ] 42/75 batches, loss: 0.0965Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0963Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0951Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0955Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0953Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0961Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0961Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0962Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0966Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0960Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0955Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0963Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0960Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0958Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0953Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0946Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0938Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0934Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0939Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0942Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0941Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0935Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0939Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0946Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0943Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0952Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0947Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0943Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0939Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0940Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0937Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0932Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0939Epoch 7/15: [==============================] 75/75 batches, loss: 0.0938
[2025-05-02 11:04:46,735][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0938
[2025-05-02 11:04:46,946][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0529, Metrics: {'mse': 0.05282490700483322, 'rmse': 0.2298366963842659, 'r2': 0.13906359672546387}
[2025-05-02 11:04:46,947][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.1246Epoch 8/15: [                              ] 2/75 batches, loss: 0.1208Epoch 8/15: [=                             ] 3/75 batches, loss: 0.1123Epoch 8/15: [=                             ] 4/75 batches, loss: 0.1084Epoch 8/15: [==                            ] 5/75 batches, loss: 0.1027Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0987Epoch 8/15: [==                            ] 7/75 batches, loss: 0.1010Epoch 8/15: [===                           ] 8/75 batches, loss: 0.1090Epoch 8/15: [===                           ] 9/75 batches, loss: 0.1038Epoch 8/15: [====                          ] 10/75 batches, loss: 0.1082Epoch 8/15: [====                          ] 11/75 batches, loss: 0.1050Epoch 8/15: [====                          ] 12/75 batches, loss: 0.1103Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.1103Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.1088Epoch 8/15: [======                        ] 15/75 batches, loss: 0.1104Epoch 8/15: [======                        ] 16/75 batches, loss: 0.1085Epoch 8/15: [======                        ] 17/75 batches, loss: 0.1064Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.1051Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.1027Epoch 8/15: [========                      ] 20/75 batches, loss: 0.1012Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0986Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0973Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0989Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0981Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0973Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0969Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0977Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0978Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0964Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0960Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0950Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0947Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0943Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0944Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0943Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0936Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0926Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0923Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0923Epoch 8/15: [================              ] 40/75 batches, loss: 0.0923Epoch 8/15: [================              ] 41/75 batches, loss: 0.0916Epoch 8/15: [================              ] 42/75 batches, loss: 0.0907Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0896Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0907Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0918Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0921Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0914Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0906Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0904Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0902Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0894Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0888Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0880Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0882Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0885Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0901Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0905Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0907Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0914Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0911Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0908Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0918Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0914Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0907Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0908Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0929Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0923Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0920Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0924Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0929Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0931Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0927Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0928Epoch 8/15: [==============================] 75/75 batches, loss: 0.0924
[2025-05-02 11:04:49,239][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0924
[2025-05-02 11:04:49,444][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0494, Metrics: {'mse': 0.04936081916093826, 'rmse': 0.22217294876050564, 'r2': 0.1955210566520691}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0652Epoch 9/15: [                              ] 2/75 batches, loss: 0.0648Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0731Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0827Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0729Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0742Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0721Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0741Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0711Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0691Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0681Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0679Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0688Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0676Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0700Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0693Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0688Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0680Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0688Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0709Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0740Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0764Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0796Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0784Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0772Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0755Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0762Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0771Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0807Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0788Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0785Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0785Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0771Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0778Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0791Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0792Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0804Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0817Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0815Epoch 9/15: [================              ] 40/75 batches, loss: 0.0815Epoch 9/15: [================              ] 41/75 batches, loss: 0.0814Epoch 9/15: [================              ] 42/75 batches, loss: 0.0813Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0815Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0818Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0810Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0816Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0814Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0812Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0819Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0815Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0820Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0813Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0815Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0823Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0820Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0814Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0811Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0815Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0823Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0818Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0809Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0810Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0813Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0809Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0805Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0804Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0808Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0812Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0816Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0823Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0819Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0819Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0816Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0815Epoch 9/15: [==============================] 75/75 batches, loss: 0.0819
[2025-05-02 11:04:52,173][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0819
[2025-05-02 11:04:52,403][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0464, Metrics: {'mse': 0.046341024339199066, 'rmse': 0.215269654942816, 'r2': 0.244737446308136}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0776Epoch 10/15: [                              ] 2/75 batches, loss: 0.0801Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0795Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0749Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0716Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0763Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0919Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0911Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0960Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0982Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0981Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0956Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0929Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0915Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0926Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0905Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0877Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0868Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0856Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0855Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0861Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0847Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0846Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0837Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0827Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0818Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0805Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0792Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0795Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0794Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0799Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0798Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0808Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0799Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0826Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0827Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0821Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0817Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0828Epoch 10/15: [================              ] 40/75 batches, loss: 0.0846Epoch 10/15: [================              ] 41/75 batches, loss: 0.0837Epoch 10/15: [================              ] 42/75 batches, loss: 0.0837Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0842Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0831Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0832Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0834Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0843Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0840Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0842Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0838Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0836Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0847Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0845Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0842Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0848Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0854Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0851Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0842Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0843Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0836Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0841Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0841Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0841Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0846Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0836Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0837Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0841Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0839Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0836Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0833Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0829Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0833Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0835Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0834Epoch 10/15: [==============================] 75/75 batches, loss: 0.0846
[2025-05-02 11:04:55,090][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0846
[2025-05-02 11:04:55,295][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0437, Metrics: {'mse': 0.04373569041490555, 'rmse': 0.20913079738504692, 'r2': 0.2871989607810974}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.1170Epoch 11/15: [                              ] 2/75 batches, loss: 0.0870Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0886Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0851Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0877Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0815Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0897Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0842Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0832Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0925Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0910Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0874Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0854Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0824Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0810Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0811Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0778Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0773Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0784Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0788Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0772Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0767Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0763Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0752Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0753Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0755Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0753Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0747Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0753Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0751Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0753Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0744Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0738Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0732Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0731Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0726Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0723Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0717Epoch 11/15: [================              ] 40/75 batches, loss: 0.0725Epoch 11/15: [================              ] 41/75 batches, loss: 0.0714Epoch 11/15: [================              ] 42/75 batches, loss: 0.0715Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0711Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0712Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0712Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0724Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0731Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0730Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0730Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0727Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0737Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0752Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0747Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0752Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0750Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0745Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0768Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0767Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0762Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0756Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0755Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0759Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0756Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0756Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0751Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0750Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0754Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0751Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0752Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0752Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0748Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0755Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0757Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0758Epoch 11/15: [==============================] 75/75 batches, loss: 0.0760
[2025-05-02 11:04:57,965][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0760
[2025-05-02 11:04:58,196][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0436, Metrics: {'mse': 0.04371461644768715, 'rmse': 0.20908040665659502, 'r2': 0.287542462348938}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.1192Epoch 12/15: [                              ] 2/75 batches, loss: 0.1024Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0848Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0796Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0783Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0780Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0755Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0714Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0700Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0700Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0664Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0660Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0696Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0717Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0708Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0703Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0719Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0719Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0726Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0736Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0729Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0742Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0722Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0726Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0739Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0730Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0737Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0742Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0732Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0739Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0732Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0734Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0731Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0723Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0724Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0735Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0745Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0752Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0761Epoch 12/15: [================              ] 40/75 batches, loss: 0.0767Epoch 12/15: [================              ] 41/75 batches, loss: 0.0762Epoch 12/15: [================              ] 42/75 batches, loss: 0.0773Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0769Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0764Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0755Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0757Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0766Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0765Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0757Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0766Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0769Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0765Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0761Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0755Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0766Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0761Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0754Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0746Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0742Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0736Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0732Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0732Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0731Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0728Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0727Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0727Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0739Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0740Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0737Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0737Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0735Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0736Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0741Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0739Epoch 12/15: [==============================] 75/75 batches, loss: 0.0740
[2025-05-02 11:05:02,773][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0740
[2025-05-02 11:05:02,997][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0495, Metrics: {'mse': 0.04931630194187164, 'rmse': 0.22207274020435655, 'r2': 0.19624656438827515}
[2025-05-02 11:05:02,998][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0424Epoch 13/15: [                              ] 2/75 batches, loss: 0.0433Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0511Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0520Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0498Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0515Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0490Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0575Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0569Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0550Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0520Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0550Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0561Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0621Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0651Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0622Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0628Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0662Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0649Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0634Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0636Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0633Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0649Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0660Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0653Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0655Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0656Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0652Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0646Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0646Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0641Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0637Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0650Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0654Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0665Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0666Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0666Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0665Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0671Epoch 13/15: [================              ] 40/75 batches, loss: 0.0673Epoch 13/15: [================              ] 41/75 batches, loss: 0.0673Epoch 13/15: [================              ] 42/75 batches, loss: 0.0674Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0668Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0669Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0679Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0676Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0670Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0670Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0675Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0671Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0664Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0668Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0663Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0667Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0666Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0669Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0673Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0677Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0673Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0675Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0674Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0679Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0680Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0682Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0680Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0682Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0683Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0685Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0693Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0690Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0695Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0687Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0686Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0688Epoch 13/15: [==============================] 75/75 batches, loss: 0.0685
[2025-05-02 11:05:05,278][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0685
[2025-05-02 11:05:05,481][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0444, Metrics: {'mse': 0.044347964227199554, 'rmse': 0.2105895634337076, 'r2': 0.2772201895713806}
[2025-05-02 11:05:05,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0198Epoch 14/15: [                              ] 2/75 batches, loss: 0.0439Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0491Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0501Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0498Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0495Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0499Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0500Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0524Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0526Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0537Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0531Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0560Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0572Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0580Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0574Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0575Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0579Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0591Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0582Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0587Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0581Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0574Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0569Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0587Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0607Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0601Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0636Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0644Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0642Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0650Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0652Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0652Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0655Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0655Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0652Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0650Epoch 14/15: [================              ] 40/75 batches, loss: 0.0647Epoch 14/15: [================              ] 41/75 batches, loss: 0.0657Epoch 14/15: [================              ] 42/75 batches, loss: 0.0659Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0654Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0646Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0641Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0645Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0648Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0647Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0648Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0649Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0649Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0663Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0666Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0665Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0664Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0663Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0658Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0655Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0648Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0651Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0648Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0650Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0649Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0644Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0641Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0641Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0642Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0640Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0645Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0646Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0649Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0647Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0645Epoch 14/15: [==============================] 75/75 batches, loss: 0.0647
[2025-05-02 11:05:07,765][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0647
[2025-05-02 11:05:07,975][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0536, Metrics: {'mse': 0.05339943990111351, 'rmse': 0.23108318827018445, 'r2': 0.1296999454498291}
[2025-05-02 11:05:07,976][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0560Epoch 15/15: [                              ] 2/75 batches, loss: 0.0443Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0387Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0395Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0448Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0454Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0440Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0444Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0523Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0587Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0596Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0578Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0611Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0652Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0630Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0649Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0648Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0656Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0655Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0658Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0652Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0642Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0643Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0646Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0656Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0673Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0671Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0687Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0692Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0708Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0699Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0695Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0694Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0694Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0687Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0700Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0705Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0694Epoch 15/15: [================              ] 40/75 batches, loss: 0.0697Epoch 15/15: [================              ] 41/75 batches, loss: 0.0697Epoch 15/15: [================              ] 42/75 batches, loss: 0.0691Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0686Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0681Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0683Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0682Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0679Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0678Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0684Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0689Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0686Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0698Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0701Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0707Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0704Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0701Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0700Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0696Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0697Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0697Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0695Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0688Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0684Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0683Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0687Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0684Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0681Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0680Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0681Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0683Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0684Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0680Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0678Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0679Epoch 15/15: [==============================] 75/75 batches, loss: 0.0683
[2025-05-02 11:05:10,263][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0683
[2025-05-02 11:05:10,484][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0456, Metrics: {'mse': 0.04549632966518402, 'rmse': 0.213298686505998, 'r2': 0.2585042119026184}
[2025-05-02 11:05:10,485][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:05:10,485][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-02 11:05:10,485][src.training.lm_trainer][INFO] - Training completed in 43.62 seconds
[2025-05-02 11:05:10,485][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:05:13,299][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03618081659078598, 'rmse': 0.19021255634364936, 'r2': 0.09695464372634888}
[2025-05-02 11:05:13,300][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04371461644768715, 'rmse': 0.20908040665659502, 'r2': 0.287542462348938}
[2025-05-02 11:05:13,300][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.039813533425331116, 'rmse': 0.1995332890154701, 'r2': 0.23521625995635986}
[2025-05-02 11:05:14,996][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/ja/model.pt
[2025-05-02 11:05:14,998][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▄▂▂▁▁▁
wandb:     best_val_mse █▄▄▄▂▂▁▁▁
wandb:      best_val_r2 ▁▅▅▅▇▇███
wandb:    best_val_rmse █▄▄▄▂▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▅▆▅▅▆▆▆▆▆▆▅
wandb:       train_loss █▅▅▄▄▃▃▂▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▄▂▃▃▂▁▁▁▂▁▃▁
wandb:          val_mse █▄▄▄▂▃▃▂▁▁▁▂▁▃▁
wandb:           val_r2 ▁▅▅▅▇▆▆▇███▇█▆█
wandb:         val_rmse █▄▄▄▂▃▃▂▂▁▁▂▁▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04358
wandb:     best_val_mse 0.04371
wandb:      best_val_r2 0.28754
wandb:    best_val_rmse 0.20908
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.03981
wandb:    final_test_r2 0.23522
wandb:  final_test_rmse 0.19953
wandb:  final_train_mse 0.03618
wandb:   final_train_r2 0.09695
wandb: final_train_rmse 0.19021
wandb:    final_val_mse 0.04371
wandb:     final_val_r2 0.28754
wandb:   final_val_rmse 0.20908
wandb:    learning_rate 2e-05
wandb:       train_loss 0.06825
wandb:       train_time 43.61814
wandb:         val_loss 0.04562
wandb:          val_mse 0.0455
wandb:           val_r2 0.2585
wandb:         val_rmse 0.2133
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110416-l40b7imt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_110416-l40b7imt/logs
Experiment  completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/ja/results.json
Running experiment: 
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=-1"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name="         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:05:27,126][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type
experiment_name: ''
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: false
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:05:27,126][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:05:27,126][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:05:27,126][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:05:27,130][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-02 11:05:27,130][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:05:28,562][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
slurmstepd: error: *** JOB 64437392 ON k28i22 CANCELLED AT 2025-05-02T11:05:30 ***
