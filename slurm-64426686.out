SLURM_JOB_ID: 64426686
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed Apr 30 12:19:19 CEST 2025
Walltime: 00-00:30:00
========================================================================
Activating conda environment...
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.4.26)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Wed Apr 30 12:19:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:17:00.0 Off |                    0 |
| N/A   34C    P0             44W /  300W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Verifying finetuning model configuration...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Fine-tuning model check:
- Trainable parameters: 394,712,833
- Total parameters: 394,712,833
- Percentage trainable: 100.00%
- Encoder trainable: 394,121,472 / 394,121,472 (100.00%)
- Head layer: 768 → 768 features
- Head layer: 768 → 1 features
SUCCESS: Model is properly set up for fine-tuning with maximum head size.
===== Running priority experiments =====
Running priority experiment: en, question_type
Running experiment: finetune_question_type_en
Output directory: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.head_hidden_size=768"         "model.head_layers=2"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 12:20:18,236][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 12:20:18,236][__main__][INFO] - Normalized task: question_type
[2025-04-30 12:20:18,236][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 12:20:18,236][__main__][INFO] - Determined Task Type: classification
[2025-04-30 12:20:18,240][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-30 12:20:18,240][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 12:20:19,769][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 12:20:22,005][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 12:20:22,006][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:20:22,103][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,141][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,235][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-30 12:20:22,244][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:20:22,244][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-30 12:20:22,245][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:20:22,265][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,295][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,309][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-30 12:20:22,310][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:20:22,310][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-30 12:20:22,311][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:20:22,330][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,359][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:20:22,372][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-30 12:20:22,373][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:20:22,373][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-30 12:20:22,374][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-30 12:20:22,375][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:20:22,375][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:20:22,375][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:20:22,375][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:20:22,375][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-30 12:20:22,376][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:20:22,376][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-30 12:20:22,376][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-30 12:20:22,376][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:20:22,377][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-30 12:20:22,377][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-30 12:20:22,377][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 12:20:22,378][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 12:20:22,378][__main__][INFO] - Using model type: lm_probe
[2025-04-30 12:20:22,378][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 12:20:26,088][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 12:20:26,089][src.models.model_factory][INFO] - Language model parameters trainable
[2025-04-30 12:20:26,089][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-04-30 12:20:26,089][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-04-30 12:20:26,090][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-30 12:20:26,091][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 73,921 trainable parameters
[2025-04-30 12:20:26,091][src.models.model_factory][INFO] - Probe configuration: hidden_size=96, dropout=0.1
[2025-04-30 12:20:26,091][__main__][INFO] - Successfully created lm_probe model for en
[2025-04-30 12:20:26,092][__main__][INFO] - Total parameters: 394,195,393
[2025-04-30 12:20:26,092][__main__][INFO] - Trainable parameters: 394,195,393 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.8574Epoch 1/10: [                              ] 2/75 batches, loss: 0.7771Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7261Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7103Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7121Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7142Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7021Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7054Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7117Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7124Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7009Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7022Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6987Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7009Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7065Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7033Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7047Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7039Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7069Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7077Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7097Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7160Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7133Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7176Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7147Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7149Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7115Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7135Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7135Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7144Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7178Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7146Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7144Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7153Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7164Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7198Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7206Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7210Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7212Epoch 1/10: [================              ] 40/75 batches, loss: 0.7199Epoch 1/10: [================              ] 41/75 batches, loss: 0.7174Epoch 1/10: [================              ] 42/75 batches, loss: 0.7158Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7154Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7144Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7148Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7150Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7133Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7141Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7139Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7137Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7130Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7119Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7118Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7103Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7091Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7070Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7054Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7035Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7024Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7009Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7000Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6998Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6995Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6985Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6982Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6979Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6970Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6956Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6945Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6931Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6939Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6940Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6932Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6927Epoch 1/10: [==============================] 75/75 batches, loss: 0.6903
[2025-04-30 12:20:36,482][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6903
[2025-04-30 12:20:36,732][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6832, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9428571428571428, 'precision': 0.9705882352941176, 'recall': 0.9166666666666666}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6480Epoch 2/10: [                              ] 2/75 batches, loss: 0.6452Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6318Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6084Epoch 2/10: [==                            ] 5/75 batches, loss: 0.5993Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6042Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6077Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6176Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6251Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6222Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6176Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6218Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6160Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6107Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6048Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6064Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6013Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5965Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5995Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5988Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5936Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5958Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5962Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5979Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5990Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5980Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5969Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5945Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5926Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5911Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5926Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5911Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5896Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5900Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5882Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5861Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5865Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5853Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5839Epoch 2/10: [================              ] 40/75 batches, loss: 0.5851Epoch 2/10: [================              ] 41/75 batches, loss: 0.5828Epoch 2/10: [================              ] 42/75 batches, loss: 0.5827Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5823Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5818Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5806Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5805Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5791Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5792Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5781Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5769Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5767Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5758Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5746Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5764Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5735Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5725Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5720Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5706Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5688Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5669Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5666Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5664Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5650Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5646Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5637Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5628Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5628Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5611Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5609Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5596Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5600Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5601Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5595Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5586Epoch 2/10: [==============================] 75/75 batches, loss: 0.5574
[2025-04-30 12:20:44,705][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5574
[2025-04-30 12:20:44,964][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5443, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.958904109589041, 'precision': 0.9459459459459459, 'recall': 0.9722222222222222}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6068Epoch 3/10: [                              ] 2/75 batches, loss: 0.5475Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5527Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5430Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5234Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5181Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5116Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5179Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5252Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5167Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5241Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5237Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5211Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5177Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5197Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5132Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5179Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5177Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5143Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5139Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5131Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5130Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5153Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5144Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5133Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5125Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5158Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5181Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5163Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5161Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5168Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5167Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5150Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5156Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5154Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5152Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5156Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5149Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5147Epoch 3/10: [================              ] 40/75 batches, loss: 0.5146Epoch 3/10: [================              ] 41/75 batches, loss: 0.5154Epoch 3/10: [================              ] 42/75 batches, loss: 0.5152Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5179Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5191Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5189Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5187Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5171Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5155Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5171Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5164Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5158Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5173Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5163Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5149Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5145Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5145Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5140Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5148Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5167Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5166Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5169Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5156Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5148Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5147Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5164Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5159Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5155Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5151Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5147Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5151Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5150Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5145Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5151Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5153Epoch 3/10: [==============================] 75/75 batches, loss: 0.5158
[2025-04-30 12:20:52,960][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5158
[2025-04-30 12:20:53,242][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5365, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5110Epoch 4/10: [                              ] 2/75 batches, loss: 0.4972Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5092Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5272Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5089Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5151Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5148Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5228Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5210Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5234Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5271Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5234Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5168Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5160Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5206Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5199Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5219Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5210Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5202Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5231Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5201Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5208Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5201Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5185Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5162Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5167Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5163Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5176Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5164Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5192Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5172Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5161Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5158Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5142Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5132Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5130Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5135Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5121Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5126Epoch 4/10: [================              ] 40/75 batches, loss: 0.5130Epoch 4/10: [================              ] 41/75 batches, loss: 0.5157Epoch 4/10: [================              ] 42/75 batches, loss: 0.5144Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5153Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5135Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5123Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5138Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5132Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5121Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5120Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5107Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5114Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5113Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5108Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5126Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5129Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5128Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5119Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5122Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5125Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5112Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5111Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5102Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5094Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5098Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5090Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5082Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5089Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5092Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5078Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5078Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5077Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5080Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5080Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5086Epoch 4/10: [==============================] 75/75 batches, loss: 0.5099
[2025-04-30 12:21:01,312][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5099
[2025-04-30 12:21:01,597][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5318, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.958904109589041, 'precision': 0.9459459459459459, 'recall': 0.9722222222222222}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4337Epoch 5/10: [                              ] 2/75 batches, loss: 0.4455Epoch 5/10: [=                             ] 3/75 batches, loss: 0.4653Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4972Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4948Epoch 5/10: [==                            ] 6/75 batches, loss: 0.5005Epoch 5/10: [==                            ] 7/75 batches, loss: 0.5013Epoch 5/10: [===                           ] 8/75 batches, loss: 0.5017Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4995Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4954Epoch 5/10: [====                          ] 11/75 batches, loss: 0.5051Epoch 5/10: [====                          ] 12/75 batches, loss: 0.5071Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.5106Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.5051Epoch 5/10: [======                        ] 15/75 batches, loss: 0.5052Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4977Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4981Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4985Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4914Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4921Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4950Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4955Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4959Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4963Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4952Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4989Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.5018Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4991Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.5018Epoch 5/10: [============                  ] 30/75 batches, loss: 0.5019Epoch 5/10: [============                  ] 31/75 batches, loss: 0.5005Epoch 5/10: [============                  ] 32/75 batches, loss: 0.5014Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.5029Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.5023Epoch 5/10: [==============                ] 35/75 batches, loss: 0.5044Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5037Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5044Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5069Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5038Epoch 5/10: [================              ] 40/75 batches, loss: 0.5027Epoch 5/10: [================              ] 41/75 batches, loss: 0.5016Epoch 5/10: [================              ] 42/75 batches, loss: 0.5033Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5017Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5018Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5008Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5014Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5014Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5020Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5019Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5010Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5015Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5011Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5021Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5021Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5030Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5027Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5035Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5032Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5036Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5036Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5040Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5044Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5044Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5052Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5034Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5034Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5052Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5059Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5062Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5069Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5068Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5061Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5052Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5061Epoch 5/10: [==============================] 75/75 batches, loss: 0.5086
[2025-04-30 12:21:09,550][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5086
[2025-04-30 12:21:09,843][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5315, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.6006Epoch 6/10: [                              ] 2/75 batches, loss: 0.5407Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5128Epoch 6/10: [=                             ] 4/75 batches, loss: 0.5168Epoch 6/10: [==                            ] 5/75 batches, loss: 0.4906Epoch 6/10: [==                            ] 6/75 batches, loss: 0.5008Epoch 6/10: [==                            ] 7/75 batches, loss: 0.4945Epoch 6/10: [===                           ] 8/75 batches, loss: 0.4987Epoch 6/10: [===                           ] 9/75 batches, loss: 0.4967Epoch 6/10: [====                          ] 10/75 batches, loss: 0.4998Epoch 6/10: [====                          ] 11/75 batches, loss: 0.4980Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5025Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5027Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5079Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5062Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5060Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5047Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5081Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5054Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5030Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5053Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5041Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5021Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5022Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5014Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5042Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5042Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5051Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5034Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5027Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5004Epoch 6/10: [============                  ] 32/75 batches, loss: 0.4983Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.4992Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.4994Epoch 6/10: [==============                ] 35/75 batches, loss: 0.4995Epoch 6/10: [==============                ] 36/75 batches, loss: 0.4996Epoch 6/10: [==============                ] 37/75 batches, loss: 0.4978Epoch 6/10: [===============               ] 38/75 batches, loss: 0.4967Epoch 6/10: [===============               ] 39/75 batches, loss: 0.4988Epoch 6/10: [================              ] 40/75 batches, loss: 0.4983Epoch 6/10: [================              ] 41/75 batches, loss: 0.4990Epoch 6/10: [================              ] 42/75 batches, loss: 0.5008Epoch 6/10: [=================             ] 43/75 batches, loss: 0.4998Epoch 6/10: [=================             ] 44/75 batches, loss: 0.4972Epoch 6/10: [==================            ] 45/75 batches, loss: 0.4974Epoch 6/10: [==================            ] 46/75 batches, loss: 0.4985Epoch 6/10: [==================            ] 47/75 batches, loss: 0.4987Epoch 6/10: [===================           ] 48/75 batches, loss: 0.4968Epoch 6/10: [===================           ] 49/75 batches, loss: 0.4960Epoch 6/10: [====================          ] 50/75 batches, loss: 0.4950Epoch 6/10: [====================          ] 51/75 batches, loss: 0.4947Epoch 6/10: [====================          ] 52/75 batches, loss: 0.4963Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.4951Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.4948Epoch 6/10: [======================        ] 55/75 batches, loss: 0.4967Epoch 6/10: [======================        ] 56/75 batches, loss: 0.4969Epoch 6/10: [======================        ] 57/75 batches, loss: 0.4974Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.4992Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5007Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5012Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5028Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5036Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5044Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5047Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5046Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5053Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5060Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5067Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5073Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5066Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5076Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5069Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5062Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5068Epoch 6/10: [==============================] 75/75 batches, loss: 0.5062
[2025-04-30 12:21:17,838][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5062
[2025-04-30 12:21:18,127][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5313, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.4569Epoch 7/10: [                              ] 2/75 batches, loss: 0.5041Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5277Epoch 7/10: [=                             ] 4/75 batches, loss: 0.5455Epoch 7/10: [==                            ] 5/75 batches, loss: 0.5229Epoch 7/10: [==                            ] 6/75 batches, loss: 0.5119Epoch 7/10: [==                            ] 7/75 batches, loss: 0.5176Epoch 7/10: [===                           ] 8/75 batches, loss: 0.5130Epoch 7/10: [===                           ] 9/75 batches, loss: 0.5041Epoch 7/10: [====                          ] 10/75 batches, loss: 0.5088Epoch 7/10: [====                          ] 11/75 batches, loss: 0.5192Epoch 7/10: [====                          ] 12/75 batches, loss: 0.5081Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.5078Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.5041Epoch 7/10: [======                        ] 15/75 batches, loss: 0.5089Epoch 7/10: [======                        ] 16/75 batches, loss: 0.5131Epoch 7/10: [======                        ] 17/75 batches, loss: 0.5070Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.5081Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.5054Epoch 7/10: [========                      ] 20/75 batches, loss: 0.5053Epoch 7/10: [========                      ] 21/75 batches, loss: 0.5041Epoch 7/10: [========                      ] 22/75 batches, loss: 0.5041Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.5052Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5031Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5060Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5087Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5129Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.5118Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.5090Epoch 7/10: [============                  ] 30/75 batches, loss: 0.5110Epoch 7/10: [============                  ] 31/75 batches, loss: 0.5115Epoch 7/10: [============                  ] 32/75 batches, loss: 0.5120Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.5104Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.5081Epoch 7/10: [==============                ] 35/75 batches, loss: 0.5066Epoch 7/10: [==============                ] 36/75 batches, loss: 0.5059Epoch 7/10: [==============                ] 37/75 batches, loss: 0.5046Epoch 7/10: [===============               ] 38/75 batches, loss: 0.5027Epoch 7/10: [===============               ] 39/75 batches, loss: 0.5037Epoch 7/10: [================              ] 40/75 batches, loss: 0.5049Epoch 7/10: [================              ] 41/75 batches, loss: 0.5055Epoch 7/10: [================              ] 42/75 batches, loss: 0.5060Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5074Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5084Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5083Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5067Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5072Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5071Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5078Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5077Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5091Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5094Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5084Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5092Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5096Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5091Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5094Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5097Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5096Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5083Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5063Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5051Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5051Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5051Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5058Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5065Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5061Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5057Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5057Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5060Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5053Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5063Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5069Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5069Epoch 7/10: [==============================] 75/75 batches, loss: 0.5056
[2025-04-30 12:21:26,211][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5056
[2025-04-30 12:21:26,514][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5316, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-04-30 12:21:26,515][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.5036Epoch 8/10: [                              ] 2/75 batches, loss: 0.5158Epoch 8/10: [=                             ] 3/75 batches, loss: 0.5281Epoch 8/10: [=                             ] 4/75 batches, loss: 0.4983Epoch 8/10: [==                            ] 5/75 batches, loss: 0.4994Epoch 8/10: [==                            ] 6/75 batches, loss: 0.5081Epoch 8/10: [==                            ] 7/75 batches, loss: 0.5008Epoch 8/10: [===                           ] 8/75 batches, loss: 0.5041Epoch 8/10: [===                           ] 9/75 batches, loss: 0.5147Epoch 8/10: [====                          ] 10/75 batches, loss: 0.5112Epoch 8/10: [====                          ] 11/75 batches, loss: 0.5062Epoch 8/10: [====                          ] 12/75 batches, loss: 0.5172Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.5126Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.5131Epoch 8/10: [======                        ] 15/75 batches, loss: 0.5125Epoch 8/10: [======                        ] 16/75 batches, loss: 0.5149Epoch 8/10: [======                        ] 17/75 batches, loss: 0.5101Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.5084Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.5069Epoch 8/10: [========                      ] 20/75 batches, loss: 0.5032Epoch 8/10: [========                      ] 21/75 batches, loss: 0.5033Epoch 8/10: [========                      ] 22/75 batches, loss: 0.5055Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.5044Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.5064Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.5082Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.5108Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.5087Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.5094Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.5109Epoch 8/10: [============                  ] 30/75 batches, loss: 0.5091Epoch 8/10: [============                  ] 31/75 batches, loss: 0.5066Epoch 8/10: [============                  ] 32/75 batches, loss: 0.5050Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.5050Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.5036Epoch 8/10: [==============                ] 35/75 batches, loss: 0.5022Epoch 8/10: [==============                ] 36/75 batches, loss: 0.5010Epoch 8/10: [==============                ] 37/75 batches, loss: 0.5017Epoch 8/10: [===============               ] 38/75 batches, loss: 0.5023Epoch 8/10: [===============               ] 39/75 batches, loss: 0.5012Epoch 8/10: [================              ] 40/75 batches, loss: 0.5024Epoch 8/10: [================              ] 41/75 batches, loss: 0.5019Epoch 8/10: [================              ] 42/75 batches, loss: 0.5019Epoch 8/10: [=================             ] 43/75 batches, loss: 0.5012Epoch 8/10: [=================             ] 44/75 batches, loss: 0.5024Epoch 8/10: [==================            ] 45/75 batches, loss: 0.5019Epoch 8/10: [==================            ] 46/75 batches, loss: 0.5019Epoch 8/10: [==================            ] 47/75 batches, loss: 0.5009Epoch 8/10: [===================           ] 48/75 batches, loss: 0.5010Epoch 8/10: [===================           ] 49/75 batches, loss: 0.5020Epoch 8/10: [====================          ] 50/75 batches, loss: 0.5021Epoch 8/10: [====================          ] 51/75 batches, loss: 0.5007Epoch 8/10: [====================          ] 52/75 batches, loss: 0.5012Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.5008Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.5017Epoch 8/10: [======================        ] 55/75 batches, loss: 0.5022Epoch 8/10: [======================        ] 56/75 batches, loss: 0.5018Epoch 8/10: [======================        ] 57/75 batches, loss: 0.5010Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.5019Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.5019Epoch 8/10: [========================      ] 60/75 batches, loss: 0.5039Epoch 8/10: [========================      ] 61/75 batches, loss: 0.5035Epoch 8/10: [========================      ] 62/75 batches, loss: 0.5028Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.5024Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.5024Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.5031Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.5034Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.5031Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.5041Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.5041Epoch 8/10: [============================  ] 70/75 batches, loss: 0.5052Epoch 8/10: [============================  ] 71/75 batches, loss: 0.5058Epoch 8/10: [============================  ] 72/75 batches, loss: 0.5054Epoch 8/10: [============================= ] 73/75 batches, loss: 0.5054Epoch 8/10: [============================= ] 74/75 batches, loss: 0.5061Epoch 8/10: [==============================] 75/75 batches, loss: 0.5060
[2025-04-30 12:21:34,151][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5060
[2025-04-30 12:21:34,451][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5316, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-04-30 12:21:34,452][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.5039Epoch 9/10: [                              ] 2/75 batches, loss: 0.4801Epoch 9/10: [=                             ] 3/75 batches, loss: 0.4930Epoch 9/10: [=                             ] 4/75 batches, loss: 0.5016Epoch 9/10: [==                            ] 5/75 batches, loss: 0.4783Epoch 9/10: [==                            ] 6/75 batches, loss: 0.4904Epoch 9/10: [==                            ] 7/75 batches, loss: 0.4923Epoch 9/10: [===                           ] 8/75 batches, loss: 0.4907Epoch 9/10: [===                           ] 9/75 batches, loss: 0.4895Epoch 9/10: [====                          ] 10/75 batches, loss: 0.4886Epoch 9/10: [====                          ] 11/75 batches, loss: 0.4878Epoch 9/10: [====                          ] 12/75 batches, loss: 0.4892Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.4885Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.4828Epoch 9/10: [======                        ] 15/75 batches, loss: 0.4826Epoch 9/10: [======                        ] 16/75 batches, loss: 0.4825Epoch 9/10: [======                        ] 17/75 batches, loss: 0.4865Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.4896Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.4954Epoch 9/10: [========                      ] 20/75 batches, loss: 0.4922Epoch 9/10: [========                      ] 21/75 batches, loss: 0.4939Epoch 9/10: [========                      ] 22/75 batches, loss: 0.4965Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.4978Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.5001Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.5021Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.4985Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.4969Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.4955Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.4958Epoch 9/10: [============                  ] 30/75 batches, loss: 0.4976Epoch 9/10: [============                  ] 31/75 batches, loss: 0.4986Epoch 9/10: [============                  ] 32/75 batches, loss: 0.4995Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.5011Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.5053Epoch 9/10: [==============                ] 35/75 batches, loss: 0.5057Epoch 9/10: [==============                ] 36/75 batches, loss: 0.5083Epoch 9/10: [==============                ] 37/75 batches, loss: 0.5088Epoch 9/10: [===============               ] 38/75 batches, loss: 0.5075Epoch 9/10: [===============               ] 39/75 batches, loss: 0.5068Epoch 9/10: [================              ] 40/75 batches, loss: 0.5055Epoch 9/10: [================              ] 41/75 batches, loss: 0.5055Epoch 9/10: [================              ] 42/75 batches, loss: 0.5060Epoch 9/10: [=================             ] 43/75 batches, loss: 0.5054Epoch 9/10: [=================             ] 44/75 batches, loss: 0.5048Epoch 9/10: [==================            ] 45/75 batches, loss: 0.5043Epoch 9/10: [==================            ] 46/75 batches, loss: 0.5042Epoch 9/10: [==================            ] 47/75 batches, loss: 0.5042Epoch 9/10: [===================           ] 48/75 batches, loss: 0.5042Epoch 9/10: [===================           ] 49/75 batches, loss: 0.5032Epoch 9/10: [====================          ] 50/75 batches, loss: 0.5037Epoch 9/10: [====================          ] 51/75 batches, loss: 0.5047Epoch 9/10: [====================          ] 52/75 batches, loss: 0.5037Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.5042Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.5046Epoch 9/10: [======================        ] 55/75 batches, loss: 0.5059Epoch 9/10: [======================        ] 56/75 batches, loss: 0.5058Epoch 9/10: [======================        ] 57/75 batches, loss: 0.5054Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.5050Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.5053Epoch 9/10: [========================      ] 60/75 batches, loss: 0.5065Epoch 9/10: [========================      ] 61/75 batches, loss: 0.5057Epoch 9/10: [========================      ] 62/75 batches, loss: 0.5055Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.5055Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.5058Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.5069Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.5072Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.5064Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.5081Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.5088Epoch 9/10: [============================  ] 70/75 batches, loss: 0.5090Epoch 9/10: [============================  ] 71/75 batches, loss: 0.5083Epoch 9/10: [============================  ] 72/75 batches, loss: 0.5086Epoch 9/10: [============================= ] 73/75 batches, loss: 0.5082Epoch 9/10: [============================= ] 74/75 batches, loss: 0.5062Epoch 9/10: [==============================] 75/75 batches, loss: 0.5055
[2025-04-30 12:21:42,082][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5055
[2025-04-30 12:21:42,386][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5330, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-04-30 12:21:42,386][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-30 12:21:42,387][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-04-30 12:21:42,387][src.training.lm_trainer][INFO] - Training completed in 74.59 seconds
[2025-04-30 12:21:42,387][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 12:21:45,407][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488, 'precision': 0.9933333333333333, 'recall': 1.0}
[2025-04-30 12:21:45,408][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
[2025-04-30 12:21:45,408][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9636363636363636, 'f1': 0.9642857142857143, 'precision': 0.9473684210526315, 'recall': 0.9818181818181818}
[2025-04-30 12:21:47,435][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
[2025-04-30 12:21:47,441][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁█████
wandb:           best_val_f1 ▁█████
wandb:         best_val_loss █▂▁▁▁▁
wandb:    best_val_precision █▄▁▄▁▁
wandb:       best_val_recall ▁▆█▆██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▄▄▄▄▄▄
wandb:            train_loss █▃▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁████████
wandb:                val_f1 ▁████████
wandb:              val_loss █▂▁▁▁▁▁▁▁
wandb:         val_precision █▄▁▄▁▁▁▁▁
wandb:            val_recall ▁▆█▆█████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.95833
wandb:           best_val_f1 0.96
wandb:         best_val_loss 0.53133
wandb:    best_val_precision 0.92308
wandb:       best_val_recall 1
wandb:      early_stop_epoch 9
wandb:                 epoch 9
wandb:   final_test_accuracy 0.96364
wandb:         final_test_f1 0.96429
wandb:  final_test_precision 0.94737
wandb:     final_test_recall 0.98182
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99666
wandb: final_train_precision 0.99333
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.95833
wandb:          final_val_f1 0.96
wandb:   final_val_precision 0.92308
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50551
wandb:            train_time 74.59277
wandb:          val_accuracy 0.95833
wandb:                val_f1 0.96
wandb:              val_loss 0.533
wandb:         val_precision 0.92308
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_122018-d331yqmm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_122018-d331yqmm/logs
Experiment finetune_question_type_en completed successfully in 101 seconds
Running experiment: finetune_question_type_control1_en
Output directory: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/control1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.head_hidden_size=768"         "model.head_layers=2"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         "experiment_name=finetune_question_type_control1_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en/control1"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 12:22:12,702][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/control1
experiment_name: finetune_question_type_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 12:22:12,702][__main__][INFO] - Normalized task: question_type
[2025-04-30 12:22:12,702][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 12:22:12,702][__main__][INFO] - Determined Task Type: classification
[2025-04-30 12:22:12,706][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-30 12:22:12,706][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 12:22:14,062][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 12:22:16,454][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 12:22:16,454][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:22:16,493][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-30 12:22:16,519][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-30 12:22:16,607][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-30 12:22:16,618][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:22:16,618][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-30 12:22:16,619][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:22:16,637][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:22:16,668][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:22:16,679][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-30 12:22:16,680][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:22:16,680][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-30 12:22:16,681][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 12:22:16,699][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:22:16,730][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 12:22:16,743][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-30 12:22:16,745][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 12:22:16,745][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-30 12:22:16,746][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-30 12:22:16,747][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:22:16,747][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:22:16,747][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:22:16,747][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:22:16,747][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-30 12:22:16,747][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:22:16,748][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-30 12:22:16,748][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-30 12:22:16,748][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 12:22:16,749][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-30 12:22:16,749][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-30 12:22:16,749][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 12:22:16,750][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 12:22:16,750][__main__][INFO] - Using model type: lm_probe
[2025-04-30 12:22:16,750][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 12:22:21,148][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 12:22:21,149][src.models.model_factory][INFO] - Language model parameters trainable
[2025-04-30 12:22:21,149][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-04-30 12:22:21,149][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-04-30 12:22:21,151][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-30 12:22:21,152][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 73,921 trainable parameters
[2025-04-30 12:22:21,152][src.models.model_factory][INFO] - Probe configuration: hidden_size=96, dropout=0.1
[2025-04-30 12:22:21,152][__main__][INFO] - Successfully created lm_probe model for en
[2025-04-30 12:22:21,153][__main__][INFO] - Total parameters: 394,195,393
[2025-04-30 12:22:21,153][__main__][INFO] - Trainable parameters: 394,195,393 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7914Epoch 1/10: [                              ] 2/75 batches, loss: 0.7245Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7238Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7173Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7233Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7130Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7151Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7049Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7101Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7189Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7135Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7095Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7203Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7157Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7076Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7064Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7106Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7141Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7125Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7176Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7184Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7158Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7165Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7182Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7189Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7225Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7190Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7178Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7183Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7196Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7188Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7188Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7180Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7202Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7223Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7252Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7235Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7236Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7237Epoch 1/10: [================              ] 40/75 batches, loss: 0.7246Epoch 1/10: [================              ] 41/75 batches, loss: 0.7262Epoch 1/10: [================              ] 42/75 batches, loss: 0.7254Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7277Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7270Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7255Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7227Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7242Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7249Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7242Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7236Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7236Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7256Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7268Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7262Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7247Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7275Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7296Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7307Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7307Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7310Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7292Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7301Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7293Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7303Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7293Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7297Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7282Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7270Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7273Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7258Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7251Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7241Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7256Epoch 1/10: [============================= ] 74/75 batches, loss: 0.7258Epoch 1/10: [==============================] 75/75 batches, loss: 0.7258
[2025-04-30 12:22:31,027][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7258
[2025-04-30 12:22:31,289][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7219, Metrics: {'accuracy': 0.4861111111111111, 'f1': 0.5194805194805194, 'precision': 0.4878048780487805, 'recall': 0.5555555555555556}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.7657Epoch 2/10: [                              ] 2/75 batches, loss: 0.7192Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6883Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6903Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6976Epoch 2/10: [==                            ] 6/75 batches, loss: 0.7100Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6951Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6913Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6912Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6904Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6889Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6999Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.7036Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.7075Epoch 2/10: [======                        ] 15/75 batches, loss: 0.7097Epoch 2/10: [======                        ] 16/75 batches, loss: 0.7103Epoch 2/10: [======                        ] 17/75 batches, loss: 0.7128Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.7128Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.7100Epoch 2/10: [========                      ] 20/75 batches, loss: 0.7100Epoch 2/10: [========                      ] 21/75 batches, loss: 0.7107Epoch 2/10: [========                      ] 22/75 batches, loss: 0.7086Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.7071Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.7047Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.7030Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.7034Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.7020Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.7015Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.7044Epoch 2/10: [============                  ] 30/75 batches, loss: 0.7034Epoch 2/10: [============                  ] 31/75 batches, loss: 0.7067Epoch 2/10: [============                  ] 32/75 batches, loss: 0.7071Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.7061Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.7077Epoch 2/10: [==============                ] 35/75 batches, loss: 0.7036Epoch 2/10: [==============                ] 36/75 batches, loss: 0.7047Epoch 2/10: [==============                ] 37/75 batches, loss: 0.7061Epoch 2/10: [===============               ] 38/75 batches, loss: 0.7064Epoch 2/10: [===============               ] 39/75 batches, loss: 0.7060Epoch 2/10: [================              ] 40/75 batches, loss: 0.7060Epoch 2/10: [================              ] 41/75 batches, loss: 0.7079Epoch 2/10: [================              ] 42/75 batches, loss: 0.7088Epoch 2/10: [=================             ] 43/75 batches, loss: 0.7107Epoch 2/10: [=================             ] 44/75 batches, loss: 0.7102Epoch 2/10: [==================            ] 45/75 batches, loss: 0.7101Epoch 2/10: [==================            ] 46/75 batches, loss: 0.7115Epoch 2/10: [==================            ] 47/75 batches, loss: 0.7121Epoch 2/10: [===================           ] 48/75 batches, loss: 0.7099Epoch 2/10: [===================           ] 49/75 batches, loss: 0.7114Epoch 2/10: [====================          ] 50/75 batches, loss: 0.7111Epoch 2/10: [====================          ] 51/75 batches, loss: 0.7128Epoch 2/10: [====================          ] 52/75 batches, loss: 0.7147Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.7155Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.7172Epoch 2/10: [======================        ] 55/75 batches, loss: 0.7150Epoch 2/10: [======================        ] 56/75 batches, loss: 0.7162Epoch 2/10: [======================        ] 57/75 batches, loss: 0.7151Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.7147Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.7150Epoch 2/10: [========================      ] 60/75 batches, loss: 0.7158Epoch 2/10: [========================      ] 61/75 batches, loss: 0.7163Epoch 2/10: [========================      ] 62/75 batches, loss: 0.7168Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.7176Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.7180Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.7167Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.7171Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.7183Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.7184Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.7175Epoch 2/10: [============================  ] 70/75 batches, loss: 0.7190Epoch 2/10: [============================  ] 71/75 batches, loss: 0.7197Epoch 2/10: [============================  ] 72/75 batches, loss: 0.7188Epoch 2/10: [============================= ] 73/75 batches, loss: 0.7186Epoch 2/10: [============================= ] 74/75 batches, loss: 0.7187Epoch 2/10: [==============================] 75/75 batches, loss: 0.7199
[2025-04-30 12:22:39,298][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.7199
[2025-04-30 12:22:39,569][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.7322, Metrics: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'precision': 0.5, 'recall': 1.0}
[2025-04-30 12:22:39,570][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6497Epoch 3/10: [                              ] 2/75 batches, loss: 0.7249Epoch 3/10: [=                             ] 3/75 batches, loss: 0.7284Epoch 3/10: [=                             ] 4/75 batches, loss: 0.7445Epoch 3/10: [==                            ] 5/75 batches, loss: 0.7271Epoch 3/10: [==                            ] 6/75 batches, loss: 0.7302Epoch 3/10: [==                            ] 7/75 batches, loss: 0.7253Epoch 3/10: [===                           ] 8/75 batches, loss: 0.7331Epoch 3/10: [===                           ] 9/75 batches, loss: 0.7413Epoch 3/10: [====                          ] 10/75 batches, loss: 0.7468Epoch 3/10: [====                          ] 11/75 batches, loss: 0.7488Epoch 3/10: [====                          ] 12/75 batches, loss: 0.7429Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.7398Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.7395Epoch 3/10: [======                        ] 15/75 batches, loss: 0.7392Epoch 3/10: [======                        ] 16/75 batches, loss: 0.7336Epoch 3/10: [======                        ] 17/75 batches, loss: 0.7299Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.7292Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.7252Epoch 3/10: [========                      ] 20/75 batches, loss: 0.7284Epoch 3/10: [========                      ] 21/75 batches, loss: 0.7286Epoch 3/10: [========                      ] 22/75 batches, loss: 0.7294Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.7281Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.7267Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.7273Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.7248Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.7200Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.7202Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.7203Epoch 3/10: [============                  ] 30/75 batches, loss: 0.7220Epoch 3/10: [============                  ] 31/75 batches, loss: 0.7221Epoch 3/10: [============                  ] 32/75 batches, loss: 0.7221Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.7217Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.7214Epoch 3/10: [==============                ] 35/75 batches, loss: 0.7210Epoch 3/10: [==============                ] 36/75 batches, loss: 0.7213Epoch 3/10: [==============                ] 37/75 batches, loss: 0.7205Epoch 3/10: [===============               ] 38/75 batches, loss: 0.7211Epoch 3/10: [===============               ] 39/75 batches, loss: 0.7199Epoch 3/10: [================              ] 40/75 batches, loss: 0.7198Epoch 3/10: [================              ] 41/75 batches, loss: 0.7205Epoch 3/10: [================              ] 42/75 batches, loss: 0.7189Epoch 3/10: [=================             ] 43/75 batches, loss: 0.7195Epoch 3/10: [=================             ] 44/75 batches, loss: 0.7195Epoch 3/10: [==================            ] 45/75 batches, loss: 0.7217Epoch 3/10: [==================            ] 46/75 batches, loss: 0.7223Epoch 3/10: [==================            ] 47/75 batches, loss: 0.7216Epoch 3/10: [===================           ] 48/75 batches, loss: 0.7216Epoch 3/10: [===================           ] 49/75 batches, loss: 0.7237Epoch 3/10: [====================          ] 50/75 batches, loss: 0.7233Epoch 3/10: [====================          ] 51/75 batches, loss: 0.7242Epoch 3/10: [====================          ] 52/75 batches, loss: 0.7220Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.7195Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.7208Epoch 3/10: [======================        ] 55/75 batches, loss: 0.7205Epoch 3/10: [======================        ] 56/75 batches, loss: 0.7197Epoch 3/10: [======================        ] 57/75 batches, loss: 0.7185Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.7202Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.7199Epoch 3/10: [========================      ] 60/75 batches, loss: 0.7199Epoch 3/10: [========================      ] 61/75 batches, loss: 0.7205Epoch 3/10: [========================      ] 62/75 batches, loss: 0.7188Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.7176Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.7186Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.7176Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.7175Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.7182Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.7190Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.7188Epoch 3/10: [============================  ] 70/75 batches, loss: 0.7204Epoch 3/10: [============================  ] 71/75 batches, loss: 0.7213Epoch 3/10: [============================  ] 72/75 batches, loss: 0.7209Epoch 3/10: [============================= ] 73/75 batches, loss: 0.7205Epoch 3/10: [============================= ] 74/75 batches, loss: 0.7222Epoch 3/10: [==============================] 75/75 batches, loss: 0.7220
[2025-04-30 12:22:47,144][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.7220
[2025-04-30 12:22:47,402][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.7311, Metrics: {'accuracy': 0.5138888888888888, 'f1': 0.6728971962616822, 'precision': 0.5070422535211268, 'recall': 1.0}
[2025-04-30 12:22:47,402][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.8001Epoch 4/10: [                              ] 2/75 batches, loss: 0.8037Epoch 4/10: [=                             ] 3/75 batches, loss: 0.7774Epoch 4/10: [=                             ] 4/75 batches, loss: 0.7672Epoch 4/10: [==                            ] 5/75 batches, loss: 0.7524Epoch 4/10: [==                            ] 6/75 batches, loss: 0.7386Epoch 4/10: [==                            ] 7/75 batches, loss: 0.7406Epoch 4/10: [===                           ] 8/75 batches, loss: 0.7324Epoch 4/10: [===                           ] 9/75 batches, loss: 0.7248Epoch 4/10: [====                          ] 10/75 batches, loss: 0.7247Epoch 4/10: [====                          ] 11/75 batches, loss: 0.7123Epoch 4/10: [====                          ] 12/75 batches, loss: 0.7107Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.7174Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.7150Epoch 4/10: [======                        ] 15/75 batches, loss: 0.7136Epoch 4/10: [======                        ] 16/75 batches, loss: 0.7134Epoch 4/10: [======                        ] 17/75 batches, loss: 0.7190Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.7203Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.7194Epoch 4/10: [========                      ] 20/75 batches, loss: 0.7218Epoch 4/10: [========                      ] 21/75 batches, loss: 0.7271Epoch 4/10: [========                      ] 22/75 batches, loss: 0.7294Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.7297Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.7259Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.7246Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.7271Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.7294Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.7262Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.7233Epoch 4/10: [============                  ] 30/75 batches, loss: 0.7233Epoch 4/10: [============                  ] 31/75 batches, loss: 0.7219Epoch 4/10: [============                  ] 32/75 batches, loss: 0.7214Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.7181Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.7178Epoch 4/10: [==============                ] 35/75 batches, loss: 0.7164Epoch 4/10: [==============                ] 36/75 batches, loss: 0.7194Epoch 4/10: [==============                ] 37/75 batches, loss: 0.7221Epoch 4/10: [===============               ] 38/75 batches, loss: 0.7235Epoch 4/10: [===============               ] 39/75 batches, loss: 0.7267Epoch 4/10: [================              ] 40/75 batches, loss: 0.7251Epoch 4/10: [================              ] 41/75 batches, loss: 0.7247Epoch 4/10: [================              ] 42/75 batches, loss: 0.7244Epoch 4/10: [=================             ] 43/75 batches, loss: 0.7233Epoch 4/10: [=================             ] 44/75 batches, loss: 0.7233Epoch 4/10: [==================            ] 45/75 batches, loss: 0.7217Epoch 4/10: [==================            ] 46/75 batches, loss: 0.7231Epoch 4/10: [==================            ] 47/75 batches, loss: 0.7235Epoch 4/10: [===================           ] 48/75 batches, loss: 0.7228Epoch 4/10: [===================           ] 49/75 batches, loss: 0.7247Epoch 4/10: [====================          ] 50/75 batches, loss: 0.7247Epoch 4/10: [====================          ] 51/75 batches, loss: 0.7243Epoch 4/10: [====================          ] 52/75 batches, loss: 0.7243Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.7245Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.7227Epoch 4/10: [======================        ] 55/75 batches, loss: 0.7232Epoch 4/10: [======================        ] 56/75 batches, loss: 0.7228Epoch 4/10: [======================        ] 57/75 batches, loss: 0.7229Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.7215Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.7210Epoch 4/10: [========================      ] 60/75 batches, loss: 0.7224Epoch 4/10: [========================      ] 61/75 batches, loss: 0.7226Epoch 4/10: [========================      ] 62/75 batches, loss: 0.7229Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.7224Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.7231Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.7219Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.7209Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.7224Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.7233Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.7228Epoch 4/10: [============================  ] 70/75 batches, loss: 0.7235Epoch 4/10: [============================  ] 71/75 batches, loss: 0.7226Epoch 4/10: [============================  ] 72/75 batches, loss: 0.7224Epoch 4/10: [============================= ] 73/75 batches, loss: 0.7228Epoch 4/10: [============================= ] 74/75 batches, loss: 0.7231Epoch 4/10: [==============================] 75/75 batches, loss: 0.7217
[2025-04-30 12:22:54,972][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.7217
[2025-04-30 12:22:55,222][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.7299, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-04-30 12:22:55,222][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-30 12:22:55,222][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-04-30 12:22:55,223][src.training.lm_trainer][INFO] - Training completed in 32.51 seconds
[2025-04-30 12:22:55,223][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 12:22:58,180][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5092281879194631, 'f1': 0.6168958742632613, 'precision': 0.505907626208378, 'recall': 0.790268456375839}
[2025-04-30 12:22:58,181][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4861111111111111, 'f1': 0.5194805194805194, 'precision': 0.4878048780487805, 'recall': 0.5555555555555556}
[2025-04-30 12:22:58,181][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5181818181818182, 'f1': 0.5891472868217055, 'precision': 0.5135135135135135, 'recall': 0.6909090909090909}
[2025-04-30 12:22:59,918][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/control1/en/model.pt
[2025-04-30 12:22:59,924][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▄▃
wandb:            train_time ▁
wandb:          val_accuracy ▁▅█▅
wandb:                val_f1 ▆██▁
wandb:              val_loss ▁█▇▆
wandb:         val_precision ███▁
wandb:            val_recall ▅██▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.48611
wandb:           best_val_f1 0.51948
wandb:         best_val_loss 0.72187
wandb:    best_val_precision 0.4878
wandb:       best_val_recall 0.55556
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.51818
wandb:         final_test_f1 0.58915
wandb:  final_test_precision 0.51351
wandb:     final_test_recall 0.69091
wandb:  final_train_accuracy 0.50923
wandb:        final_train_f1 0.6169
wandb: final_train_precision 0.50591
wandb:    final_train_recall 0.79027
wandb:    final_val_accuracy 0.48611
wandb:          final_val_f1 0.51948
wandb:   final_val_precision 0.4878
wandb:      final_val_recall 0.55556
wandb:         learning_rate 2e-05
wandb:            train_loss 0.72168
wandb:            train_time 32.5091
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.72986
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_122212-egsqosq7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_122212-egsqosq7/logs
Experiment finetune_question_type_control1_en completed successfully in 59 seconds
