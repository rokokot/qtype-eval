SLURM_JOB_ID: 64430090
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed Apr 30 15:46:54 CEST 2025
Walltime: 00-00:30:00
========================================================================
========= Running main tasks experiments ==========
Processing language: ar
Processing layer: 2
Running question_type experiment for language ar, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_2_question_type_ar
Running complexity experiment for language ar, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_2_complexity_ar
Processing layer: 6
Running question_type experiment for language ar, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_6_question_type_ar
Running complexity experiment for language ar, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_6_complexity_ar
Processing layer: 11
Running question_type experiment for language ar, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_11/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_11_question_type_ar
Running complexity experiment for language ar, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_11/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_11_complexity_ar
Processing language: ja
Processing layer: 2
Running question_type experiment for language ja, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_2/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_2_question_type_ja
Running complexity experiment for language ja, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_2/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_2_complexity_ja
Processing layer: 6
Running question_type experiment for language ja, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_6/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_6_question_type_ja
Running complexity experiment for language ja, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_6/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_6_complexity_ja
Processing layer: 11
Running question_type experiment for language ja, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_11/question_type"         "wandb.mode=offline"
Standard experiment completed successfully: layer_11_question_type_ja
Running complexity experiment for language ja, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_11/complexity"         "wandb.mode=offline"
Standard experiment completed successfully: layer_11_complexity_ja
========= Running submetric tasks experiments ==========
Processing language: ar
Processing layer: 2
Running single_submetric experiment for language ar, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_avg_verb_edges_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_2_avg_verb_edges_ar
Running single_submetric experiment for language ar, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_lexical_density_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_2_lexical_density_ar
Processing layer: 6
Running single_submetric experiment for language ar, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_avg_verb_edges_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_6_avg_verb_edges_ar
Running single_submetric experiment for language ar, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_lexical_density_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_6_lexical_density_ar
Processing layer: 11
Running single_submetric experiment for language ar, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_avg_verb_edges_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_11/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_11_avg_verb_edges_ar
Running single_submetric experiment for language ar, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_lexical_density_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_11/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_11_lexical_density_ar
Processing language: ja
Processing layer: 2
Running single_submetric experiment for language ja, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_avg_verb_edges_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_2/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_2_avg_verb_edges_ja
Running single_submetric experiment for language ja, layer 2
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_lexical_density_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_2/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_2_lexical_density_ja
Processing layer: 6
Running single_submetric experiment for language ja, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_avg_verb_edges_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_6/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_6_avg_verb_edges_ja
Running single_submetric experiment for language ja, layer 6
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_lexical_density_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_6/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_6_lexical_density_ja
Processing layer: 11
Running single_submetric experiment for language ja, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_avg_verb_edges_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_11/avg_verb_edges"         "wandb.mode=offline" "experiment.submetric=avg_verb_edges"
Error in standard experiment: layer_11_avg_verb_edges_ja
Running single_submetric experiment for language ja, layer 11
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment="         "experiment.tasks=single_submetric"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_lexical_density_ja"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ja/layer_11/lexical_density"         "wandb.mode=offline" "experiment.submetric=lexical_density"
Error in standard experiment: layer_11_lexical_density_ja
========= Running control experiments for main tasks ==========
Processing language: ar
Processing layer: 2
Running question_type control experiment for language ar, layer 2, control 1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "experiment.use_controls=true"         "experiment.control_index=1"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_question_type_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/question_type/control1"         "wandb.mode=offline"
Control experiment completed successfully: layer_2_question_type_control1_ar
Running complexity control experiment for language ar, layer 2, control 1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=2"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "experiment.use_controls=true"         "experiment.control_index=1"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_2_complexity_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/complexity/control1"         "wandb.mode=offline"
Control experiment completed successfully: layer_2_complexity_control1_ar
Processing layer: 6
Running question_type control experiment for language ar, layer 6, control 1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "experiment.use_controls=true"         "experiment.control_index=1"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_question_type_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/question_type/control1"         "wandb.mode=offline"
Control experiment completed successfully: layer_6_question_type_control1_ar
Running complexity control experiment for language ar, layer 6, control 1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=6"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "experiment.use_controls=true"         "experiment.control_index=1"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_6_complexity_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_6/complexity/control1"         "wandb.mode=offline"
Control experiment completed successfully: layer_6_complexity_control1_ar
Processing layer: 11
Running question_type control experiment for language ar, layer 11, control 1
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.layer_wise=true"         "model.layer_index=11"         "model.freeze_model=true"         "model.probe_hidden_size=96"         "experiment.use_controls=true"         "experiment.control_index=1"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.lr=1e-4"         "training.batch_size=16"         "+training.gradient_accumulation_steps=2"         "experiment_name=layer_11_question_type_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_11/question_type/control1"         "wandb.mode=offline"
slurmstepd: error: *** JOB 64430090 ON k28i22 CANCELLED AT 2025-04-30T16:17:14 DUE TO TIME LIMIT ***
