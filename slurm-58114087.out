SLURM_JOB_ID: 58114087
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r22g39
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Tue Apr 29 21:08:40 CEST 2025
Walltime: 01-12:00:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:09:10,619][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:09:10,619][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:09:10,619][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:09:10,619][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:09:10,624][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 21:09:10,624][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:09:12,418][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:09:15,275][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:09:15,276][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:09:15,424][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,485][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,623][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:09:15,633][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:09:15,634][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:09:15,636][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:09:15,670][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,713][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,730][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:09:15,731][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:09:15,731][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:09:15,732][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:09:15,764][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,807][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:09:15,823][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:09:15,825][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:09:15,825][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:09:15,826][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:09:15,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:09:15,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:09:15,827][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:09:15,827][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:09:15,827][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 21:09:15,828][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:09:15,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:09:15,828][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 21:09:15,829][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:09:15,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:09:15,829][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 21:09:15,830][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 21:09:15,830][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:09:15,830][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:09:15,830][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:09:15,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:09:15,830][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:09:15,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:09:23,155][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:09:23,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,170][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,171][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,172][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,173][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:09:23,174][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:09:23,174][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:09:23,175][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:09:23,175][__main__][INFO] - Successfully created model for ar
[2025-04-29 21:09:23,175][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.6814Epoch 1/10: [                              ] 2/63 batches, loss: 0.6934Epoch 1/10: [=                             ] 3/63 batches, loss: 0.6982Epoch 1/10: [=                             ] 4/63 batches, loss: 0.6912Epoch 1/10: [==                            ] 5/63 batches, loss: 0.6994Epoch 1/10: [==                            ] 6/63 batches, loss: 0.6993Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7009Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7013Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7023Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7016Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7003Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.6970Epoch 1/10: [======                        ] 13/63 batches, loss: 0.6974Epoch 1/10: [======                        ] 14/63 batches, loss: 0.6977Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.6963Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.6952Epoch 1/10: [========                      ] 17/63 batches, loss: 0.6948Epoch 1/10: [========                      ] 18/63 batches, loss: 0.6954Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.6948Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.6947Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.6945Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.6962Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.6950Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.6948Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.6950Epoch 1/10: [============                  ] 26/63 batches, loss: 0.6951Epoch 1/10: [============                  ] 27/63 batches, loss: 0.6950Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.6950Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.6946Epoch 1/10: [==============                ] 30/63 batches, loss: 0.6956Epoch 1/10: [==============                ] 31/63 batches, loss: 0.6952Epoch 1/10: [===============               ] 32/63 batches, loss: 0.6952Epoch 1/10: [===============               ] 33/63 batches, loss: 0.6950Epoch 1/10: [================              ] 34/63 batches, loss: 0.6940Epoch 1/10: [================              ] 35/63 batches, loss: 0.6937Epoch 1/10: [=================             ] 36/63 batches, loss: 0.6936Epoch 1/10: [=================             ] 37/63 batches, loss: 0.6937Epoch 1/10: [==================            ] 38/63 batches, loss: 0.6930Epoch 1/10: [==================            ] 39/63 batches, loss: 0.6929Epoch 1/10: [===================           ] 40/63 batches, loss: 0.6926Epoch 1/10: [===================           ] 41/63 batches, loss: 0.6921Epoch 1/10: [====================          ] 42/63 batches, loss: 0.6910Epoch 1/10: [====================          ] 43/63 batches, loss: 0.6918Epoch 1/10: [====================          ] 44/63 batches, loss: 0.6919Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.6918Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.6918Epoch 1/10: [======================        ] 47/63 batches, loss: 0.6914Epoch 1/10: [======================        ] 48/63 batches, loss: 0.6916Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.6927Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.6924Epoch 1/10: [========================      ] 51/63 batches, loss: 0.6929Epoch 1/10: [========================      ] 52/63 batches, loss: 0.6932Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.6929Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.6932Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.6927Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.6930Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.6926Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.6925Epoch 1/10: [============================  ] 59/63 batches, loss: 0.6924Epoch 1/10: [============================  ] 60/63 batches, loss: 0.6918Epoch 1/10: [============================= ] 61/63 batches, loss: 0.6916Epoch 1/10: [============================= ] 62/63 batches, loss: 0.6906Epoch 1/10: [==============================] 63/63 batches, loss: 0.6908
[2025-04-29 21:09:41,429][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6908
[2025-04-29 21:09:41,831][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.6803Epoch 2/10: [                              ] 2/63 batches, loss: 0.6822Epoch 2/10: [=                             ] 3/63 batches, loss: 0.6796Epoch 2/10: [=                             ] 4/63 batches, loss: 0.6802Epoch 2/10: [==                            ] 5/63 batches, loss: 0.6788Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6739Epoch 2/10: [===                           ] 7/63 batches, loss: 0.6703Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6748Epoch 2/10: [====                          ] 9/63 batches, loss: 0.6748Epoch 2/10: [====                          ] 10/63 batches, loss: 0.6737Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.6763Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.6759Epoch 2/10: [======                        ] 13/63 batches, loss: 0.6746Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6775Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6781Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6767Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6755Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6738Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6741Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6758Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6751Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6740Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6708Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6683Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6682Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6671Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6630Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6601Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6595Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6583Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6584Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6574Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6555Epoch 2/10: [================              ] 34/63 batches, loss: 0.6555Epoch 2/10: [================              ] 35/63 batches, loss: 0.6539Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6545Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6534Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6531Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6515Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6502Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6487Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6465Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6467Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6456Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6423Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6430Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6408Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6407Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6400Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6403Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6403Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6393Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6386Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6379Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6374Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6368Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6359Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6352Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6351Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6349Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6339Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6338Epoch 2/10: [==============================] 63/63 batches, loss: 0.6340
[2025-04-29 21:09:55,049][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6340
[2025-04-29 21:09:55,368][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6606, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6545Epoch 3/10: [                              ] 2/63 batches, loss: 0.6433Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6582Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6472Epoch 3/10: [==                            ] 5/63 batches, loss: 0.6370Epoch 3/10: [==                            ] 6/63 batches, loss: 0.6371Epoch 3/10: [===                           ] 7/63 batches, loss: 0.6302Epoch 3/10: [===                           ] 8/63 batches, loss: 0.6316Epoch 3/10: [====                          ] 9/63 batches, loss: 0.6241Epoch 3/10: [====                          ] 10/63 batches, loss: 0.6308Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.6292Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.6239Epoch 3/10: [======                        ] 13/63 batches, loss: 0.6184Epoch 3/10: [======                        ] 14/63 batches, loss: 0.6068Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.6075Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.6011Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5983Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5943Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5902Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5887Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5820Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5822Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5790Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5790Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5789Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5763Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5761Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5756Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5736Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5734Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5702Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5684Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5663Epoch 3/10: [================              ] 34/63 batches, loss: 0.5666Epoch 3/10: [================              ] 35/63 batches, loss: 0.5629Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5582Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5557Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5527Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5491Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5465Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5453Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5448Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5433Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5405Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5391Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5417Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5386Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5352Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5315Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5287Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5265Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5237Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5209Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5187Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5177Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5150Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5144Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5115Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5077Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5044Epoch 3/10: [============================= ] 61/63 batches, loss: 0.4995Epoch 3/10: [============================= ] 62/63 batches, loss: 0.4971Epoch 3/10: [==============================] 63/63 batches, loss: 0.4930
[2025-04-29 21:10:08,689][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.4930
[2025-04-29 21:10:09,014][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.3824, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.3609Epoch 4/10: [                              ] 2/63 batches, loss: 0.3413Epoch 4/10: [=                             ] 3/63 batches, loss: 0.3560Epoch 4/10: [=                             ] 4/63 batches, loss: 0.3643Epoch 4/10: [==                            ] 5/63 batches, loss: 0.3584Epoch 4/10: [==                            ] 6/63 batches, loss: 0.3781Epoch 4/10: [===                           ] 7/63 batches, loss: 0.3619Epoch 4/10: [===                           ] 8/63 batches, loss: 0.3619Epoch 4/10: [====                          ] 9/63 batches, loss: 0.3534Epoch 4/10: [====                          ] 10/63 batches, loss: 0.3504Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.3408Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.3313Epoch 4/10: [======                        ] 13/63 batches, loss: 0.3313Epoch 4/10: [======                        ] 14/63 batches, loss: 0.3290Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.3212Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.3283Epoch 4/10: [========                      ] 17/63 batches, loss: 0.3328Epoch 4/10: [========                      ] 18/63 batches, loss: 0.3313Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.3299Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.3296Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.3256Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.3220Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.3198Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.3157Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.3118Epoch 4/10: [============                  ] 26/63 batches, loss: 0.3070Epoch 4/10: [============                  ] 27/63 batches, loss: 0.3019Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.3001Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.2977Epoch 4/10: [==============                ] 30/63 batches, loss: 0.2953Epoch 4/10: [==============                ] 31/63 batches, loss: 0.2922Epoch 4/10: [===============               ] 32/63 batches, loss: 0.2896Epoch 4/10: [===============               ] 33/63 batches, loss: 0.2849Epoch 4/10: [================              ] 34/63 batches, loss: 0.2806Epoch 4/10: [================              ] 35/63 batches, loss: 0.2773Epoch 4/10: [=================             ] 36/63 batches, loss: 0.2719Epoch 4/10: [=================             ] 37/63 batches, loss: 0.2700Epoch 4/10: [==================            ] 38/63 batches, loss: 0.2659Epoch 4/10: [==================            ] 39/63 batches, loss: 0.2641Epoch 4/10: [===================           ] 40/63 batches, loss: 0.2607Epoch 4/10: [===================           ] 41/63 batches, loss: 0.2572Epoch 4/10: [====================          ] 42/63 batches, loss: 0.2533Epoch 4/10: [====================          ] 43/63 batches, loss: 0.2499Epoch 4/10: [====================          ] 44/63 batches, loss: 0.2489Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.2454Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.2420Epoch 4/10: [======================        ] 47/63 batches, loss: 0.2401Epoch 4/10: [======================        ] 48/63 batches, loss: 0.2386Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.2357Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.2335Epoch 4/10: [========================      ] 51/63 batches, loss: 0.2310Epoch 4/10: [========================      ] 52/63 batches, loss: 0.2280Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.2248Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.2224Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.2198Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.2171Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.2152Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.2132Epoch 4/10: [============================  ] 59/63 batches, loss: 0.2117Epoch 4/10: [============================  ] 60/63 batches, loss: 0.2104Epoch 4/10: [============================= ] 61/63 batches, loss: 0.2076Epoch 4/10: [============================= ] 62/63 batches, loss: 0.2056Epoch 4/10: [==============================] 63/63 batches, loss: 0.2036
[2025-04-29 21:10:22,278][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2036
[2025-04-29 21:10:22,636][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1426, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0943Epoch 5/10: [                              ] 2/63 batches, loss: 0.0929Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0834Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0746Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0717Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0682Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0628Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0580Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0565Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0575Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0579Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0576Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0547Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0551Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0547Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0540Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0532Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0533Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0525Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0514Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0509Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0502Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0493Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0488Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0479Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0472Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0465Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0467Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0472Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0468Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0464Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0466Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0460Epoch 5/10: [================              ] 34/63 batches, loss: 0.0454Epoch 5/10: [================              ] 35/63 batches, loss: 0.0451Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0450Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0445Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0443Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0441Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0439Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0434Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0429Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0424Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0418Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0413Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0409Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0405Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0403Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0404Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0402Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0397Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0392Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0387Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0383Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0381Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0378Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0374Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0372Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0370Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0365Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0364Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0365Epoch 5/10: [==============================] 63/63 batches, loss: 0.0362
[2025-04-29 21:10:35,916][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0362
[2025-04-29 21:10:36,314][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1215, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0169Epoch 6/10: [                              ] 2/63 batches, loss: 0.0160Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0197Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0199Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0210Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0220Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0215Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0205Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0194Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0190Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0185Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0190Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0190Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0186Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0182Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0183Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0180Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0179Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0178Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0175Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0169Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0166Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0162Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0161Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0160Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0162Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0160Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0158Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0158Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0161Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0159Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0158Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0159Epoch 6/10: [================              ] 34/63 batches, loss: 0.0157Epoch 6/10: [================              ] 35/63 batches, loss: 0.0155Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0154Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0154Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0154Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0153Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0151Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0150Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0150Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0149Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0149Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0148Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0149Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0148Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0147Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0147Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0147Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0146Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0146Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0145Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0144Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0144Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0145Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0145Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0145Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0144Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0143Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0141Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0140Epoch 6/10: [==============================] 63/63 batches, loss: 0.0139
[2025-04-29 21:10:49,698][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0139
[2025-04-29 21:10:50,033][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
[2025-04-29 21:10:50,034][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0074Epoch 7/10: [                              ] 2/63 batches, loss: 0.0076Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0080Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0076Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0081Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0085Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0089Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0089Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0091Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0091Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0088Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0087Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0086Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0087Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0092Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0091Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0092Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0093Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0092Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0098Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0097Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0097Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0097Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0098Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0103Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0102Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0101Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0100Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0098Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0098Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0100Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0099Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0100Epoch 7/10: [================              ] 34/63 batches, loss: 0.0100Epoch 7/10: [================              ] 35/63 batches, loss: 0.0099Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0099Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0098Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0098Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0097Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0098Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0097Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0097Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0097Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0097Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0096Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0097Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0097Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0099Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0099Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0097Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0098Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0099Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0099Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0098Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0098Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0097Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0097Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0096Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0097Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0096Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0096Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0095Epoch 7/10: [==============================] 63/63 batches, loss: 0.0096
[2025-04-29 21:11:02,834][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0096
[2025-04-29 21:11:03,182][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1377, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
[2025-04-29 21:11:03,183][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0107Epoch 8/10: [                              ] 2/63 batches, loss: 0.0081Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0072Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0072Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0070Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0070Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0068Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0078Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0080Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0082Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0078Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0081Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0079Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0079Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0077Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0075Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0074Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0072Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0071Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0071Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0071Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0070Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0070Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0068Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0067Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0068Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0068Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0069Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0068Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0069Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0068Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0068Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0067Epoch 8/10: [================              ] 34/63 batches, loss: 0.0067Epoch 8/10: [================              ] 35/63 batches, loss: 0.0067Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0067Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0067Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0066Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0066Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0066Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0067Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0067Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0068Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0068Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0069Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0069Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0068Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0068Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0068Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0067Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0067Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0067Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0067Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0067Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0066Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0066Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0066Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0066Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0066Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0066Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0066Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0065Epoch 8/10: [==============================] 63/63 batches, loss: 0.0065
[2025-04-29 21:11:15,986][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0065
[2025-04-29 21:11:16,384][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1654, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
[2025-04-29 21:11:16,385][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:11:16,385][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:11:16,386][src.training.lm_trainer][INFO] - Training completed in 108.42 seconds
[2025-04-29 21:11:16,386][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:11:21,173][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0}
[2025-04-29 21:11:21,174][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
[2025-04-29 21:11:21,174][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8571428571428571, 'f1': 0.7924528301886793}
[2025-04-29 21:11:23,426][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅███
wandb:          best_val_f1 ▁▄███
wandb:        best_val_loss ██▄▁▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▃▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅██████
wandb:               val_f1 ▁▄██████
wandb:             val_loss ██▄▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.1215
wandb:                epoch 8
wandb:  final_test_accuracy 0.85714
wandb:        final_test_f1 0.79245
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 2e-05
wandb:           train_loss 0.00652
wandb:           train_time 108.41839
wandb:         val_accuracy 0.97727
wandb:               val_f1 0.97561
wandb:             val_loss 0.1654
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_210910-kenx4550
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_210910-kenx4550/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:11:45,569][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:11:45,569][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:11:45,569][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:11:45,569][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:11:45,574][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 21:11:45,574][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:11:46,930][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:11:49,814][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:11:49,815][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:11:49,864][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:49,889][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:49,966][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:11:49,977][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:11:49,978][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:11:49,979][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:11:49,997][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:50,023][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:50,035][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:11:50,036][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:11:50,037][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:11:50,037][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:11:50,054][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:50,077][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:11:50,087][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:11:50,088][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:11:50,089][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:11:50,089][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:11:50,090][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:11:50,090][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:11:50,090][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:11:50,090][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:11:50,090][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:11:50,091][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:11:50,091][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:11:50,092][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:11:50,092][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:11:50,092][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:11:50,093][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:11:50,093][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 21:11:50,093][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:11:50,093][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 21:11:50,093][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:11:50,093][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:11:50,094][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:11:50,094][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:11:54,152][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:11:54,152][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,152][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,152][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,153][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,154][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,155][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,156][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,157][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,158][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,159][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,160][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,161][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,162][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,163][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,164][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,165][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,166][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,167][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,168][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,169][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:11:54,170][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:11:54,170][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:11:54,171][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:11:54,171][__main__][INFO] - Successfully created model for ar
[2025-04-29 21:11:54,171][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.1457Epoch 1/10: [                              ] 2/63 batches, loss: 0.1388Epoch 1/10: [=                             ] 3/63 batches, loss: 0.1425Epoch 1/10: [=                             ] 4/63 batches, loss: 0.1343Epoch 1/10: [==                            ] 5/63 batches, loss: 0.1290Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1291Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1270Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1214Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1180Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1176Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1173Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1201Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1181Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1190Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1166Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1178Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1196Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1186Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1181Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1173Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1158Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1130Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1114Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1122Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1108Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1089Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1084Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1060Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1066Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1070Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1086Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1078Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1064Epoch 1/10: [================              ] 34/63 batches, loss: 0.1059Epoch 1/10: [================              ] 35/63 batches, loss: 0.1056Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1035Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1024Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1019Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1005Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1008Epoch 1/10: [===================           ] 41/63 batches, loss: 0.0994Epoch 1/10: [====================          ] 42/63 batches, loss: 0.0981Epoch 1/10: [====================          ] 43/63 batches, loss: 0.0970Epoch 1/10: [====================          ] 44/63 batches, loss: 0.0965Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.0957Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.0961Epoch 1/10: [======================        ] 47/63 batches, loss: 0.0951Epoch 1/10: [======================        ] 48/63 batches, loss: 0.0946Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.0937Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.0930Epoch 1/10: [========================      ] 51/63 batches, loss: 0.0917Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0907Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0905Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0896Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0883Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0872Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0868Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0862Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0859Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0853Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0853Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0852Epoch 1/10: [==============================] 63/63 batches, loss: 0.0842
[2025-04-29 21:12:09,422][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0842
[2025-04-29 21:12:09,801][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0706, Metrics: {'mse': 0.0717705562710762, 'rmse': 0.2679002729955238, 'r2': -0.10622298717498779}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0724Epoch 2/10: [                              ] 2/63 batches, loss: 0.0901Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0982Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0979Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0929Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0858Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0851Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0826Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0829Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0795Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0770Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0757Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0784Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0794Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0783Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0807Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0793Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0774Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0761Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0781Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0762Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0746Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0726Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0716Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0707Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0703Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0699Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0691Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0684Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0674Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0662Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0657Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0647Epoch 2/10: [================              ] 34/63 batches, loss: 0.0640Epoch 2/10: [================              ] 35/63 batches, loss: 0.0631Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0629Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0622Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0620Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0612Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0609Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0605Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0601Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0593Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0584Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0580Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0572Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0566Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0571Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0567Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0559Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0555Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0554Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0554Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0557Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0557Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0554Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0547Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0546Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0541Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0543Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0537Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0535Epoch 2/10: [==============================] 63/63 batches, loss: 0.0531
[2025-04-29 21:12:23,152][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-29 21:12:23,499][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0651, Metrics: {'mse': 0.06509795039892197, 'rmse': 0.2551429999018628, 'r2': -0.0033758878707885742}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0371Epoch 3/10: [                              ] 2/63 batches, loss: 0.0281Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0317Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0292Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0287Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0300Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0322Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0343Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0348Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0339Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0339Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0325Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0337Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0336Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0360Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0362Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0364Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0358Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0359Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0362Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0360Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0354Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0357Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0352Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0352Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0344Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0341Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0338Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0341Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0335Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0333Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0332Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0330Epoch 3/10: [================              ] 34/63 batches, loss: 0.0328Epoch 3/10: [================              ] 35/63 batches, loss: 0.0327Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0323Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0321Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0317Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0317Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0318Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0317Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0319Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0319Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0320Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0317Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0314Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0314Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0315Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0315Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0314Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0312Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0310Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0306Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0303Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0300Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0298Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0294Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0292Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0293Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0290Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0289Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0291Epoch 3/10: [==============================] 63/63 batches, loss: 0.0292
[2025-04-29 21:12:36,897][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0292
[2025-04-29 21:12:37,234][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0496, Metrics: {'mse': 0.049061119556427, 'rmse': 0.2214974481939397, 'r2': 0.24380499124526978}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0390Epoch 4/10: [                              ] 2/63 batches, loss: 0.0241Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0302Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0299Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0342Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0330Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0307Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0317Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0306Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0308Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0307Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0307Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0295Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0283Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0285Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0275Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0265Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0260Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0260Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0257Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0257Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0254Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0256Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0253Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0255Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0252Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0249Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0247Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0250Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0255Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0251Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0249Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0245Epoch 4/10: [================              ] 34/63 batches, loss: 0.0245Epoch 4/10: [================              ] 35/63 batches, loss: 0.0247Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0246Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0245Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0242Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0245Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0245Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0246Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0248Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0248Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0250Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0248Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0247Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0248Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0249Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0249Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0248Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0251Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0251Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0251Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0252Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0249Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0252Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0251Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0250Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0253Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0253Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0253Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0254Epoch 4/10: [==============================] 63/63 batches, loss: 0.0253
[2025-04-29 21:12:50,621][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0253
[2025-04-29 21:12:50,945][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0328, Metrics: {'mse': 0.032667871564626694, 'rmse': 0.18074255604208625, 'r2': 0.49647945165634155}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0197Epoch 5/10: [                              ] 2/63 batches, loss: 0.0202Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0176Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0202Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0191Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0186Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0182Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0176Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0169Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0170Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0165Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0173Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0165Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0160Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0155Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0152Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0155Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0157Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0154Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0155Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0160Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0158Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0156Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0155Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0153Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0155Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0154Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0153Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0152Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0151Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0151Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0151Epoch 5/10: [================              ] 34/63 batches, loss: 0.0156Epoch 5/10: [================              ] 35/63 batches, loss: 0.0159Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0159Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0167Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0165Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0170Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0178Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0176Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0181Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0179Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0179Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0176Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0175Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0177Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0177Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0176Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0179Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0179Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0181Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0179Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0180Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0180Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0178Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0178Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0179Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0179Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0178Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0177Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0176Epoch 5/10: [==============================] 63/63 batches, loss: 0.0177
[2025-04-29 21:13:04,284][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0177
[2025-04-29 21:13:04,626][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0402, Metrics: {'mse': 0.03997764363884926, 'rmse': 0.19994410128545742, 'r2': 0.3838115334510803}
[2025-04-29 21:13:04,627][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0260Epoch 6/10: [                              ] 2/63 batches, loss: 0.0302Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0257Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0268Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0247Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0277Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0263Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0243Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0234Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0232Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0233Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0237Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0228Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0227Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0225Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0219Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0217Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0212Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0215Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0222Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0220Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0221Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0222Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0217Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0216Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0211Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0216Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0213Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0213Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0209Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0206Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0205Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0206Epoch 6/10: [================              ] 34/63 batches, loss: 0.0204Epoch 6/10: [================              ] 35/63 batches, loss: 0.0203Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0201Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0198Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0198Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0198Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0195Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0193Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0190Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0189Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0192Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0190Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0192Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0190Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0190Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0189Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0188Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0186Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0184Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0183Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0182Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0182Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0182Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0183Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0184Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0184Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0186Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0184Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0183Epoch 6/10: [==============================] 63/63 batches, loss: 0.0182
[2025-04-29 21:13:17,398][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0182
[2025-04-29 21:13:17,735][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0243, Metrics: {'mse': 0.024227410554885864, 'rmse': 0.1556515677880755, 'r2': 0.6265749931335449}
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0229Epoch 7/10: [                              ] 2/63 batches, loss: 0.0273Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0222Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0198Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0195Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0187Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0180Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0176Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0167Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0169Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0175Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0174Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0177Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0180Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0172Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0176Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0175Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0175Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0170Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0172Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0167Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0167Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0163Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0158Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0154Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0157Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0156Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0159Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0157Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0160Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0160Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0160Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0158Epoch 7/10: [================              ] 34/63 batches, loss: 0.0155Epoch 7/10: [================              ] 35/63 batches, loss: 0.0153Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0152Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0150Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0150Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0153Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0152Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0150Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0149Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0149Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0148Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0147Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0147Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0147Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0146Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0148Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0148Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0148Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0147Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0146Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0146Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0146Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0145Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0147Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0146Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0146Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0146Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0146Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0145Epoch 7/10: [==============================] 63/63 batches, loss: 0.0148
[2025-04-29 21:13:31,147][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0148
[2025-04-29 21:13:31,481][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0295, Metrics: {'mse': 0.0288498867303133, 'rmse': 0.16985254407960249, 'r2': 0.555327296257019}
[2025-04-29 21:13:31,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0334Epoch 8/10: [                              ] 2/63 batches, loss: 0.0234Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0191Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0205Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0176Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0171Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0172Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0156Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0155Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0150Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0155Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0156Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0148Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0143Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0141Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0144Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0147Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0147Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0149Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0147Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0149Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0145Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0141Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0139Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0142Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0139Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0139Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0139Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0136Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0136Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0138Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0143Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0143Epoch 8/10: [================              ] 34/63 batches, loss: 0.0142Epoch 8/10: [================              ] 35/63 batches, loss: 0.0146Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0146Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0145Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0145Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0146Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0146Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0145Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0149Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0148Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0150Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0151Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0149Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0152Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0156Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0156Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0155Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0154Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0153Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0153Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0152Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0152Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0152Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0153Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0153Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0152Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0151Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0149Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0150Epoch 8/10: [==============================] 63/63 batches, loss: 0.0151
[2025-04-29 21:13:44,280][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0151
[2025-04-29 21:13:44,616][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0273, Metrics: {'mse': 0.026704899966716766, 'rmse': 0.1634163393504969, 'r2': 0.5883886814117432}
[2025-04-29 21:13:44,617][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.0177Epoch 9/10: [                              ] 2/63 batches, loss: 0.0119Epoch 9/10: [=                             ] 3/63 batches, loss: 0.0131Epoch 9/10: [=                             ] 4/63 batches, loss: 0.0131Epoch 9/10: [==                            ] 5/63 batches, loss: 0.0126Epoch 9/10: [==                            ] 6/63 batches, loss: 0.0115Epoch 9/10: [===                           ] 7/63 batches, loss: 0.0113Epoch 9/10: [===                           ] 8/63 batches, loss: 0.0112Epoch 9/10: [====                          ] 9/63 batches, loss: 0.0132Epoch 9/10: [====                          ] 10/63 batches, loss: 0.0132Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.0137Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.0133Epoch 9/10: [======                        ] 13/63 batches, loss: 0.0128Epoch 9/10: [======                        ] 14/63 batches, loss: 0.0133Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.0137Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.0135Epoch 9/10: [========                      ] 17/63 batches, loss: 0.0139Epoch 9/10: [========                      ] 18/63 batches, loss: 0.0135Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.0134Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.0129Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.0130Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.0129Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.0127Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.0124Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.0122Epoch 9/10: [============                  ] 26/63 batches, loss: 0.0119Epoch 9/10: [============                  ] 27/63 batches, loss: 0.0122Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.0122Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.0119Epoch 9/10: [==============                ] 30/63 batches, loss: 0.0120Epoch 9/10: [==============                ] 31/63 batches, loss: 0.0118Epoch 9/10: [===============               ] 32/63 batches, loss: 0.0117Epoch 9/10: [===============               ] 33/63 batches, loss: 0.0115Epoch 9/10: [================              ] 34/63 batches, loss: 0.0115Epoch 9/10: [================              ] 35/63 batches, loss: 0.0115Epoch 9/10: [=================             ] 36/63 batches, loss: 0.0113Epoch 9/10: [=================             ] 37/63 batches, loss: 0.0112Epoch 9/10: [==================            ] 38/63 batches, loss: 0.0113Epoch 9/10: [==================            ] 39/63 batches, loss: 0.0115Epoch 9/10: [===================           ] 40/63 batches, loss: 0.0114Epoch 9/10: [===================           ] 41/63 batches, loss: 0.0113Epoch 9/10: [====================          ] 42/63 batches, loss: 0.0111Epoch 9/10: [====================          ] 43/63 batches, loss: 0.0112Epoch 9/10: [====================          ] 44/63 batches, loss: 0.0111Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.0110Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.0111Epoch 9/10: [======================        ] 47/63 batches, loss: 0.0113Epoch 9/10: [======================        ] 48/63 batches, loss: 0.0113Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.0114Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.0114Epoch 9/10: [========================      ] 51/63 batches, loss: 0.0114Epoch 9/10: [========================      ] 52/63 batches, loss: 0.0113Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.0112Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.0111Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.0110Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.0110Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.0110Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.0110Epoch 9/10: [============================  ] 59/63 batches, loss: 0.0109Epoch 9/10: [============================  ] 60/63 batches, loss: 0.0108Epoch 9/10: [============================= ] 61/63 batches, loss: 0.0110Epoch 9/10: [============================= ] 62/63 batches, loss: 0.0112Epoch 9/10: [==============================] 63/63 batches, loss: 0.0113
[2025-04-29 21:13:57,410][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0113
[2025-04-29 21:13:57,834][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0247, Metrics: {'mse': 0.023842977359890938, 'rmse': 0.1544117138040082, 'r2': 0.632500410079956}
[2025-04-29 21:13:57,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:13:57,835][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 21:13:57,835][src.training.lm_trainer][INFO] - Training completed in 121.73 seconds
[2025-04-29 21:13:57,835][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:14:02,549][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.009174106642603874, 'rmse': 0.09578155690217127, 'r2': 0.7011435031890869}
[2025-04-29 21:14:02,549][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.024227410554885864, 'rmse': 0.1556515677880755, 'r2': 0.6265749931335449}
[2025-04-29 21:14:02,549][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.043659549206495285, 'rmse': 0.20894867601039085, 'r2': 0.24732881784439087}
[2025-04-29 21:14:04,798][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▂▁
wandb:     best_val_mse █▇▅▂▁
wandb:      best_val_r2 ▁▂▄▇█
wandb:    best_val_rmse █▇▅▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▂▃▁▂▁▁
wandb:          val_mse █▇▅▂▃▁▂▁▁
wandb:           val_r2 ▁▂▄▇▆█▇██
wandb:         val_rmse █▇▅▃▄▁▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02427
wandb:     best_val_mse 0.02423
wandb:      best_val_r2 0.62657
wandb:    best_val_rmse 0.15565
wandb:            epoch 9
wandb:   final_test_mse 0.04366
wandb:    final_test_r2 0.24733
wandb:  final_test_rmse 0.20895
wandb:  final_train_mse 0.00917
wandb:   final_train_r2 0.70114
wandb: final_train_rmse 0.09578
wandb:    final_val_mse 0.02423
wandb:     final_val_r2 0.62657
wandb:   final_val_rmse 0.15565
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01135
wandb:       train_time 121.73372
wandb:         val_loss 0.02471
wandb:          val_mse 0.02384
wandb:           val_r2 0.6325
wandb:         val_rmse 0.15441
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211145-3a9c4q1h
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211145-3a9c4q1h/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:14:27,010][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:14:27,010][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:14:27,010][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:14:27,010][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:14:27,014][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 21:14:27,015][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:14:28,406][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:14:31,160][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:14:31,160][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:14:31,213][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,238][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,322][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 21:14:31,334][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:14:31,335][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 21:14:31,336][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:14:31,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,378][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,389][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 21:14:31,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:14:31,390][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 21:14:31,391][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:14:31,410][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,435][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:14:31,446][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 21:14:31,447][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:14:31,447][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 21:14:31,448][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 21:14:31,449][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:14:31,449][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:14:31,449][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:14:31,449][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:14:31,449][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 21:14:31,449][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:14:31,450][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:14:31,450][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:14:31,450][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:14:31,451][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:14:31,451][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:14:31,451][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 21:14:31,452][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:14:31,452][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 21:14:31,452][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:14:31,452][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:14:31,452][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:14:35,604][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,616][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,617][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,618][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,619][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,620][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:14:35,621][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:14:35,621][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:14:35,622][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:14:35,623][__main__][INFO] - Successfully created model for en
[2025-04-29 21:14:35,623][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7234Epoch 1/10: [                              ] 2/75 batches, loss: 0.7107Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7003Epoch 1/10: [=                             ] 4/75 batches, loss: 0.6965Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6958Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6945Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6912Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6916Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6932Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6931Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6906Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6904Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6906Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.6908Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6926Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6909Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6911Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6909Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6912Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6914Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6918Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6935Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6923Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6933Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6928Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6926Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6915Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6919Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6913Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6915Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6915Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6909Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6909Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6906Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6908Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6906Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6907Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6904Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6902Epoch 1/10: [================              ] 40/75 batches, loss: 0.6899Epoch 1/10: [================              ] 41/75 batches, loss: 0.6893Epoch 1/10: [================              ] 42/75 batches, loss: 0.6888Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6888Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6883Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6882Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6876Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6872Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6873Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6867Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6866Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6861Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6860Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6860Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6844Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6837Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6835Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6830Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6821Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6813Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6808Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6808Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6804Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6803Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6795Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6796Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6794Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6784Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6777Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6777Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6763Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6765Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6752Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6746Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6738Epoch 1/10: [==============================] 75/75 batches, loss: 0.6728
[2025-04-29 21:14:53,479][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6728
[2025-04-29 21:14:53,897][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.5972, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9113924050632911}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6179Epoch 2/10: [                              ] 2/75 batches, loss: 0.5886Epoch 2/10: [=                             ] 3/75 batches, loss: 0.5830Epoch 2/10: [=                             ] 4/75 batches, loss: 0.5458Epoch 2/10: [==                            ] 5/75 batches, loss: 0.5436Epoch 2/10: [==                            ] 6/75 batches, loss: 0.5476Epoch 2/10: [==                            ] 7/75 batches, loss: 0.5535Epoch 2/10: [===                           ] 8/75 batches, loss: 0.5540Epoch 2/10: [===                           ] 9/75 batches, loss: 0.5520Epoch 2/10: [====                          ] 10/75 batches, loss: 0.5548Epoch 2/10: [====                          ] 11/75 batches, loss: 0.5476Epoch 2/10: [====                          ] 12/75 batches, loss: 0.5443Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5352Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5280Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5240Epoch 2/10: [======                        ] 16/75 batches, loss: 0.5206Epoch 2/10: [======                        ] 17/75 batches, loss: 0.5202Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5176Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5191Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5156Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5102Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5085Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5042Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5018Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.4964Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.4904Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.4870Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.4846Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.4818Epoch 2/10: [============                  ] 30/75 batches, loss: 0.4789Epoch 2/10: [============                  ] 31/75 batches, loss: 0.4778Epoch 2/10: [============                  ] 32/75 batches, loss: 0.4743Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.4718Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.4677Epoch 2/10: [==============                ] 35/75 batches, loss: 0.4632Epoch 2/10: [==============                ] 36/75 batches, loss: 0.4612Epoch 2/10: [==============                ] 37/75 batches, loss: 0.4587Epoch 2/10: [===============               ] 38/75 batches, loss: 0.4552Epoch 2/10: [===============               ] 39/75 batches, loss: 0.4510Epoch 2/10: [================              ] 40/75 batches, loss: 0.4454Epoch 2/10: [================              ] 41/75 batches, loss: 0.4412Epoch 2/10: [================              ] 42/75 batches, loss: 0.4393Epoch 2/10: [=================             ] 43/75 batches, loss: 0.4367Epoch 2/10: [=================             ] 44/75 batches, loss: 0.4336Epoch 2/10: [==================            ] 45/75 batches, loss: 0.4290Epoch 2/10: [==================            ] 46/75 batches, loss: 0.4239Epoch 2/10: [==================            ] 47/75 batches, loss: 0.4213Epoch 2/10: [===================           ] 48/75 batches, loss: 0.4174Epoch 2/10: [===================           ] 49/75 batches, loss: 0.4135Epoch 2/10: [====================          ] 50/75 batches, loss: 0.4109Epoch 2/10: [====================          ] 51/75 batches, loss: 0.4097Epoch 2/10: [====================          ] 52/75 batches, loss: 0.4076Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.4044Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.4014Epoch 2/10: [======================        ] 55/75 batches, loss: 0.3989Epoch 2/10: [======================        ] 56/75 batches, loss: 0.3957Epoch 2/10: [======================        ] 57/75 batches, loss: 0.3933Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.3899Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.3868Epoch 2/10: [========================      ] 60/75 batches, loss: 0.3833Epoch 2/10: [========================      ] 61/75 batches, loss: 0.3793Epoch 2/10: [========================      ] 62/75 batches, loss: 0.3758Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.3725Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.3703Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.3674Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.3646Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.3624Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.3588Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.3560Epoch 2/10: [============================  ] 70/75 batches, loss: 0.3537Epoch 2/10: [============================  ] 71/75 batches, loss: 0.3509Epoch 2/10: [============================  ] 72/75 batches, loss: 0.3478Epoch 2/10: [============================= ] 73/75 batches, loss: 0.3453Epoch 2/10: [============================= ] 74/75 batches, loss: 0.3433Epoch 2/10: [==============================] 75/75 batches, loss: 0.3414
[2025-04-29 21:15:09,596][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.3414
[2025-04-29 21:15:10,010][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2238, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.2153Epoch 3/10: [                              ] 2/75 batches, loss: 0.1911Epoch 3/10: [=                             ] 3/75 batches, loss: 0.1685Epoch 3/10: [=                             ] 4/75 batches, loss: 0.1748Epoch 3/10: [==                            ] 5/75 batches, loss: 0.1849Epoch 3/10: [==                            ] 6/75 batches, loss: 0.1734Epoch 3/10: [==                            ] 7/75 batches, loss: 0.1706Epoch 3/10: [===                           ] 8/75 batches, loss: 0.1726Epoch 3/10: [===                           ] 9/75 batches, loss: 0.1782Epoch 3/10: [====                          ] 10/75 batches, loss: 0.1779Epoch 3/10: [====                          ] 11/75 batches, loss: 0.1752Epoch 3/10: [====                          ] 12/75 batches, loss: 0.1848Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.1815Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.1774Epoch 3/10: [======                        ] 15/75 batches, loss: 0.1747Epoch 3/10: [======                        ] 16/75 batches, loss: 0.1732Epoch 3/10: [======                        ] 17/75 batches, loss: 0.1834Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.1797Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.1760Epoch 3/10: [========                      ] 20/75 batches, loss: 0.1748Epoch 3/10: [========                      ] 21/75 batches, loss: 0.1715Epoch 3/10: [========                      ] 22/75 batches, loss: 0.1663Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.1650Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.1644Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.1640Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.1622Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.1712Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.1690Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.1679Epoch 3/10: [============                  ] 30/75 batches, loss: 0.1646Epoch 3/10: [============                  ] 31/75 batches, loss: 0.1627Epoch 3/10: [============                  ] 32/75 batches, loss: 0.1615Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.1600Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.1587Epoch 3/10: [==============                ] 35/75 batches, loss: 0.1565Epoch 3/10: [==============                ] 36/75 batches, loss: 0.1545Epoch 3/10: [==============                ] 37/75 batches, loss: 0.1524Epoch 3/10: [===============               ] 38/75 batches, loss: 0.1513Epoch 3/10: [===============               ] 39/75 batches, loss: 0.1495Epoch 3/10: [================              ] 40/75 batches, loss: 0.1479Epoch 3/10: [================              ] 41/75 batches, loss: 0.1466Epoch 3/10: [================              ] 42/75 batches, loss: 0.1451Epoch 3/10: [=================             ] 43/75 batches, loss: 0.1433Epoch 3/10: [=================             ] 44/75 batches, loss: 0.1436Epoch 3/10: [==================            ] 45/75 batches, loss: 0.1416Epoch 3/10: [==================            ] 46/75 batches, loss: 0.1397Epoch 3/10: [==================            ] 47/75 batches, loss: 0.1386Epoch 3/10: [===================           ] 48/75 batches, loss: 0.1376Epoch 3/10: [===================           ] 49/75 batches, loss: 0.1358Epoch 3/10: [====================          ] 50/75 batches, loss: 0.1343Epoch 3/10: [====================          ] 51/75 batches, loss: 0.1329Epoch 3/10: [====================          ] 52/75 batches, loss: 0.1322Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.1316Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.1320Epoch 3/10: [======================        ] 55/75 batches, loss: 0.1307Epoch 3/10: [======================        ] 56/75 batches, loss: 0.1298Epoch 3/10: [======================        ] 57/75 batches, loss: 0.1289Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.1279Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.1266Epoch 3/10: [========================      ] 60/75 batches, loss: 0.1256Epoch 3/10: [========================      ] 61/75 batches, loss: 0.1245Epoch 3/10: [========================      ] 62/75 batches, loss: 0.1236Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.1225Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.1216Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.1233Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.1224Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.1216Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.1206Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.1199Epoch 3/10: [============================  ] 70/75 batches, loss: 0.1191Epoch 3/10: [============================  ] 71/75 batches, loss: 0.1186Epoch 3/10: [============================  ] 72/75 batches, loss: 0.1181Epoch 3/10: [============================= ] 73/75 batches, loss: 0.1176Epoch 3/10: [============================= ] 74/75 batches, loss: 0.1166Epoch 3/10: [==============================] 75/75 batches, loss: 0.1159
[2025-04-29 21:15:25,785][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1159
[2025-04-29 21:15:26,202][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2105, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.1046Epoch 4/10: [                              ] 2/75 batches, loss: 0.1092Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0989Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0991Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0925Epoch 4/10: [==                            ] 6/75 batches, loss: 0.1140Epoch 4/10: [==                            ] 7/75 batches, loss: 0.1072Epoch 4/10: [===                           ] 8/75 batches, loss: 0.1053Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0981Epoch 4/10: [====                          ] 10/75 batches, loss: 0.1091Epoch 4/10: [====                          ] 11/75 batches, loss: 0.1235Epoch 4/10: [====                          ] 12/75 batches, loss: 0.1191Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.1149Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.1108Epoch 4/10: [======                        ] 15/75 batches, loss: 0.1065Epoch 4/10: [======                        ] 16/75 batches, loss: 0.1032Epoch 4/10: [======                        ] 17/75 batches, loss: 0.1009Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0987Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0964Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0936Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0925Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0904Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0896Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0875Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0863Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0853Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0844Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0831Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0820Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0813Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0801Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0793Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0788Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0784Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0775Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0770Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0759Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0747Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0739Epoch 4/10: [================              ] 40/75 batches, loss: 0.0733Epoch 4/10: [================              ] 41/75 batches, loss: 0.0722Epoch 4/10: [================              ] 42/75 batches, loss: 0.0716Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0710Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0700Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0697Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0694Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0688Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0682Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0677Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0727Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0730Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0722Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0717Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0714Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0712Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0714Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0716Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0712Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0709Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0712Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0706Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0701Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0699Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0692Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0687Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0686Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0684Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0679Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0678Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0672Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0668Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0665Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0662Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0657Epoch 4/10: [==============================] 75/75 batches, loss: 0.0651
[2025-04-29 21:15:41,950][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0651
[2025-04-29 21:15:42,525][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1464, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0311Epoch 5/10: [                              ] 2/75 batches, loss: 0.0282Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0286Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0613Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0550Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0517Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0481Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0472Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0445Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0444Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0427Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0420Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0408Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0387Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0380Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0365Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0359Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0351Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0346Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0339Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0334Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0331Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0324Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0317Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0320Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0376Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0369Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0460Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0453Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0444Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0439Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0434Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0426Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0420Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0414Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0408Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0419Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0413Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0408Epoch 5/10: [================              ] 40/75 batches, loss: 0.0403Epoch 5/10: [================              ] 41/75 batches, loss: 0.0397Epoch 5/10: [================              ] 42/75 batches, loss: 0.0394Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0388Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0383Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0378Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0374Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0370Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0365Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0434Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0429Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0424Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0421Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0415Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0409Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0405Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0400Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0394Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0391Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0388Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0385Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0380Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0378Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0374Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0371Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0369Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0366Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0364Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0362Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0359Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0357Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0355Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0351Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0349Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0348Epoch 5/10: [==============================] 75/75 batches, loss: 0.0345
[2025-04-29 21:15:58,353][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0345
[2025-04-29 21:15:58,778][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1641, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 21:15:58,779][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0115Epoch 6/10: [                              ] 2/75 batches, loss: 0.0139Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0166Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0161Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0164Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0168Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0162Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0157Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0161Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0163Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0169Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0168Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0164Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0163Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0162Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0171Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0165Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0227Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0221Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0217Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0213Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0212Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0206Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0204Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0204Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0201Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0200Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0199Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0199Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0196Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0197Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0194Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0190Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0188Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0187Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0185Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0184Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0185Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0185Epoch 6/10: [================              ] 40/75 batches, loss: 0.0182Epoch 6/10: [================              ] 41/75 batches, loss: 0.0180Epoch 6/10: [================              ] 42/75 batches, loss: 0.0180Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0178Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0177Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0176Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0176Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0174Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0173Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0172Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0238Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0236Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0234Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0230Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0227Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0226Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0225Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0224Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0222Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0236Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0233Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0231Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0229Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0226Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0224Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0254Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0252Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0250Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0248Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0246Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0245Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0245Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0242Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0241Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0241Epoch 6/10: [==============================] 75/75 batches, loss: 0.0239
[2025-04-29 21:16:13,955][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0239
[2025-04-29 21:16:14,396][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1620, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 21:16:14,397][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0162Epoch 7/10: [                              ] 2/75 batches, loss: 0.0141Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0115Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0115Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0105Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0112Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0112Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0114Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0123Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0133Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0139Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0136Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0134Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0132Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0129Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0131Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0130Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0133Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0135Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0134Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0131Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0130Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0129Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0130Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0129Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0128Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0130Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0128Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0126Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0169Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0166Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0165Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0164Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0164Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0165Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0165Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0166Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0169Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0176Epoch 7/10: [================              ] 40/75 batches, loss: 0.0173Epoch 7/10: [================              ] 41/75 batches, loss: 0.0172Epoch 7/10: [================              ] 42/75 batches, loss: 0.0170Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0170Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0168Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0166Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0165Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0164Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0163Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0190Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0189Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0187Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0187Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0185Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0183Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0181Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0179Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0177Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0176Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0175Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0173Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0173Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0172Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0170Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0173Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0173Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0171Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0170Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0169Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0167Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0166Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0165Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0165Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0165Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0163Epoch 7/10: [==============================] 75/75 batches, loss: 0.0164
[2025-04-29 21:16:29,589][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0164
[2025-04-29 21:16:30,026][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2467, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
[2025-04-29 21:16:30,027][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:16:30,027][src.training.lm_trainer][INFO] - Early stopping at epoch 7
[2025-04-29 21:16:30,027][src.training.lm_trainer][INFO] - Training completed in 112.25 seconds
[2025-04-29 21:16:30,027][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:16:35,725][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488}
[2025-04-29 21:16:35,725][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-29 21:16:35,725][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9363636363636364, 'f1': 0.9380530973451328}
[2025-04-29 21:16:37,957][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁███
wandb:          best_val_f1 ▁███
wandb:        best_val_loss █▂▂▁
wandb:                epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁
wandb:           train_loss █▄▂▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████▆
wandb:               val_f1 ▁█████▆
wandb:             val_loss █▂▂▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.1464
wandb:                epoch 7
wandb:  final_test_accuracy 0.93636
wandb:        final_test_f1 0.93805
wandb: final_train_accuracy 0.99664
wandb:       final_train_f1 0.99666
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 2e-05
wandb:           train_loss 0.0164
wandb:           train_time 112.25411
wandb:         val_accuracy 0.93056
wandb:               val_f1 0.93506
wandb:             val_loss 0.24672
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211427-4rmk79df
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211427-4rmk79df/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:17:03,779][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/en
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:17:03,779][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:17:03,779][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:17:03,779][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:17:03,784][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 21:17:03,784][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:17:05,173][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:17:07,902][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:17:07,902][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:17:07,969][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:07,994][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:08,077][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 21:17:08,089][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:17:08,089][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 21:17:08,090][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:17:08,109][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:08,134][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:08,146][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 21:17:08,147][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:17:08,147][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 21:17:08,148][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:17:08,169][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:08,196][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:17:08,208][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 21:17:08,209][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:17:08,209][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 21:17:08,210][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 21:17:08,211][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:17:08,211][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:17:08,211][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:17:08,211][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:17:08,211][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:17:08,212][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:17:08,212][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:17:08,212][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:17:08,213][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:17:08,213][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:17:08,213][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:17:08,214][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 21:17:08,214][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 21:17:08,214][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 21:17:08,214][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 21:17:08,214][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:17:08,214][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:17:08,215][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,300][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,301][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,302][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,303][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,304][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,305][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,306][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,307][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,308][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,309][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,310][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,311][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,312][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,313][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,314][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,315][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,316][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,316][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:17:12,316][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:17:12,317][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:17:12,318][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:17:12,318][__main__][INFO] - Successfully created model for en
[2025-04-29 21:17:12,318][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.0702Epoch 1/10: [                              ] 2/75 batches, loss: 0.0912Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1138Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1255Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1182Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1127Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1126Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1054Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1042Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1033Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1023Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1013Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1001Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.0995Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1011Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1030Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1014Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1023Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1008Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1021Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1002Epoch 1/10: [========                      ] 22/75 batches, loss: 0.0981Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.0980Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.0964Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.0946Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.0938Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.0926Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.0923Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.0904Epoch 1/10: [============                  ] 30/75 batches, loss: 0.0899Epoch 1/10: [============                  ] 31/75 batches, loss: 0.0899Epoch 1/10: [============                  ] 32/75 batches, loss: 0.0886Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.0884Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.0877Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0865Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0848Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0841Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0837Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0826Epoch 1/10: [================              ] 40/75 batches, loss: 0.0824Epoch 1/10: [================              ] 41/75 batches, loss: 0.0817Epoch 1/10: [================              ] 42/75 batches, loss: 0.0808Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0804Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0802Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0789Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0784Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0778Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0770Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0767Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0762Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0755Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0766Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0767Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0760Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0754Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0756Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0755Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0751Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0746Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0745Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0748Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0750Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0746Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0739Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0735Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0728Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0724Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0720Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0719Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0717Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0719Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0716Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0712Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0709Epoch 1/10: [==============================] 75/75 batches, loss: 0.0713
[2025-04-29 21:17:29,947][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0713
[2025-04-29 21:17:30,343][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0504, Metrics: {'mse': 0.054246142506599426, 'rmse': 0.23290801297207323, 'r2': -0.29616665840148926}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0476Epoch 2/10: [                              ] 2/75 batches, loss: 0.0437Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0466Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0484Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0528Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0529Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0530Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0539Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0554Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0555Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0565Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0574Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0556Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0570Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0555Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0545Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0549Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0546Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0540Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0528Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0558Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0574Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0576Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0579Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0603Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0603Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0604Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0613Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0627Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0630Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0636Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0641Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0641Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0643Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0645Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0643Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0638Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0645Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0648Epoch 2/10: [================              ] 40/75 batches, loss: 0.0642Epoch 2/10: [================              ] 41/75 batches, loss: 0.0650Epoch 2/10: [================              ] 42/75 batches, loss: 0.0650Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0643Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0637Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0637Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0635Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0635Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0634Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0633Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0626Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0618Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0611Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0612Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0611Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0609Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0604Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0609Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0610Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0619Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0623Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0624Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0622Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0623Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0620Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0621Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0616Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0614Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0619Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0622Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0624Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0629Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0630Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0629Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0631Epoch 2/10: [==============================] 75/75 batches, loss: 0.0638
[2025-04-29 21:17:46,073][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0638
[2025-04-29 21:17:46,480][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0606, Metrics: {'mse': 0.06526431441307068, 'rmse': 0.25546881299499297, 'r2': -0.5594366788864136}
[2025-04-29 21:17:46,481][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0806Epoch 3/10: [                              ] 2/75 batches, loss: 0.0739Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0756Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0741Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0705Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0697Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0736Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0802Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0770Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0822Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0814Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0796Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0794Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0771Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0764Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0784Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0760Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0761Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0740Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0742Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0732Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0717Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0716Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0716Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0721Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0735Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0728Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0711Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0703Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0696Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0682Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0668Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0659Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0648Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0641Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0632Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0624Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0621Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0617Epoch 3/10: [================              ] 40/75 batches, loss: 0.0607Epoch 3/10: [================              ] 41/75 batches, loss: 0.0597Epoch 3/10: [================              ] 42/75 batches, loss: 0.0590Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0587Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0584Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0581Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0582Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0581Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0584Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0584Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0581Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0579Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0579Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0572Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0570Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0568Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0562Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0556Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0555Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0556Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0555Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0557Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0554Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0550Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0550Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0549Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0549Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0549Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0545Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0547Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0544Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0544Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0546Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0544Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0541Epoch 3/10: [==============================] 75/75 batches, loss: 0.0535
[2025-04-29 21:18:01,644][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0535
[2025-04-29 21:18:02,052][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0342, Metrics: {'mse': 0.034175898879766464, 'rmse': 0.1848672466386798, 'r2': 0.18339520692825317}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0294Epoch 4/10: [                              ] 2/75 batches, loss: 0.0364Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0311Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0287Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0293Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0308Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0334Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0327Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0327Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0318Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0325Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0334Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0326Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0333Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0335Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0321Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0324Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0317Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0309Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0314Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0313Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0312Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0316Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0321Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0326Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0321Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0325Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0320Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0321Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0326Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0321Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0318Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0323Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0330Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0326Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0327Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0328Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0327Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0326Epoch 4/10: [================              ] 40/75 batches, loss: 0.0332Epoch 4/10: [================              ] 41/75 batches, loss: 0.0331Epoch 4/10: [================              ] 42/75 batches, loss: 0.0334Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0334Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0338Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0338Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0335Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0332Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0329Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0328Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0331Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0331Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0331Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0331Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0330Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0331Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0331Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0331Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0327Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0326Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0322Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0322Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0323Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0321Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0320Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0319Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0320Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0321Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0320Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0320Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0317Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0315Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0314Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0313Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0311Epoch 4/10: [==============================] 75/75 batches, loss: 0.0309
[2025-04-29 21:18:17,840][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0309
[2025-04-29 21:18:18,249][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0356, Metrics: {'mse': 0.03552708402276039, 'rmse': 0.18848629664450514, 'r2': 0.1511097550392151}
[2025-04-29 21:18:18,250][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0379Epoch 5/10: [                              ] 2/75 batches, loss: 0.0424Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0412Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0379Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0358Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0337Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0346Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0318Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0304Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0306Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0296Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0287Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0313Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0301Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0289Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0286Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0281Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0273Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0273Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0280Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0272Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0268Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0260Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0262Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0271Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0276Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0273Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0275Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0278Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0279Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0275Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0278Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0281Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0277Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0276Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0274Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0273Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0269Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0271Epoch 5/10: [================              ] 40/75 batches, loss: 0.0270Epoch 5/10: [================              ] 41/75 batches, loss: 0.0267Epoch 5/10: [================              ] 42/75 batches, loss: 0.0266Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0265Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0264Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0260Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0262Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0259Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0257Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0260Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0257Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0257Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0258Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0255Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0253Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0252Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0252Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0250Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0249Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0251Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0250Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0251Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0252Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0251Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0252Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0254Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0252Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0250Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0251Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0250Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0249Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0250Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0249Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0247Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0245Epoch 5/10: [==============================] 75/75 batches, loss: 0.0244
[2025-04-29 21:18:33,416][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0244
[2025-04-29 21:18:33,886][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0334, Metrics: {'mse': 0.03311040624976158, 'rmse': 0.1819626506999763, 'r2': 0.20885425806045532}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0092Epoch 6/10: [                              ] 2/75 batches, loss: 0.0166Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0193Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0184Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0216Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0202Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0198Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0224Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0210Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0213Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0210Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0211Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0204Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0198Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0214Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0213Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0228Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0229Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0231Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0226Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0220Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0220Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0226Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0224Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0220Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0217Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0216Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0210Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0207Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0207Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0207Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0206Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0204Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0205Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0204Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0202Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0199Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0199Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0198Epoch 6/10: [================              ] 40/75 batches, loss: 0.0196Epoch 6/10: [================              ] 41/75 batches, loss: 0.0195Epoch 6/10: [================              ] 42/75 batches, loss: 0.0197Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0197Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0196Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0195Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0194Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0199Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0197Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0197Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0196Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0195Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0194Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0194Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0195Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0194Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0193Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0193Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0191Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0192Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0190Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0189Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0188Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0188Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0189Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0188Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0187Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0188Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0188Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0188Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0189Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0189Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0190Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0192Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0192Epoch 6/10: [==============================] 75/75 batches, loss: 0.0191
[2025-04-29 21:18:49,685][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0191
[2025-04-29 21:18:50,086][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0363, Metrics: {'mse': 0.03547845780849457, 'rmse': 0.18835726109840992, 'r2': 0.15227168798446655}
[2025-04-29 21:18:50,087][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0187Epoch 7/10: [                              ] 2/75 batches, loss: 0.0192Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0175Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0163Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0158Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0181Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0182Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0193Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0193Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0208Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0206Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0213Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0211Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0206Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0210Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0203Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0201Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0200Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0199Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0194Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0196Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0205Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0207Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0204Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0203Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0200Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0198Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0198Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0195Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0196Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0200Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0197Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0196Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0197Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0200Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0202Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0206Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0208Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0210Epoch 7/10: [================              ] 40/75 batches, loss: 0.0210Epoch 7/10: [================              ] 41/75 batches, loss: 0.0209Epoch 7/10: [================              ] 42/75 batches, loss: 0.0205Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0202Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0199Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0198Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0198Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0196Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0196Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0198Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0198Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0203Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0204Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0203Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0205Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0207Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0205Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0204Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0203Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0203Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0202Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0201Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0200Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0201Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0199Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0199Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0198Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0197Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0198Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0198Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0197Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0196Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0194Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0195Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0194Epoch 7/10: [==============================] 75/75 batches, loss: 0.0195
[2025-04-29 21:19:05,312][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0195
[2025-04-29 21:19:05,729][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0299, Metrics: {'mse': 0.02911628596484661, 'rmse': 0.1706349494237526, 'r2': 0.3042904734611511}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0142Epoch 8/10: [                              ] 2/75 batches, loss: 0.0164Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0163Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0153Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0173Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0167Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0168Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0170Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0161Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0149Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0145Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0145Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0141Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0139Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0147Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0143Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0145Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0144Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0141Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0148Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0145Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0147Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0145Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0147Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0147Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0146Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0147Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0148Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0145Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0142Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0140Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0139Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0138Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0141Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0143Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0143Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0143Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0146Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0151Epoch 8/10: [================              ] 40/75 batches, loss: 0.0149Epoch 8/10: [================              ] 41/75 batches, loss: 0.0149Epoch 8/10: [================              ] 42/75 batches, loss: 0.0154Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0155Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0155Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0155Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0154Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0155Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0153Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0156Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0155Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0156Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0159Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0159Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0161Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0160Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0160Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0162Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0160Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0160Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0158Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0157Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0156Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0156Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0156Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0155Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0154Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0154Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0155Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0157Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0157Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0157Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0157Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0157Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0157Epoch 8/10: [==============================] 75/75 batches, loss: 0.0157
[2025-04-29 21:19:21,473][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0157
[2025-04-29 21:19:21,960][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0236, Metrics: {'mse': 0.02348492108285427, 'rmse': 0.153247907270717, 'r2': 0.43884724378585815}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0243Epoch 9/10: [                              ] 2/75 batches, loss: 0.0188Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0173Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0199Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0219Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0206Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0196Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0188Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0178Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0169Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0163Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0155Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0156Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0168Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0163Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0164Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0165Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0163Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0162Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0163Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0158Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0155Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0152Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0149Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0149Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0146Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0144Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0145Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0145Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0148Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0149Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0147Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0150Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0150Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0151Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0149Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0146Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0148Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0148Epoch 9/10: [================              ] 40/75 batches, loss: 0.0146Epoch 9/10: [================              ] 41/75 batches, loss: 0.0149Epoch 9/10: [================              ] 42/75 batches, loss: 0.0151Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0152Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0153Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0152Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0151Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0153Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0155Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0155Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0154Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0154Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0154Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0153Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0152Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0152Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0151Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0150Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0149Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0149Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0148Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0150Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0150Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0151Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0152Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0151Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0151Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0150Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0150Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0148Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0152Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0152Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0152Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0151Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0150Epoch 9/10: [==============================] 75/75 batches, loss: 0.0150
[2025-04-29 21:19:37,699][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0150
[2025-04-29 21:19:38,123][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0387, Metrics: {'mse': 0.03847069665789604, 'rmse': 0.19613948265939737, 'r2': 0.08077460527420044}
[2025-04-29 21:19:38,124][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0317Epoch 10/10: [                              ] 2/75 batches, loss: 0.0174Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0175Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0159Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0147Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0137Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0129Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0118Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0116Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0115Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0114Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0115Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0118Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0122Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0134Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0130Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0127Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0127Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0134Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0132Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0129Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0129Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0130Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0127Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0130Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0135Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0135Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0133Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0133Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0130Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0131Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0131Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0128Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0129Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0128Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0129Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0128Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0128Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0127Epoch 10/10: [================              ] 40/75 batches, loss: 0.0127Epoch 10/10: [================              ] 41/75 batches, loss: 0.0128Epoch 10/10: [================              ] 42/75 batches, loss: 0.0127Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0128Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0128Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0128Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0128Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0128Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0127Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0128Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0127Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0127Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0127Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0126Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0126Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0125Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0124Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0123Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0124Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0124Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0124Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0123Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0122Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0122Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0121Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0120Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0119Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0119Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0118Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0118Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0118Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0117Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0117Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0117Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0116Epoch 10/10: [==============================] 75/75 batches, loss: 0.0117
[2025-04-29 21:19:53,312][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0117
[2025-04-29 21:19:53,722][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0249, Metrics: {'mse': 0.024643395096063614, 'rmse': 0.15698214897262558, 'r2': 0.4111664891242981}
[2025-04-29 21:19:53,723][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-04-29 21:19:53,723][src.training.lm_trainer][INFO] - Training completed in 159.47 seconds
[2025-04-29 21:19:53,723][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:19:59,325][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.015083781443536282, 'rmse': 0.12281604717436675, 'r2': 0.43777376413345337}
[2025-04-29 21:19:59,326][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02348492108285427, 'rmse': 0.153247907270717, 'r2': 0.43884724378585815}
[2025-04-29 21:19:59,326][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.018679635599255562, 'rmse': 0.1366734634055037, 'r2': 0.5153007507324219}
[2025-04-29 21:20:01,595][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▃▁
wandb:     best_val_mse █▃▃▂▁
wandb:      best_val_r2 ▁▆▆▇█
wandb:    best_val_rmse █▄▄▃▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▃▃▃▃▂▁▄▁
wandb:          val_mse ▆█▃▃▃▃▂▁▄▁
wandb:           val_r2 ▃▁▆▆▆▆▇█▅█
wandb:         val_rmse ▆█▃▃▃▃▂▁▄▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0236
wandb:     best_val_mse 0.02348
wandb:      best_val_r2 0.43885
wandb:    best_val_rmse 0.15325
wandb:            epoch 10
wandb:   final_test_mse 0.01868
wandb:    final_test_r2 0.5153
wandb:  final_test_rmse 0.13667
wandb:  final_train_mse 0.01508
wandb:   final_train_r2 0.43777
wandb: final_train_rmse 0.12282
wandb:    final_val_mse 0.02348
wandb:     final_val_r2 0.43885
wandb:   final_val_rmse 0.15325
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01172
wandb:       train_time 159.47158
wandb:         val_loss 0.0249
wandb:          val_mse 0.02464
wandb:           val_r2 0.41117
wandb:         val_rmse 0.15698
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211703-xquopbua
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_211703-xquopbua/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:20:31,313][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:20:31,313][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:20:31,313][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:20:31,313][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:20:31,318][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 21:20:31,318][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:20:32,838][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:20:35,546][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:20:35,546][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:20:35,603][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,628][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,721][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 21:20:35,732][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:20:35,733][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 21:20:35,734][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:20:35,774][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,801][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,812][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 21:20:35,813][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:20:35,813][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 21:20:35,815][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:20:35,836][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,877][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:20:35,898][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 21:20:35,899][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:20:35,900][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 21:20:35,901][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 21:20:35,901][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:20:35,901][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:20:35,901][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:20:35,901][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:20:35,902][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 21:20:35,902][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:20:35,902][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:20:35,903][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 21:20:35,903][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:20:35,903][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:20:35,904][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:20:35,904][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:20:35,904][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 21:20:35,904][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:20:35,904][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 21:20:35,904][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:20:35,904][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:20:35,905][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:20:40,049][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,050][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,051][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,052][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,053][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,054][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,055][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,056][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,057][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,058][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,059][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,060][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,061][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,062][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,063][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,064][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,065][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,066][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,067][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,067][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:20:40,067][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:20:40,068][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:20:40,069][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:20:40,069][__main__][INFO] - Successfully created model for fi
[2025-04-29 21:20:40,069][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.6868Epoch 1/10: [                              ] 2/75 batches, loss: 0.6951Epoch 1/10: [=                             ] 3/75 batches, loss: 0.6979Epoch 1/10: [=                             ] 4/75 batches, loss: 0.6999Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6997Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7042Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7025Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7027Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6999Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6981Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6959Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6966Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6968Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.6961Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6952Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6940Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6949Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6944Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6946Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6939Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6949Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6951Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6946Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6951Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6950Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6957Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6953Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6951Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6950Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6951Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6951Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6947Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6948Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6947Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6939Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6936Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6928Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6924Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6920Epoch 1/10: [================              ] 40/75 batches, loss: 0.6920Epoch 1/10: [================              ] 41/75 batches, loss: 0.6908Epoch 1/10: [================              ] 42/75 batches, loss: 0.6906Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6910Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6902Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6896Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6899Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6901Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6902Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6893Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6886Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6882Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6878Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6870Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6864Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6856Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6858Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6854Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6843Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6835Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6833Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6832Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6823Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6819Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6810Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6806Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6804Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6796Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6788Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6779Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6770Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6769Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6758Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6748Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6731Epoch 1/10: [==============================] 75/75 batches, loss: 0.6718
[2025-04-29 21:20:58,547][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6718
[2025-04-29 21:20:58,923][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6236, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.90625}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5820Epoch 2/10: [                              ] 2/75 batches, loss: 0.5954Epoch 2/10: [=                             ] 3/75 batches, loss: 0.5779Epoch 2/10: [=                             ] 4/75 batches, loss: 0.5825Epoch 2/10: [==                            ] 5/75 batches, loss: 0.5885Epoch 2/10: [==                            ] 6/75 batches, loss: 0.5995Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6009Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6020Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6009Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6073Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6023Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6035Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5976Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5981Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5988Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6000Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6013Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5987Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5926Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5898Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5875Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5836Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5771Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5751Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5745Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5726Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5708Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5673Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5656Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5620Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5583Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5574Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5543Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5528Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5493Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5451Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5413Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5386Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5337Epoch 2/10: [================              ] 40/75 batches, loss: 0.5305Epoch 2/10: [================              ] 41/75 batches, loss: 0.5280Epoch 2/10: [================              ] 42/75 batches, loss: 0.5252Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5218Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5203Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5183Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5161Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5131Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5133Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5097Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5100Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5076Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5038Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5006Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.4963Epoch 2/10: [======================        ] 55/75 batches, loss: 0.4947Epoch 2/10: [======================        ] 56/75 batches, loss: 0.4916Epoch 2/10: [======================        ] 57/75 batches, loss: 0.4892Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.4882Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.4873Epoch 2/10: [========================      ] 60/75 batches, loss: 0.4849Epoch 2/10: [========================      ] 61/75 batches, loss: 0.4828Epoch 2/10: [========================      ] 62/75 batches, loss: 0.4809Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.4778Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.4754Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.4740Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.4728Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.4715Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.4694Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.4661Epoch 2/10: [============================  ] 70/75 batches, loss: 0.4631Epoch 2/10: [============================  ] 71/75 batches, loss: 0.4599Epoch 2/10: [============================  ] 72/75 batches, loss: 0.4573Epoch 2/10: [============================= ] 73/75 batches, loss: 0.4568Epoch 2/10: [============================= ] 74/75 batches, loss: 0.4545Epoch 2/10: [==============================] 75/75 batches, loss: 0.4523
[2025-04-29 21:21:14,638][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4523
[2025-04-29 21:21:15,035][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.3364, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.2918Epoch 3/10: [                              ] 2/75 batches, loss: 0.2521Epoch 3/10: [=                             ] 3/75 batches, loss: 0.3144Epoch 3/10: [=                             ] 4/75 batches, loss: 0.2895Epoch 3/10: [==                            ] 5/75 batches, loss: 0.2774Epoch 3/10: [==                            ] 6/75 batches, loss: 0.2589Epoch 3/10: [==                            ] 7/75 batches, loss: 0.2514Epoch 3/10: [===                           ] 8/75 batches, loss: 0.2456Epoch 3/10: [===                           ] 9/75 batches, loss: 0.2449Epoch 3/10: [====                          ] 10/75 batches, loss: 0.2474Epoch 3/10: [====                          ] 11/75 batches, loss: 0.2510Epoch 3/10: [====                          ] 12/75 batches, loss: 0.2655Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.2657Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.2596Epoch 3/10: [======                        ] 15/75 batches, loss: 0.2552Epoch 3/10: [======                        ] 16/75 batches, loss: 0.2509Epoch 3/10: [======                        ] 17/75 batches, loss: 0.2476Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.2493Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.2462Epoch 3/10: [========                      ] 20/75 batches, loss: 0.2476Epoch 3/10: [========                      ] 21/75 batches, loss: 0.2504Epoch 3/10: [========                      ] 22/75 batches, loss: 0.2471Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.2435Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.2399Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.2378Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.2356Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.2316Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.2308Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.2298Epoch 3/10: [============                  ] 30/75 batches, loss: 0.2282Epoch 3/10: [============                  ] 31/75 batches, loss: 0.2255Epoch 3/10: [============                  ] 32/75 batches, loss: 0.2234Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.2291Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.2277Epoch 3/10: [==============                ] 35/75 batches, loss: 0.2265Epoch 3/10: [==============                ] 36/75 batches, loss: 0.2259Epoch 3/10: [==============                ] 37/75 batches, loss: 0.2226Epoch 3/10: [===============               ] 38/75 batches, loss: 0.2208Epoch 3/10: [===============               ] 39/75 batches, loss: 0.2197Epoch 3/10: [================              ] 40/75 batches, loss: 0.2167Epoch 3/10: [================              ] 41/75 batches, loss: 0.2138Epoch 3/10: [================              ] 42/75 batches, loss: 0.2112Epoch 3/10: [=================             ] 43/75 batches, loss: 0.2092Epoch 3/10: [=================             ] 44/75 batches, loss: 0.2065Epoch 3/10: [==================            ] 45/75 batches, loss: 0.2042Epoch 3/10: [==================            ] 46/75 batches, loss: 0.2035Epoch 3/10: [==================            ] 47/75 batches, loss: 0.2012Epoch 3/10: [===================           ] 48/75 batches, loss: 0.2002Epoch 3/10: [===================           ] 49/75 batches, loss: 0.1988Epoch 3/10: [====================          ] 50/75 batches, loss: 0.1989Epoch 3/10: [====================          ] 51/75 batches, loss: 0.1970Epoch 3/10: [====================          ] 52/75 batches, loss: 0.1985Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.1984Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.1971Epoch 3/10: [======================        ] 55/75 batches, loss: 0.1956Epoch 3/10: [======================        ] 56/75 batches, loss: 0.1934Epoch 3/10: [======================        ] 57/75 batches, loss: 0.1962Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.1949Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.1932Epoch 3/10: [========================      ] 60/75 batches, loss: 0.1913Epoch 3/10: [========================      ] 61/75 batches, loss: 0.1896Epoch 3/10: [========================      ] 62/75 batches, loss: 0.1884Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.1916Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.1899Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.1899Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.1890Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.1877Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.1878Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.1880Epoch 3/10: [============================  ] 70/75 batches, loss: 0.1892Epoch 3/10: [============================  ] 71/75 batches, loss: 0.1885Epoch 3/10: [============================  ] 72/75 batches, loss: 0.1870Epoch 3/10: [============================= ] 73/75 batches, loss: 0.1856Epoch 3/10: [============================= ] 74/75 batches, loss: 0.1918Epoch 3/10: [==============================] 75/75 batches, loss: 0.1902
[2025-04-29 21:21:30,825][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1902
[2025-04-29 21:21:31,193][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2137, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.2394Epoch 4/10: [                              ] 2/75 batches, loss: 0.1900Epoch 4/10: [=                             ] 3/75 batches, loss: 0.1910Epoch 4/10: [=                             ] 4/75 batches, loss: 0.1726Epoch 4/10: [==                            ] 5/75 batches, loss: 0.1642Epoch 4/10: [==                            ] 6/75 batches, loss: 0.1581Epoch 4/10: [==                            ] 7/75 batches, loss: 0.1501Epoch 4/10: [===                           ] 8/75 batches, loss: 0.1410Epoch 4/10: [===                           ] 9/75 batches, loss: 0.1372Epoch 4/10: [====                          ] 10/75 batches, loss: 0.1327Epoch 4/10: [====                          ] 11/75 batches, loss: 0.1348Epoch 4/10: [====                          ] 12/75 batches, loss: 0.1285Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.1247Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.1196Epoch 4/10: [======                        ] 15/75 batches, loss: 0.1265Epoch 4/10: [======                        ] 16/75 batches, loss: 0.1242Epoch 4/10: [======                        ] 17/75 batches, loss: 0.1227Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.1213Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.1210Epoch 4/10: [========                      ] 20/75 batches, loss: 0.1176Epoch 4/10: [========                      ] 21/75 batches, loss: 0.1150Epoch 4/10: [========                      ] 22/75 batches, loss: 0.1148Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.1115Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.1097Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.1068Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.1112Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.1085Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.1127Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.1131Epoch 4/10: [============                  ] 30/75 batches, loss: 0.1124Epoch 4/10: [============                  ] 31/75 batches, loss: 0.1107Epoch 4/10: [============                  ] 32/75 batches, loss: 0.1092Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.1095Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.1101Epoch 4/10: [==============                ] 35/75 batches, loss: 0.1086Epoch 4/10: [==============                ] 36/75 batches, loss: 0.1143Epoch 4/10: [==============                ] 37/75 batches, loss: 0.1128Epoch 4/10: [===============               ] 38/75 batches, loss: 0.1137Epoch 4/10: [===============               ] 39/75 batches, loss: 0.1127Epoch 4/10: [================              ] 40/75 batches, loss: 0.1114Epoch 4/10: [================              ] 41/75 batches, loss: 0.1101Epoch 4/10: [================              ] 42/75 batches, loss: 0.1089Epoch 4/10: [=================             ] 43/75 batches, loss: 0.1078Epoch 4/10: [=================             ] 44/75 batches, loss: 0.1070Epoch 4/10: [==================            ] 45/75 batches, loss: 0.1062Epoch 4/10: [==================            ] 46/75 batches, loss: 0.1055Epoch 4/10: [==================            ] 47/75 batches, loss: 0.1054Epoch 4/10: [===================           ] 48/75 batches, loss: 0.1083Epoch 4/10: [===================           ] 49/75 batches, loss: 0.1081Epoch 4/10: [====================          ] 50/75 batches, loss: 0.1076Epoch 4/10: [====================          ] 51/75 batches, loss: 0.1068Epoch 4/10: [====================          ] 52/75 batches, loss: 0.1066Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.1064Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.1055Epoch 4/10: [======================        ] 55/75 batches, loss: 0.1053Epoch 4/10: [======================        ] 56/75 batches, loss: 0.1046Epoch 4/10: [======================        ] 57/75 batches, loss: 0.1043Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.1032Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.1025Epoch 4/10: [========================      ] 60/75 batches, loss: 0.1013Epoch 4/10: [========================      ] 61/75 batches, loss: 0.1000Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0993Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0990Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0981Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0976Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0966Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0962Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0951Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0987Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0981Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0970Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0962Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0982Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0975Epoch 4/10: [==============================] 75/75 batches, loss: 0.0975
[2025-04-29 21:21:46,970][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0975
[2025-04-29 21:21:47,407][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2461, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 21:21:47,408][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.1076Epoch 5/10: [                              ] 2/75 batches, loss: 0.1163Epoch 5/10: [=                             ] 3/75 batches, loss: 0.1092Epoch 5/10: [=                             ] 4/75 batches, loss: 0.1066Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0974Epoch 5/10: [==                            ] 6/75 batches, loss: 0.1142Epoch 5/10: [==                            ] 7/75 batches, loss: 0.1047Epoch 5/10: [===                           ] 8/75 batches, loss: 0.1028Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0985Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0943Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0934Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0916Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0935Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0889Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0845Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0825Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0795Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0772Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0764Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0748Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0749Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0739Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0724Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0717Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0718Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0707Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0724Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0782Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0765Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0824Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0810Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0806Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0794Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0778Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0778Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0775Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0780Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0826Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0815Epoch 5/10: [================              ] 40/75 batches, loss: 0.0857Epoch 5/10: [================              ] 41/75 batches, loss: 0.0868Epoch 5/10: [================              ] 42/75 batches, loss: 0.0871Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0865Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0876Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0919Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0913Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0956Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0955Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0944Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0932Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0971Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0962Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0952Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0938Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0926Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0913Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0903Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0890Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0878Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0870Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0859Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0886Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0925Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0916Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0908Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0955Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0982Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0981Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0980Epoch 5/10: [============================  ] 70/75 batches, loss: 0.1029Epoch 5/10: [============================  ] 71/75 batches, loss: 0.1022Epoch 5/10: [============================  ] 72/75 batches, loss: 0.1027Epoch 5/10: [============================= ] 73/75 batches, loss: 0.1020Epoch 5/10: [============================= ] 74/75 batches, loss: 0.1056Epoch 5/10: [==============================] 75/75 batches, loss: 0.1052
[2025-04-29 21:22:02,623][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1052
[2025-04-29 21:22:03,038][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1566, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.1527Epoch 6/10: [                              ] 2/75 batches, loss: 0.1009Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0794Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0975Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0847Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0768Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0707Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0947Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0873Epoch 6/10: [====                          ] 10/75 batches, loss: 0.1021Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0973Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0929Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0916Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0896Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0888Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0866Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0882Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.1087Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.1099Epoch 6/10: [========                      ] 20/75 batches, loss: 0.1082Epoch 6/10: [========                      ] 21/75 batches, loss: 0.1075Epoch 6/10: [========                      ] 22/75 batches, loss: 0.1055Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.1031Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.1030Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.1004Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0978Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0951Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0927Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0910Epoch 6/10: [============                  ] 30/75 batches, loss: 0.1001Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0980Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0959Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0947Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0938Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0930Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0931Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0927Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0921Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0911Epoch 6/10: [================              ] 40/75 batches, loss: 0.0901Epoch 6/10: [================              ] 41/75 batches, loss: 0.0892Epoch 6/10: [================              ] 42/75 batches, loss: 0.0884Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0873Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0858Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0917Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0900Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0887Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0873Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0860Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0846Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0837Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0830Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0823Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0816Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0810Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0805Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0831Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0826Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0821Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0820Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0813Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0810Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0806Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0797Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0791Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0832Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0823Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0815Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0805Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0818Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0807Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0799Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0790Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0781Epoch 6/10: [==============================] 75/75 batches, loss: 0.0772
[2025-04-29 21:22:18,848][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0772
[2025-04-29 21:22:19,233][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
[2025-04-29 21:22:19,234][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.2713Epoch 7/10: [                              ] 2/75 batches, loss: 0.1414Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0994Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0780Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0646Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0560Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0491Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0450Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0453Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0439Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0405Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0379Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0377Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0356Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0359Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0351Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0337Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0324Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0324Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0317Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0309Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0300Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0291Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0283Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0277Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0370Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0359Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0349Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0342Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0341Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0336Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0328Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0321Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0316Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0310Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0367Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0361Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0359Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0355Epoch 7/10: [================              ] 40/75 batches, loss: 0.0352Epoch 7/10: [================              ] 41/75 batches, loss: 0.0367Epoch 7/10: [================              ] 42/75 batches, loss: 0.0388Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0387Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0381Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0378Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0375Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0371Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0368Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0362Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0359Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0354Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0352Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0352Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0378Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0398Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0393Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0390Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0385Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0380Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0377Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0372Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0367Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0367Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0363Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0359Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0355Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0352Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0349Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0346Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0345Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0378Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0378Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0376Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0372Epoch 7/10: [==============================] 75/75 batches, loss: 0.0369
[2025-04-29 21:22:34,431][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0369
[2025-04-29 21:22:34,851][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2106, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
[2025-04-29 21:22:34,851][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0113Epoch 8/10: [                              ] 2/75 batches, loss: 0.0097Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0084Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0116Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0134Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0133Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0138Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0132Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0328Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0706Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0655Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0613Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0619Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0598Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0567Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0538Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0520Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0499Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0483Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0466Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0508Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0490Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0530Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0512Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0500Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0484Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0469Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0457Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0455Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0443Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0431Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0475Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0464Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0453Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0443Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0475Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0550Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0538Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0542Epoch 8/10: [================              ] 40/75 batches, loss: 0.0536Epoch 8/10: [================              ] 41/75 batches, loss: 0.0528Epoch 8/10: [================              ] 42/75 batches, loss: 0.0523Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0515Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0507Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0508Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0504Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0504Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0501Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0497Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0491Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0486Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0482Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0475Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0468Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0462Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0457Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0453Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0448Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0442Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0437Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0431Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0428Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0423Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0418Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0414Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0410Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0406Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0403Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0398Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0396Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0392Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0390Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0386Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0382Epoch 8/10: [==============================] 75/75 batches, loss: 0.0379
[2025-04-29 21:22:50,061][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0379
[2025-04-29 21:22:50,550][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.2047, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
[2025-04-29 21:22:50,551][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:22:50,551][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:22:50,551][src.training.lm_trainer][INFO] - Training completed in 127.75 seconds
[2025-04-29 21:22:50,551][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:22:56,126][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9891213389121339, 'f1': 0.9891395154553049}
[2025-04-29 21:22:56,126][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-29 21:22:56,126][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8363636363636363, 'f1': 0.8571428571428571}
[2025-04-29 21:22:58,380][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/fi/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁███
wandb:          best_val_f1 ▁███
wandb:        best_val_loss █▄▂▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▃▂▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁████▆▃▆
wandb:               val_f1 ▁████▅▂▅
wandb:             val_loss █▄▂▂▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.95082
wandb:        best_val_loss 0.1566
wandb:                epoch 8
wandb:  final_test_accuracy 0.83636
wandb:        final_test_f1 0.85714
wandb: final_train_accuracy 0.98912
wandb:       final_train_f1 0.98914
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.95082
wandb:        learning_rate 2e-05
wandb:           train_loss 0.0379
wandb:           train_time 127.74842
wandb:         val_accuracy 0.93651
wandb:               val_f1 0.93333
wandb:             val_loss 0.2047
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212031-xo2fs0hg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212031-xo2fs0hg/logs
Experiment finetune_question_type_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/results.json
Running experiment: finetune_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:23:14,484][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi
experiment_name: finetune_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:23:14,484][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:23:14,484][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:23:14,484][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:23:14,489][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 21:23:14,489][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:23:16,106][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:23:18,936][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:23:18,936][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:23:18,982][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,097][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 21:23:19,108][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:23:19,109][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 21:23:19,110][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:23:19,137][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,164][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,176][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 21:23:19,178][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:23:19,178][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 21:23:19,179][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:23:19,201][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,227][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:23:19,239][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 21:23:19,240][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:23:19,241][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 21:23:19,242][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 21:23:19,242][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:23:19,242][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:23:19,242][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:23:19,242][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:23:19,243][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:23:19,243][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 21:23:19,243][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 21:23:19,243][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 21:23:19,243][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:23:19,243][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:23:19,243][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:23:19,244][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:23:19,244][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:23:19,244][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:23:19,245][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:23:19,245][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:23:19,245][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 21:23:19,245][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 21:23:19,245][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 21:23:19,245][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 21:23:19,245][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:23:19,246][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:23:19,246][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:23:23,492][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,493][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,494][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,495][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,496][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,497][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,498][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,499][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,500][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,501][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,502][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,503][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,504][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,505][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,506][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,507][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,508][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,509][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,509][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:23:23,509][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:23:23,510][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:23:23,511][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:23:23,511][__main__][INFO] - Successfully created model for fi
[2025-04-29 21:23:23,511][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1047Epoch 1/10: [                              ] 2/75 batches, loss: 0.1062Epoch 1/10: [=                             ] 3/75 batches, loss: 0.0899Epoch 1/10: [=                             ] 4/75 batches, loss: 0.0829Epoch 1/10: [==                            ] 5/75 batches, loss: 0.0819Epoch 1/10: [==                            ] 6/75 batches, loss: 0.0815Epoch 1/10: [==                            ] 7/75 batches, loss: 0.0813Epoch 1/10: [===                           ] 8/75 batches, loss: 0.0821Epoch 1/10: [===                           ] 9/75 batches, loss: 0.0808Epoch 1/10: [====                          ] 10/75 batches, loss: 0.0811Epoch 1/10: [====                          ] 11/75 batches, loss: 0.0810Epoch 1/10: [====                          ] 12/75 batches, loss: 0.0787Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.0758Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.0759Epoch 1/10: [======                        ] 15/75 batches, loss: 0.0758Epoch 1/10: [======                        ] 16/75 batches, loss: 0.0739Epoch 1/10: [======                        ] 17/75 batches, loss: 0.0730Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.0727Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.0721Epoch 1/10: [========                      ] 20/75 batches, loss: 0.0719Epoch 1/10: [========                      ] 21/75 batches, loss: 0.0721Epoch 1/10: [========                      ] 22/75 batches, loss: 0.0715Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.0709Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.0695Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.0695Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.0690Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.0687Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.0685Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.0689Epoch 1/10: [============                  ] 30/75 batches, loss: 0.0687Epoch 1/10: [============                  ] 31/75 batches, loss: 0.0692Epoch 1/10: [============                  ] 32/75 batches, loss: 0.0685Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.0675Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.0672Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0672Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0665Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0667Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0655Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0650Epoch 1/10: [================              ] 40/75 batches, loss: 0.0647Epoch 1/10: [================              ] 41/75 batches, loss: 0.0645Epoch 1/10: [================              ] 42/75 batches, loss: 0.0639Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0639Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0632Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0625Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0620Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0613Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0608Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0607Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0602Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0600Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0605Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0603Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0599Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0594Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0593Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0591Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0584Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0585Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0582Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0579Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0572Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0570Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0565Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0560Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0556Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0550Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0545Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0546Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0541Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0536Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0533Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0527Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0525Epoch 1/10: [==============================] 75/75 batches, loss: 0.0520
[2025-04-29 21:23:42,982][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0520
[2025-04-29 21:23:43,425][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1213, Metrics: {'mse': 0.12147491425275803, 'rmse': 0.3485325153450651, 'r2': -0.8528661727905273}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0471Epoch 2/10: [                              ] 2/75 batches, loss: 0.0451Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0409Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0401Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0369Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0412Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0388Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0367Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0363Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0366Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0383Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0375Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0377Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0362Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0354Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0355Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0358Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0349Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0361Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0369Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0379Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0381Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0379Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0376Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0375Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0375Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0375Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0371Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0371Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0368Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0367Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0359Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0358Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0355Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0351Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0345Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0342Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0338Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0338Epoch 2/10: [================              ] 40/75 batches, loss: 0.0336Epoch 2/10: [================              ] 41/75 batches, loss: 0.0330Epoch 2/10: [================              ] 42/75 batches, loss: 0.0330Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0329Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0329Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0330Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0327Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0326Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0322Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0322Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0323Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0322Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0320Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0319Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0318Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0317Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0315Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0314Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0315Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0314Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0313Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0310Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0310Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0310Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0308Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0304Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0307Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0306Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0307Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0304Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0304Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0302Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0299Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0297Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0295Epoch 2/10: [==============================] 75/75 batches, loss: 0.0294
[2025-04-29 21:23:59,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0294
[2025-04-29 21:23:59,516][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0762, Metrics: {'mse': 0.07613969594240189, 'rmse': 0.27593422394186967, 'r2': -0.16136455535888672}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0312Epoch 3/10: [                              ] 2/75 batches, loss: 0.0404Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0313Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0277Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0250Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0239Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0230Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0255Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0253Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0250Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0253Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0256Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0251Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0246Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0243Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0239Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0239Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0237Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0234Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0238Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0241Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0235Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0232Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0230Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0235Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0244Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0244Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0243Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0242Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0239Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0241Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0239Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0241Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0239Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0239Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0235Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0239Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0239Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0239Epoch 3/10: [================              ] 40/75 batches, loss: 0.0239Epoch 3/10: [================              ] 41/75 batches, loss: 0.0236Epoch 3/10: [================              ] 42/75 batches, loss: 0.0239Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0239Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0239Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0236Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0235Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0232Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0233Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0233Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0234Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0232Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0229Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0230Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0232Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0238Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0238Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0243Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0241Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0244Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0241Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0240Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0239Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0241Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0240Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0238Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0237Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0235Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0235Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0236Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0234Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0233Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0234Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0234Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0233Epoch 3/10: [==============================] 75/75 batches, loss: 0.0231
[2025-04-29 21:24:15,319][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0231
[2025-04-29 21:24:15,708][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0353, Metrics: {'mse': 0.03519731014966965, 'rmse': 0.18760946178076854, 'r2': 0.46313273906707764}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0356Epoch 4/10: [                              ] 2/75 batches, loss: 0.0272Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0246Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0251Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0236Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0242Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0250Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0239Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0231Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0221Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0207Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0218Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0216Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0212Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0214Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0211Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0208Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0223Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0218Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0216Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0217Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0228Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0226Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0232Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0230Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0228Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0230Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0233Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0231Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0228Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0223Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0219Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0220Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0220Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0222Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0223Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0221Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0222Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0219Epoch 4/10: [================              ] 40/75 batches, loss: 0.0217Epoch 4/10: [================              ] 41/75 batches, loss: 0.0215Epoch 4/10: [================              ] 42/75 batches, loss: 0.0216Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0215Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0214Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0213Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0211Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0212Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0214Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0212Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0213Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0215Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0214Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0214Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0212Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0212Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0213Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0211Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0211Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0211Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0210Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0211Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0209Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0210Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0208Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0208Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0208Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0210Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0209Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0210Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0209Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0207Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0208Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0207Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0207Epoch 4/10: [==============================] 75/75 batches, loss: 0.0206
[2025-04-29 21:24:31,462][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0206
[2025-04-29 21:24:31,987][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0343, Metrics: {'mse': 0.034385617822408676, 'rmse': 0.18543359410422017, 'r2': 0.4755135774612427}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0221Epoch 5/10: [                              ] 2/75 batches, loss: 0.0190Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0146Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0142Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0138Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0129Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0126Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0143Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0136Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0140Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0139Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0141Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0150Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0153Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0156Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0152Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0157Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0161Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0161Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0159Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0158Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0157Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0155Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0161Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0158Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0160Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0159Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0162Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0161Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0161Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0159Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0157Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0157Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0162Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0164Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0166Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0163Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0164Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0164Epoch 5/10: [================              ] 40/75 batches, loss: 0.0165Epoch 5/10: [================              ] 41/75 batches, loss: 0.0167Epoch 5/10: [================              ] 42/75 batches, loss: 0.0166Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0174Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0173Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0173Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0171Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0169Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0180Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0180Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0178Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0178Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0178Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0177Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0179Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0181Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0181Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0181Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0181Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0180Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0179Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0178Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0180Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0180Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0180Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0178Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0178Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0177Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0176Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0175Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0176Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0176Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0175Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0173Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0173Epoch 5/10: [==============================] 75/75 batches, loss: 0.0173
[2025-04-29 21:24:47,861][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0173
[2025-04-29 21:24:48,305][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0726, Metrics: {'mse': 0.0729241669178009, 'rmse': 0.2700447498430601, 'r2': -0.11231791973114014}
[2025-04-29 21:24:48,305][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0275Epoch 6/10: [                              ] 2/75 batches, loss: 0.0265Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0257Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0244Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0243Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0268Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0243Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0238Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0228Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0222Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0210Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0200Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0195Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0190Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0185Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0183Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0179Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0172Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0174Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0171Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0167Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0167Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0164Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0165Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0165Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0166Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0163Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0160Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0161Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0162Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0160Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0158Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0158Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0157Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0159Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0158Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0158Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0157Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0158Epoch 6/10: [================              ] 40/75 batches, loss: 0.0158Epoch 6/10: [================              ] 41/75 batches, loss: 0.0159Epoch 6/10: [================              ] 42/75 batches, loss: 0.0160Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0158Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0159Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0160Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0159Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0158Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0158Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0157Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0159Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0158Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0156Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0156Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0155Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0154Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0156Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0155Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0154Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0153Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0152Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0151Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0151Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0152Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0151Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0151Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0150Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0150Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0149Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0149Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0148Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0147Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0149Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0149Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0149Epoch 6/10: [==============================] 75/75 batches, loss: 0.0148
[2025-04-29 21:25:03,547][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0148
[2025-04-29 21:25:03,942][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0582, Metrics: {'mse': 0.05841447040438652, 'rmse': 0.24169085709721524, 'r2': 0.10899960994720459}
[2025-04-29 21:25:03,943][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0139Epoch 7/10: [                              ] 2/75 batches, loss: 0.0142Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0119Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0106Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0107Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0163Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0177Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0178Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0180Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0169Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0164Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0158Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0153Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0151Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0149Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0144Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0145Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0148Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0147Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0146Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0143Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0144Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0142Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0138Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0137Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0138Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0138Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0138Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0136Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0135Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0134Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0137Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0137Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0135Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0133Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0136Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0135Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0135Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0135Epoch 7/10: [================              ] 40/75 batches, loss: 0.0134Epoch 7/10: [================              ] 41/75 batches, loss: 0.0133Epoch 7/10: [================              ] 42/75 batches, loss: 0.0132Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0131Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0133Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0135Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0135Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0134Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0133Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0132Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0131Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0134Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0134Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0132Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0131Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0131Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0131Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0130Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0130Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0129Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0129Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0128Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0127Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0127Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0129Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0128Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0129Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0129Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0129Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0128Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0127Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0128Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0129Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0129Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0128Epoch 7/10: [==============================] 75/75 batches, loss: 0.0128
[2025-04-29 21:25:19,159][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-29 21:25:19,620][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0253, Metrics: {'mse': 0.025147302076220512, 'rmse': 0.1585790089394574, 'r2': 0.6164262890815735}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0120Epoch 8/10: [                              ] 2/75 batches, loss: 0.0094Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0088Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0083Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0078Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0071Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0074Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0077Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0080Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0081Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0084Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0085Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0100Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0105Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0105Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0104Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0105Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0113Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0112Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0111Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0112Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0115Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0114Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0115Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0114Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0116Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0116Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0115Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0114Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0112Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0112Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0111Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0112Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0112Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0112Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0115Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0114Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0113Epoch 8/10: [================              ] 40/75 batches, loss: 0.0117Epoch 8/10: [================              ] 41/75 batches, loss: 0.0118Epoch 8/10: [================              ] 42/75 batches, loss: 0.0118Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0118Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0121Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0120Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0123Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0122Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0123Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0122Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0122Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0121Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0122Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0121Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0123Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0123Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0122Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0124Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0123Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0122Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0124Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0124Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0125Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0126Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0125Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0124Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0124Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0124Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0124Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0124Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0126Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0126Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0126Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0125Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0127Epoch 8/10: [==============================] 75/75 batches, loss: 0.0126
[2025-04-29 21:25:35,385][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0126
[2025-04-29 21:25:35,816][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0407, Metrics: {'mse': 0.04074390232563019, 'rmse': 0.20185118856630543, 'r2': 0.3785301446914673}
[2025-04-29 21:25:35,817][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0139Epoch 9/10: [                              ] 2/75 batches, loss: 0.0164Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0160Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0145Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0131Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0122Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0119Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0117Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0124Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0131Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0135Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0141Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0143Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0138Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0139Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0138Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0138Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0135Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0130Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0128Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0127Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0130Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0130Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0128Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0130Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0133Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0134Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0134Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0132Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0133Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0131Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0131Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0134Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0134Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0131Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0130Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0128Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0126Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0125Epoch 9/10: [================              ] 40/75 batches, loss: 0.0125Epoch 9/10: [================              ] 41/75 batches, loss: 0.0125Epoch 9/10: [================              ] 42/75 batches, loss: 0.0126Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0124Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0124Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0124Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0123Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0123Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0122Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0122Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0121Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0120Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0120Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0120Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0120Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0122Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0121Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0120Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0121Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0121Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0120Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0119Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0118Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0118Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0120Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0119Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0119Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0119Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0118Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0117Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0117Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0117Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0116Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0116Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0116Epoch 9/10: [==============================] 75/75 batches, loss: 0.0115
[2025-04-29 21:25:51,037][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0115
[2025-04-29 21:25:51,426][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0299, Metrics: {'mse': 0.02980719693005085, 'rmse': 0.17264760910609464, 'r2': 0.5453485250473022}
[2025-04-29 21:25:51,427][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0127Epoch 10/10: [                              ] 2/75 batches, loss: 0.0098Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0082Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0106Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0097Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0116Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0109Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0111Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0108Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0103Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0100Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0095Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0094Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0095Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0096Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0098Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0101Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0102Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0100Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0102Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0100Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0101Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0102Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0100Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0099Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0100Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0104Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0103Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0101Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0101Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0100Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0105Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0105Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0106Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0106Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0104Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0103Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0102Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0102Epoch 10/10: [================              ] 40/75 batches, loss: 0.0102Epoch 10/10: [================              ] 41/75 batches, loss: 0.0104Epoch 10/10: [================              ] 42/75 batches, loss: 0.0103Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0102Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0101Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0101Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0102Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0100Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0100Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0100Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0099Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0099Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0099Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0100Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0099Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0099Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0098Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0098Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0097Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0097Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0096Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0097Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0097Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0096Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0096Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0095Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0096Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0096Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0096Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0096Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0096Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0096Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0096Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0096Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0096Epoch 10/10: [==============================] 75/75 batches, loss: 0.0096
[2025-04-29 21:26:06,644][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0096
[2025-04-29 21:26:07,041][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0206, Metrics: {'mse': 0.02050848677754402, 'rmse': 0.14320784467878853, 'r2': 0.6871824860572815}
[2025-04-29 21:26:07,646][src.training.lm_trainer][INFO] - Training completed in 160.59 seconds
[2025-04-29 21:26:07,646][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:26:13,175][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.005812661722302437, 'rmse': 0.0762408140191488, 'r2': 0.7123590707778931}
[2025-04-29 21:26:13,175][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.02050848677754402, 'rmse': 0.14320784467878853, 'r2': 0.6871824860572815}
[2025-04-29 21:26:13,175][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.021732095628976822, 'rmse': 0.14741809803744188, 'r2': 0.44963932037353516}
[2025-04-29 21:26:15,415][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/fi/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▂▁▁
wandb:     best_val_mse █▅▂▂▁▁
wandb:      best_val_r2 ▁▄▇▇██
wandb:    best_val_rmse █▆▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▃▂▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▂▅▄▁▂▂▁
wandb:          val_mse █▅▂▂▅▄▁▂▂▁
wandb:           val_r2 ▁▄▇▇▄▅█▇▇█
wandb:         val_rmse █▆▃▂▅▄▂▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02061
wandb:     best_val_mse 0.02051
wandb:      best_val_r2 0.68718
wandb:    best_val_rmse 0.14321
wandb:            epoch 10
wandb:   final_test_mse 0.02173
wandb:    final_test_r2 0.44964
wandb:  final_test_rmse 0.14742
wandb:  final_train_mse 0.00581
wandb:   final_train_r2 0.71236
wandb: final_train_rmse 0.07624
wandb:    final_val_mse 0.02051
wandb:     final_val_r2 0.68718
wandb:   final_val_rmse 0.14321
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00956
wandb:       train_time 160.59244
wandb:         val_loss 0.02061
wandb:          val_mse 0.02051
wandb:           val_r2 0.68718
wandb:         val_rmse 0.14321
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212314-y8sq0zc8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212314-y8sq0zc8/logs
Experiment finetune_complexity_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/results.json
Running experiment: finetune_question_type_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:26:48,102][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/id
experiment_name: finetune_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:26:48,102][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:26:48,103][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:26:48,103][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:26:48,107][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 21:26:48,107][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:26:49,526][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:26:52,398][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:26:52,399][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:52,455][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,480][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,567][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 21:26:52,577][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:52,578][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 21:26:52,579][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:52,599][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,628][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,640][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 21:26:52,641][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:52,642][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 21:26:52,643][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:52,664][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,692][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:52,704][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 21:26:52,706][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:52,706][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 21:26:52,707][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 21:26:52,707][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:52,708][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:52,708][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:52,708][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:52,708][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 21:26:52,708][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 21:26:52,708][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 21:26:52,708][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:52,709][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:26:52,709][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 21:26:52,709][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:52,710][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:26:52,710][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 21:26:52,710][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:26:52,711][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:26:52,711][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:26:56,960][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,961][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,963][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,964][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,965][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,966][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,967][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,968][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,969][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,970][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,971][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,972][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,973][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,974][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,975][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,976][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,977][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,977][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:26:56,978][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:26:56,978][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:26:56,979][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:26:56,979][__main__][INFO] - Successfully created model for id
[2025-04-29 21:26:56,979][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.7010Epoch 1/10: [=                             ] 2/60 batches, loss: 0.7070Epoch 1/10: [=                             ] 3/60 batches, loss: 0.7013Epoch 1/10: [==                            ] 4/60 batches, loss: 0.7019Epoch 1/10: [==                            ] 5/60 batches, loss: 0.7000Epoch 1/10: [===                           ] 6/60 batches, loss: 0.6953Epoch 1/10: [===                           ] 7/60 batches, loss: 0.6946Epoch 1/10: [====                          ] 8/60 batches, loss: 0.6949Epoch 1/10: [====                          ] 9/60 batches, loss: 0.6935Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.6942Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.6922Epoch 1/10: [======                        ] 12/60 batches, loss: 0.6935Epoch 1/10: [======                        ] 13/60 batches, loss: 0.6975Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.6977Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.6968Epoch 1/10: [========                      ] 16/60 batches, loss: 0.6968Epoch 1/10: [========                      ] 17/60 batches, loss: 0.6962Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.6952Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.6954Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.6942Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.6942Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.6921Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.6923Epoch 1/10: [============                  ] 24/60 batches, loss: 0.6921Epoch 1/10: [============                  ] 25/60 batches, loss: 0.6923Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.6914Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.6909Epoch 1/10: [==============                ] 28/60 batches, loss: 0.6910Epoch 1/10: [==============                ] 29/60 batches, loss: 0.6914Epoch 1/10: [===============               ] 30/60 batches, loss: 0.6924Epoch 1/10: [===============               ] 31/60 batches, loss: 0.6926Epoch 1/10: [================              ] 32/60 batches, loss: 0.6933Epoch 1/10: [================              ] 33/60 batches, loss: 0.6924Epoch 1/10: [=================             ] 34/60 batches, loss: 0.6916Epoch 1/10: [=================             ] 35/60 batches, loss: 0.6923Epoch 1/10: [==================            ] 36/60 batches, loss: 0.6927Epoch 1/10: [==================            ] 37/60 batches, loss: 0.6932Epoch 1/10: [===================           ] 38/60 batches, loss: 0.6938Epoch 1/10: [===================           ] 39/60 batches, loss: 0.6937Epoch 1/10: [====================          ] 40/60 batches, loss: 0.6931Epoch 1/10: [====================          ] 41/60 batches, loss: 0.6928Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.6921Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.6919Epoch 1/10: [======================        ] 44/60 batches, loss: 0.6916Epoch 1/10: [======================        ] 45/60 batches, loss: 0.6915Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.6912Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.6907Epoch 1/10: [========================      ] 48/60 batches, loss: 0.6907Epoch 1/10: [========================      ] 49/60 batches, loss: 0.6903Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.6902Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.6899Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.6902Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.6898Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.6885Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.6880Epoch 1/10: [============================  ] 56/60 batches, loss: 0.6882Epoch 1/10: [============================  ] 57/60 batches, loss: 0.6879Epoch 1/10: [============================= ] 58/60 batches, loss: 0.6876Epoch 1/10: [============================= ] 59/60 batches, loss: 0.6870Epoch 1/10: [==============================] 60/60 batches, loss: 0.6853
[2025-04-29 21:27:11,614][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6853
[2025-04-29 21:27:12,037][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6853, Metrics: {'accuracy': 0.5, 'f1': 0.6666666666666666}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.6700Epoch 2/10: [=                             ] 2/60 batches, loss: 0.6667Epoch 2/10: [=                             ] 3/60 batches, loss: 0.6718Epoch 2/10: [==                            ] 4/60 batches, loss: 0.6728Epoch 2/10: [==                            ] 5/60 batches, loss: 0.6635Epoch 2/10: [===                           ] 6/60 batches, loss: 0.6602Epoch 2/10: [===                           ] 7/60 batches, loss: 0.6513Epoch 2/10: [====                          ] 8/60 batches, loss: 0.6490Epoch 2/10: [====                          ] 9/60 batches, loss: 0.6507Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.6520Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.6443Epoch 2/10: [======                        ] 12/60 batches, loss: 0.6376Epoch 2/10: [======                        ] 13/60 batches, loss: 0.6347Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.6297Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.6292Epoch 2/10: [========                      ] 16/60 batches, loss: 0.6277Epoch 2/10: [========                      ] 17/60 batches, loss: 0.6272Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.6224Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.6199Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.6163Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.6162Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.6144Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.6161Epoch 2/10: [============                  ] 24/60 batches, loss: 0.6170Epoch 2/10: [============                  ] 25/60 batches, loss: 0.6164Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.6162Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.6171Epoch 2/10: [==============                ] 28/60 batches, loss: 0.6145Epoch 2/10: [==============                ] 29/60 batches, loss: 0.6133Epoch 2/10: [===============               ] 30/60 batches, loss: 0.6135Epoch 2/10: [===============               ] 31/60 batches, loss: 0.6133Epoch 2/10: [================              ] 32/60 batches, loss: 0.6139Epoch 2/10: [================              ] 33/60 batches, loss: 0.6119Epoch 2/10: [=================             ] 34/60 batches, loss: 0.6092Epoch 2/10: [=================             ] 35/60 batches, loss: 0.6052Epoch 2/10: [==================            ] 36/60 batches, loss: 0.6032Epoch 2/10: [==================            ] 37/60 batches, loss: 0.6020Epoch 2/10: [===================           ] 38/60 batches, loss: 0.5990Epoch 2/10: [===================           ] 39/60 batches, loss: 0.5995Epoch 2/10: [====================          ] 40/60 batches, loss: 0.5990Epoch 2/10: [====================          ] 41/60 batches, loss: 0.5976Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.5970Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.5945Epoch 2/10: [======================        ] 44/60 batches, loss: 0.5941Epoch 2/10: [======================        ] 45/60 batches, loss: 0.5930Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.5959Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.5958Epoch 2/10: [========================      ] 48/60 batches, loss: 0.5963Epoch 2/10: [========================      ] 49/60 batches, loss: 0.5969Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.5975Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.5971Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.5959Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.5943Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.5935Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.5932Epoch 2/10: [============================  ] 56/60 batches, loss: 0.5919Epoch 2/10: [============================  ] 57/60 batches, loss: 0.5926Epoch 2/10: [============================= ] 58/60 batches, loss: 0.5921Epoch 2/10: [============================= ] 59/60 batches, loss: 0.5926Epoch 2/10: [==============================] 60/60 batches, loss: 0.5927
[2025-04-29 21:27:24,729][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5927
[2025-04-29 21:27:25,527][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6797, Metrics: {'accuracy': 0.5972222222222222, 'f1': 0.7010309278350515}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.7780Epoch 3/10: [=                             ] 2/60 batches, loss: 0.7097Epoch 3/10: [=                             ] 3/60 batches, loss: 0.6852Epoch 3/10: [==                            ] 4/60 batches, loss: 0.6717Epoch 3/10: [==                            ] 5/60 batches, loss: 0.6452Epoch 3/10: [===                           ] 6/60 batches, loss: 0.6464Epoch 3/10: [===                           ] 7/60 batches, loss: 0.6451Epoch 3/10: [====                          ] 8/60 batches, loss: 0.6368Epoch 3/10: [====                          ] 9/60 batches, loss: 0.6408Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.6443Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.6343Epoch 3/10: [======                        ] 12/60 batches, loss: 0.6325Epoch 3/10: [======                        ] 13/60 batches, loss: 0.6296Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.6309Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.6318Epoch 3/10: [========                      ] 16/60 batches, loss: 0.6319Epoch 3/10: [========                      ] 17/60 batches, loss: 0.6311Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.6341Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.6342Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.6347Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.6360Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.6329Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.6293Epoch 3/10: [============                  ] 24/60 batches, loss: 0.6261Epoch 3/10: [============                  ] 25/60 batches, loss: 0.6221Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.6201Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.6175Epoch 3/10: [==============                ] 28/60 batches, loss: 0.6154Epoch 3/10: [==============                ] 29/60 batches, loss: 0.6147Epoch 3/10: [===============               ] 30/60 batches, loss: 0.6107Epoch 3/10: [===============               ] 31/60 batches, loss: 0.6090Epoch 3/10: [================              ] 32/60 batches, loss: 0.6068Epoch 3/10: [================              ] 33/60 batches, loss: 0.6018Epoch 3/10: [=================             ] 34/60 batches, loss: 0.5950Epoch 3/10: [=================             ] 35/60 batches, loss: 0.5906Epoch 3/10: [==================            ] 36/60 batches, loss: 0.5849Epoch 3/10: [==================            ] 37/60 batches, loss: 0.5801Epoch 3/10: [===================           ] 38/60 batches, loss: 0.5747Epoch 3/10: [===================           ] 39/60 batches, loss: 0.5702Epoch 3/10: [====================          ] 40/60 batches, loss: 0.5679Epoch 3/10: [====================          ] 41/60 batches, loss: 0.5676Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.5641Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.5625Epoch 3/10: [======================        ] 44/60 batches, loss: 0.5608Epoch 3/10: [======================        ] 45/60 batches, loss: 0.5568Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.5548Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.5520Epoch 3/10: [========================      ] 48/60 batches, loss: 0.5493Epoch 3/10: [========================      ] 49/60 batches, loss: 0.5468Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.5450Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.5447Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.5412Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.5414Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.5404Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.5387Epoch 3/10: [============================  ] 56/60 batches, loss: 0.5358Epoch 3/10: [============================  ] 57/60 batches, loss: 0.5350Epoch 3/10: [============================= ] 58/60 batches, loss: 0.5320Epoch 3/10: [============================= ] 59/60 batches, loss: 0.5295Epoch 3/10: [==============================] 60/60 batches, loss: 0.5273
[2025-04-29 21:27:38,253][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5273
[2025-04-29 21:27:38,677][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4221, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.78125}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.3922Epoch 4/10: [=                             ] 2/60 batches, loss: 0.3903Epoch 4/10: [=                             ] 3/60 batches, loss: 0.4202Epoch 4/10: [==                            ] 4/60 batches, loss: 0.4095Epoch 4/10: [==                            ] 5/60 batches, loss: 0.3906Epoch 4/10: [===                           ] 6/60 batches, loss: 0.3717Epoch 4/10: [===                           ] 7/60 batches, loss: 0.3706Epoch 4/10: [====                          ] 8/60 batches, loss: 0.3592Epoch 4/10: [====                          ] 9/60 batches, loss: 0.3548Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.3507Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.3538Epoch 4/10: [======                        ] 12/60 batches, loss: 0.3575Epoch 4/10: [======                        ] 13/60 batches, loss: 0.3474Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.3437Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.3432Epoch 4/10: [========                      ] 16/60 batches, loss: 0.3337Epoch 4/10: [========                      ] 17/60 batches, loss: 0.3328Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.3317Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.3240Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.3206Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.3209Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.3184Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.3156Epoch 4/10: [============                  ] 24/60 batches, loss: 0.3151Epoch 4/10: [============                  ] 25/60 batches, loss: 0.3168Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.3192Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.3286Epoch 4/10: [==============                ] 28/60 batches, loss: 0.3311Epoch 4/10: [==============                ] 29/60 batches, loss: 0.3329Epoch 4/10: [===============               ] 30/60 batches, loss: 0.3426Epoch 4/10: [===============               ] 31/60 batches, loss: 0.3429Epoch 4/10: [================              ] 32/60 batches, loss: 0.3460Epoch 4/10: [================              ] 33/60 batches, loss: 0.3470Epoch 4/10: [=================             ] 34/60 batches, loss: 0.3463Epoch 4/10: [=================             ] 35/60 batches, loss: 0.3421Epoch 4/10: [==================            ] 36/60 batches, loss: 0.3404Epoch 4/10: [==================            ] 37/60 batches, loss: 0.3368Epoch 4/10: [===================           ] 38/60 batches, loss: 0.3344Epoch 4/10: [===================           ] 39/60 batches, loss: 0.3361Epoch 4/10: [====================          ] 40/60 batches, loss: 0.3352Epoch 4/10: [====================          ] 41/60 batches, loss: 0.3332Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.3356Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.3391Epoch 4/10: [======================        ] 44/60 batches, loss: 0.3382Epoch 4/10: [======================        ] 45/60 batches, loss: 0.3400Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.3419Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.3430Epoch 4/10: [========================      ] 48/60 batches, loss: 0.3437Epoch 4/10: [========================      ] 49/60 batches, loss: 0.3433Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.3446Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.3439Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.3427Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.3422Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.3411Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.3397Epoch 4/10: [============================  ] 56/60 batches, loss: 0.3396Epoch 4/10: [============================  ] 57/60 batches, loss: 0.3393Epoch 4/10: [============================= ] 58/60 batches, loss: 0.3371Epoch 4/10: [============================= ] 59/60 batches, loss: 0.3344Epoch 4/10: [==============================] 60/60 batches, loss: 0.3306
[2025-04-29 21:27:51,386][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3306
[2025-04-29 21:27:51,817][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3427, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.1464Epoch 5/10: [=                             ] 2/60 batches, loss: 0.1614Epoch 5/10: [=                             ] 3/60 batches, loss: 0.1421Epoch 5/10: [==                            ] 4/60 batches, loss: 0.1545Epoch 5/10: [==                            ] 5/60 batches, loss: 0.1506Epoch 5/10: [===                           ] 6/60 batches, loss: 0.1526Epoch 5/10: [===                           ] 7/60 batches, loss: 0.1414Epoch 5/10: [====                          ] 8/60 batches, loss: 0.1421Epoch 5/10: [====                          ] 9/60 batches, loss: 0.1473Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.1498Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.1509Epoch 5/10: [======                        ] 12/60 batches, loss: 0.1500Epoch 5/10: [======                        ] 13/60 batches, loss: 0.1515Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.1452Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.1415Epoch 5/10: [========                      ] 16/60 batches, loss: 0.1385Epoch 5/10: [========                      ] 17/60 batches, loss: 0.1426Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.1374Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.1403Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.1600Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.1704Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.1821Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.1832Epoch 5/10: [============                  ] 24/60 batches, loss: 0.1835Epoch 5/10: [============                  ] 25/60 batches, loss: 0.1849Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.1888Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.1907Epoch 5/10: [==============                ] 28/60 batches, loss: 0.1898Epoch 5/10: [==============                ] 29/60 batches, loss: 0.1940Epoch 5/10: [===============               ] 30/60 batches, loss: 0.1957Epoch 5/10: [===============               ] 31/60 batches, loss: 0.1957Epoch 5/10: [================              ] 32/60 batches, loss: 0.1937Epoch 5/10: [================              ] 33/60 batches, loss: 0.1908Epoch 5/10: [=================             ] 34/60 batches, loss: 0.1880Epoch 5/10: [=================             ] 35/60 batches, loss: 0.1852Epoch 5/10: [==================            ] 36/60 batches, loss: 0.1836Epoch 5/10: [==================            ] 37/60 batches, loss: 0.1810Epoch 5/10: [===================           ] 38/60 batches, loss: 0.1849Epoch 5/10: [===================           ] 39/60 batches, loss: 0.1829Epoch 5/10: [====================          ] 40/60 batches, loss: 0.1803Epoch 5/10: [====================          ] 41/60 batches, loss: 0.1786Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.1777Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.1751Epoch 5/10: [======================        ] 44/60 batches, loss: 0.1724Epoch 5/10: [======================        ] 45/60 batches, loss: 0.1696Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.1687Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.1665Epoch 5/10: [========================      ] 48/60 batches, loss: 0.1647Epoch 5/10: [========================      ] 49/60 batches, loss: 0.1675Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.1715Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.1693Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.1676Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.1653Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.1633Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.1644Epoch 5/10: [============================  ] 56/60 batches, loss: 0.1639Epoch 5/10: [============================  ] 57/60 batches, loss: 0.1632Epoch 5/10: [============================= ] 58/60 batches, loss: 0.1678Epoch 5/10: [============================= ] 59/60 batches, loss: 0.1662Epoch 5/10: [==============================] 60/60 batches, loss: 0.1649
[2025-04-29 21:28:04,578][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1649
[2025-04-29 21:28:05,050][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3830, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8125}
[2025-04-29 21:28:05,051][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.0672Epoch 6/10: [=                             ] 2/60 batches, loss: 0.0620Epoch 6/10: [=                             ] 3/60 batches, loss: 0.0562Epoch 6/10: [==                            ] 4/60 batches, loss: 0.0497Epoch 6/10: [==                            ] 5/60 batches, loss: 0.0494Epoch 6/10: [===                           ] 6/60 batches, loss: 0.0596Epoch 6/10: [===                           ] 7/60 batches, loss: 0.0631Epoch 6/10: [====                          ] 8/60 batches, loss: 0.0707Epoch 6/10: [====                          ] 9/60 batches, loss: 0.0691Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.0665Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.0762Epoch 6/10: [======                        ] 12/60 batches, loss: 0.0765Epoch 6/10: [======                        ] 13/60 batches, loss: 0.0749Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.0943Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.0902Epoch 6/10: [========                      ] 16/60 batches, loss: 0.0875Epoch 6/10: [========                      ] 17/60 batches, loss: 0.0882Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.0858Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.0825Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.0839Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.0899Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.0881Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.0902Epoch 6/10: [============                  ] 24/60 batches, loss: 0.0882Epoch 6/10: [============                  ] 25/60 batches, loss: 0.0892Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.0881Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.0873Epoch 6/10: [==============                ] 28/60 batches, loss: 0.0866Epoch 6/10: [==============                ] 29/60 batches, loss: 0.0854Epoch 6/10: [===============               ] 30/60 batches, loss: 0.0840Epoch 6/10: [===============               ] 31/60 batches, loss: 0.0832Epoch 6/10: [================              ] 32/60 batches, loss: 0.0822Epoch 6/10: [================              ] 33/60 batches, loss: 0.0807Epoch 6/10: [=================             ] 34/60 batches, loss: 0.0798Epoch 6/10: [=================             ] 35/60 batches, loss: 0.0796Epoch 6/10: [==================            ] 36/60 batches, loss: 0.0833Epoch 6/10: [==================            ] 37/60 batches, loss: 0.0826Epoch 6/10: [===================           ] 38/60 batches, loss: 0.0815Epoch 6/10: [===================           ] 39/60 batches, loss: 0.0805Epoch 6/10: [====================          ] 40/60 batches, loss: 0.0815Epoch 6/10: [====================          ] 41/60 batches, loss: 0.0806Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.0852Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.0847Epoch 6/10: [======================        ] 44/60 batches, loss: 0.0839Epoch 6/10: [======================        ] 45/60 batches, loss: 0.0853Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.0856Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.0860Epoch 6/10: [========================      ] 48/60 batches, loss: 0.0856Epoch 6/10: [========================      ] 49/60 batches, loss: 0.0905Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.0893Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.0921Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.0912Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.0909Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.0900Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.0959Epoch 6/10: [============================  ] 56/60 batches, loss: 0.0974Epoch 6/10: [============================  ] 57/60 batches, loss: 0.0963Epoch 6/10: [============================= ] 58/60 batches, loss: 0.0964Epoch 6/10: [============================= ] 59/60 batches, loss: 0.0969Epoch 6/10: [==============================] 60/60 batches, loss: 0.0960
[2025-04-29 21:28:17,203][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0960
[2025-04-29 21:28:17,623][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4021, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7586206896551724}
[2025-04-29 21:28:17,623][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.0563Epoch 7/10: [=                             ] 2/60 batches, loss: 0.0576Epoch 7/10: [=                             ] 3/60 batches, loss: 0.1318Epoch 7/10: [==                            ] 4/60 batches, loss: 0.1153Epoch 7/10: [==                            ] 5/60 batches, loss: 0.1201Epoch 7/10: [===                           ] 6/60 batches, loss: 0.1056Epoch 7/10: [===                           ] 7/60 batches, loss: 0.1017Epoch 7/10: [====                          ] 8/60 batches, loss: 0.0952Epoch 7/10: [====                          ] 9/60 batches, loss: 0.0920Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.0882Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.0939Epoch 7/10: [======                        ] 12/60 batches, loss: 0.0906Epoch 7/10: [======                        ] 13/60 batches, loss: 0.1034Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.1050Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.1006Epoch 7/10: [========                      ] 16/60 batches, loss: 0.0996Epoch 7/10: [========                      ] 17/60 batches, loss: 0.0972Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.0944Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.0981Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.0964Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.0938Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.0922Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.0921Epoch 7/10: [============                  ] 24/60 batches, loss: 0.0933Epoch 7/10: [============                  ] 25/60 batches, loss: 0.0920Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.0943Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.0933Epoch 7/10: [==============                ] 28/60 batches, loss: 0.0905Epoch 7/10: [==============                ] 29/60 batches, loss: 0.0988Epoch 7/10: [===============               ] 30/60 batches, loss: 0.1056Epoch 7/10: [===============               ] 31/60 batches, loss: 0.1031Epoch 7/10: [================              ] 32/60 batches, loss: 0.1025Epoch 7/10: [================              ] 33/60 batches, loss: 0.1003Epoch 7/10: [=================             ] 34/60 batches, loss: 0.1011Epoch 7/10: [=================             ] 35/60 batches, loss: 0.1030Epoch 7/10: [==================            ] 36/60 batches, loss: 0.1070Epoch 7/10: [==================            ] 37/60 batches, loss: 0.1109Epoch 7/10: [===================           ] 38/60 batches, loss: 0.1092Epoch 7/10: [===================           ] 39/60 batches, loss: 0.1085Epoch 7/10: [====================          ] 40/60 batches, loss: 0.1070Epoch 7/10: [====================          ] 41/60 batches, loss: 0.1056Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.1068Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.1051Epoch 7/10: [======================        ] 44/60 batches, loss: 0.1043Epoch 7/10: [======================        ] 45/60 batches, loss: 0.1037Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.1027Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.1021Epoch 7/10: [========================      ] 48/60 batches, loss: 0.1024Epoch 7/10: [========================      ] 49/60 batches, loss: 0.1020Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.1018Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.1004Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.0995Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.0987Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.0989Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.0984Epoch 7/10: [============================  ] 56/60 batches, loss: 0.0976Epoch 7/10: [============================  ] 57/60 batches, loss: 0.1010Epoch 7/10: [============================= ] 58/60 batches, loss: 0.1000Epoch 7/10: [============================= ] 59/60 batches, loss: 0.0990Epoch 7/10: [==============================] 60/60 batches, loss: 0.0980
[2025-04-29 21:28:29,781][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0980
[2025-04-29 21:28:30,202][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3940, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7868852459016393}
[2025-04-29 21:28:30,202][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:28:30,203][src.training.lm_trainer][INFO] - Early stopping at epoch 7
[2025-04-29 21:28:30,203][src.training.lm_trainer][INFO] - Training completed in 91.45 seconds
[2025-04-29 21:28:30,203][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:28:34,914][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9821802935010482, 'f1': 0.9812981298129813}
[2025-04-29 21:28:34,914][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
[2025-04-29 21:28:34,915][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7727272727272727, 'f1': 0.7311827956989247}
[2025-04-29 21:28:37,164][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/id/id/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▃▇█
wandb:          best_val_f1 ▁▃▇█
wandb:        best_val_loss ██▃▁
wandb:                epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▄▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▃▇██▇█
wandb:               val_f1 ▁▃▆▇█▅▇
wandb:             val_loss ██▃▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.83333
wandb:          best_val_f1 0.8
wandb:        best_val_loss 0.34274
wandb:                epoch 7
wandb:  final_test_accuracy 0.77273
wandb:        final_test_f1 0.73118
wandb: final_train_accuracy 0.98218
wandb:       final_train_f1 0.9813
wandb:   final_val_accuracy 0.83333
wandb:         final_val_f1 0.8
wandb:        learning_rate 2e-05
wandb:           train_loss 0.09802
wandb:           train_time 91.45395
wandb:         val_accuracy 0.81944
wandb:               val_f1 0.78689
wandb:             val_loss 0.39403
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212648-aq5up9a3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212648-aq5up9a3/logs
Experiment finetune_question_type_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/id/results.json
Running experiment: finetune_complexity_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:28:55,226][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/id
experiment_name: finetune_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:28:55,226][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:28:55,227][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:28:55,227][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:28:55,231][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 21:28:55,232][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:28:59,790][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:29:02,630][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:29:02,631][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:02,840][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:02,935][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:03,167][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 21:29:03,177][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:03,177][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 21:29:03,181][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:03,261][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:03,348][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:03,376][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 21:29:03,378][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:03,378][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 21:29:03,381][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:03,464][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:03,563][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:03,599][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 21:29:03,601][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:03,601][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 21:29:03,604][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 21:29:03,605][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:03,605][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:03,605][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:03,605][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:03,605][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:03,605][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:03,606][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:03,606][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:03,606][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:03,607][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:03,607][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:03,607][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 21:29:03,608][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 21:29:03,608][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 21:29:03,608][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 21:29:03,608][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:29:03,608][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:29:03,609][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:29:13,597][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:29:13,599][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,599][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,600][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,601][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,602][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,603][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,604][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,605][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,606][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,607][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,608][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,609][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,610][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,611][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,612][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,613][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,614][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,615][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:29:13,616][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:29:13,616][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:29:13,617][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:29:13,618][__main__][INFO] - Successfully created model for id
[2025-04-29 21:29:13,618][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.0866Epoch 1/10: [=                             ] 2/60 batches, loss: 0.1093Epoch 1/10: [=                             ] 3/60 batches, loss: 0.1070Epoch 1/10: [==                            ] 4/60 batches, loss: 0.1167Epoch 1/10: [==                            ] 5/60 batches, loss: 0.1147Epoch 1/10: [===                           ] 6/60 batches, loss: 0.1182Epoch 1/10: [===                           ] 7/60 batches, loss: 0.1156Epoch 1/10: [====                          ] 8/60 batches, loss: 0.1127Epoch 1/10: [====                          ] 9/60 batches, loss: 0.1140Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.1164Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.1174Epoch 1/10: [======                        ] 12/60 batches, loss: 0.1157Epoch 1/10: [======                        ] 13/60 batches, loss: 0.1120Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.1134Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.1101Epoch 1/10: [========                      ] 16/60 batches, loss: 0.1091Epoch 1/10: [========                      ] 17/60 batches, loss: 0.1075Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.1070Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.1062Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.1075Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.1085Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.1069Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.1048Epoch 1/10: [============                  ] 24/60 batches, loss: 0.1068Epoch 1/10: [============                  ] 25/60 batches, loss: 0.1065Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.1056Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.1040Epoch 1/10: [==============                ] 28/60 batches, loss: 0.1038Epoch 1/10: [==============                ] 29/60 batches, loss: 0.1041Epoch 1/10: [===============               ] 30/60 batches, loss: 0.1043Epoch 1/10: [===============               ] 31/60 batches, loss: 0.1030Epoch 1/10: [================              ] 32/60 batches, loss: 0.1028Epoch 1/10: [================              ] 33/60 batches, loss: 0.1032Epoch 1/10: [=================             ] 34/60 batches, loss: 0.1029Epoch 1/10: [=================             ] 35/60 batches, loss: 0.1025Epoch 1/10: [==================            ] 36/60 batches, loss: 0.1013Epoch 1/10: [==================            ] 37/60 batches, loss: 0.0999Epoch 1/10: [===================           ] 38/60 batches, loss: 0.0996Epoch 1/10: [===================           ] 39/60 batches, loss: 0.0997Epoch 1/10: [====================          ] 40/60 batches, loss: 0.0997Epoch 1/10: [====================          ] 41/60 batches, loss: 0.0984Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.0984Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.0982Epoch 1/10: [======================        ] 44/60 batches, loss: 0.0979Epoch 1/10: [======================        ] 45/60 batches, loss: 0.0971Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.0962Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.0955Epoch 1/10: [========================      ] 48/60 batches, loss: 0.0946Epoch 1/10: [========================      ] 49/60 batches, loss: 0.0940Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.0936Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.0931Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.0923Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.0917Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.0910Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.0900Epoch 1/10: [============================  ] 56/60 batches, loss: 0.0892Epoch 1/10: [============================  ] 57/60 batches, loss: 0.0891Epoch 1/10: [============================= ] 58/60 batches, loss: 0.0886Epoch 1/10: [============================= ] 59/60 batches, loss: 0.0875Epoch 1/10: [==============================] 60/60 batches, loss: 0.0869
[2025-04-29 21:29:31,471][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0869
[2025-04-29 21:29:31,862][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1157, Metrics: {'mse': 0.11039131134748459, 'rmse': 0.33225187937389394, 'r2': -1.640362024307251}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.0186Epoch 2/10: [=                             ] 2/60 batches, loss: 0.0423Epoch 2/10: [=                             ] 3/60 batches, loss: 0.0388Epoch 2/10: [==                            ] 4/60 batches, loss: 0.0386Epoch 2/10: [==                            ] 5/60 batches, loss: 0.0436Epoch 2/10: [===                           ] 6/60 batches, loss: 0.0488Epoch 2/10: [===                           ] 7/60 batches, loss: 0.0499Epoch 2/10: [====                          ] 8/60 batches, loss: 0.0491Epoch 2/10: [====                          ] 9/60 batches, loss: 0.0485Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.0486Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.0487Epoch 2/10: [======                        ] 12/60 batches, loss: 0.0491Epoch 2/10: [======                        ] 13/60 batches, loss: 0.0486Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.0482Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.0475Epoch 2/10: [========                      ] 16/60 batches, loss: 0.0477Epoch 2/10: [========                      ] 17/60 batches, loss: 0.0485Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.0485Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.0488Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.0519Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.0523Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.0517Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.0531Epoch 2/10: [============                  ] 24/60 batches, loss: 0.0534Epoch 2/10: [============                  ] 25/60 batches, loss: 0.0553Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.0554Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.0556Epoch 2/10: [==============                ] 28/60 batches, loss: 0.0553Epoch 2/10: [==============                ] 29/60 batches, loss: 0.0548Epoch 2/10: [===============               ] 30/60 batches, loss: 0.0541Epoch 2/10: [===============               ] 31/60 batches, loss: 0.0548Epoch 2/10: [================              ] 32/60 batches, loss: 0.0550Epoch 2/10: [================              ] 33/60 batches, loss: 0.0548Epoch 2/10: [=================             ] 34/60 batches, loss: 0.0555Epoch 2/10: [=================             ] 35/60 batches, loss: 0.0553Epoch 2/10: [==================            ] 36/60 batches, loss: 0.0562Epoch 2/10: [==================            ] 37/60 batches, loss: 0.0553Epoch 2/10: [===================           ] 38/60 batches, loss: 0.0556Epoch 2/10: [===================           ] 39/60 batches, loss: 0.0549Epoch 2/10: [====================          ] 40/60 batches, loss: 0.0556Epoch 2/10: [====================          ] 41/60 batches, loss: 0.0551Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.0549Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.0542Epoch 2/10: [======================        ] 44/60 batches, loss: 0.0546Epoch 2/10: [======================        ] 45/60 batches, loss: 0.0541Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.0541Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.0535Epoch 2/10: [========================      ] 48/60 batches, loss: 0.0533Epoch 2/10: [========================      ] 49/60 batches, loss: 0.0525Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.0524Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.0521Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.0516Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.0514Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.0515Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.0518Epoch 2/10: [============================  ] 56/60 batches, loss: 0.0517Epoch 2/10: [============================  ] 57/60 batches, loss: 0.0514Epoch 2/10: [============================= ] 58/60 batches, loss: 0.0519Epoch 2/10: [============================= ] 59/60 batches, loss: 0.0516Epoch 2/10: [==============================] 60/60 batches, loss: 0.0514
[2025-04-29 21:29:44,571][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0514
[2025-04-29 21:29:45,027][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0686, Metrics: {'mse': 0.06576036661863327, 'rmse': 0.25643784162762184, 'r2': -0.5728698968887329}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.0474Epoch 3/10: [=                             ] 2/60 batches, loss: 0.0362Epoch 3/10: [=                             ] 3/60 batches, loss: 0.0411Epoch 3/10: [==                            ] 4/60 batches, loss: 0.0405Epoch 3/10: [==                            ] 5/60 batches, loss: 0.0364Epoch 3/10: [===                           ] 6/60 batches, loss: 0.0449Epoch 3/10: [===                           ] 7/60 batches, loss: 0.0469Epoch 3/10: [====                          ] 8/60 batches, loss: 0.0476Epoch 3/10: [====                          ] 9/60 batches, loss: 0.0465Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.0473Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.0496Epoch 3/10: [======                        ] 12/60 batches, loss: 0.0542Epoch 3/10: [======                        ] 13/60 batches, loss: 0.0515Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.0536Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.0541Epoch 3/10: [========                      ] 16/60 batches, loss: 0.0533Epoch 3/10: [========                      ] 17/60 batches, loss: 0.0523Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.0513Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.0511Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.0496Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.0491Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.0498Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.0491Epoch 3/10: [============                  ] 24/60 batches, loss: 0.0496Epoch 3/10: [============                  ] 25/60 batches, loss: 0.0497Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.0501Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.0509Epoch 3/10: [==============                ] 28/60 batches, loss: 0.0515Epoch 3/10: [==============                ] 29/60 batches, loss: 0.0511Epoch 3/10: [===============               ] 30/60 batches, loss: 0.0527Epoch 3/10: [===============               ] 31/60 batches, loss: 0.0530Epoch 3/10: [================              ] 32/60 batches, loss: 0.0528Epoch 3/10: [================              ] 33/60 batches, loss: 0.0524Epoch 3/10: [=================             ] 34/60 batches, loss: 0.0521Epoch 3/10: [=================             ] 35/60 batches, loss: 0.0526Epoch 3/10: [==================            ] 36/60 batches, loss: 0.0533Epoch 3/10: [==================            ] 37/60 batches, loss: 0.0534Epoch 3/10: [===================           ] 38/60 batches, loss: 0.0528Epoch 3/10: [===================           ] 39/60 batches, loss: 0.0534Epoch 3/10: [====================          ] 40/60 batches, loss: 0.0529Epoch 3/10: [====================          ] 41/60 batches, loss: 0.0529Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.0523Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.0531Epoch 3/10: [======================        ] 44/60 batches, loss: 0.0527Epoch 3/10: [======================        ] 45/60 batches, loss: 0.0525Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.0526Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.0525Epoch 3/10: [========================      ] 48/60 batches, loss: 0.0524Epoch 3/10: [========================      ] 49/60 batches, loss: 0.0520Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.0515Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.0514Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.0514Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.0514Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.0509Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.0505Epoch 3/10: [============================  ] 56/60 batches, loss: 0.0505Epoch 3/10: [============================  ] 57/60 batches, loss: 0.0501Epoch 3/10: [============================= ] 58/60 batches, loss: 0.0501Epoch 3/10: [============================= ] 59/60 batches, loss: 0.0495Epoch 3/10: [==============================] 60/60 batches, loss: 0.0498
[2025-04-29 21:29:57,799][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0498
[2025-04-29 21:29:58,290][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0432, Metrics: {'mse': 0.04153232276439667, 'rmse': 0.20379480553830773, 'r2': 0.006621420383453369}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.0289Epoch 4/10: [=                             ] 2/60 batches, loss: 0.0448Epoch 4/10: [=                             ] 3/60 batches, loss: 0.0382Epoch 4/10: [==                            ] 4/60 batches, loss: 0.0329Epoch 4/10: [==                            ] 5/60 batches, loss: 0.0347Epoch 4/10: [===                           ] 6/60 batches, loss: 0.0353Epoch 4/10: [===                           ] 7/60 batches, loss: 0.0354Epoch 4/10: [====                          ] 8/60 batches, loss: 0.0343Epoch 4/10: [====                          ] 9/60 batches, loss: 0.0344Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.0337Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.0318Epoch 4/10: [======                        ] 12/60 batches, loss: 0.0311Epoch 4/10: [======                        ] 13/60 batches, loss: 0.0338Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.0354Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.0355Epoch 4/10: [========                      ] 16/60 batches, loss: 0.0357Epoch 4/10: [========                      ] 17/60 batches, loss: 0.0355Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.0348Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.0351Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.0353Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.0349Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.0341Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.0342Epoch 4/10: [============                  ] 24/60 batches, loss: 0.0344Epoch 4/10: [============                  ] 25/60 batches, loss: 0.0356Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.0363Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.0379Epoch 4/10: [==============                ] 28/60 batches, loss: 0.0384Epoch 4/10: [==============                ] 29/60 batches, loss: 0.0395Epoch 4/10: [===============               ] 30/60 batches, loss: 0.0412Epoch 4/10: [===============               ] 31/60 batches, loss: 0.0415Epoch 4/10: [================              ] 32/60 batches, loss: 0.0415Epoch 4/10: [================              ] 33/60 batches, loss: 0.0408Epoch 4/10: [=================             ] 34/60 batches, loss: 0.0412Epoch 4/10: [=================             ] 35/60 batches, loss: 0.0414Epoch 4/10: [==================            ] 36/60 batches, loss: 0.0410Epoch 4/10: [==================            ] 37/60 batches, loss: 0.0405Epoch 4/10: [===================           ] 38/60 batches, loss: 0.0401Epoch 4/10: [===================           ] 39/60 batches, loss: 0.0402Epoch 4/10: [====================          ] 40/60 batches, loss: 0.0403Epoch 4/10: [====================          ] 41/60 batches, loss: 0.0399Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.0397Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.0393Epoch 4/10: [======================        ] 44/60 batches, loss: 0.0394Epoch 4/10: [======================        ] 45/60 batches, loss: 0.0390Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.0390Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.0390Epoch 4/10: [========================      ] 48/60 batches, loss: 0.0387Epoch 4/10: [========================      ] 49/60 batches, loss: 0.0385Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.0384Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.0392Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.0392Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.0388Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.0389Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.0393Epoch 4/10: [============================  ] 56/60 batches, loss: 0.0399Epoch 4/10: [============================  ] 57/60 batches, loss: 0.0397Epoch 4/10: [============================= ] 58/60 batches, loss: 0.0399Epoch 4/10: [============================= ] 59/60 batches, loss: 0.0401Epoch 4/10: [==============================] 60/60 batches, loss: 0.0407
[2025-04-29 21:30:11,008][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0407
[2025-04-29 21:30:11,435][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0811, Metrics: {'mse': 0.08130574226379395, 'rmse': 0.28514161790905573, 'r2': -0.9446873664855957}
[2025-04-29 21:30:11,436][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.0408Epoch 5/10: [=                             ] 2/60 batches, loss: 0.0471Epoch 5/10: [=                             ] 3/60 batches, loss: 0.0448Epoch 5/10: [==                            ] 4/60 batches, loss: 0.0418Epoch 5/10: [==                            ] 5/60 batches, loss: 0.0377Epoch 5/10: [===                           ] 6/60 batches, loss: 0.0361Epoch 5/10: [===                           ] 7/60 batches, loss: 0.0364Epoch 5/10: [====                          ] 8/60 batches, loss: 0.0339Epoch 5/10: [====                          ] 9/60 batches, loss: 0.0367Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.0355Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.0365Epoch 5/10: [======                        ] 12/60 batches, loss: 0.0359Epoch 5/10: [======                        ] 13/60 batches, loss: 0.0368Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.0368Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.0372Epoch 5/10: [========                      ] 16/60 batches, loss: 0.0365Epoch 5/10: [========                      ] 17/60 batches, loss: 0.0359Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.0357Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.0353Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.0362Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.0372Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.0380Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.0384Epoch 5/10: [============                  ] 24/60 batches, loss: 0.0382Epoch 5/10: [============                  ] 25/60 batches, loss: 0.0374Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.0370Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.0380Epoch 5/10: [==============                ] 28/60 batches, loss: 0.0372Epoch 5/10: [==============                ] 29/60 batches, loss: 0.0376Epoch 5/10: [===============               ] 30/60 batches, loss: 0.0375Epoch 5/10: [===============               ] 31/60 batches, loss: 0.0379Epoch 5/10: [================              ] 32/60 batches, loss: 0.0391Epoch 5/10: [================              ] 33/60 batches, loss: 0.0383Epoch 5/10: [=================             ] 34/60 batches, loss: 0.0390Epoch 5/10: [=================             ] 35/60 batches, loss: 0.0391Epoch 5/10: [==================            ] 36/60 batches, loss: 0.0386Epoch 5/10: [==================            ] 37/60 batches, loss: 0.0381Epoch 5/10: [===================           ] 38/60 batches, loss: 0.0376Epoch 5/10: [===================           ] 39/60 batches, loss: 0.0385Epoch 5/10: [====================          ] 40/60 batches, loss: 0.0382Epoch 5/10: [====================          ] 41/60 batches, loss: 0.0379Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.0374Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.0372Epoch 5/10: [======================        ] 44/60 batches, loss: 0.0375Epoch 5/10: [======================        ] 45/60 batches, loss: 0.0374Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.0374Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.0372Epoch 5/10: [========================      ] 48/60 batches, loss: 0.0367Epoch 5/10: [========================      ] 49/60 batches, loss: 0.0363Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.0359Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.0356Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.0352Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.0352Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.0354Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.0354Epoch 5/10: [============================  ] 56/60 batches, loss: 0.0354Epoch 5/10: [============================  ] 57/60 batches, loss: 0.0353Epoch 5/10: [============================= ] 58/60 batches, loss: 0.0351Epoch 5/10: [============================= ] 59/60 batches, loss: 0.0351Epoch 5/10: [==============================] 60/60 batches, loss: 0.0351
[2025-04-29 21:30:23,606][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0351
[2025-04-29 21:30:24,010][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0310, Metrics: {'mse': 0.029994746670126915, 'rmse': 0.1731899150358557, 'r2': 0.28257954120635986}
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.0155Epoch 6/10: [=                             ] 2/60 batches, loss: 0.0217Epoch 6/10: [=                             ] 3/60 batches, loss: 0.0239Epoch 6/10: [==                            ] 4/60 batches, loss: 0.0229Epoch 6/10: [==                            ] 5/60 batches, loss: 0.0217Epoch 6/10: [===                           ] 6/60 batches, loss: 0.0236Epoch 6/10: [===                           ] 7/60 batches, loss: 0.0226Epoch 6/10: [====                          ] 8/60 batches, loss: 0.0230Epoch 6/10: [====                          ] 9/60 batches, loss: 0.0252Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.0252Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.0257Epoch 6/10: [======                        ] 12/60 batches, loss: 0.0258Epoch 6/10: [======                        ] 13/60 batches, loss: 0.0269Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.0299Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.0292Epoch 6/10: [========                      ] 16/60 batches, loss: 0.0293Epoch 6/10: [========                      ] 17/60 batches, loss: 0.0293Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.0306Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.0302Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.0295Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.0299Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.0296Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.0299Epoch 6/10: [============                  ] 24/60 batches, loss: 0.0291Epoch 6/10: [============                  ] 25/60 batches, loss: 0.0291Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.0290Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.0293Epoch 6/10: [==============                ] 28/60 batches, loss: 0.0291Epoch 6/10: [==============                ] 29/60 batches, loss: 0.0295Epoch 6/10: [===============               ] 30/60 batches, loss: 0.0292Epoch 6/10: [===============               ] 31/60 batches, loss: 0.0295Epoch 6/10: [================              ] 32/60 batches, loss: 0.0293Epoch 6/10: [================              ] 33/60 batches, loss: 0.0293Epoch 6/10: [=================             ] 34/60 batches, loss: 0.0291Epoch 6/10: [=================             ] 35/60 batches, loss: 0.0292Epoch 6/10: [==================            ] 36/60 batches, loss: 0.0296Epoch 6/10: [==================            ] 37/60 batches, loss: 0.0296Epoch 6/10: [===================           ] 38/60 batches, loss: 0.0292Epoch 6/10: [===================           ] 39/60 batches, loss: 0.0288Epoch 6/10: [====================          ] 40/60 batches, loss: 0.0284Epoch 6/10: [====================          ] 41/60 batches, loss: 0.0285Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.0285Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.0282Epoch 6/10: [======================        ] 44/60 batches, loss: 0.0281Epoch 6/10: [======================        ] 45/60 batches, loss: 0.0282Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.0286Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.0287Epoch 6/10: [========================      ] 48/60 batches, loss: 0.0284Epoch 6/10: [========================      ] 49/60 batches, loss: 0.0281Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.0279Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.0279Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.0277Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.0277Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.0274Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.0271Epoch 6/10: [============================  ] 56/60 batches, loss: 0.0268Epoch 6/10: [============================  ] 57/60 batches, loss: 0.0268Epoch 6/10: [============================= ] 58/60 batches, loss: 0.0271Epoch 6/10: [============================= ] 59/60 batches, loss: 0.0272Epoch 6/10: [==============================] 60/60 batches, loss: 0.0269
[2025-04-29 21:30:36,802][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0269
[2025-04-29 21:30:37,221][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0315, Metrics: {'mse': 0.02896486409008503, 'rmse': 0.17019066980914385, 'r2': 0.3072124719619751}
[2025-04-29 21:30:37,222][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.0305Epoch 7/10: [=                             ] 2/60 batches, loss: 0.0272Epoch 7/10: [=                             ] 3/60 batches, loss: 0.0250Epoch 7/10: [==                            ] 4/60 batches, loss: 0.0232Epoch 7/10: [==                            ] 5/60 batches, loss: 0.0212Epoch 7/10: [===                           ] 6/60 batches, loss: 0.0206Epoch 7/10: [===                           ] 7/60 batches, loss: 0.0194Epoch 7/10: [====                          ] 8/60 batches, loss: 0.0230Epoch 7/10: [====                          ] 9/60 batches, loss: 0.0226Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.0223Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.0230Epoch 7/10: [======                        ] 12/60 batches, loss: 0.0222Epoch 7/10: [======                        ] 13/60 batches, loss: 0.0214Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.0208Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.0213Epoch 7/10: [========                      ] 16/60 batches, loss: 0.0215Epoch 7/10: [========                      ] 17/60 batches, loss: 0.0210Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.0210Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.0204Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.0201Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.0198Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.0196Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.0192Epoch 7/10: [============                  ] 24/60 batches, loss: 0.0193Epoch 7/10: [============                  ] 25/60 batches, loss: 0.0190Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.0187Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.0183Epoch 7/10: [==============                ] 28/60 batches, loss: 0.0182Epoch 7/10: [==============                ] 29/60 batches, loss: 0.0185Epoch 7/10: [===============               ] 30/60 batches, loss: 0.0185Epoch 7/10: [===============               ] 31/60 batches, loss: 0.0185Epoch 7/10: [================              ] 32/60 batches, loss: 0.0186Epoch 7/10: [================              ] 33/60 batches, loss: 0.0189Epoch 7/10: [=================             ] 34/60 batches, loss: 0.0190Epoch 7/10: [=================             ] 35/60 batches, loss: 0.0187Epoch 7/10: [==================            ] 36/60 batches, loss: 0.0185Epoch 7/10: [==================            ] 37/60 batches, loss: 0.0184Epoch 7/10: [===================           ] 38/60 batches, loss: 0.0182Epoch 7/10: [===================           ] 39/60 batches, loss: 0.0182Epoch 7/10: [====================          ] 40/60 batches, loss: 0.0183Epoch 7/10: [====================          ] 41/60 batches, loss: 0.0186Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.0184Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.0183Epoch 7/10: [======================        ] 44/60 batches, loss: 0.0184Epoch 7/10: [======================        ] 45/60 batches, loss: 0.0190Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.0189Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.0189Epoch 7/10: [========================      ] 48/60 batches, loss: 0.0187Epoch 7/10: [========================      ] 49/60 batches, loss: 0.0186Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.0186Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.0184Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.0184Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.0185Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.0185Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.0184Epoch 7/10: [============================  ] 56/60 batches, loss: 0.0183Epoch 7/10: [============================  ] 57/60 batches, loss: 0.0183Epoch 7/10: [============================= ] 58/60 batches, loss: 0.0182Epoch 7/10: [============================= ] 59/60 batches, loss: 0.0182Epoch 7/10: [==============================] 60/60 batches, loss: 0.0181
[2025-04-29 21:30:49,422][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0181
[2025-04-29 21:30:49,926][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0180, Metrics: {'mse': 0.017849735915660858, 'rmse': 0.1336029038444182, 'r2': 0.5730663537979126}
Epoch 8/10: [Epoch 8/10: [                              ] 1/60 batches, loss: 0.0197Epoch 8/10: [=                             ] 2/60 batches, loss: 0.0172Epoch 8/10: [=                             ] 3/60 batches, loss: 0.0149Epoch 8/10: [==                            ] 4/60 batches, loss: 0.0167Epoch 8/10: [==                            ] 5/60 batches, loss: 0.0164Epoch 8/10: [===                           ] 6/60 batches, loss: 0.0151Epoch 8/10: [===                           ] 7/60 batches, loss: 0.0175Epoch 8/10: [====                          ] 8/60 batches, loss: 0.0168Epoch 8/10: [====                          ] 9/60 batches, loss: 0.0162Epoch 8/10: [=====                         ] 10/60 batches, loss: 0.0155Epoch 8/10: [=====                         ] 11/60 batches, loss: 0.0154Epoch 8/10: [======                        ] 12/60 batches, loss: 0.0151Epoch 8/10: [======                        ] 13/60 batches, loss: 0.0149Epoch 8/10: [=======                       ] 14/60 batches, loss: 0.0149Epoch 8/10: [=======                       ] 15/60 batches, loss: 0.0142Epoch 8/10: [========                      ] 16/60 batches, loss: 0.0139Epoch 8/10: [========                      ] 17/60 batches, loss: 0.0143Epoch 8/10: [=========                     ] 18/60 batches, loss: 0.0140Epoch 8/10: [=========                     ] 19/60 batches, loss: 0.0138Epoch 8/10: [==========                    ] 20/60 batches, loss: 0.0138Epoch 8/10: [==========                    ] 21/60 batches, loss: 0.0136Epoch 8/10: [===========                   ] 22/60 batches, loss: 0.0139Epoch 8/10: [===========                   ] 23/60 batches, loss: 0.0137Epoch 8/10: [============                  ] 24/60 batches, loss: 0.0143Epoch 8/10: [============                  ] 25/60 batches, loss: 0.0142Epoch 8/10: [=============                 ] 26/60 batches, loss: 0.0143Epoch 8/10: [=============                 ] 27/60 batches, loss: 0.0142Epoch 8/10: [==============                ] 28/60 batches, loss: 0.0145Epoch 8/10: [==============                ] 29/60 batches, loss: 0.0146Epoch 8/10: [===============               ] 30/60 batches, loss: 0.0149Epoch 8/10: [===============               ] 31/60 batches, loss: 0.0147Epoch 8/10: [================              ] 32/60 batches, loss: 0.0153Epoch 8/10: [================              ] 33/60 batches, loss: 0.0158Epoch 8/10: [=================             ] 34/60 batches, loss: 0.0159Epoch 8/10: [=================             ] 35/60 batches, loss: 0.0157Epoch 8/10: [==================            ] 36/60 batches, loss: 0.0157Epoch 8/10: [==================            ] 37/60 batches, loss: 0.0158Epoch 8/10: [===================           ] 38/60 batches, loss: 0.0156Epoch 8/10: [===================           ] 39/60 batches, loss: 0.0154Epoch 8/10: [====================          ] 40/60 batches, loss: 0.0158Epoch 8/10: [====================          ] 41/60 batches, loss: 0.0156Epoch 8/10: [=====================         ] 42/60 batches, loss: 0.0159Epoch 8/10: [=====================         ] 43/60 batches, loss: 0.0159Epoch 8/10: [======================        ] 44/60 batches, loss: 0.0160Epoch 8/10: [======================        ] 45/60 batches, loss: 0.0160Epoch 8/10: [=======================       ] 46/60 batches, loss: 0.0159Epoch 8/10: [=======================       ] 47/60 batches, loss: 0.0158Epoch 8/10: [========================      ] 48/60 batches, loss: 0.0158Epoch 8/10: [========================      ] 49/60 batches, loss: 0.0157Epoch 8/10: [=========================     ] 50/60 batches, loss: 0.0158Epoch 8/10: [=========================     ] 51/60 batches, loss: 0.0158Epoch 8/10: [==========================    ] 52/60 batches, loss: 0.0157Epoch 8/10: [==========================    ] 53/60 batches, loss: 0.0158Epoch 8/10: [===========================   ] 54/60 batches, loss: 0.0160Epoch 8/10: [===========================   ] 55/60 batches, loss: 0.0158Epoch 8/10: [============================  ] 56/60 batches, loss: 0.0158Epoch 8/10: [============================  ] 57/60 batches, loss: 0.0159Epoch 8/10: [============================= ] 58/60 batches, loss: 0.0158Epoch 8/10: [============================= ] 59/60 batches, loss: 0.0159Epoch 8/10: [==============================] 60/60 batches, loss: 0.0160
[2025-04-29 21:31:02,641][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0160
[2025-04-29 21:31:03,130][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0170, Metrics: {'mse': 0.01568892039358616, 'rmse': 0.12525542061558118, 'r2': 0.6247491836547852}
Epoch 9/10: [Epoch 9/10: [                              ] 1/60 batches, loss: 0.0154Epoch 9/10: [=                             ] 2/60 batches, loss: 0.0142Epoch 9/10: [=                             ] 3/60 batches, loss: 0.0135Epoch 9/10: [==                            ] 4/60 batches, loss: 0.0130Epoch 9/10: [==                            ] 5/60 batches, loss: 0.0121Epoch 9/10: [===                           ] 6/60 batches, loss: 0.0121Epoch 9/10: [===                           ] 7/60 batches, loss: 0.0117Epoch 9/10: [====                          ] 8/60 batches, loss: 0.0134Epoch 9/10: [====                          ] 9/60 batches, loss: 0.0129Epoch 9/10: [=====                         ] 10/60 batches, loss: 0.0133Epoch 9/10: [=====                         ] 11/60 batches, loss: 0.0134Epoch 9/10: [======                        ] 12/60 batches, loss: 0.0138Epoch 9/10: [======                        ] 13/60 batches, loss: 0.0137Epoch 9/10: [=======                       ] 14/60 batches, loss: 0.0133Epoch 9/10: [=======                       ] 15/60 batches, loss: 0.0130Epoch 9/10: [========                      ] 16/60 batches, loss: 0.0135Epoch 9/10: [========                      ] 17/60 batches, loss: 0.0136Epoch 9/10: [=========                     ] 18/60 batches, loss: 0.0132Epoch 9/10: [=========                     ] 19/60 batches, loss: 0.0129Epoch 9/10: [==========                    ] 20/60 batches, loss: 0.0129Epoch 9/10: [==========                    ] 21/60 batches, loss: 0.0133Epoch 9/10: [===========                   ] 22/60 batches, loss: 0.0130Epoch 9/10: [===========                   ] 23/60 batches, loss: 0.0131Epoch 9/10: [============                  ] 24/60 batches, loss: 0.0133Epoch 9/10: [============                  ] 25/60 batches, loss: 0.0132Epoch 9/10: [=============                 ] 26/60 batches, loss: 0.0131Epoch 9/10: [=============                 ] 27/60 batches, loss: 0.0132Epoch 9/10: [==============                ] 28/60 batches, loss: 0.0130Epoch 9/10: [==============                ] 29/60 batches, loss: 0.0129Epoch 9/10: [===============               ] 30/60 batches, loss: 0.0127Epoch 9/10: [===============               ] 31/60 batches, loss: 0.0125Epoch 9/10: [================              ] 32/60 batches, loss: 0.0124Epoch 9/10: [================              ] 33/60 batches, loss: 0.0128Epoch 9/10: [=================             ] 34/60 batches, loss: 0.0129Epoch 9/10: [=================             ] 35/60 batches, loss: 0.0128Epoch 9/10: [==================            ] 36/60 batches, loss: 0.0127Epoch 9/10: [==================            ] 37/60 batches, loss: 0.0127Epoch 9/10: [===================           ] 38/60 batches, loss: 0.0128Epoch 9/10: [===================           ] 39/60 batches, loss: 0.0127Epoch 9/10: [====================          ] 40/60 batches, loss: 0.0129Epoch 9/10: [====================          ] 41/60 batches, loss: 0.0131Epoch 9/10: [=====================         ] 42/60 batches, loss: 0.0132Epoch 9/10: [=====================         ] 43/60 batches, loss: 0.0131Epoch 9/10: [======================        ] 44/60 batches, loss: 0.0131Epoch 9/10: [======================        ] 45/60 batches, loss: 0.0129Epoch 9/10: [=======================       ] 46/60 batches, loss: 0.0128Epoch 9/10: [=======================       ] 47/60 batches, loss: 0.0128Epoch 9/10: [========================      ] 48/60 batches, loss: 0.0129Epoch 9/10: [========================      ] 49/60 batches, loss: 0.0128Epoch 9/10: [=========================     ] 50/60 batches, loss: 0.0127Epoch 9/10: [=========================     ] 51/60 batches, loss: 0.0126Epoch 9/10: [==========================    ] 52/60 batches, loss: 0.0128Epoch 9/10: [==========================    ] 53/60 batches, loss: 0.0127Epoch 9/10: [===========================   ] 54/60 batches, loss: 0.0128Epoch 9/10: [===========================   ] 55/60 batches, loss: 0.0129Epoch 9/10: [============================  ] 56/60 batches, loss: 0.0131Epoch 9/10: [============================  ] 57/60 batches, loss: 0.0131Epoch 9/10: [============================= ] 58/60 batches, loss: 0.0132Epoch 9/10: [============================= ] 59/60 batches, loss: 0.0131Epoch 9/10: [==============================] 60/60 batches, loss: 0.0130
[2025-04-29 21:31:15,862][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0130
[2025-04-29 21:31:16,300][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0160, Metrics: {'mse': 0.015276872552931309, 'rmse': 0.12359964624921589, 'r2': 0.6346046328544617}
Epoch 10/10: [Epoch 10/10: [                              ] 1/60 batches, loss: 0.0114Epoch 10/10: [=                             ] 2/60 batches, loss: 0.0096Epoch 10/10: [=                             ] 3/60 batches, loss: 0.0101Epoch 10/10: [==                            ] 4/60 batches, loss: 0.0095Epoch 10/10: [==                            ] 5/60 batches, loss: 0.0090Epoch 10/10: [===                           ] 6/60 batches, loss: 0.0095Epoch 10/10: [===                           ] 7/60 batches, loss: 0.0112Epoch 10/10: [====                          ] 8/60 batches, loss: 0.0114Epoch 10/10: [====                          ] 9/60 batches, loss: 0.0123Epoch 10/10: [=====                         ] 10/60 batches, loss: 0.0122Epoch 10/10: [=====                         ] 11/60 batches, loss: 0.0123Epoch 10/10: [======                        ] 12/60 batches, loss: 0.0122Epoch 10/10: [======                        ] 13/60 batches, loss: 0.0117Epoch 10/10: [=======                       ] 14/60 batches, loss: 0.0114Epoch 10/10: [=======                       ] 15/60 batches, loss: 0.0128Epoch 10/10: [========                      ] 16/60 batches, loss: 0.0137Epoch 10/10: [========                      ] 17/60 batches, loss: 0.0135Epoch 10/10: [=========                     ] 18/60 batches, loss: 0.0134Epoch 10/10: [=========                     ] 19/60 batches, loss: 0.0130Epoch 10/10: [==========                    ] 20/60 batches, loss: 0.0129Epoch 10/10: [==========                    ] 21/60 batches, loss: 0.0128Epoch 10/10: [===========                   ] 22/60 batches, loss: 0.0126Epoch 10/10: [===========                   ] 23/60 batches, loss: 0.0126Epoch 10/10: [============                  ] 24/60 batches, loss: 0.0130Epoch 10/10: [============                  ] 25/60 batches, loss: 0.0129Epoch 10/10: [=============                 ] 26/60 batches, loss: 0.0132Epoch 10/10: [=============                 ] 27/60 batches, loss: 0.0128Epoch 10/10: [==============                ] 28/60 batches, loss: 0.0128Epoch 10/10: [==============                ] 29/60 batches, loss: 0.0127Epoch 10/10: [===============               ] 30/60 batches, loss: 0.0127Epoch 10/10: [===============               ] 31/60 batches, loss: 0.0126Epoch 10/10: [================              ] 32/60 batches, loss: 0.0126Epoch 10/10: [================              ] 33/60 batches, loss: 0.0127Epoch 10/10: [=================             ] 34/60 batches, loss: 0.0125Epoch 10/10: [=================             ] 35/60 batches, loss: 0.0125Epoch 10/10: [==================            ] 36/60 batches, loss: 0.0125Epoch 10/10: [==================            ] 37/60 batches, loss: 0.0125Epoch 10/10: [===================           ] 38/60 batches, loss: 0.0126Epoch 10/10: [===================           ] 39/60 batches, loss: 0.0129Epoch 10/10: [====================          ] 40/60 batches, loss: 0.0129Epoch 10/10: [====================          ] 41/60 batches, loss: 0.0128Epoch 10/10: [=====================         ] 42/60 batches, loss: 0.0128Epoch 10/10: [=====================         ] 43/60 batches, loss: 0.0128Epoch 10/10: [======================        ] 44/60 batches, loss: 0.0127Epoch 10/10: [======================        ] 45/60 batches, loss: 0.0126Epoch 10/10: [=======================       ] 46/60 batches, loss: 0.0125Epoch 10/10: [=======================       ] 47/60 batches, loss: 0.0124Epoch 10/10: [========================      ] 48/60 batches, loss: 0.0123Epoch 10/10: [========================      ] 49/60 batches, loss: 0.0122Epoch 10/10: [=========================     ] 50/60 batches, loss: 0.0122Epoch 10/10: [=========================     ] 51/60 batches, loss: 0.0123Epoch 10/10: [==========================    ] 52/60 batches, loss: 0.0123Epoch 10/10: [==========================    ] 53/60 batches, loss: 0.0125Epoch 10/10: [===========================   ] 54/60 batches, loss: 0.0125Epoch 10/10: [===========================   ] 55/60 batches, loss: 0.0123Epoch 10/10: [============================  ] 56/60 batches, loss: 0.0124Epoch 10/10: [============================  ] 57/60 batches, loss: 0.0123Epoch 10/10: [============================= ] 58/60 batches, loss: 0.0125Epoch 10/10: [============================= ] 59/60 batches, loss: 0.0125Epoch 10/10: [==============================] 60/60 batches, loss: 0.0124
[2025-04-29 21:31:29,052][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0124
[2025-04-29 21:31:29,479][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0229, Metrics: {'mse': 0.020594067871570587, 'rmse': 0.14350633390749898, 'r2': 0.5074268579483032}
[2025-04-29 21:31:29,480][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-04-29 21:31:29,480][src.training.lm_trainer][INFO] - Training completed in 130.73 seconds
[2025-04-29 21:31:29,480][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:31:34,236][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.012259178794920444, 'rmse': 0.11072117591012319, 'r2': 0.6621581315994263}
[2025-04-29 21:31:34,236][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.015276872552931309, 'rmse': 0.12359964624921589, 'r2': 0.6346046328544617}
[2025-04-29 21:31:34,236][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03745632246136665, 'rmse': 0.19353635953320672, 'r2': 0.08136987686157227}
[2025-04-29 21:31:36,504][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/id/id/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁▁
wandb:     best_val_mse █▅▃▂▁▁▁
wandb:      best_val_r2 ▁▄▆▇███
wandb:    best_val_rmse █▅▄▃▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▅▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▆▂▂▁▁▁▁
wandb:          val_mse █▅▃▆▂▂▁▁▁▁
wandb:           val_r2 ▁▄▆▃▇▇████
wandb:         val_rmse █▅▄▆▃▃▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01599
wandb:     best_val_mse 0.01528
wandb:      best_val_r2 0.6346
wandb:    best_val_rmse 0.1236
wandb:            epoch 10
wandb:   final_test_mse 0.03746
wandb:    final_test_r2 0.08137
wandb:  final_test_rmse 0.19354
wandb:  final_train_mse 0.01226
wandb:   final_train_r2 0.66216
wandb: final_train_rmse 0.11072
wandb:    final_val_mse 0.01528
wandb:     final_val_r2 0.6346
wandb:   final_val_rmse 0.1236
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01243
wandb:       train_time 130.72539
wandb:         val_loss 0.02289
wandb:          val_mse 0.02059
wandb:           val_r2 0.50743
wandb:         val_rmse 0.14351
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212855-3ht9hp0k
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212855-3ht9hp0k/logs
Experiment finetune_complexity_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/id/results.json
Running experiment: finetune_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:32:07,446][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ja
experiment_name: finetune_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:32:07,446][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:32:07,446][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:32:07,447][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:32:07,451][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-04-29 21:32:07,451][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:32:09,043][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:32:11,771][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:32:11,772][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:11,841][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:11,880][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:11,962][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-29 21:32:11,974][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:11,974][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-29 21:32:11,976][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:11,994][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:12,022][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:12,045][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-29 21:32:12,046][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:12,046][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-29 21:32:12,047][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:12,067][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:12,104][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:12,116][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-29 21:32:12,118][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:12,118][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-29 21:32:12,119][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-29 21:32:12,119][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:12,119][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:12,119][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:12,120][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-29 21:32:12,120][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:12,120][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:12,121][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-29 21:32:12,121][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:12,121][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:12,122][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:12,122][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-29 21:32:12,122][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-29 21:32:12,122][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-29 21:32:12,122][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:32:12,122][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-29 21:32:12,122][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:32:12,123][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:32:12,123][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:32:16,515][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,516][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,517][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,518][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,519][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,520][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,521][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,522][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,523][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,524][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,525][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,526][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,527][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,528][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,529][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,530][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,531][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:32:16,532][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:32:16,533][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:32:16,533][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:32:16,534][__main__][INFO] - Successfully created model for ja
[2025-04-29 21:32:16,534][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7083Epoch 1/10: [                              ] 2/75 batches, loss: 0.6930Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7012Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7016Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7014Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7018Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7021Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6988Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6979Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6989Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6989Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6981Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7006Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7007Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6987Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6987Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6983Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6975Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6948Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6943Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6952Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6934Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6919Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6917Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6915Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6914Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6910Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6904Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6901Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6897Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6896Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6896Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6895Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6891Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6888Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6883Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6879Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6885Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6869Epoch 1/10: [================              ] 40/75 batches, loss: 0.6868Epoch 1/10: [================              ] 41/75 batches, loss: 0.6860Epoch 1/10: [================              ] 42/75 batches, loss: 0.6854Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6862Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6861Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6857Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6842Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6841Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6847Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6841Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6839Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6834Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6832Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6832Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6821Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6813Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6814Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6808Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6803Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6797Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6799Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6795Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6792Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6788Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6779Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6774Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6766Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6751Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6742Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6728Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6713Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6714Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6701Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6693Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6687Epoch 1/10: [==============================] 75/75 batches, loss: 0.6677
[2025-04-29 21:32:34,332][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6677
[2025-04-29 21:32:34,722][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.5462, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5466Epoch 2/10: [                              ] 2/75 batches, loss: 0.4994Epoch 2/10: [=                             ] 3/75 batches, loss: 0.5257Epoch 2/10: [=                             ] 4/75 batches, loss: 0.5412Epoch 2/10: [==                            ] 5/75 batches, loss: 0.5418Epoch 2/10: [==                            ] 6/75 batches, loss: 0.5394Epoch 2/10: [==                            ] 7/75 batches, loss: 0.5413Epoch 2/10: [===                           ] 8/75 batches, loss: 0.5405Epoch 2/10: [===                           ] 9/75 batches, loss: 0.5357Epoch 2/10: [====                          ] 10/75 batches, loss: 0.5336Epoch 2/10: [====                          ] 11/75 batches, loss: 0.5336Epoch 2/10: [====                          ] 12/75 batches, loss: 0.5362Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5400Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5335Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5282Epoch 2/10: [======                        ] 16/75 batches, loss: 0.5225Epoch 2/10: [======                        ] 17/75 batches, loss: 0.5181Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5167Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5140Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5125Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5102Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5057Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5052Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5024Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5017Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5023Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.4999Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5003Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5008Epoch 2/10: [============                  ] 30/75 batches, loss: 0.4989Epoch 2/10: [============                  ] 31/75 batches, loss: 0.4974Epoch 2/10: [============                  ] 32/75 batches, loss: 0.4951Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.4931Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.4866Epoch 2/10: [==============                ] 35/75 batches, loss: 0.4855Epoch 2/10: [==============                ] 36/75 batches, loss: 0.4842Epoch 2/10: [==============                ] 37/75 batches, loss: 0.4837Epoch 2/10: [===============               ] 38/75 batches, loss: 0.4819Epoch 2/10: [===============               ] 39/75 batches, loss: 0.4810Epoch 2/10: [================              ] 40/75 batches, loss: 0.4798Epoch 2/10: [================              ] 41/75 batches, loss: 0.4774Epoch 2/10: [================              ] 42/75 batches, loss: 0.4752Epoch 2/10: [=================             ] 43/75 batches, loss: 0.4727Epoch 2/10: [=================             ] 44/75 batches, loss: 0.4729Epoch 2/10: [==================            ] 45/75 batches, loss: 0.4700Epoch 2/10: [==================            ] 46/75 batches, loss: 0.4672Epoch 2/10: [==================            ] 47/75 batches, loss: 0.4648Epoch 2/10: [===================           ] 48/75 batches, loss: 0.4633Epoch 2/10: [===================           ] 49/75 batches, loss: 0.4619Epoch 2/10: [====================          ] 50/75 batches, loss: 0.4594Epoch 2/10: [====================          ] 51/75 batches, loss: 0.4585Epoch 2/10: [====================          ] 52/75 batches, loss: 0.4569Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.4550Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.4519Epoch 2/10: [======================        ] 55/75 batches, loss: 0.4497Epoch 2/10: [======================        ] 56/75 batches, loss: 0.4459Epoch 2/10: [======================        ] 57/75 batches, loss: 0.4444Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.4423Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.4409Epoch 2/10: [========================      ] 60/75 batches, loss: 0.4403Epoch 2/10: [========================      ] 61/75 batches, loss: 0.4396Epoch 2/10: [========================      ] 62/75 batches, loss: 0.4374Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.4369Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.4357Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.4333Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.4321Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.4287Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.4266Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.4246Epoch 2/10: [============================  ] 70/75 batches, loss: 0.4229Epoch 2/10: [============================  ] 71/75 batches, loss: 0.4207Epoch 2/10: [============================  ] 72/75 batches, loss: 0.4196Epoch 2/10: [============================= ] 73/75 batches, loss: 0.4183Epoch 2/10: [============================= ] 74/75 batches, loss: 0.4151Epoch 2/10: [==============================] 75/75 batches, loss: 0.4130
[2025-04-29 21:32:50,422][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4130
[2025-04-29 21:32:50,746][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2074, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.2309Epoch 3/10: [                              ] 2/75 batches, loss: 0.2411Epoch 3/10: [=                             ] 3/75 batches, loss: 0.2469Epoch 3/10: [=                             ] 4/75 batches, loss: 0.2348Epoch 3/10: [==                            ] 5/75 batches, loss: 0.2428Epoch 3/10: [==                            ] 6/75 batches, loss: 0.2294Epoch 3/10: [==                            ] 7/75 batches, loss: 0.2306Epoch 3/10: [===                           ] 8/75 batches, loss: 0.2289Epoch 3/10: [===                           ] 9/75 batches, loss: 0.2220Epoch 3/10: [====                          ] 10/75 batches, loss: 0.2249Epoch 3/10: [====                          ] 11/75 batches, loss: 0.2353Epoch 3/10: [====                          ] 12/75 batches, loss: 0.2435Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.2374Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.2281Epoch 3/10: [======                        ] 15/75 batches, loss: 0.2238Epoch 3/10: [======                        ] 16/75 batches, loss: 0.2200Epoch 3/10: [======                        ] 17/75 batches, loss: 0.2174Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.2176Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.2111Epoch 3/10: [========                      ] 20/75 batches, loss: 0.2056Epoch 3/10: [========                      ] 21/75 batches, loss: 0.2046Epoch 3/10: [========                      ] 22/75 batches, loss: 0.2099Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.2093Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.2052Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.2057Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.2047Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.2044Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.2050Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.2036Epoch 3/10: [============                  ] 30/75 batches, loss: 0.2030Epoch 3/10: [============                  ] 31/75 batches, loss: 0.2032Epoch 3/10: [============                  ] 32/75 batches, loss: 0.2060Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.2032Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.2012Epoch 3/10: [==============                ] 35/75 batches, loss: 0.1986Epoch 3/10: [==============                ] 36/75 batches, loss: 0.1976Epoch 3/10: [==============                ] 37/75 batches, loss: 0.1973Epoch 3/10: [===============               ] 38/75 batches, loss: 0.1970Epoch 3/10: [===============               ] 39/75 batches, loss: 0.1966Epoch 3/10: [================              ] 40/75 batches, loss: 0.1942Epoch 3/10: [================              ] 41/75 batches, loss: 0.1914Epoch 3/10: [================              ] 42/75 batches, loss: 0.1935Epoch 3/10: [=================             ] 43/75 batches, loss: 0.1916Epoch 3/10: [=================             ] 44/75 batches, loss: 0.1894Epoch 3/10: [==================            ] 45/75 batches, loss: 0.1897Epoch 3/10: [==================            ] 46/75 batches, loss: 0.1890Epoch 3/10: [==================            ] 47/75 batches, loss: 0.1880Epoch 3/10: [===================           ] 48/75 batches, loss: 0.1865Epoch 3/10: [===================           ] 49/75 batches, loss: 0.1847Epoch 3/10: [====================          ] 50/75 batches, loss: 0.1830Epoch 3/10: [====================          ] 51/75 batches, loss: 0.1816Epoch 3/10: [====================          ] 52/75 batches, loss: 0.1803Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.1782Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.1781Epoch 3/10: [======================        ] 55/75 batches, loss: 0.1768Epoch 3/10: [======================        ] 56/75 batches, loss: 0.1759Epoch 3/10: [======================        ] 57/75 batches, loss: 0.1751Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.1731Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.1720Epoch 3/10: [========================      ] 60/75 batches, loss: 0.1715Epoch 3/10: [========================      ] 61/75 batches, loss: 0.1703Epoch 3/10: [========================      ] 62/75 batches, loss: 0.1690Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.1704Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.1695Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.1681Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.1671Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.1656Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.1641Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.1633Epoch 3/10: [============================  ] 70/75 batches, loss: 0.1627Epoch 3/10: [============================  ] 71/75 batches, loss: 0.1620Epoch 3/10: [============================  ] 72/75 batches, loss: 0.1608Epoch 3/10: [============================= ] 73/75 batches, loss: 0.1600Epoch 3/10: [============================= ] 74/75 batches, loss: 0.1590Epoch 3/10: [==============================] 75/75 batches, loss: 0.1582
[2025-04-29 21:33:06,511][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1582
[2025-04-29 21:33:06,836][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0466, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0986Epoch 4/10: [                              ] 2/75 batches, loss: 0.0865Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0888Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0840Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0810Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0790Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0771Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0825Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0814Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0793Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0786Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0772Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0773Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0937Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0916Epoch 4/10: [======                        ] 16/75 batches, loss: 0.1034Epoch 4/10: [======                        ] 17/75 batches, loss: 0.1039Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.1039Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.1076Epoch 4/10: [========                      ] 20/75 batches, loss: 0.1067Epoch 4/10: [========                      ] 21/75 batches, loss: 0.1067Epoch 4/10: [========                      ] 22/75 batches, loss: 0.1080Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.1058Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.1030Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.1026Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.1026Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.1026Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.1006Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0998Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0973Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0959Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0948Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0951Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0951Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0957Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0959Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0959Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0950Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0951Epoch 4/10: [================              ] 40/75 batches, loss: 0.0937Epoch 4/10: [================              ] 41/75 batches, loss: 0.0930Epoch 4/10: [================              ] 42/75 batches, loss: 0.0919Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0907Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0904Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0901Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0890Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0881Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0881Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0882Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0871Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0873Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0866Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0863Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0856Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0848Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0844Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0837Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0835Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0827Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0819Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0819Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0815Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0813Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0810Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0806Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0799Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0794Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0834Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0827Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0818Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0848Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0842Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0834Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0836Epoch 4/10: [==============================] 75/75 batches, loss: 0.0832
[2025-04-29 21:33:22,554][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0832
[2025-04-29 21:33:22,868][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0122, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.2918Epoch 5/10: [                              ] 2/75 batches, loss: 0.1579Epoch 5/10: [=                             ] 3/75 batches, loss: 0.1918Epoch 5/10: [=                             ] 4/75 batches, loss: 0.1482Epoch 5/10: [==                            ] 5/75 batches, loss: 0.1261Epoch 5/10: [==                            ] 6/75 batches, loss: 0.1131Epoch 5/10: [==                            ] 7/75 batches, loss: 0.1027Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0974Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0904Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0841Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0791Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0743Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0715Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0698Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0678Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0660Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0651Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0639Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0658Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0665Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0660Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0647Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0631Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0617Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0640Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0629Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0621Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0617Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0615Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0604Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0593Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0586Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0614Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0661Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0653Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0641Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0632Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0627Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0620Epoch 5/10: [================              ] 40/75 batches, loss: 0.0619Epoch 5/10: [================              ] 41/75 batches, loss: 0.0617Epoch 5/10: [================              ] 42/75 batches, loss: 0.0616Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0613Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0611Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0603Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0626Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0618Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0613Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0605Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0602Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0597Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0591Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0588Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0584Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0583Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0629Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0627Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0625Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0650Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0649Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0647Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0645Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0643Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0646Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0640Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0638Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0636Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0636Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0629Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0625Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0620Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0614Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0610Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0605Epoch 5/10: [==============================] 75/75 batches, loss: 0.0601
[2025-04-29 21:33:38,700][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0601
[2025-04-29 21:33:39,076][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0048, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0327Epoch 6/10: [                              ] 2/75 batches, loss: 0.0259Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0267Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0404Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0428Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0433Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0550Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0558Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0512Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0541Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0524Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0503Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0484Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0617Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0592Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0564Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0544Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0525Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0511Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0496Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0489Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0474Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0472Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0464Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0474Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0475Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0471Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0471Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0462Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0457Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0450Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0443Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0438Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0429Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0424Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0415Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0411Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0408Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0405Epoch 6/10: [================              ] 40/75 batches, loss: 0.0401Epoch 6/10: [================              ] 41/75 batches, loss: 0.0395Epoch 6/10: [================              ] 42/75 batches, loss: 0.0391Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0385Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0383Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0375Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0371Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0370Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0367Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0363Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0361Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0357Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0352Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0350Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0349Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0345Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0363Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0359Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0355Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0370Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0367Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0363Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0360Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0357Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0354Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0379Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0376Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0372Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0368Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0365Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0361Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0358Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0356Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0355Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0351Epoch 6/10: [==============================] 75/75 batches, loss: 0.0351
[2025-04-29 21:33:54,875][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0351
[2025-04-29 21:33:55,194][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0075, Metrics: {'accuracy': 1.0, 'f1': 1.0}
[2025-04-29 21:33:55,195][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0140Epoch 7/10: [                              ] 2/75 batches, loss: 0.0135Epoch 7/10: [=                             ] 3/75 batches, loss: 0.2079Epoch 7/10: [=                             ] 4/75 batches, loss: 0.1587Epoch 7/10: [==                            ] 5/75 batches, loss: 0.1378Epoch 7/10: [==                            ] 6/75 batches, loss: 0.1580Epoch 7/10: [==                            ] 7/75 batches, loss: 0.1451Epoch 7/10: [===                           ] 8/75 batches, loss: 0.1347Epoch 7/10: [===                           ] 9/75 batches, loss: 0.1280Epoch 7/10: [====                          ] 10/75 batches, loss: 0.1230Epoch 7/10: [====                          ] 11/75 batches, loss: 0.1213Epoch 7/10: [====                          ] 12/75 batches, loss: 0.1328Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.1286Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.1268Epoch 7/10: [======                        ] 15/75 batches, loss: 0.1259Epoch 7/10: [======                        ] 16/75 batches, loss: 0.1248Epoch 7/10: [======                        ] 17/75 batches, loss: 0.1211Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.1203Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.1167Epoch 7/10: [========                      ] 20/75 batches, loss: 0.1143Epoch 7/10: [========                      ] 21/75 batches, loss: 0.1107Epoch 7/10: [========                      ] 22/75 batches, loss: 0.1074Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.1043Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.1006Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.1128Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.1093Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.1062Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.1033Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.1012Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0987Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0965Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0943Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0922Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.1039Epoch 7/10: [==============                ] 35/75 batches, loss: 0.1014Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0995Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0977Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0956Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0950Epoch 7/10: [================              ] 40/75 batches, loss: 0.0936Epoch 7/10: [================              ] 41/75 batches, loss: 0.0922Epoch 7/10: [================              ] 42/75 batches, loss: 0.0906Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0890Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0875Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0858Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0842Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0826Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0817Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0805Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0792Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0787Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0780Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0768Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0761Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0749Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0739Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0727Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0717Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0706Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0724Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0714Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0704Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0698Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0690Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0681Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0674Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0665Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0659Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0654Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0648Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0643Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0637Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0631Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0625Epoch 7/10: [==============================] 75/75 batches, loss: 0.0620
[2025-04-29 21:34:10,486][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0620
[2025-04-29 21:34:10,834][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1499, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9795918367346939}
[2025-04-29 21:34:10,835][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0076Epoch 8/10: [                              ] 2/75 batches, loss: 0.0100Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0099Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0094Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0092Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0102Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0116Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0116Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0113Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0134Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0137Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0146Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0142Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0151Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0150Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0257Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0258Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0252Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0248Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0241Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0240Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0233Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0228Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0225Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0221Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0217Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0227Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0224Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0218Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0215Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0210Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0205Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0201Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0197Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0197Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0196Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0287Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0372Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0365Epoch 8/10: [================              ] 40/75 batches, loss: 0.0357Epoch 8/10: [================              ] 41/75 batches, loss: 0.0351Epoch 8/10: [================              ] 42/75 batches, loss: 0.0348Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0341Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0336Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0332Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0330Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0326Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0331Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0330Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0328Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0323Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0319Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0315Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0379Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0386Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0382Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0376Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0373Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0369Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0365Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0360Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0355Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0351Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0347Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0345Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0354Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0350Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0346Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0343Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0340Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0335Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0332Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0329Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0327Epoch 8/10: [==============================] 75/75 batches, loss: 0.0323
[2025-04-29 21:34:26,128][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0323
[2025-04-29 21:34:26,460][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0257, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191}
[2025-04-29 21:34:26,461][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:34:26,461][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:34:26,461][src.training.lm_trainer][INFO] - Training completed in 127.80 seconds
[2025-04-29 21:34:26,461][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:34:31,917][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9958018471872376, 'f1': 0.995787700084246}
[2025-04-29 21:34:31,917][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0}
[2025-04-29 21:34:31,918][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9021739130434783, 'f1': 0.9174311926605505}
[2025-04-29 21:34:34,173][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ja/ja/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁███
wandb:          best_val_f1 ▁▁███
wandb:        best_val_loss █▄▂▁▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▅▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████▁▁
wandb:               val_f1 ▁▁████▁▁
wandb:             val_loss █▄▂▁▁▁▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.00476
wandb:                epoch 8
wandb:  final_test_accuracy 0.90217
wandb:        final_test_f1 0.91743
wandb: final_train_accuracy 0.9958
wandb:       final_train_f1 0.99579
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 2e-05
wandb:           train_loss 0.03228
wandb:           train_time 127.79506
wandb:         val_accuracy 0.97826
wandb:               val_f1 0.97872
wandb:             val_loss 0.02572
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213207-tnpcv8eh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213207-tnpcv8eh/logs
Experiment finetune_question_type_ja completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ja/results.json
Running experiment: finetune_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:34:50,636][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ja
experiment_name: finetune_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:34:50,636][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:34:50,636][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:34:50,636][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:34:50,641][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-04-29 21:34:50,641][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:34:51,975][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:34:54,690][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:34:54,691][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:34:54,750][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,782][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,869][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-29 21:34:54,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:34:54,880][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-29 21:34:54,882][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:34:54,901][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,925][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,938][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-29 21:34:54,939][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:34:54,939][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-29 21:34:54,940][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:34:54,958][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,982][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:34:54,993][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-29 21:34:54,995][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:34:54,995][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-29 21:34:54,996][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-29 21:34:54,997][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:34:54,997][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:34:54,997][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:34:54,997][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:34:54,997][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:34:54,997][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-29 21:34:54,997][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-29 21:34:54,998][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-29 21:34:54,998][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:34:54,998][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:34:54,998][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:34:54,998][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:34:54,998][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:34:54,998][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:34:54,999][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:34:54,999][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:34:54,999][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-29 21:34:55,000][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-29 21:34:55,000][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-29 21:34:55,000][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-29 21:34:55,000][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:34:55,000][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:34:55,001][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,426][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,427][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:34:59,443][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:34:59,443][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:34:59,444][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:34:59,444][__main__][INFO] - Successfully created model for ja
[2025-04-29 21:34:59,444][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1183Epoch 1/10: [                              ] 2/75 batches, loss: 0.1495Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1423Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1327Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1215Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1247Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1291Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1288Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1288Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1280Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1271Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1229Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1220Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1211Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1184Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1236Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1231Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1221Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1227Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1209Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1196Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1189Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1185Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1170Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1167Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1156Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1143Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1128Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1120Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1103Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1097Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1082Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1077Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1074Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1065Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1057Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1063Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1047Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1039Epoch 1/10: [================              ] 40/75 batches, loss: 0.1032Epoch 1/10: [================              ] 41/75 batches, loss: 0.1022Epoch 1/10: [================              ] 42/75 batches, loss: 0.1021Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1034Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1041Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1030Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1027Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1027Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1019Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1020Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1015Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1016Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1008Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1000Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0999Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1003Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1006Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1001Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1000Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0997Epoch 1/10: [========================      ] 60/75 batches, loss: 0.1002Epoch 1/10: [========================      ] 61/75 batches, loss: 0.1009Epoch 1/10: [========================      ] 62/75 batches, loss: 0.1005Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.1010Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.1010Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.1006Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0999Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0996Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0997Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0997Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0995Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0988Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0992Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0993Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0984Epoch 1/10: [==============================] 75/75 batches, loss: 0.0974
[2025-04-29 21:35:18,742][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0974
[2025-04-29 21:35:19,056][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1484, Metrics: {'mse': 0.14778737723827362, 'rmse': 0.3844312386347832, 'r2': -1.4086275100708008}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0765Epoch 2/10: [                              ] 2/75 batches, loss: 0.0584Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0627Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0623Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0739Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0763Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0726Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0802Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0797Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0847Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0854Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0846Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0873Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0850Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0866Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0857Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0847Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0846Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0838Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0841Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0831Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0827Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0836Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0849Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0853Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0846Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0832Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0849Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0845Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0837Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0831Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0817Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0808Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0818Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0800Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0789Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0779Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0778Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0777Epoch 2/10: [================              ] 40/75 batches, loss: 0.0776Epoch 2/10: [================              ] 41/75 batches, loss: 0.0777Epoch 2/10: [================              ] 42/75 batches, loss: 0.0766Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0758Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0752Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0744Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0745Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0740Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0738Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0731Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0732Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0735Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0728Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0719Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0721Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0714Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0704Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0707Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0706Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0702Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0700Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0699Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0697Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0694Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0691Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0689Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0685Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0679Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0680Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0677Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0678Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0678Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0680Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0681Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0677Epoch 2/10: [==============================] 75/75 batches, loss: 0.0677
[2025-04-29 21:35:34,796][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0677
[2025-04-29 21:35:35,102][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0812, Metrics: {'mse': 0.08098509162664413, 'rmse': 0.2845787968676587, 'r2': -0.3198889493942261}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0651Epoch 3/10: [                              ] 2/75 batches, loss: 0.0472Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0432Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0483Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0586Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0575Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0584Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0540Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0560Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0581Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0565Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0579Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0562Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0576Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0567Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0572Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0557Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0548Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0540Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0532Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0552Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0552Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0546Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0536Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0525Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0532Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0536Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0526Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0521Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0529Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0525Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0528Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0516Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0507Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0506Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0504Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0500Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0506Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0504Epoch 3/10: [================              ] 40/75 batches, loss: 0.0500Epoch 3/10: [================              ] 41/75 batches, loss: 0.0496Epoch 3/10: [================              ] 42/75 batches, loss: 0.0492Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0495Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0491Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0489Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0485Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0479Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0474Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0471Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0466Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0461Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0461Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0461Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0465Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0467Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0462Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0461Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0463Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0460Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0457Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0453Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0453Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0453Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0450Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0450Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0448Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0447Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0446Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0443Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0441Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0445Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0447Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0445Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0448Epoch 3/10: [==============================] 75/75 batches, loss: 0.0445
[2025-04-29 21:35:50,887][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-29 21:35:51,202][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0361, Metrics: {'mse': 0.03546852245926857, 'rmse': 0.18833088556917202, 'r2': 0.42193663120269775}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0207Epoch 4/10: [                              ] 2/75 batches, loss: 0.0225Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0240Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0229Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0253Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0271Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0277Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0288Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0321Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0352Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0351Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0342Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0362Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0356Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0337Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0344Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0331Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0342Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0330Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0327Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0323Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0327Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0321Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0319Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0322Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0318Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0318Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0314Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0314Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0316Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0311Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0320Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0319Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0319Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0323Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0327Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0326Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0324Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0323Epoch 4/10: [================              ] 40/75 batches, loss: 0.0335Epoch 4/10: [================              ] 41/75 batches, loss: 0.0337Epoch 4/10: [================              ] 42/75 batches, loss: 0.0331Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0327Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0323Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0322Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0317Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0317Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0317Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0318Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0316Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0315Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0312Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0310Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0309Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0311Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0308Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0307Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0307Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0307Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0306Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0307Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0304Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0307Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0306Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0304Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0301Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0300Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0298Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0297Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0296Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0294Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0295Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0295Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0296Epoch 4/10: [==============================] 75/75 batches, loss: 0.0294
[2025-04-29 21:36:06,938][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0294
[2025-04-29 21:36:07,313][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0351, Metrics: {'mse': 0.03519738093018532, 'rmse': 0.1876096504185894, 'r2': 0.4263557195663452}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0147Epoch 5/10: [                              ] 2/75 batches, loss: 0.0166Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0231Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0217Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0197Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0185Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0182Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0174Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0174Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0172Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0174Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0178Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0174Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0182Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0187Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0181Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0178Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0180Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0177Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0176Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0176Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0185Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0183Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0189Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0190Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0189Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0188Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0185Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0184Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0184Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0182Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0186Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0184Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0184Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0182Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0181Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0180Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0183Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0183Epoch 5/10: [================              ] 40/75 batches, loss: 0.0188Epoch 5/10: [================              ] 41/75 batches, loss: 0.0193Epoch 5/10: [================              ] 42/75 batches, loss: 0.0198Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0196Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0196Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0196Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0194Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0192Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0192Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0194Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0193Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0194Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0194Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0192Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0193Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0194Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0194Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0193Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0192Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0193Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0191Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0192Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0193Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0192Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0194Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0193Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0191Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0195Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0196Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0196Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0194Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0194Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0194Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0193Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0194Epoch 5/10: [==============================] 75/75 batches, loss: 0.0197
[2025-04-29 21:36:23,123][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0197
[2025-04-29 21:36:23,465][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0243, Metrics: {'mse': 0.024641355499625206, 'rmse': 0.1569756525695154, 'r2': 0.5983970165252686}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0185Epoch 6/10: [                              ] 2/75 batches, loss: 0.0202Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0254Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0231Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0212Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0195Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0209Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0194Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0202Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0200Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0193Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0202Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0192Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0195Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0189Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0183Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0180Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0181Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0179Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0182Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0183Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0185Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0189Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0189Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0185Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0184Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0182Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0181Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0179Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0176Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0176Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0179Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0179Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0179Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0178Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0177Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0176Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0176Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0174Epoch 6/10: [================              ] 40/75 batches, loss: 0.0173Epoch 6/10: [================              ] 41/75 batches, loss: 0.0173Epoch 6/10: [================              ] 42/75 batches, loss: 0.0172Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0171Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0172Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0172Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0174Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0175Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0175Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0180Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0180Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0180Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0181Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0182Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0189Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0190Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0191Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0192Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0191Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0192Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0194Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0194Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0195Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0195Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0194Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0192Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0194Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0193Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0192Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0192Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0194Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0193Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0193Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0191Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0191Epoch 6/10: [==============================] 75/75 batches, loss: 0.0191
[2025-04-29 21:36:39,247][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0191
[2025-04-29 21:36:39,571][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0265, Metrics: {'mse': 0.02670883946120739, 'rmse': 0.16342839245739216, 'r2': 0.5647013187408447}
[2025-04-29 21:36:39,572][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0168Epoch 7/10: [                              ] 2/75 batches, loss: 0.0165Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0164Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0166Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0165Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0152Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0169Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0171Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0173Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0185Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0197Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0194Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0194Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0192Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0191Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0183Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0181Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0185Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0186Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0193Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0197Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0206Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0203Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0198Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0198Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0196Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0202Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0204Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0208Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0206Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0207Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0202Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0200Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0202Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0199Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0197Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0195Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0194Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0192Epoch 7/10: [================              ] 40/75 batches, loss: 0.0189Epoch 7/10: [================              ] 41/75 batches, loss: 0.0191Epoch 7/10: [================              ] 42/75 batches, loss: 0.0190Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0190Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0190Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0190Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0189Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0188Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0186Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0188Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0188Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0188Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0185Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0184Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0183Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0183Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0182Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0180Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0181Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0182Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0182Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0182Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0179Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0179Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0177Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0176Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0177Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0179Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0178Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0178Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0176Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0175Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0175Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0174Epoch 7/10: [==============================] 75/75 batches, loss: 0.0174
[2025-04-29 21:36:54,812][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0174
[2025-04-29 21:36:55,228][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0458, Metrics: {'mse': 0.045695092529058456, 'rmse': 0.21376410486575723, 'r2': 0.2552648186683655}
[2025-04-29 21:36:55,229][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0205Epoch 8/10: [                              ] 2/75 batches, loss: 0.0140Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0141Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0144Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0137Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0146Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0153Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0140Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0142Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0136Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0141Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0141Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0139Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0133Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0133Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0133Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0132Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0136Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0135Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0137Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0137Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0135Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0132Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0134Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0133Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0135Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0131Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0130Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0128Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0127Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0129Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0128Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0128Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0128Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0131Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0130Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0131Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0132Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0134Epoch 8/10: [================              ] 40/75 batches, loss: 0.0134Epoch 8/10: [================              ] 41/75 batches, loss: 0.0133Epoch 8/10: [================              ] 42/75 batches, loss: 0.0132Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0131Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0132Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0133Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0133Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0132Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0132Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0131Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0131Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0130Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0130Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0130Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0129Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0132Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0132Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0131Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0130Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0129Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0130Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0129Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0129Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0129Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0128Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0127Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0127Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0126Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0125Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0126Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0125Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0125Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0127Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0126Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0125Epoch 8/10: [==============================] 75/75 batches, loss: 0.0125
[2025-04-29 21:37:10,458][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0125
[2025-04-29 21:37:10,851][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0198, Metrics: {'mse': 0.019827209413051605, 'rmse': 0.14080912404049534, 'r2': 0.676857590675354}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0137Epoch 9/10: [                              ] 2/75 batches, loss: 0.0116Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0123Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0113Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0127Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0116Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0110Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0120Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0112Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0105Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0104Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0103Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0111Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0109Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0107Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0105Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0107Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0108Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0106Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0104Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0106Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0108Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0107Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0107Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0110Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0113Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0112Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0110Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0109Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0107Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0110Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0116Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0118Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0117Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0117Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0122Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0121Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0121Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0121Epoch 9/10: [================              ] 40/75 batches, loss: 0.0120Epoch 9/10: [================              ] 41/75 batches, loss: 0.0119Epoch 9/10: [================              ] 42/75 batches, loss: 0.0117Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0117Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0116Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0116Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0116Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0115Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0115Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0116Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0115Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0115Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0115Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0114Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0113Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0115Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0116Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0115Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0114Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0113Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0113Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0113Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0112Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0113Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0114Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0113Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0113Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0113Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0114Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0113Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0112Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0113Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0113Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0113Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0113Epoch 9/10: [==============================] 75/75 batches, loss: 0.0115
[2025-04-29 21:37:26,655][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0115
[2025-04-29 21:37:26,988][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0141, Metrics: {'mse': 0.014070739969611168, 'rmse': 0.11862014993082401, 'r2': 0.7706761360168457}
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0095Epoch 10/10: [                              ] 2/75 batches, loss: 0.0094Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0113Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0116Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0104Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0096Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0094Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0094Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0088Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0084Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0086Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0088Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0089Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0085Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0083Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0082Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0081Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0087Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0088Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0087Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0087Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0085Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0086Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0088Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0086Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0085Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0084Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0083Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0084Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0084Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0084Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0083Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0082Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0082Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0083Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0082Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0082Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0082Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0082Epoch 10/10: [================              ] 40/75 batches, loss: 0.0082Epoch 10/10: [================              ] 41/75 batches, loss: 0.0082Epoch 10/10: [================              ] 42/75 batches, loss: 0.0083Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0085Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0087Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0087Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0087Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0087Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0088Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0088Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0088Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0088Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0088Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0087Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0087Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0089Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0089Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0089Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0090Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0090Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0090Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0090Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0090Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0089Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0090Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0091Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0090Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0090Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0089Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0089Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0089Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0090Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0090Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0090Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0090Epoch 10/10: [==============================] 75/75 batches, loss: 0.0092
[2025-04-29 21:37:42,763][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0092
[2025-04-29 21:37:43,082][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0190, Metrics: {'mse': 0.01900576986372471, 'rmse': 0.13786141542768487, 'r2': 0.6902453899383545}
[2025-04-29 21:37:43,083][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-04-29 21:37:43,083][src.training.lm_trainer][INFO] - Training completed in 160.18 seconds
[2025-04-29 21:37:43,083][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:37:48,518][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.006702850107103586, 'rmse': 0.08187093566769336, 'r2': 0.8327020406723022}
[2025-04-29 21:37:48,519][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.014070739969611168, 'rmse': 0.11862014993082401, 'r2': 0.7706761360168457}
[2025-04-29 21:37:48,519][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.015467599965631962, 'rmse': 0.1243688062402786, 'r2': 0.7028807401657104}
[2025-04-29 21:37:50,763][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ja/ja/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▂▁▁
wandb:     best_val_mse █▅▂▂▂▁▁
wandb:      best_val_r2 ▁▄▇▇▇██
wandb:    best_val_rmse █▅▃▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▄▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▂▂▂▃▁▁▁
wandb:          val_mse █▅▂▂▂▂▃▁▁▁
wandb:           val_r2 ▁▄▇▇▇▇▆███
wandb:         val_rmse █▅▃▃▂▂▄▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01408
wandb:     best_val_mse 0.01407
wandb:      best_val_r2 0.77068
wandb:    best_val_rmse 0.11862
wandb:            epoch 10
wandb:   final_test_mse 0.01547
wandb:    final_test_r2 0.70288
wandb:  final_test_rmse 0.12437
wandb:  final_train_mse 0.0067
wandb:   final_train_r2 0.8327
wandb: final_train_rmse 0.08187
wandb:    final_val_mse 0.01407
wandb:     final_val_r2 0.77068
wandb:   final_val_rmse 0.11862
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00915
wandb:       train_time 160.17628
wandb:         val_loss 0.01902
wandb:          val_mse 0.01901
wandb:           val_r2 0.69025
wandb:         val_rmse 0.13786
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213450-hinepwzl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213450-hinepwzl/logs
Experiment finetune_complexity_ja completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ja/results.json
Running experiment: finetune_question_type_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ko"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:38:22,998][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ko
experiment_name: finetune_question_type_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:38:22,998][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:38:22,998][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:38:22,998][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:38:23,002][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-04-29 21:38:23,003][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:38:24,605][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:38:27,428][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:38:27,429][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:38:27,494][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,519][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,609][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-29 21:38:27,617][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:38:27,618][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-29 21:38:27,619][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:38:27,641][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,668][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,719][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-29 21:38:27,720][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:38:27,721][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-29 21:38:27,722][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:38:27,746][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,778][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:38:27,793][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-29 21:38:27,794][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:38:27,794][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-29 21:38:27,796][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-29 21:38:27,796][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:38:27,796][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:38:27,797][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:38:27,797][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:38:27,797][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-29 21:38:27,797][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-29 21:38:27,797][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-29 21:38:27,797][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:38:27,797][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:38:27,798][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:38:27,798][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:38:27,798][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:38:27,799][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:38:27,799][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-29 21:38:27,799][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:38:27,800][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:38:27,800][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:38:32,257][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,258][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,259][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,260][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,261][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,262][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,263][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,264][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,265][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,266][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,267][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,268][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,269][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,270][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,271][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,272][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,273][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:38:32,274][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:38:32,274][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:38:32,275][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:38:32,275][__main__][INFO] - Successfully created model for ko
[2025-04-29 21:38:32,276][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/47 batches, loss: 0.7018Epoch 1/10: [=                             ] 2/47 batches, loss: 0.6933Epoch 1/10: [=                             ] 3/47 batches, loss: 0.6955Epoch 1/10: [==                            ] 4/47 batches, loss: 0.7003Epoch 1/10: [===                           ] 5/47 batches, loss: 0.7023Epoch 1/10: [===                           ] 6/47 batches, loss: 0.7017Epoch 1/10: [====                          ] 7/47 batches, loss: 0.7024Epoch 1/10: [=====                         ] 8/47 batches, loss: 0.7039Epoch 1/10: [=====                         ] 9/47 batches, loss: 0.7004Epoch 1/10: [======                        ] 10/47 batches, loss: 0.6982Epoch 1/10: [=======                       ] 11/47 batches, loss: 0.6985Epoch 1/10: [=======                       ] 12/47 batches, loss: 0.6989Epoch 1/10: [========                      ] 13/47 batches, loss: 0.6989Epoch 1/10: [========                      ] 14/47 batches, loss: 0.6994Epoch 1/10: [=========                     ] 15/47 batches, loss: 0.6971Epoch 1/10: [==========                    ] 16/47 batches, loss: 0.6952Epoch 1/10: [==========                    ] 17/47 batches, loss: 0.6952Epoch 1/10: [===========                   ] 18/47 batches, loss: 0.6938Epoch 1/10: [============                  ] 19/47 batches, loss: 0.6934Epoch 1/10: [============                  ] 20/47 batches, loss: 0.6924Epoch 1/10: [=============                 ] 21/47 batches, loss: 0.6934Epoch 1/10: [==============                ] 22/47 batches, loss: 0.6915Epoch 1/10: [==============                ] 23/47 batches, loss: 0.6911Epoch 1/10: [===============               ] 24/47 batches, loss: 0.6916Epoch 1/10: [===============               ] 25/47 batches, loss: 0.6914Epoch 1/10: [================              ] 26/47 batches, loss: 0.6914Epoch 1/10: [=================             ] 27/47 batches, loss: 0.6932Epoch 1/10: [=================             ] 28/47 batches, loss: 0.6934Epoch 1/10: [==================            ] 29/47 batches, loss: 0.6923Epoch 1/10: [===================           ] 30/47 batches, loss: 0.6917Epoch 1/10: [===================           ] 31/47 batches, loss: 0.6930Epoch 1/10: [====================          ] 32/47 batches, loss: 0.6921Epoch 1/10: [=====================         ] 33/47 batches, loss: 0.6924Epoch 1/10: [=====================         ] 34/47 batches, loss: 0.6922Epoch 1/10: [======================        ] 35/47 batches, loss: 0.6926Epoch 1/10: [======================        ] 36/47 batches, loss: 0.6922Epoch 1/10: [=======================       ] 37/47 batches, loss: 0.6927Epoch 1/10: [========================      ] 38/47 batches, loss: 0.6917Epoch 1/10: [========================      ] 39/47 batches, loss: 0.6927Epoch 1/10: [=========================     ] 40/47 batches, loss: 0.6923Epoch 1/10: [==========================    ] 41/47 batches, loss: 0.6921Epoch 1/10: [==========================    ] 42/47 batches, loss: 0.6918Epoch 1/10: [===========================   ] 43/47 batches, loss: 0.6916Epoch 1/10: [============================  ] 44/47 batches, loss: 0.6922Epoch 1/10: [============================  ] 45/47 batches, loss: 0.6913Epoch 1/10: [============================= ] 46/47 batches, loss: 0.6917Epoch 1/10: [==============================] 47/47 batches, loss: 0.6892
[2025-04-29 21:38:44,571][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6892
[2025-04-29 21:38:45,061][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6921, Metrics: {'accuracy': 0.5, 'f1': 0.6666666666666666}
Epoch 2/10: [Epoch 2/10: [                              ] 1/47 batches, loss: 0.6924Epoch 2/10: [=                             ] 2/47 batches, loss: 0.6932Epoch 2/10: [=                             ] 3/47 batches, loss: 0.6931Epoch 2/10: [==                            ] 4/47 batches, loss: 0.6820Epoch 2/10: [===                           ] 5/47 batches, loss: 0.6730Epoch 2/10: [===                           ] 6/47 batches, loss: 0.6763Epoch 2/10: [====                          ] 7/47 batches, loss: 0.6767Epoch 2/10: [=====                         ] 8/47 batches, loss: 0.6785Epoch 2/10: [=====                         ] 9/47 batches, loss: 0.6793Epoch 2/10: [======                        ] 10/47 batches, loss: 0.6773Epoch 2/10: [=======                       ] 11/47 batches, loss: 0.6778Epoch 2/10: [=======                       ] 12/47 batches, loss: 0.6758Epoch 2/10: [========                      ] 13/47 batches, loss: 0.6790Epoch 2/10: [========                      ] 14/47 batches, loss: 0.6796Epoch 2/10: [=========                     ] 15/47 batches, loss: 0.6803Epoch 2/10: [==========                    ] 16/47 batches, loss: 0.6842Epoch 2/10: [==========                    ] 17/47 batches, loss: 0.6825Epoch 2/10: [===========                   ] 18/47 batches, loss: 0.6837Epoch 2/10: [============                  ] 19/47 batches, loss: 0.6842Epoch 2/10: [============                  ] 20/47 batches, loss: 0.6840Epoch 2/10: [=============                 ] 21/47 batches, loss: 0.6836Epoch 2/10: [==============                ] 22/47 batches, loss: 0.6840Epoch 2/10: [==============                ] 23/47 batches, loss: 0.6814Epoch 2/10: [===============               ] 24/47 batches, loss: 0.6811Epoch 2/10: [===============               ] 25/47 batches, loss: 0.6812Epoch 2/10: [================              ] 26/47 batches, loss: 0.6796Epoch 2/10: [=================             ] 27/47 batches, loss: 0.6806Epoch 2/10: [=================             ] 28/47 batches, loss: 0.6819Epoch 2/10: [==================            ] 29/47 batches, loss: 0.6799Epoch 2/10: [===================           ] 30/47 batches, loss: 0.6798Epoch 2/10: [===================           ] 31/47 batches, loss: 0.6800Epoch 2/10: [====================          ] 32/47 batches, loss: 0.6806Epoch 2/10: [=====================         ] 33/47 batches, loss: 0.6793Epoch 2/10: [=====================         ] 34/47 batches, loss: 0.6810Epoch 2/10: [======================        ] 35/47 batches, loss: 0.6819Epoch 2/10: [======================        ] 36/47 batches, loss: 0.6816Epoch 2/10: [=======================       ] 37/47 batches, loss: 0.6811Epoch 2/10: [========================      ] 38/47 batches, loss: 0.6819Epoch 2/10: [========================      ] 39/47 batches, loss: 0.6817Epoch 2/10: [=========================     ] 40/47 batches, loss: 0.6793Epoch 2/10: [==========================    ] 41/47 batches, loss: 0.6794Epoch 2/10: [==========================    ] 42/47 batches, loss: 0.6806Epoch 2/10: [===========================   ] 43/47 batches, loss: 0.6799Epoch 2/10: [============================  ] 44/47 batches, loss: 0.6786Epoch 2/10: [============================  ] 45/47 batches, loss: 0.6777Epoch 2/10: [============================= ] 46/47 batches, loss: 0.6763Epoch 2/10: [==============================] 47/47 batches, loss: 0.6775
[2025-04-29 21:38:55,115][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6775
[2025-04-29 21:38:55,719][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6402, Metrics: {'accuracy': 0.5138888888888888, 'f1': 0.6728971962616822}
Epoch 3/10: [Epoch 3/10: [                              ] 1/47 batches, loss: 0.6542Epoch 3/10: [=                             ] 2/47 batches, loss: 0.6494Epoch 3/10: [=                             ] 3/47 batches, loss: 0.6341Epoch 3/10: [==                            ] 4/47 batches, loss: 0.6415Epoch 3/10: [===                           ] 5/47 batches, loss: 0.6386Epoch 3/10: [===                           ] 6/47 batches, loss: 0.6307Epoch 3/10: [====                          ] 7/47 batches, loss: 0.6205Epoch 3/10: [=====                         ] 8/47 batches, loss: 0.6248Epoch 3/10: [=====                         ] 9/47 batches, loss: 0.6292Epoch 3/10: [======                        ] 10/47 batches, loss: 0.6297Epoch 3/10: [=======                       ] 11/47 batches, loss: 0.6355Epoch 3/10: [=======                       ] 12/47 batches, loss: 0.6352Epoch 3/10: [========                      ] 13/47 batches, loss: 0.6331Epoch 3/10: [========                      ] 14/47 batches, loss: 0.6295Epoch 3/10: [=========                     ] 15/47 batches, loss: 0.6285Epoch 3/10: [==========                    ] 16/47 batches, loss: 0.6298Epoch 3/10: [==========                    ] 17/47 batches, loss: 0.6289Epoch 3/10: [===========                   ] 18/47 batches, loss: 0.6292Epoch 3/10: [============                  ] 19/47 batches, loss: 0.6276Epoch 3/10: [============                  ] 20/47 batches, loss: 0.6263Epoch 3/10: [=============                 ] 21/47 batches, loss: 0.6219Epoch 3/10: [==============                ] 22/47 batches, loss: 0.6178Epoch 3/10: [==============                ] 23/47 batches, loss: 0.6158Epoch 3/10: [===============               ] 24/47 batches, loss: 0.6144Epoch 3/10: [===============               ] 25/47 batches, loss: 0.6096Epoch 3/10: [================              ] 26/47 batches, loss: 0.6055Epoch 3/10: [=================             ] 27/47 batches, loss: 0.6033Epoch 3/10: [=================             ] 28/47 batches, loss: 0.6027Epoch 3/10: [==================            ] 29/47 batches, loss: 0.5998Epoch 3/10: [===================           ] 30/47 batches, loss: 0.5980Epoch 3/10: [===================           ] 31/47 batches, loss: 0.5949Epoch 3/10: [====================          ] 32/47 batches, loss: 0.5926Epoch 3/10: [=====================         ] 33/47 batches, loss: 0.5874Epoch 3/10: [=====================         ] 34/47 batches, loss: 0.5870Epoch 3/10: [======================        ] 35/47 batches, loss: 0.5840Epoch 3/10: [======================        ] 36/47 batches, loss: 0.5796Epoch 3/10: [=======================       ] 37/47 batches, loss: 0.5766Epoch 3/10: [========================      ] 38/47 batches, loss: 0.5751Epoch 3/10: [========================      ] 39/47 batches, loss: 0.5732Epoch 3/10: [=========================     ] 40/47 batches, loss: 0.5702Epoch 3/10: [==========================    ] 41/47 batches, loss: 0.5686Epoch 3/10: [==========================    ] 42/47 batches, loss: 0.5680Epoch 3/10: [===========================   ] 43/47 batches, loss: 0.5685Epoch 3/10: [============================  ] 44/47 batches, loss: 0.5647Epoch 3/10: [============================  ] 45/47 batches, loss: 0.5634Epoch 3/10: [============================= ] 46/47 batches, loss: 0.5601Epoch 3/10: [==============================] 47/47 batches, loss: 0.5598
[2025-04-29 21:39:05,878][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5598
[2025-04-29 21:39:06,367][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4156, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 4/10: [Epoch 4/10: [                              ] 1/47 batches, loss: 0.4249Epoch 4/10: [=                             ] 2/47 batches, loss: 0.4305Epoch 4/10: [=                             ] 3/47 batches, loss: 0.4175Epoch 4/10: [==                            ] 4/47 batches, loss: 0.4218Epoch 4/10: [===                           ] 5/47 batches, loss: 0.4391Epoch 4/10: [===                           ] 6/47 batches, loss: 0.4393Epoch 4/10: [====                          ] 7/47 batches, loss: 0.4306Epoch 4/10: [=====                         ] 8/47 batches, loss: 0.4312Epoch 4/10: [=====                         ] 9/47 batches, loss: 0.4292Epoch 4/10: [======                        ] 10/47 batches, loss: 0.4265Epoch 4/10: [=======                       ] 11/47 batches, loss: 0.4276Epoch 4/10: [=======                       ] 12/47 batches, loss: 0.4254Epoch 4/10: [========                      ] 13/47 batches, loss: 0.4241Epoch 4/10: [========                      ] 14/47 batches, loss: 0.4149Epoch 4/10: [=========                     ] 15/47 batches, loss: 0.4145Epoch 4/10: [==========                    ] 16/47 batches, loss: 0.4139Epoch 4/10: [==========                    ] 17/47 batches, loss: 0.4063Epoch 4/10: [===========                   ] 18/47 batches, loss: 0.4042Epoch 4/10: [============                  ] 19/47 batches, loss: 0.4016Epoch 4/10: [============                  ] 20/47 batches, loss: 0.3997Epoch 4/10: [=============                 ] 21/47 batches, loss: 0.3942Epoch 4/10: [==============                ] 22/47 batches, loss: 0.3884Epoch 4/10: [==============                ] 23/47 batches, loss: 0.3860Epoch 4/10: [===============               ] 24/47 batches, loss: 0.3852Epoch 4/10: [===============               ] 25/47 batches, loss: 0.3850Epoch 4/10: [================              ] 26/47 batches, loss: 0.3829Epoch 4/10: [=================             ] 27/47 batches, loss: 0.3813Epoch 4/10: [=================             ] 28/47 batches, loss: 0.3811Epoch 4/10: [==================            ] 29/47 batches, loss: 0.3769Epoch 4/10: [===================           ] 30/47 batches, loss: 0.3734Epoch 4/10: [===================           ] 31/47 batches, loss: 0.3713Epoch 4/10: [====================          ] 32/47 batches, loss: 0.3676Epoch 4/10: [=====================         ] 33/47 batches, loss: 0.3659Epoch 4/10: [=====================         ] 34/47 batches, loss: 0.3637Epoch 4/10: [======================        ] 35/47 batches, loss: 0.3612Epoch 4/10: [======================        ] 36/47 batches, loss: 0.3591Epoch 4/10: [=======================       ] 37/47 batches, loss: 0.3572Epoch 4/10: [========================      ] 38/47 batches, loss: 0.3558Epoch 4/10: [========================      ] 39/47 batches, loss: 0.3536Epoch 4/10: [=========================     ] 40/47 batches, loss: 0.3534Epoch 4/10: [==========================    ] 41/47 batches, loss: 0.3499Epoch 4/10: [==========================    ] 42/47 batches, loss: 0.3446Epoch 4/10: [===========================   ] 43/47 batches, loss: 0.3424Epoch 4/10: [============================  ] 44/47 batches, loss: 0.3378Epoch 4/10: [============================  ] 45/47 batches, loss: 0.3337Epoch 4/10: [============================= ] 46/47 batches, loss: 0.3303Epoch 4/10: [==============================] 47/47 batches, loss: 0.3279
[2025-04-29 21:39:16,443][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3279
[2025-04-29 21:39:16,860][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2401, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10: [Epoch 5/10: [                              ] 1/47 batches, loss: 0.3290Epoch 5/10: [=                             ] 2/47 batches, loss: 0.2864Epoch 5/10: [=                             ] 3/47 batches, loss: 0.2743Epoch 5/10: [==                            ] 4/47 batches, loss: 0.2742Epoch 5/10: [===                           ] 5/47 batches, loss: 0.2626Epoch 5/10: [===                           ] 6/47 batches, loss: 0.2459Epoch 5/10: [====                          ] 7/47 batches, loss: 0.2481Epoch 5/10: [=====                         ] 8/47 batches, loss: 0.2353Epoch 5/10: [=====                         ] 9/47 batches, loss: 0.2346Epoch 5/10: [======                        ] 10/47 batches, loss: 0.2270Epoch 5/10: [=======                       ] 11/47 batches, loss: 0.2288Epoch 5/10: [=======                       ] 12/47 batches, loss: 0.2316Epoch 5/10: [========                      ] 13/47 batches, loss: 0.2284Epoch 5/10: [========                      ] 14/47 batches, loss: 0.2268Epoch 5/10: [=========                     ] 15/47 batches, loss: 0.2262Epoch 5/10: [==========                    ] 16/47 batches, loss: 0.2248Epoch 5/10: [==========                    ] 17/47 batches, loss: 0.2355Epoch 5/10: [===========                   ] 18/47 batches, loss: 0.2327Epoch 5/10: [============                  ] 19/47 batches, loss: 0.2342Epoch 5/10: [============                  ] 20/47 batches, loss: 0.2317Epoch 5/10: [=============                 ] 21/47 batches, loss: 0.2336Epoch 5/10: [==============                ] 22/47 batches, loss: 0.2298Epoch 5/10: [==============                ] 23/47 batches, loss: 0.2270Epoch 5/10: [===============               ] 24/47 batches, loss: 0.2219Epoch 5/10: [===============               ] 25/47 batches, loss: 0.2192Epoch 5/10: [================              ] 26/47 batches, loss: 0.2250Epoch 5/10: [=================             ] 27/47 batches, loss: 0.2202Epoch 5/10: [=================             ] 28/47 batches, loss: 0.2184Epoch 5/10: [==================            ] 29/47 batches, loss: 0.2168Epoch 5/10: [===================           ] 30/47 batches, loss: 0.2157Epoch 5/10: [===================           ] 31/47 batches, loss: 0.2119Epoch 5/10: [====================          ] 32/47 batches, loss: 0.2085Epoch 5/10: [=====================         ] 33/47 batches, loss: 0.2037Epoch 5/10: [=====================         ] 34/47 batches, loss: 0.2018Epoch 5/10: [======================        ] 35/47 batches, loss: 0.1988Epoch 5/10: [======================        ] 36/47 batches, loss: 0.1958Epoch 5/10: [=======================       ] 37/47 batches, loss: 0.1944Epoch 5/10: [========================      ] 38/47 batches, loss: 0.1920Epoch 5/10: [========================      ] 39/47 batches, loss: 0.1906Epoch 5/10: [=========================     ] 40/47 batches, loss: 0.1900Epoch 5/10: [==========================    ] 41/47 batches, loss: 0.1882Epoch 5/10: [==========================    ] 42/47 batches, loss: 0.1865Epoch 5/10: [===========================   ] 43/47 batches, loss: 0.1840Epoch 5/10: [============================  ] 44/47 batches, loss: 0.1812Epoch 5/10: [============================  ] 45/47 batches, loss: 0.1790Epoch 5/10: [============================= ] 46/47 batches, loss: 0.1808Epoch 5/10: [==============================] 47/47 batches, loss: 0.1789
[2025-04-29 21:39:26,937][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1789
[2025-04-29 21:39:27,360][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1877, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10: [Epoch 6/10: [                              ] 1/47 batches, loss: 0.0729Epoch 6/10: [=                             ] 2/47 batches, loss: 0.0742Epoch 6/10: [=                             ] 3/47 batches, loss: 0.0697Epoch 6/10: [==                            ] 4/47 batches, loss: 0.0805Epoch 6/10: [===                           ] 5/47 batches, loss: 0.0791Epoch 6/10: [===                           ] 6/47 batches, loss: 0.0717Epoch 6/10: [====                          ] 7/47 batches, loss: 0.0702Epoch 6/10: [=====                         ] 8/47 batches, loss: 0.0802Epoch 6/10: [=====                         ] 9/47 batches, loss: 0.0822Epoch 6/10: [======                        ] 10/47 batches, loss: 0.0827Epoch 6/10: [=======                       ] 11/47 batches, loss: 0.0839Epoch 6/10: [=======                       ] 12/47 batches, loss: 0.0818Epoch 6/10: [========                      ] 13/47 batches, loss: 0.0803Epoch 6/10: [========                      ] 14/47 batches, loss: 0.0792Epoch 6/10: [=========                     ] 15/47 batches, loss: 0.0772Epoch 6/10: [==========                    ] 16/47 batches, loss: 0.0753Epoch 6/10: [==========                    ] 17/47 batches, loss: 0.0749Epoch 6/10: [===========                   ] 18/47 batches, loss: 0.0738Epoch 6/10: [============                  ] 19/47 batches, loss: 0.0851Epoch 6/10: [============                  ] 20/47 batches, loss: 0.0834Epoch 6/10: [=============                 ] 21/47 batches, loss: 0.0833Epoch 6/10: [==============                ] 22/47 batches, loss: 0.0823Epoch 6/10: [==============                ] 23/47 batches, loss: 0.0826Epoch 6/10: [===============               ] 24/47 batches, loss: 0.0875Epoch 6/10: [===============               ] 25/47 batches, loss: 0.0864Epoch 6/10: [================              ] 26/47 batches, loss: 0.0904Epoch 6/10: [=================             ] 27/47 batches, loss: 0.0889Epoch 6/10: [=================             ] 28/47 batches, loss: 0.0887Epoch 6/10: [==================            ] 29/47 batches, loss: 0.0871Epoch 6/10: [===================           ] 30/47 batches, loss: 0.0854Epoch 6/10: [===================           ] 31/47 batches, loss: 0.0850Epoch 6/10: [====================          ] 32/47 batches, loss: 0.0839Epoch 6/10: [=====================         ] 33/47 batches, loss: 0.0829Epoch 6/10: [=====================         ] 34/47 batches, loss: 0.0814Epoch 6/10: [======================        ] 35/47 batches, loss: 0.0801Epoch 6/10: [======================        ] 36/47 batches, loss: 0.0856Epoch 6/10: [=======================       ] 37/47 batches, loss: 0.0854Epoch 6/10: [========================      ] 38/47 batches, loss: 0.0845Epoch 6/10: [========================      ] 39/47 batches, loss: 0.0850Epoch 6/10: [=========================     ] 40/47 batches, loss: 0.0849Epoch 6/10: [==========================    ] 41/47 batches, loss: 0.0862Epoch 6/10: [==========================    ] 42/47 batches, loss: 0.0853Epoch 6/10: [===========================   ] 43/47 batches, loss: 0.0851Epoch 6/10: [============================  ] 44/47 batches, loss: 0.0842Epoch 6/10: [============================  ] 45/47 batches, loss: 0.0841Epoch 6/10: [============================= ] 46/47 batches, loss: 0.0844Epoch 6/10: [==============================] 47/47 batches, loss: 0.0830
[2025-04-29 21:39:37,460][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0830
[2025-04-29 21:39:37,894][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2046, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
[2025-04-29 21:39:37,895][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/47 batches, loss: 0.0423Epoch 7/10: [=                             ] 2/47 batches, loss: 0.0381Epoch 7/10: [=                             ] 3/47 batches, loss: 0.0349Epoch 7/10: [==                            ] 4/47 batches, loss: 0.0667Epoch 7/10: [===                           ] 5/47 batches, loss: 0.0724Epoch 7/10: [===                           ] 6/47 batches, loss: 0.0668Epoch 7/10: [====                          ] 7/47 batches, loss: 0.0649Epoch 7/10: [=====                         ] 8/47 batches, loss: 0.0818Epoch 7/10: [=====                         ] 9/47 batches, loss: 0.0770Epoch 7/10: [======                        ] 10/47 batches, loss: 0.0766Epoch 7/10: [=======                       ] 11/47 batches, loss: 0.0756Epoch 7/10: [=======                       ] 12/47 batches, loss: 0.0748Epoch 7/10: [========                      ] 13/47 batches, loss: 0.0732Epoch 7/10: [========                      ] 14/47 batches, loss: 0.0740Epoch 7/10: [=========                     ] 15/47 batches, loss: 0.0704Epoch 7/10: [==========                    ] 16/47 batches, loss: 0.0683Epoch 7/10: [==========                    ] 17/47 batches, loss: 0.0668Epoch 7/10: [===========                   ] 18/47 batches, loss: 0.0645Epoch 7/10: [============                  ] 19/47 batches, loss: 0.0630Epoch 7/10: [============                  ] 20/47 batches, loss: 0.0622Epoch 7/10: [=============                 ] 21/47 batches, loss: 0.0608Epoch 7/10: [==============                ] 22/47 batches, loss: 0.0599Epoch 7/10: [==============                ] 23/47 batches, loss: 0.0581Epoch 7/10: [===============               ] 24/47 batches, loss: 0.0593Epoch 7/10: [===============               ] 25/47 batches, loss: 0.0579Epoch 7/10: [================              ] 26/47 batches, loss: 0.0569Epoch 7/10: [=================             ] 27/47 batches, loss: 0.0563Epoch 7/10: [=================             ] 28/47 batches, loss: 0.0559Epoch 7/10: [==================            ] 29/47 batches, loss: 0.0553Epoch 7/10: [===================           ] 30/47 batches, loss: 0.0545Epoch 7/10: [===================           ] 31/47 batches, loss: 0.0537Epoch 7/10: [====================          ] 32/47 batches, loss: 0.0535Epoch 7/10: [=====================         ] 33/47 batches, loss: 0.0545Epoch 7/10: [=====================         ] 34/47 batches, loss: 0.0543Epoch 7/10: [======================        ] 35/47 batches, loss: 0.0579Epoch 7/10: [======================        ] 36/47 batches, loss: 0.0586Epoch 7/10: [=======================       ] 37/47 batches, loss: 0.0577Epoch 7/10: [========================      ] 38/47 batches, loss: 0.0570Epoch 7/10: [========================      ] 39/47 batches, loss: 0.0564Epoch 7/10: [=========================     ] 40/47 batches, loss: 0.0566Epoch 7/10: [==========================    ] 41/47 batches, loss: 0.0558Epoch 7/10: [==========================    ] 42/47 batches, loss: 0.0554Epoch 7/10: [===========================   ] 43/47 batches, loss: 0.0548Epoch 7/10: [============================  ] 44/47 batches, loss: 0.0542Epoch 7/10: [============================  ] 45/47 batches, loss: 0.0536Epoch 7/10: [============================= ] 46/47 batches, loss: 0.0530Epoch 7/10: [==============================] 47/47 batches, loss: 0.0521
[2025-04-29 21:39:47,428][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0521
[2025-04-29 21:39:47,845][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2469, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
[2025-04-29 21:39:47,846][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/47 batches, loss: 0.0108Epoch 8/10: [=                             ] 2/47 batches, loss: 0.0127Epoch 8/10: [=                             ] 3/47 batches, loss: 0.0177Epoch 8/10: [==                            ] 4/47 batches, loss: 0.0186Epoch 8/10: [===                           ] 5/47 batches, loss: 0.0182Epoch 8/10: [===                           ] 6/47 batches, loss: 0.0319Epoch 8/10: [====                          ] 7/47 batches, loss: 0.0316Epoch 8/10: [=====                         ] 8/47 batches, loss: 0.0299Epoch 8/10: [=====                         ] 9/47 batches, loss: 0.0295Epoch 8/10: [======                        ] 10/47 batches, loss: 0.0289Epoch 8/10: [=======                       ] 11/47 batches, loss: 0.0277Epoch 8/10: [=======                       ] 12/47 batches, loss: 0.0268Epoch 8/10: [========                      ] 13/47 batches, loss: 0.0261Epoch 8/10: [========                      ] 14/47 batches, loss: 0.0265Epoch 8/10: [=========                     ] 15/47 batches, loss: 0.0262Epoch 8/10: [==========                    ] 16/47 batches, loss: 0.0255Epoch 8/10: [==========                    ] 17/47 batches, loss: 0.0260Epoch 8/10: [===========                   ] 18/47 batches, loss: 0.0265Epoch 8/10: [============                  ] 19/47 batches, loss: 0.0259Epoch 8/10: [============                  ] 20/47 batches, loss: 0.0256Epoch 8/10: [=============                 ] 21/47 batches, loss: 0.0256Epoch 8/10: [==============                ] 22/47 batches, loss: 0.0256Epoch 8/10: [==============                ] 23/47 batches, loss: 0.0253Epoch 8/10: [===============               ] 24/47 batches, loss: 0.0256Epoch 8/10: [===============               ] 25/47 batches, loss: 0.0252Epoch 8/10: [================              ] 26/47 batches, loss: 0.0247Epoch 8/10: [=================             ] 27/47 batches, loss: 0.0246Epoch 8/10: [=================             ] 28/47 batches, loss: 0.0243Epoch 8/10: [==================            ] 29/47 batches, loss: 0.0241Epoch 8/10: [===================           ] 30/47 batches, loss: 0.0239Epoch 8/10: [===================           ] 31/47 batches, loss: 0.0237Epoch 8/10: [====================          ] 32/47 batches, loss: 0.0237Epoch 8/10: [=====================         ] 33/47 batches, loss: 0.0236Epoch 8/10: [=====================         ] 34/47 batches, loss: 0.0234Epoch 8/10: [======================        ] 35/47 batches, loss: 0.0234Epoch 8/10: [======================        ] 36/47 batches, loss: 0.0244Epoch 8/10: [=======================       ] 37/47 batches, loss: 0.0244Epoch 8/10: [========================      ] 38/47 batches, loss: 0.0239Epoch 8/10: [========================      ] 39/47 batches, loss: 0.0237Epoch 8/10: [=========================     ] 40/47 batches, loss: 0.0239Epoch 8/10: [==========================    ] 41/47 batches, loss: 0.0238Epoch 8/10: [==========================    ] 42/47 batches, loss: 0.0301Epoch 8/10: [===========================   ] 43/47 batches, loss: 0.0304Epoch 8/10: [============================  ] 44/47 batches, loss: 0.0300Epoch 8/10: [============================  ] 45/47 batches, loss: 0.0352Epoch 8/10: [============================= ] 46/47 batches, loss: 0.0354Epoch 8/10: [==============================] 47/47 batches, loss: 0.0351
[2025-04-29 21:39:57,379][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0351
[2025-04-29 21:39:57,856][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.2616, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
[2025-04-29 21:39:57,857][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:39:57,857][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:39:57,857][src.training.lm_trainer][INFO] - Training completed in 83.38 seconds
[2025-04-29 21:39:57,858][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:40:02,021][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9878213802435724, 'f1': 0.986627043090639}
[2025-04-29 21:40:02,022][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
[2025-04-29 21:40:02,022][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8727272727272727, 'f1': 0.8627450980392157}
[2025-04-29 21:40:04,279][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ko/ko/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁███
wandb:          best_val_f1 ▁▁▇██
wandb:        best_val_loss █▇▄▂▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁██████
wandb:               val_f1 ▁▁▇██▇▇█
wandb:             val_loss █▇▄▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.18769
wandb:                epoch 8
wandb:  final_test_accuracy 0.87273
wandb:        final_test_f1 0.86275
wandb: final_train_accuracy 0.98782
wandb:       final_train_f1 0.98663
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 2e-05
wandb:           train_loss 0.03505
wandb:           train_time 83.37524
wandb:         val_accuracy 0.93056
wandb:               val_f1 0.93151
wandb:             val_loss 0.26158
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213823-li3ufj2p
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213823-li3ufj2p/logs
Experiment finetune_question_type_ko completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ko/results.json
Running experiment: finetune_complexity_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ko"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:40:19,732][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ko
experiment_name: finetune_complexity_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:40:19,732][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:40:19,732][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:40:19,732][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:40:19,737][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-04-29 21:40:19,738][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:40:21,137][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:40:23,849][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:40:23,850][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:40:23,911][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:23,947][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:24,049][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-29 21:40:24,057][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:40:24,057][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-29 21:40:24,059][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:40:24,089][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:24,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:24,130][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-29 21:40:24,131][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:40:24,132][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-29 21:40:24,132][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:40:24,154][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:24,188][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:40:24,200][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-29 21:40:24,202][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:40:24,202][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-29 21:40:24,203][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-29 21:40:24,204][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:40:24,204][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:40:24,204][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:40:24,204][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:40:24,204][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:40:24,204][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:40:24,205][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:40:24,205][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:40:24,206][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:40:24,206][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:40:24,206][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:40:24,207][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-29 21:40:24,207][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-29 21:40:24,207][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-29 21:40:24,207][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-29 21:40:24,207][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:40:24,207][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:40:24,208][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:40:28,570][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,571][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,572][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,573][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,574][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,575][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,576][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,577][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,578][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,579][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,580][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,581][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,582][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,583][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,584][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,585][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,586][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,587][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:40:28,587][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:40:28,588][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:40:28,589][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:40:28,589][__main__][INFO] - Successfully created model for ko
[2025-04-29 21:40:28,589][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/47 batches, loss: 0.0635Epoch 1/10: [=                             ] 2/47 batches, loss: 0.0804Epoch 1/10: [=                             ] 3/47 batches, loss: 0.0987Epoch 1/10: [==                            ] 4/47 batches, loss: 0.0973Epoch 1/10: [===                           ] 5/47 batches, loss: 0.0960Epoch 1/10: [===                           ] 6/47 batches, loss: 0.0900Epoch 1/10: [====                          ] 7/47 batches, loss: 0.0930Epoch 1/10: [=====                         ] 8/47 batches, loss: 0.0902Epoch 1/10: [=====                         ] 9/47 batches, loss: 0.0889Epoch 1/10: [======                        ] 10/47 batches, loss: 0.0916Epoch 1/10: [=======                       ] 11/47 batches, loss: 0.0929Epoch 1/10: [=======                       ] 12/47 batches, loss: 0.0945Epoch 1/10: [========                      ] 13/47 batches, loss: 0.0929Epoch 1/10: [========                      ] 14/47 batches, loss: 0.0932Epoch 1/10: [=========                     ] 15/47 batches, loss: 0.0919Epoch 1/10: [==========                    ] 16/47 batches, loss: 0.0955Epoch 1/10: [==========                    ] 17/47 batches, loss: 0.0942Epoch 1/10: [===========                   ] 18/47 batches, loss: 0.0927Epoch 1/10: [============                  ] 19/47 batches, loss: 0.0923Epoch 1/10: [============                  ] 20/47 batches, loss: 0.0913Epoch 1/10: [=============                 ] 21/47 batches, loss: 0.0903Epoch 1/10: [==============                ] 22/47 batches, loss: 0.0884Epoch 1/10: [==============                ] 23/47 batches, loss: 0.0875Epoch 1/10: [===============               ] 24/47 batches, loss: 0.0866Epoch 1/10: [===============               ] 25/47 batches, loss: 0.0864Epoch 1/10: [================              ] 26/47 batches, loss: 0.0866Epoch 1/10: [=================             ] 27/47 batches, loss: 0.0856Epoch 1/10: [=================             ] 28/47 batches, loss: 0.0853Epoch 1/10: [==================            ] 29/47 batches, loss: 0.0852Epoch 1/10: [===================           ] 30/47 batches, loss: 0.0849Epoch 1/10: [===================           ] 31/47 batches, loss: 0.0843Epoch 1/10: [====================          ] 32/47 batches, loss: 0.0825Epoch 1/10: [=====================         ] 33/47 batches, loss: 0.0821Epoch 1/10: [=====================         ] 34/47 batches, loss: 0.0822Epoch 1/10: [======================        ] 35/47 batches, loss: 0.0818Epoch 1/10: [======================        ] 36/47 batches, loss: 0.0817Epoch 1/10: [=======================       ] 37/47 batches, loss: 0.0820Epoch 1/10: [========================      ] 38/47 batches, loss: 0.0814Epoch 1/10: [========================      ] 39/47 batches, loss: 0.0805Epoch 1/10: [=========================     ] 40/47 batches, loss: 0.0809Epoch 1/10: [==========================    ] 41/47 batches, loss: 0.0803Epoch 1/10: [==========================    ] 42/47 batches, loss: 0.0800Epoch 1/10: [===========================   ] 43/47 batches, loss: 0.0793Epoch 1/10: [============================  ] 44/47 batches, loss: 0.0787Epoch 1/10: [============================  ] 45/47 batches, loss: 0.0778Epoch 1/10: [============================= ] 46/47 batches, loss: 0.0776Epoch 1/10: [==============================] 47/47 batches, loss: 0.0772
[2025-04-29 21:40:41,998][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0772
[2025-04-29 21:40:42,523][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1250, Metrics: {'mse': 0.1307724118232727, 'rmse': 0.3616246836476635, 'r2': -1.7758417129516602}
Epoch 2/10: [Epoch 2/10: [                              ] 1/47 batches, loss: 0.0453Epoch 2/10: [=                             ] 2/47 batches, loss: 0.0472Epoch 2/10: [=                             ] 3/47 batches, loss: 0.0651Epoch 2/10: [==                            ] 4/47 batches, loss: 0.0610Epoch 2/10: [===                           ] 5/47 batches, loss: 0.0588Epoch 2/10: [===                           ] 6/47 batches, loss: 0.0563Epoch 2/10: [====                          ] 7/47 batches, loss: 0.0593Epoch 2/10: [=====                         ] 8/47 batches, loss: 0.0648Epoch 2/10: [=====                         ] 9/47 batches, loss: 0.0631Epoch 2/10: [======                        ] 10/47 batches, loss: 0.0625Epoch 2/10: [=======                       ] 11/47 batches, loss: 0.0617Epoch 2/10: [=======                       ] 12/47 batches, loss: 0.0634Epoch 2/10: [========                      ] 13/47 batches, loss: 0.0609Epoch 2/10: [========                      ] 14/47 batches, loss: 0.0592Epoch 2/10: [=========                     ] 15/47 batches, loss: 0.0601Epoch 2/10: [==========                    ] 16/47 batches, loss: 0.0580Epoch 2/10: [==========                    ] 17/47 batches, loss: 0.0580Epoch 2/10: [===========                   ] 18/47 batches, loss: 0.0593Epoch 2/10: [============                  ] 19/47 batches, loss: 0.0570Epoch 2/10: [============                  ] 20/47 batches, loss: 0.0575Epoch 2/10: [=============                 ] 21/47 batches, loss: 0.0580Epoch 2/10: [==============                ] 22/47 batches, loss: 0.0584Epoch 2/10: [==============                ] 23/47 batches, loss: 0.0594Epoch 2/10: [===============               ] 24/47 batches, loss: 0.0586Epoch 2/10: [===============               ] 25/47 batches, loss: 0.0571Epoch 2/10: [================              ] 26/47 batches, loss: 0.0558Epoch 2/10: [=================             ] 27/47 batches, loss: 0.0554Epoch 2/10: [=================             ] 28/47 batches, loss: 0.0547Epoch 2/10: [==================            ] 29/47 batches, loss: 0.0538Epoch 2/10: [===================           ] 30/47 batches, loss: 0.0536Epoch 2/10: [===================           ] 31/47 batches, loss: 0.0535Epoch 2/10: [====================          ] 32/47 batches, loss: 0.0529Epoch 2/10: [=====================         ] 33/47 batches, loss: 0.0524Epoch 2/10: [=====================         ] 34/47 batches, loss: 0.0524Epoch 2/10: [======================        ] 35/47 batches, loss: 0.0524Epoch 2/10: [======================        ] 36/47 batches, loss: 0.0520Epoch 2/10: [=======================       ] 37/47 batches, loss: 0.0514Epoch 2/10: [========================      ] 38/47 batches, loss: 0.0512Epoch 2/10: [========================      ] 39/47 batches, loss: 0.0514Epoch 2/10: [=========================     ] 40/47 batches, loss: 0.0511Epoch 2/10: [==========================    ] 41/47 batches, loss: 0.0506Epoch 2/10: [==========================    ] 42/47 batches, loss: 0.0501Epoch 2/10: [===========================   ] 43/47 batches, loss: 0.0502Epoch 2/10: [============================  ] 44/47 batches, loss: 0.0497Epoch 2/10: [============================  ] 45/47 batches, loss: 0.0490Epoch 2/10: [============================= ] 46/47 batches, loss: 0.0481Epoch 2/10: [==============================] 47/47 batches, loss: 0.0479
[2025-04-29 21:40:52,990][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0479
[2025-04-29 21:40:53,537][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0798, Metrics: {'mse': 0.08441633731126785, 'rmse': 0.29054489723839216, 'r2': -0.7918640375137329}
Epoch 3/10: [Epoch 3/10: [                              ] 1/47 batches, loss: 0.0303Epoch 3/10: [=                             ] 2/47 batches, loss: 0.0206Epoch 3/10: [=                             ] 3/47 batches, loss: 0.0217Epoch 3/10: [==                            ] 4/47 batches, loss: 0.0225Epoch 3/10: [===                           ] 5/47 batches, loss: 0.0214Epoch 3/10: [===                           ] 6/47 batches, loss: 0.0210Epoch 3/10: [====                          ] 7/47 batches, loss: 0.0210Epoch 3/10: [=====                         ] 8/47 batches, loss: 0.0206Epoch 3/10: [=====                         ] 9/47 batches, loss: 0.0210Epoch 3/10: [======                        ] 10/47 batches, loss: 0.0212Epoch 3/10: [=======                       ] 11/47 batches, loss: 0.0212Epoch 3/10: [=======                       ] 12/47 batches, loss: 0.0213Epoch 3/10: [========                      ] 13/47 batches, loss: 0.0218Epoch 3/10: [========                      ] 14/47 batches, loss: 0.0219Epoch 3/10: [=========                     ] 15/47 batches, loss: 0.0223Epoch 3/10: [==========                    ] 16/47 batches, loss: 0.0225Epoch 3/10: [==========                    ] 17/47 batches, loss: 0.0229Epoch 3/10: [===========                   ] 18/47 batches, loss: 0.0223Epoch 3/10: [============                  ] 19/47 batches, loss: 0.0217Epoch 3/10: [============                  ] 20/47 batches, loss: 0.0217Epoch 3/10: [=============                 ] 21/47 batches, loss: 0.0218Epoch 3/10: [==============                ] 22/47 batches, loss: 0.0218Epoch 3/10: [==============                ] 23/47 batches, loss: 0.0221Epoch 3/10: [===============               ] 24/47 batches, loss: 0.0223Epoch 3/10: [===============               ] 25/47 batches, loss: 0.0225Epoch 3/10: [================              ] 26/47 batches, loss: 0.0221Epoch 3/10: [=================             ] 27/47 batches, loss: 0.0219Epoch 3/10: [=================             ] 28/47 batches, loss: 0.0217Epoch 3/10: [==================            ] 29/47 batches, loss: 0.0219Epoch 3/10: [===================           ] 30/47 batches, loss: 0.0220Epoch 3/10: [===================           ] 31/47 batches, loss: 0.0232Epoch 3/10: [====================          ] 32/47 batches, loss: 0.0232Epoch 3/10: [=====================         ] 33/47 batches, loss: 0.0231Epoch 3/10: [=====================         ] 34/47 batches, loss: 0.0236Epoch 3/10: [======================        ] 35/47 batches, loss: 0.0235Epoch 3/10: [======================        ] 36/47 batches, loss: 0.0233Epoch 3/10: [=======================       ] 37/47 batches, loss: 0.0235Epoch 3/10: [========================      ] 38/47 batches, loss: 0.0239Epoch 3/10: [========================      ] 39/47 batches, loss: 0.0241Epoch 3/10: [=========================     ] 40/47 batches, loss: 0.0239Epoch 3/10: [==========================    ] 41/47 batches, loss: 0.0239Epoch 3/10: [==========================    ] 42/47 batches, loss: 0.0239Epoch 3/10: [===========================   ] 43/47 batches, loss: 0.0237Epoch 3/10: [============================  ] 44/47 batches, loss: 0.0238Epoch 3/10: [============================  ] 45/47 batches, loss: 0.0241Epoch 3/10: [============================= ] 46/47 batches, loss: 0.0247Epoch 3/10: [==============================] 47/47 batches, loss: 0.0242
[2025-04-29 21:41:03,700][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0242
[2025-04-29 21:41:04,135][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0335, Metrics: {'mse': 0.03511868417263031, 'rmse': 0.1873997976856707, 'r2': 0.254552960395813}
Epoch 4/10: [Epoch 4/10: [                              ] 1/47 batches, loss: 0.0197Epoch 4/10: [=                             ] 2/47 batches, loss: 0.0197Epoch 4/10: [=                             ] 3/47 batches, loss: 0.0221Epoch 4/10: [==                            ] 4/47 batches, loss: 0.0237Epoch 4/10: [===                           ] 5/47 batches, loss: 0.0263Epoch 4/10: [===                           ] 6/47 batches, loss: 0.0253Epoch 4/10: [====                          ] 7/47 batches, loss: 0.0235Epoch 4/10: [=====                         ] 8/47 batches, loss: 0.0229Epoch 4/10: [=====                         ] 9/47 batches, loss: 0.0223Epoch 4/10: [======                        ] 10/47 batches, loss: 0.0219Epoch 4/10: [=======                       ] 11/47 batches, loss: 0.0205Epoch 4/10: [=======                       ] 12/47 batches, loss: 0.0202Epoch 4/10: [========                      ] 13/47 batches, loss: 0.0206Epoch 4/10: [========                      ] 14/47 batches, loss: 0.0203Epoch 4/10: [=========                     ] 15/47 batches, loss: 0.0198Epoch 4/10: [==========                    ] 16/47 batches, loss: 0.0191Epoch 4/10: [==========                    ] 17/47 batches, loss: 0.0187Epoch 4/10: [===========                   ] 18/47 batches, loss: 0.0184Epoch 4/10: [============                  ] 19/47 batches, loss: 0.0190Epoch 4/10: [============                  ] 20/47 batches, loss: 0.0192Epoch 4/10: [=============                 ] 21/47 batches, loss: 0.0196Epoch 4/10: [==============                ] 22/47 batches, loss: 0.0193Epoch 4/10: [==============                ] 23/47 batches, loss: 0.0188Epoch 4/10: [===============               ] 24/47 batches, loss: 0.0188Epoch 4/10: [===============               ] 25/47 batches, loss: 0.0184Epoch 4/10: [================              ] 26/47 batches, loss: 0.0184Epoch 4/10: [=================             ] 27/47 batches, loss: 0.0179Epoch 4/10: [=================             ] 28/47 batches, loss: 0.0180Epoch 4/10: [==================            ] 29/47 batches, loss: 0.0180Epoch 4/10: [===================           ] 30/47 batches, loss: 0.0180Epoch 4/10: [===================           ] 31/47 batches, loss: 0.0179Epoch 4/10: [====================          ] 32/47 batches, loss: 0.0178Epoch 4/10: [=====================         ] 33/47 batches, loss: 0.0180Epoch 4/10: [=====================         ] 34/47 batches, loss: 0.0181Epoch 4/10: [======================        ] 35/47 batches, loss: 0.0184Epoch 4/10: [======================        ] 36/47 batches, loss: 0.0183Epoch 4/10: [=======================       ] 37/47 batches, loss: 0.0182Epoch 4/10: [========================      ] 38/47 batches, loss: 0.0181Epoch 4/10: [========================      ] 39/47 batches, loss: 0.0181Epoch 4/10: [=========================     ] 40/47 batches, loss: 0.0181Epoch 4/10: [==========================    ] 41/47 batches, loss: 0.0181Epoch 4/10: [==========================    ] 42/47 batches, loss: 0.0181Epoch 4/10: [===========================   ] 43/47 batches, loss: 0.0179Epoch 4/10: [============================  ] 44/47 batches, loss: 0.0178Epoch 4/10: [============================  ] 45/47 batches, loss: 0.0182Epoch 4/10: [============================= ] 46/47 batches, loss: 0.0184Epoch 4/10: [==============================] 47/47 batches, loss: 0.0182
[2025-04-29 21:41:14,191][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0182
[2025-04-29 21:41:14,610][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0364, Metrics: {'mse': 0.038128312677145004, 'rmse': 0.1952647246103223, 'r2': 0.19066905975341797}
[2025-04-29 21:41:14,610][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/47 batches, loss: 0.0161Epoch 5/10: [=                             ] 2/47 batches, loss: 0.0201Epoch 5/10: [=                             ] 3/47 batches, loss: 0.0217Epoch 5/10: [==                            ] 4/47 batches, loss: 0.0236Epoch 5/10: [===                           ] 5/47 batches, loss: 0.0228Epoch 5/10: [===                           ] 6/47 batches, loss: 0.0232Epoch 5/10: [====                          ] 7/47 batches, loss: 0.0227Epoch 5/10: [=====                         ] 8/47 batches, loss: 0.0215Epoch 5/10: [=====                         ] 9/47 batches, loss: 0.0207Epoch 5/10: [======                        ] 10/47 batches, loss: 0.0202Epoch 5/10: [=======                       ] 11/47 batches, loss: 0.0197Epoch 5/10: [=======                       ] 12/47 batches, loss: 0.0194Epoch 5/10: [========                      ] 13/47 batches, loss: 0.0187Epoch 5/10: [========                      ] 14/47 batches, loss: 0.0193Epoch 5/10: [=========                     ] 15/47 batches, loss: 0.0209Epoch 5/10: [==========                    ] 16/47 batches, loss: 0.0212Epoch 5/10: [==========                    ] 17/47 batches, loss: 0.0214Epoch 5/10: [===========                   ] 18/47 batches, loss: 0.0222Epoch 5/10: [============                  ] 19/47 batches, loss: 0.0217Epoch 5/10: [============                  ] 20/47 batches, loss: 0.0215Epoch 5/10: [=============                 ] 21/47 batches, loss: 0.0215Epoch 5/10: [==============                ] 22/47 batches, loss: 0.0223Epoch 5/10: [==============                ] 23/47 batches, loss: 0.0222Epoch 5/10: [===============               ] 24/47 batches, loss: 0.0228Epoch 5/10: [===============               ] 25/47 batches, loss: 0.0234Epoch 5/10: [================              ] 26/47 batches, loss: 0.0238Epoch 5/10: [=================             ] 27/47 batches, loss: 0.0242Epoch 5/10: [=================             ] 28/47 batches, loss: 0.0245Epoch 5/10: [==================            ] 29/47 batches, loss: 0.0247Epoch 5/10: [===================           ] 30/47 batches, loss: 0.0249Epoch 5/10: [===================           ] 31/47 batches, loss: 0.0250Epoch 5/10: [====================          ] 32/47 batches, loss: 0.0249Epoch 5/10: [=====================         ] 33/47 batches, loss: 0.0246Epoch 5/10: [=====================         ] 34/47 batches, loss: 0.0251Epoch 5/10: [======================        ] 35/47 batches, loss: 0.0249Epoch 5/10: [======================        ] 36/47 batches, loss: 0.0250Epoch 5/10: [=======================       ] 37/47 batches, loss: 0.0253Epoch 5/10: [========================      ] 38/47 batches, loss: 0.0254Epoch 5/10: [========================      ] 39/47 batches, loss: 0.0251Epoch 5/10: [=========================     ] 40/47 batches, loss: 0.0262Epoch 5/10: [==========================    ] 41/47 batches, loss: 0.0262Epoch 5/10: [==========================    ] 42/47 batches, loss: 0.0259Epoch 5/10: [===========================   ] 43/47 batches, loss: 0.0255Epoch 5/10: [============================  ] 44/47 batches, loss: 0.0254Epoch 5/10: [============================  ] 45/47 batches, loss: 0.0253Epoch 5/10: [============================= ] 46/47 batches, loss: 0.0252Epoch 5/10: [==============================] 47/47 batches, loss: 0.0249
[2025-04-29 21:41:24,155][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0249
[2025-04-29 21:41:24,572][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0244, Metrics: {'mse': 0.023718159645795822, 'rmse': 0.1540070116773773, 'r2': 0.49654626846313477}
Epoch 6/10: [Epoch 6/10: [                              ] 1/47 batches, loss: 0.0153Epoch 6/10: [=                             ] 2/47 batches, loss: 0.0123Epoch 6/10: [=                             ] 3/47 batches, loss: 0.0129Epoch 6/10: [==                            ] 4/47 batches, loss: 0.0155Epoch 6/10: [===                           ] 5/47 batches, loss: 0.0150Epoch 6/10: [===                           ] 6/47 batches, loss: 0.0149Epoch 6/10: [====                          ] 7/47 batches, loss: 0.0155Epoch 6/10: [=====                         ] 8/47 batches, loss: 0.0164Epoch 6/10: [=====                         ] 9/47 batches, loss: 0.0159Epoch 6/10: [======                        ] 10/47 batches, loss: 0.0155Epoch 6/10: [=======                       ] 11/47 batches, loss: 0.0150Epoch 6/10: [=======                       ] 12/47 batches, loss: 0.0149Epoch 6/10: [========                      ] 13/47 batches, loss: 0.0148Epoch 6/10: [========                      ] 14/47 batches, loss: 0.0150Epoch 6/10: [=========                     ] 15/47 batches, loss: 0.0150Epoch 6/10: [==========                    ] 16/47 batches, loss: 0.0148Epoch 6/10: [==========                    ] 17/47 batches, loss: 0.0144Epoch 6/10: [===========                   ] 18/47 batches, loss: 0.0145Epoch 6/10: [============                  ] 19/47 batches, loss: 0.0142Epoch 6/10: [============                  ] 20/47 batches, loss: 0.0150Epoch 6/10: [=============                 ] 21/47 batches, loss: 0.0149Epoch 6/10: [==============                ] 22/47 batches, loss: 0.0150Epoch 6/10: [==============                ] 23/47 batches, loss: 0.0150Epoch 6/10: [===============               ] 24/47 batches, loss: 0.0148Epoch 6/10: [===============               ] 25/47 batches, loss: 0.0147Epoch 6/10: [================              ] 26/47 batches, loss: 0.0146Epoch 6/10: [=================             ] 27/47 batches, loss: 0.0143Epoch 6/10: [=================             ] 28/47 batches, loss: 0.0144Epoch 6/10: [==================            ] 29/47 batches, loss: 0.0143Epoch 6/10: [===================           ] 30/47 batches, loss: 0.0142Epoch 6/10: [===================           ] 31/47 batches, loss: 0.0143Epoch 6/10: [====================          ] 32/47 batches, loss: 0.0140Epoch 6/10: [=====================         ] 33/47 batches, loss: 0.0140Epoch 6/10: [=====================         ] 34/47 batches, loss: 0.0142Epoch 6/10: [======================        ] 35/47 batches, loss: 0.0142Epoch 6/10: [======================        ] 36/47 batches, loss: 0.0141Epoch 6/10: [=======================       ] 37/47 batches, loss: 0.0140Epoch 6/10: [========================      ] 38/47 batches, loss: 0.0139Epoch 6/10: [========================      ] 39/47 batches, loss: 0.0141Epoch 6/10: [=========================     ] 40/47 batches, loss: 0.0142Epoch 6/10: [==========================    ] 41/47 batches, loss: 0.0144Epoch 6/10: [==========================    ] 42/47 batches, loss: 0.0145Epoch 6/10: [===========================   ] 43/47 batches, loss: 0.0147Epoch 6/10: [============================  ] 44/47 batches, loss: 0.0148Epoch 6/10: [============================  ] 45/47 batches, loss: 0.0150Epoch 6/10: [============================= ] 46/47 batches, loss: 0.0149Epoch 6/10: [==============================] 47/47 batches, loss: 0.0148
[2025-04-29 21:41:34,661][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0148
[2025-04-29 21:41:35,192][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0448, Metrics: {'mse': 0.04764651507139206, 'rmse': 0.21828081700275923, 'r2': -0.011369109153747559}
[2025-04-29 21:41:35,193][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/47 batches, loss: 0.0063Epoch 7/10: [=                             ] 2/47 batches, loss: 0.0096Epoch 7/10: [=                             ] 3/47 batches, loss: 0.0108Epoch 7/10: [==                            ] 4/47 batches, loss: 0.0142Epoch 7/10: [===                           ] 5/47 batches, loss: 0.0151Epoch 7/10: [===                           ] 6/47 batches, loss: 0.0177Epoch 7/10: [====                          ] 7/47 batches, loss: 0.0168Epoch 7/10: [=====                         ] 8/47 batches, loss: 0.0161Epoch 7/10: [=====                         ] 9/47 batches, loss: 0.0153Epoch 7/10: [======                        ] 10/47 batches, loss: 0.0147Epoch 7/10: [=======                       ] 11/47 batches, loss: 0.0139Epoch 7/10: [=======                       ] 12/47 batches, loss: 0.0133Epoch 7/10: [========                      ] 13/47 batches, loss: 0.0138Epoch 7/10: [========                      ] 14/47 batches, loss: 0.0147Epoch 7/10: [=========                     ] 15/47 batches, loss: 0.0151Epoch 7/10: [==========                    ] 16/47 batches, loss: 0.0146Epoch 7/10: [==========                    ] 17/47 batches, loss: 0.0147Epoch 7/10: [===========                   ] 18/47 batches, loss: 0.0153Epoch 7/10: [============                  ] 19/47 batches, loss: 0.0150Epoch 7/10: [============                  ] 20/47 batches, loss: 0.0150Epoch 7/10: [=============                 ] 21/47 batches, loss: 0.0154Epoch 7/10: [==============                ] 22/47 batches, loss: 0.0151Epoch 7/10: [==============                ] 23/47 batches, loss: 0.0148Epoch 7/10: [===============               ] 24/47 batches, loss: 0.0149Epoch 7/10: [===============               ] 25/47 batches, loss: 0.0152Epoch 7/10: [================              ] 26/47 batches, loss: 0.0153Epoch 7/10: [=================             ] 27/47 batches, loss: 0.0150Epoch 7/10: [=================             ] 28/47 batches, loss: 0.0148Epoch 7/10: [==================            ] 29/47 batches, loss: 0.0147Epoch 7/10: [===================           ] 30/47 batches, loss: 0.0151Epoch 7/10: [===================           ] 31/47 batches, loss: 0.0151Epoch 7/10: [====================          ] 32/47 batches, loss: 0.0152Epoch 7/10: [=====================         ] 33/47 batches, loss: 0.0153Epoch 7/10: [=====================         ] 34/47 batches, loss: 0.0151Epoch 7/10: [======================        ] 35/47 batches, loss: 0.0152Epoch 7/10: [======================        ] 36/47 batches, loss: 0.0152Epoch 7/10: [=======================       ] 37/47 batches, loss: 0.0153Epoch 7/10: [========================      ] 38/47 batches, loss: 0.0150Epoch 7/10: [========================      ] 39/47 batches, loss: 0.0150Epoch 7/10: [=========================     ] 40/47 batches, loss: 0.0151Epoch 7/10: [==========================    ] 41/47 batches, loss: 0.0149Epoch 7/10: [==========================    ] 42/47 batches, loss: 0.0148Epoch 7/10: [===========================   ] 43/47 batches, loss: 0.0148Epoch 7/10: [============================  ] 44/47 batches, loss: 0.0152Epoch 7/10: [============================  ] 45/47 batches, loss: 0.0153Epoch 7/10: [============================= ] 46/47 batches, loss: 0.0151Epoch 7/10: [==============================] 47/47 batches, loss: 0.0155
[2025-04-29 21:41:44,746][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0155
[2025-04-29 21:41:45,208][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0269, Metrics: {'mse': 0.028340790420770645, 'rmse': 0.16834723169915994, 'r2': 0.398423969745636}
[2025-04-29 21:41:45,209][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/47 batches, loss: 0.0101Epoch 8/10: [=                             ] 2/47 batches, loss: 0.0117Epoch 8/10: [=                             ] 3/47 batches, loss: 0.0181Epoch 8/10: [==                            ] 4/47 batches, loss: 0.0173Epoch 8/10: [===                           ] 5/47 batches, loss: 0.0202Epoch 8/10: [===                           ] 6/47 batches, loss: 0.0211Epoch 8/10: [====                          ] 7/47 batches, loss: 0.0203Epoch 8/10: [=====                         ] 8/47 batches, loss: 0.0193Epoch 8/10: [=====                         ] 9/47 batches, loss: 0.0187Epoch 8/10: [======                        ] 10/47 batches, loss: 0.0181Epoch 8/10: [=======                       ] 11/47 batches, loss: 0.0185Epoch 8/10: [=======                       ] 12/47 batches, loss: 0.0183Epoch 8/10: [========                      ] 13/47 batches, loss: 0.0187Epoch 8/10: [========                      ] 14/47 batches, loss: 0.0190Epoch 8/10: [=========                     ] 15/47 batches, loss: 0.0186Epoch 8/10: [==========                    ] 16/47 batches, loss: 0.0185Epoch 8/10: [==========                    ] 17/47 batches, loss: 0.0181Epoch 8/10: [===========                   ] 18/47 batches, loss: 0.0181Epoch 8/10: [============                  ] 19/47 batches, loss: 0.0180Epoch 8/10: [============                  ] 20/47 batches, loss: 0.0181Epoch 8/10: [=============                 ] 21/47 batches, loss: 0.0177Epoch 8/10: [==============                ] 22/47 batches, loss: 0.0174Epoch 8/10: [==============                ] 23/47 batches, loss: 0.0170Epoch 8/10: [===============               ] 24/47 batches, loss: 0.0168Epoch 8/10: [===============               ] 25/47 batches, loss: 0.0168Epoch 8/10: [================              ] 26/47 batches, loss: 0.0165Epoch 8/10: [=================             ] 27/47 batches, loss: 0.0165Epoch 8/10: [=================             ] 28/47 batches, loss: 0.0161Epoch 8/10: [==================            ] 29/47 batches, loss: 0.0161Epoch 8/10: [===================           ] 30/47 batches, loss: 0.0160Epoch 8/10: [===================           ] 31/47 batches, loss: 0.0159Epoch 8/10: [====================          ] 32/47 batches, loss: 0.0156Epoch 8/10: [=====================         ] 33/47 batches, loss: 0.0155Epoch 8/10: [=====================         ] 34/47 batches, loss: 0.0151Epoch 8/10: [======================        ] 35/47 batches, loss: 0.0150Epoch 8/10: [======================        ] 36/47 batches, loss: 0.0149Epoch 8/10: [=======================       ] 37/47 batches, loss: 0.0148Epoch 8/10: [========================      ] 38/47 batches, loss: 0.0147Epoch 8/10: [========================      ] 39/47 batches, loss: 0.0145Epoch 8/10: [=========================     ] 40/47 batches, loss: 0.0147Epoch 8/10: [==========================    ] 41/47 batches, loss: 0.0146Epoch 8/10: [==========================    ] 42/47 batches, loss: 0.0145Epoch 8/10: [===========================   ] 43/47 batches, loss: 0.0143Epoch 8/10: [============================  ] 44/47 batches, loss: 0.0142Epoch 8/10: [============================  ] 45/47 batches, loss: 0.0141Epoch 8/10: [============================= ] 46/47 batches, loss: 0.0141Epoch 8/10: [==============================] 47/47 batches, loss: 0.0140
[2025-04-29 21:41:54,765][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0140
[2025-04-29 21:41:55,166][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0156, Metrics: {'mse': 0.015568256378173828, 'rmse': 0.12477281906799184, 'r2': 0.669540286064148}
Epoch 9/10: [Epoch 9/10: [                              ] 1/47 batches, loss: 0.0140Epoch 9/10: [=                             ] 2/47 batches, loss: 0.0111Epoch 9/10: [=                             ] 3/47 batches, loss: 0.0094Epoch 9/10: [==                            ] 4/47 batches, loss: 0.0093Epoch 9/10: [===                           ] 5/47 batches, loss: 0.0094Epoch 9/10: [===                           ] 6/47 batches, loss: 0.0099Epoch 9/10: [====                          ] 7/47 batches, loss: 0.0093Epoch 9/10: [=====                         ] 8/47 batches, loss: 0.0091Epoch 9/10: [=====                         ] 9/47 batches, loss: 0.0087Epoch 9/10: [======                        ] 10/47 batches, loss: 0.0096Epoch 9/10: [=======                       ] 11/47 batches, loss: 0.0097Epoch 9/10: [=======                       ] 12/47 batches, loss: 0.0096Epoch 9/10: [========                      ] 13/47 batches, loss: 0.0098Epoch 9/10: [========                      ] 14/47 batches, loss: 0.0098Epoch 9/10: [=========                     ] 15/47 batches, loss: 0.0104Epoch 9/10: [==========                    ] 16/47 batches, loss: 0.0104Epoch 9/10: [==========                    ] 17/47 batches, loss: 0.0101Epoch 9/10: [===========                   ] 18/47 batches, loss: 0.0101Epoch 9/10: [============                  ] 19/47 batches, loss: 0.0099Epoch 9/10: [============                  ] 20/47 batches, loss: 0.0098Epoch 9/10: [=============                 ] 21/47 batches, loss: 0.0098Epoch 9/10: [==============                ] 22/47 batches, loss: 0.0096Epoch 9/10: [==============                ] 23/47 batches, loss: 0.0094Epoch 9/10: [===============               ] 24/47 batches, loss: 0.0093Epoch 9/10: [===============               ] 25/47 batches, loss: 0.0092Epoch 9/10: [================              ] 26/47 batches, loss: 0.0095Epoch 9/10: [=================             ] 27/47 batches, loss: 0.0097Epoch 9/10: [=================             ] 28/47 batches, loss: 0.0097Epoch 9/10: [==================            ] 29/47 batches, loss: 0.0096Epoch 9/10: [===================           ] 30/47 batches, loss: 0.0097Epoch 9/10: [===================           ] 31/47 batches, loss: 0.0096Epoch 9/10: [====================          ] 32/47 batches, loss: 0.0095Epoch 9/10: [=====================         ] 33/47 batches, loss: 0.0094Epoch 9/10: [=====================         ] 34/47 batches, loss: 0.0093Epoch 9/10: [======================        ] 35/47 batches, loss: 0.0092Epoch 9/10: [======================        ] 36/47 batches, loss: 0.0091Epoch 9/10: [=======================       ] 37/47 batches, loss: 0.0091Epoch 9/10: [========================      ] 38/47 batches, loss: 0.0090Epoch 9/10: [========================      ] 39/47 batches, loss: 0.0089Epoch 9/10: [=========================     ] 40/47 batches, loss: 0.0088Epoch 9/10: [==========================    ] 41/47 batches, loss: 0.0087Epoch 9/10: [==========================    ] 42/47 batches, loss: 0.0087Epoch 9/10: [===========================   ] 43/47 batches, loss: 0.0087Epoch 9/10: [============================  ] 44/47 batches, loss: 0.0086Epoch 9/10: [============================  ] 45/47 batches, loss: 0.0086Epoch 9/10: [============================= ] 46/47 batches, loss: 0.0087Epoch 9/10: [==============================] 47/47 batches, loss: 0.0088
[2025-04-29 21:42:05,313][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0088
[2025-04-29 21:42:05,730][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0147, Metrics: {'mse': 0.014456884935498238, 'rmse': 0.12023678694766522, 'r2': 0.6931308507919312}
Epoch 10/10: [Epoch 10/10: [                              ] 1/47 batches, loss: 0.0099Epoch 10/10: [=                             ] 2/47 batches, loss: 0.0111Epoch 10/10: [=                             ] 3/47 batches, loss: 0.0090Epoch 10/10: [==                            ] 4/47 batches, loss: 0.0092Epoch 10/10: [===                           ] 5/47 batches, loss: 0.0105Epoch 10/10: [===                           ] 6/47 batches, loss: 0.0111Epoch 10/10: [====                          ] 7/47 batches, loss: 0.0111Epoch 10/10: [=====                         ] 8/47 batches, loss: 0.0104Epoch 10/10: [=====                         ] 9/47 batches, loss: 0.0106Epoch 10/10: [======                        ] 10/47 batches, loss: 0.0106Epoch 10/10: [=======                       ] 11/47 batches, loss: 0.0117Epoch 10/10: [=======                       ] 12/47 batches, loss: 0.0118Epoch 10/10: [========                      ] 13/47 batches, loss: 0.0123Epoch 10/10: [========                      ] 14/47 batches, loss: 0.0120Epoch 10/10: [=========                     ] 15/47 batches, loss: 0.0119Epoch 10/10: [==========                    ] 16/47 batches, loss: 0.0117Epoch 10/10: [==========                    ] 17/47 batches, loss: 0.0118Epoch 10/10: [===========                   ] 18/47 batches, loss: 0.0116Epoch 10/10: [============                  ] 19/47 batches, loss: 0.0114Epoch 10/10: [============                  ] 20/47 batches, loss: 0.0112Epoch 10/10: [=============                 ] 21/47 batches, loss: 0.0110Epoch 10/10: [==============                ] 22/47 batches, loss: 0.0110Epoch 10/10: [==============                ] 23/47 batches, loss: 0.0109Epoch 10/10: [===============               ] 24/47 batches, loss: 0.0106Epoch 10/10: [===============               ] 25/47 batches, loss: 0.0105Epoch 10/10: [================              ] 26/47 batches, loss: 0.0104Epoch 10/10: [=================             ] 27/47 batches, loss: 0.0104Epoch 10/10: [=================             ] 28/47 batches, loss: 0.0103Epoch 10/10: [==================            ] 29/47 batches, loss: 0.0102Epoch 10/10: [===================           ] 30/47 batches, loss: 0.0100Epoch 10/10: [===================           ] 31/47 batches, loss: 0.0101Epoch 10/10: [====================          ] 32/47 batches, loss: 0.0101Epoch 10/10: [=====================         ] 33/47 batches, loss: 0.0100Epoch 10/10: [=====================         ] 34/47 batches, loss: 0.0100Epoch 10/10: [======================        ] 35/47 batches, loss: 0.0100Epoch 10/10: [======================        ] 36/47 batches, loss: 0.0099Epoch 10/10: [=======================       ] 37/47 batches, loss: 0.0099Epoch 10/10: [========================      ] 38/47 batches, loss: 0.0098Epoch 10/10: [========================      ] 39/47 batches, loss: 0.0097Epoch 10/10: [=========================     ] 40/47 batches, loss: 0.0096Epoch 10/10: [==========================    ] 41/47 batches, loss: 0.0096Epoch 10/10: [==========================    ] 42/47 batches, loss: 0.0095Epoch 10/10: [===========================   ] 43/47 batches, loss: 0.0095Epoch 10/10: [============================  ] 44/47 batches, loss: 0.0096Epoch 10/10: [============================  ] 45/47 batches, loss: 0.0095Epoch 10/10: [============================= ] 46/47 batches, loss: 0.0095Epoch 10/10: [==============================] 47/47 batches, loss: 0.0093
[2025-04-29 21:42:15,862][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0093
[2025-04-29 21:42:16,270][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0312, Metrics: {'mse': 0.03166913241147995, 'rmse': 0.17795823221048232, 'r2': 0.32777488231658936}
[2025-04-29 21:42:16,271][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-04-29 21:42:16,271][src.training.lm_trainer][INFO] - Training completed in 104.37 seconds
[2025-04-29 21:42:16,271][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:42:20,302][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.011384602636098862, 'rmse': 0.10669865339402772, 'r2': 0.48859769105911255}
[2025-04-29 21:42:20,302][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.014456884935498238, 'rmse': 0.12023678694766522, 'r2': 0.6931308507919312}
[2025-04-29 21:42:20,303][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.018415041267871857, 'rmse': 0.1357020311855053, 'r2': 0.4282096028327942}
[2025-04-29 21:42:22,548][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ko/ko/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▂▁▁
wandb:     best_val_mse █▅▂▂▁▁
wandb:      best_val_r2 ▁▄▇▇██
wandb:    best_val_rmse █▆▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▃▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▂▂▃▂▁▁▂
wandb:          val_mse █▅▂▂▂▃▂▁▁▂
wandb:           val_r2 ▁▄▇▇▇▆▇██▇
wandb:         val_rmse █▆▃▃▂▄▂▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01469
wandb:     best_val_mse 0.01446
wandb:      best_val_r2 0.69313
wandb:    best_val_rmse 0.12024
wandb:            epoch 10
wandb:   final_test_mse 0.01842
wandb:    final_test_r2 0.42821
wandb:  final_test_rmse 0.1357
wandb:  final_train_mse 0.01138
wandb:   final_train_r2 0.4886
wandb: final_train_rmse 0.1067
wandb:    final_val_mse 0.01446
wandb:     final_val_r2 0.69313
wandb:   final_val_rmse 0.12024
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00933
wandb:       train_time 104.37248
wandb:         val_loss 0.03115
wandb:          val_mse 0.03167
wandb:           val_r2 0.32777
wandb:         val_rmse 0.17796
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_214019-powu5r86
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_214019-powu5r86/logs
Experiment finetune_complexity_ko completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ko/results.json
Running experiment: finetune_question_type_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ru"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:42:55,271][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ru
experiment_name: finetune_question_type_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:42:55,271][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:42:55,271][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:42:55,271][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:42:55,276][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-04-29 21:42:55,276][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:42:56,782][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:42:59,633][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:42:59,634][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:42:59,683][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:42:59,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:42:59,816][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-29 21:42:59,829][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:42:59,829][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-29 21:42:59,831][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:42:59,859][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:42:59,897][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:42:59,946][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-29 21:42:59,947][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:42:59,947][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-29 21:42:59,948][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:42:59,970][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:42:59,999][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:43:00,137][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-29 21:43:00,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:43:00,139][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-29 21:43:00,140][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-29 21:43:00,140][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:43:00,140][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:43:00,141][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:43:00,141][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:43:00,141][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 21:43:00,141][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-29 21:43:00,141][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-29 21:43:00,141][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:43:00,141][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:43:00,142][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:43:00,142][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:43:00,142][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:43:00,143][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:43:00,143][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-29 21:43:00,143][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:43:00,144][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:43:00,144][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:43:04,369][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:43:04,369][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,370][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,371][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,372][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,373][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,374][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,375][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,376][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,377][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,378][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,379][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,380][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,381][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,382][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,383][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,384][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,385][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:43:04,386][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:43:04,386][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:43:04,387][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:43:04,388][__main__][INFO] - Successfully created model for ru
[2025-04-29 21:43:04,388][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7097Epoch 1/10: [                              ] 2/75 batches, loss: 0.6927Epoch 1/10: [=                             ] 3/75 batches, loss: 0.6954Epoch 1/10: [=                             ] 4/75 batches, loss: 0.6982Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6983Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6944Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6915Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6907Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6925Epoch 1/10: [====                          ] 10/75 batches, loss: 0.6939Epoch 1/10: [====                          ] 11/75 batches, loss: 0.6926Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6916Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.6883Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.6890Epoch 1/10: [======                        ] 15/75 batches, loss: 0.6882Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6891Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6890Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6898Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6910Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6892Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6907Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6910Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6910Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6911Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6914Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6919Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6919Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6905Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6914Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6902Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6891Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6887Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6898Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6890Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6890Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6891Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6888Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6879Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6877Epoch 1/10: [================              ] 40/75 batches, loss: 0.6876Epoch 1/10: [================              ] 41/75 batches, loss: 0.6875Epoch 1/10: [================              ] 42/75 batches, loss: 0.6871Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6872Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6880Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6881Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6878Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6881Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6883Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6881Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6877Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6874Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6871Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6874Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6864Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6866Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6864Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6863Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6860Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6854Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6855Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6857Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6855Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6853Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6848Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6849Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6846Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6839Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6833Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6827Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6820Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6821Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6809Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6810Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6806Epoch 1/10: [==============================] 75/75 batches, loss: 0.6798
[2025-04-29 21:43:22,022][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6798
[2025-04-29 21:43:22,420][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6830, Metrics: {'accuracy': 0.5, 'f1': 0.6666666666666666}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6520Epoch 2/10: [                              ] 2/75 batches, loss: 0.6562Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6428Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6551Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6447Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6464Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6479Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6464Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6407Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6402Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6430Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6434Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6401Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6376Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6370Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6401Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6383Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6358Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6316Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6265Epoch 2/10: [========                      ] 21/75 batches, loss: 0.6270Epoch 2/10: [========                      ] 22/75 batches, loss: 0.6280Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.6274Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.6263Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.6242Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.6231Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.6200Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.6209Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.6192Epoch 2/10: [============                  ] 30/75 batches, loss: 0.6182Epoch 2/10: [============                  ] 31/75 batches, loss: 0.6172Epoch 2/10: [============                  ] 32/75 batches, loss: 0.6165Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.6166Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.6183Epoch 2/10: [==============                ] 35/75 batches, loss: 0.6201Epoch 2/10: [==============                ] 36/75 batches, loss: 0.6191Epoch 2/10: [==============                ] 37/75 batches, loss: 0.6189Epoch 2/10: [===============               ] 38/75 batches, loss: 0.6193Epoch 2/10: [===============               ] 39/75 batches, loss: 0.6188Epoch 2/10: [================              ] 40/75 batches, loss: 0.6188Epoch 2/10: [================              ] 41/75 batches, loss: 0.6171Epoch 2/10: [================              ] 42/75 batches, loss: 0.6165Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6151Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6145Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6111Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6092Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6060Epoch 2/10: [===================           ] 48/75 batches, loss: 0.6053Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6041Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6018Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6021Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5990Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5977Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5947Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5943Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5945Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5935Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5924Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5903Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5893Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5881Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5854Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5833Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5829Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5817Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5807Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5787Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5768Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5754Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5729Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5706Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5689Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5676Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5656Epoch 2/10: [==============================] 75/75 batches, loss: 0.5644
[2025-04-29 21:43:38,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5644
[2025-04-29 21:43:38,739][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4932, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.4485Epoch 3/10: [                              ] 2/75 batches, loss: 0.4522Epoch 3/10: [=                             ] 3/75 batches, loss: 0.4438Epoch 3/10: [=                             ] 4/75 batches, loss: 0.4329Epoch 3/10: [==                            ] 5/75 batches, loss: 0.4425Epoch 3/10: [==                            ] 6/75 batches, loss: 0.4412Epoch 3/10: [==                            ] 7/75 batches, loss: 0.4301Epoch 3/10: [===                           ] 8/75 batches, loss: 0.4476Epoch 3/10: [===                           ] 9/75 batches, loss: 0.4388Epoch 3/10: [====                          ] 10/75 batches, loss: 0.4416Epoch 3/10: [====                          ] 11/75 batches, loss: 0.4405Epoch 3/10: [====                          ] 12/75 batches, loss: 0.4322Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.4282Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.4220Epoch 3/10: [======                        ] 15/75 batches, loss: 0.4150Epoch 3/10: [======                        ] 16/75 batches, loss: 0.4106Epoch 3/10: [======                        ] 17/75 batches, loss: 0.4034Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.4081Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.4055Epoch 3/10: [========                      ] 20/75 batches, loss: 0.4024Epoch 3/10: [========                      ] 21/75 batches, loss: 0.3961Epoch 3/10: [========                      ] 22/75 batches, loss: 0.3988Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.3971Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.3941Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.3956Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.3928Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.3895Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.3870Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.3826Epoch 3/10: [============                  ] 30/75 batches, loss: 0.3825Epoch 3/10: [============                  ] 31/75 batches, loss: 0.3803Epoch 3/10: [============                  ] 32/75 batches, loss: 0.3782Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.3763Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.3764Epoch 3/10: [==============                ] 35/75 batches, loss: 0.3729Epoch 3/10: [==============                ] 36/75 batches, loss: 0.3700Epoch 3/10: [==============                ] 37/75 batches, loss: 0.3660Epoch 3/10: [===============               ] 38/75 batches, loss: 0.3622Epoch 3/10: [===============               ] 39/75 batches, loss: 0.3595Epoch 3/10: [================              ] 40/75 batches, loss: 0.3563Epoch 3/10: [================              ] 41/75 batches, loss: 0.3508Epoch 3/10: [================              ] 42/75 batches, loss: 0.3462Epoch 3/10: [=================             ] 43/75 batches, loss: 0.3429Epoch 3/10: [=================             ] 44/75 batches, loss: 0.3399Epoch 3/10: [==================            ] 45/75 batches, loss: 0.3357Epoch 3/10: [==================            ] 46/75 batches, loss: 0.3341Epoch 3/10: [==================            ] 47/75 batches, loss: 0.3304Epoch 3/10: [===================           ] 48/75 batches, loss: 0.3273Epoch 3/10: [===================           ] 49/75 batches, loss: 0.3275Epoch 3/10: [====================          ] 50/75 batches, loss: 0.3245Epoch 3/10: [====================          ] 51/75 batches, loss: 0.3211Epoch 3/10: [====================          ] 52/75 batches, loss: 0.3189Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.3173Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.3154Epoch 3/10: [======================        ] 55/75 batches, loss: 0.3127Epoch 3/10: [======================        ] 56/75 batches, loss: 0.3091Epoch 3/10: [======================        ] 57/75 batches, loss: 0.3054Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.3035Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.3033Epoch 3/10: [========================      ] 60/75 batches, loss: 0.3039Epoch 3/10: [========================      ] 61/75 batches, loss: 0.3030Epoch 3/10: [========================      ] 62/75 batches, loss: 0.3010Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.3007Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.3019Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.3011Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.3004Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.2995Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.3020Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.3021Epoch 3/10: [============================  ] 70/75 batches, loss: 0.3028Epoch 3/10: [============================  ] 71/75 batches, loss: 0.3028Epoch 3/10: [============================  ] 72/75 batches, loss: 0.3067Epoch 3/10: [============================= ] 73/75 batches, loss: 0.3077Epoch 3/10: [============================= ] 74/75 batches, loss: 0.3091Epoch 3/10: [==============================] 75/75 batches, loss: 0.3087
[2025-04-29 21:43:54,622][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3087
[2025-04-29 21:43:55,033][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.3603, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.2473Epoch 4/10: [                              ] 2/75 batches, loss: 0.3776Epoch 4/10: [=                             ] 3/75 batches, loss: 0.3191Epoch 4/10: [=                             ] 4/75 batches, loss: 0.3431Epoch 4/10: [==                            ] 5/75 batches, loss: 0.3275Epoch 4/10: [==                            ] 6/75 batches, loss: 0.3066Epoch 4/10: [==                            ] 7/75 batches, loss: 0.3070Epoch 4/10: [===                           ] 8/75 batches, loss: 0.3056Epoch 4/10: [===                           ] 9/75 batches, loss: 0.2986Epoch 4/10: [====                          ] 10/75 batches, loss: 0.2870Epoch 4/10: [====                          ] 11/75 batches, loss: 0.2916Epoch 4/10: [====                          ] 12/75 batches, loss: 0.2831Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.2756Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.2713Epoch 4/10: [======                        ] 15/75 batches, loss: 0.2755Epoch 4/10: [======                        ] 16/75 batches, loss: 0.2699Epoch 4/10: [======                        ] 17/75 batches, loss: 0.2636Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.2611Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.2616Epoch 4/10: [========                      ] 20/75 batches, loss: 0.2611Epoch 4/10: [========                      ] 21/75 batches, loss: 0.2598Epoch 4/10: [========                      ] 22/75 batches, loss: 0.2587Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.2600Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.2566Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.2527Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.2477Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.2429Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.2387Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.2346Epoch 4/10: [============                  ] 30/75 batches, loss: 0.2306Epoch 4/10: [============                  ] 31/75 batches, loss: 0.2279Epoch 4/10: [============                  ] 32/75 batches, loss: 0.2237Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.2271Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.2242Epoch 4/10: [==============                ] 35/75 batches, loss: 0.2221Epoch 4/10: [==============                ] 36/75 batches, loss: 0.2234Epoch 4/10: [==============                ] 37/75 batches, loss: 0.2213Epoch 4/10: [===============               ] 38/75 batches, loss: 0.2179Epoch 4/10: [===============               ] 39/75 batches, loss: 0.2152Epoch 4/10: [================              ] 40/75 batches, loss: 0.2172Epoch 4/10: [================              ] 41/75 batches, loss: 0.2135Epoch 4/10: [================              ] 42/75 batches, loss: 0.2107Epoch 4/10: [=================             ] 43/75 batches, loss: 0.2080Epoch 4/10: [=================             ] 44/75 batches, loss: 0.2069Epoch 4/10: [==================            ] 45/75 batches, loss: 0.2035Epoch 4/10: [==================            ] 46/75 batches, loss: 0.2080Epoch 4/10: [==================            ] 47/75 batches, loss: 0.2077Epoch 4/10: [===================           ] 48/75 batches, loss: 0.2102Epoch 4/10: [===================           ] 49/75 batches, loss: 0.2169Epoch 4/10: [====================          ] 50/75 batches, loss: 0.2158Epoch 4/10: [====================          ] 51/75 batches, loss: 0.2136Epoch 4/10: [====================          ] 52/75 batches, loss: 0.2131Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.2124Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.2125Epoch 4/10: [======================        ] 55/75 batches, loss: 0.2112Epoch 4/10: [======================        ] 56/75 batches, loss: 0.2107Epoch 4/10: [======================        ] 57/75 batches, loss: 0.2089Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.2075Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.2057Epoch 4/10: [========================      ] 60/75 batches, loss: 0.2047Epoch 4/10: [========================      ] 61/75 batches, loss: 0.2028Epoch 4/10: [========================      ] 62/75 batches, loss: 0.2023Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.2064Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.2081Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.2113Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.2093Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.2079Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.2113Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.2095Epoch 4/10: [============================  ] 70/75 batches, loss: 0.2088Epoch 4/10: [============================  ] 71/75 batches, loss: 0.2108Epoch 4/10: [============================  ] 72/75 batches, loss: 0.2101Epoch 4/10: [============================= ] 73/75 batches, loss: 0.2083Epoch 4/10: [============================= ] 74/75 batches, loss: 0.2067Epoch 4/10: [==============================] 75/75 batches, loss: 0.2044
[2025-04-29 21:44:10,837][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2044
[2025-04-29 21:44:11,286][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3050, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.3442Epoch 5/10: [                              ] 2/75 batches, loss: 0.2675Epoch 5/10: [=                             ] 3/75 batches, loss: 0.1975Epoch 5/10: [=                             ] 4/75 batches, loss: 0.1556Epoch 5/10: [==                            ] 5/75 batches, loss: 0.1395Epoch 5/10: [==                            ] 6/75 batches, loss: 0.1261Epoch 5/10: [==                            ] 7/75 batches, loss: 0.1463Epoch 5/10: [===                           ] 8/75 batches, loss: 0.1369Epoch 5/10: [===                           ] 9/75 batches, loss: 0.1379Epoch 5/10: [====                          ] 10/75 batches, loss: 0.1340Epoch 5/10: [====                          ] 11/75 batches, loss: 0.1284Epoch 5/10: [====                          ] 12/75 batches, loss: 0.1258Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.1315Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.1352Epoch 5/10: [======                        ] 15/75 batches, loss: 0.1351Epoch 5/10: [======                        ] 16/75 batches, loss: 0.1302Epoch 5/10: [======                        ] 17/75 batches, loss: 0.1321Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.1271Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.1223Epoch 5/10: [========                      ] 20/75 batches, loss: 0.1419Epoch 5/10: [========                      ] 21/75 batches, loss: 0.1364Epoch 5/10: [========                      ] 22/75 batches, loss: 0.1420Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.1372Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.1331Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.1297Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.1393Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.1366Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.1339Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.1319Epoch 5/10: [============                  ] 30/75 batches, loss: 0.1339Epoch 5/10: [============                  ] 31/75 batches, loss: 0.1310Epoch 5/10: [============                  ] 32/75 batches, loss: 0.1285Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.1357Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.1328Epoch 5/10: [==============                ] 35/75 batches, loss: 0.1304Epoch 5/10: [==============                ] 36/75 batches, loss: 0.1289Epoch 5/10: [==============                ] 37/75 batches, loss: 0.1265Epoch 5/10: [===============               ] 38/75 batches, loss: 0.1242Epoch 5/10: [===============               ] 39/75 batches, loss: 0.1273Epoch 5/10: [================              ] 40/75 batches, loss: 0.1281Epoch 5/10: [================              ] 41/75 batches, loss: 0.1262Epoch 5/10: [================              ] 42/75 batches, loss: 0.1294Epoch 5/10: [=================             ] 43/75 batches, loss: 0.1277Epoch 5/10: [=================             ] 44/75 batches, loss: 0.1333Epoch 5/10: [==================            ] 45/75 batches, loss: 0.1384Epoch 5/10: [==================            ] 46/75 batches, loss: 0.1371Epoch 5/10: [==================            ] 47/75 batches, loss: 0.1367Epoch 5/10: [===================           ] 48/75 batches, loss: 0.1350Epoch 5/10: [===================           ] 49/75 batches, loss: 0.1333Epoch 5/10: [====================          ] 50/75 batches, loss: 0.1320Epoch 5/10: [====================          ] 51/75 batches, loss: 0.1325Epoch 5/10: [====================          ] 52/75 batches, loss: 0.1306Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.1293Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.1280Epoch 5/10: [======================        ] 55/75 batches, loss: 0.1262Epoch 5/10: [======================        ] 56/75 batches, loss: 0.1282Epoch 5/10: [======================        ] 57/75 batches, loss: 0.1265Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.1251Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.1236Epoch 5/10: [========================      ] 60/75 batches, loss: 0.1241Epoch 5/10: [========================      ] 61/75 batches, loss: 0.1227Epoch 5/10: [========================      ] 62/75 batches, loss: 0.1213Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.1202Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.1193Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.1230Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.1220Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.1238Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.1250Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.1265Epoch 5/10: [============================  ] 70/75 batches, loss: 0.1252Epoch 5/10: [============================  ] 71/75 batches, loss: 0.1240Epoch 5/10: [============================  ] 72/75 batches, loss: 0.1232Epoch 5/10: [============================= ] 73/75 batches, loss: 0.1223Epoch 5/10: [============================= ] 74/75 batches, loss: 0.1239Epoch 5/10: [==============================] 75/75 batches, loss: 0.1249
[2025-04-29 21:44:27,130][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1249
[2025-04-29 21:44:27,599][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2524, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0527Epoch 6/10: [                              ] 2/75 batches, loss: 0.0605Epoch 6/10: [=                             ] 3/75 batches, loss: 0.1034Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0927Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0884Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0835Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0801Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0909Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0961Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0928Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0941Epoch 6/10: [====                          ] 12/75 batches, loss: 0.1232Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.1227Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.1156Epoch 6/10: [======                        ] 15/75 batches, loss: 0.1256Epoch 6/10: [======                        ] 16/75 batches, loss: 0.1235Epoch 6/10: [======                        ] 17/75 batches, loss: 0.1204Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.1174Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.1297Epoch 6/10: [========                      ] 20/75 batches, loss: 0.1266Epoch 6/10: [========                      ] 21/75 batches, loss: 0.1243Epoch 6/10: [========                      ] 22/75 batches, loss: 0.1219Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.1195Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.1174Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.1165Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.1155Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.1220Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.1297Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.1270Epoch 6/10: [============                  ] 30/75 batches, loss: 0.1375Epoch 6/10: [============                  ] 31/75 batches, loss: 0.1435Epoch 6/10: [============                  ] 32/75 batches, loss: 0.1456Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.1422Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.1497Epoch 6/10: [==============                ] 35/75 batches, loss: 0.1467Epoch 6/10: [==============                ] 36/75 batches, loss: 0.1466Epoch 6/10: [==============                ] 37/75 batches, loss: 0.1497Epoch 6/10: [===============               ] 38/75 batches, loss: 0.1502Epoch 6/10: [===============               ] 39/75 batches, loss: 0.1487Epoch 6/10: [================              ] 40/75 batches, loss: 0.1557Epoch 6/10: [================              ] 41/75 batches, loss: 0.1543Epoch 6/10: [================              ] 42/75 batches, loss: 0.1546Epoch 6/10: [=================             ] 43/75 batches, loss: 0.1575Epoch 6/10: [=================             ] 44/75 batches, loss: 0.1562Epoch 6/10: [==================            ] 45/75 batches, loss: 0.1568Epoch 6/10: [==================            ] 46/75 batches, loss: 0.1559Epoch 6/10: [==================            ] 47/75 batches, loss: 0.1571Epoch 6/10: [===================           ] 48/75 batches, loss: 0.1616Epoch 6/10: [===================           ] 49/75 batches, loss: 0.1618Epoch 6/10: [====================          ] 50/75 batches, loss: 0.1625Epoch 6/10: [====================          ] 51/75 batches, loss: 0.1649Epoch 6/10: [====================          ] 52/75 batches, loss: 0.1634Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.1620Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.1629Epoch 6/10: [======================        ] 55/75 batches, loss: 0.1627Epoch 6/10: [======================        ] 56/75 batches, loss: 0.1620Epoch 6/10: [======================        ] 57/75 batches, loss: 0.1625Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.1623Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.1612Epoch 6/10: [========================      ] 60/75 batches, loss: 0.1597Epoch 6/10: [========================      ] 61/75 batches, loss: 0.1592Epoch 6/10: [========================      ] 62/75 batches, loss: 0.1592Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.1573Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.1557Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.1561Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.1545Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.1531Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.1512Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.1511Epoch 6/10: [============================  ] 70/75 batches, loss: 0.1492Epoch 6/10: [============================  ] 71/75 batches, loss: 0.1494Epoch 6/10: [============================  ] 72/75 batches, loss: 0.1480Epoch 6/10: [============================= ] 73/75 batches, loss: 0.1465Epoch 6/10: [============================= ] 74/75 batches, loss: 0.1456Epoch 6/10: [==============================] 75/75 batches, loss: 0.1519
[2025-04-29 21:44:43,399][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1519
[2025-04-29 21:44:43,839][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3815, Metrics: {'accuracy': 0.875, 'f1': 0.88}
[2025-04-29 21:44:43,841][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0585Epoch 7/10: [                              ] 2/75 batches, loss: 0.0473Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0378Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0383Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0352Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0352Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0348Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0330Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0308Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0328Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0322Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0407Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0640Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0620Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0838Epoch 7/10: [======                        ] 16/75 batches, loss: 0.1027Epoch 7/10: [======                        ] 17/75 batches, loss: 0.1178Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.1218Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.1196Epoch 7/10: [========                      ] 20/75 batches, loss: 0.1218Epoch 7/10: [========                      ] 21/75 batches, loss: 0.1177Epoch 7/10: [========                      ] 22/75 batches, loss: 0.1141Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.1104Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.1144Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.1194Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.1172Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.1136Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.1106Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.1081Epoch 7/10: [============                  ] 30/75 batches, loss: 0.1123Epoch 7/10: [============                  ] 31/75 batches, loss: 0.1112Epoch 7/10: [============                  ] 32/75 batches, loss: 0.1088Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.1065Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.1045Epoch 7/10: [==============                ] 35/75 batches, loss: 0.1023Epoch 7/10: [==============                ] 36/75 batches, loss: 0.1042Epoch 7/10: [==============                ] 37/75 batches, loss: 0.1020Epoch 7/10: [===============               ] 38/75 batches, loss: 0.1003Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0991Epoch 7/10: [================              ] 40/75 batches, loss: 0.1041Epoch 7/10: [================              ] 41/75 batches, loss: 0.1022Epoch 7/10: [================              ] 42/75 batches, loss: 0.1160Epoch 7/10: [=================             ] 43/75 batches, loss: 0.1139Epoch 7/10: [=================             ] 44/75 batches, loss: 0.1145Epoch 7/10: [==================            ] 45/75 batches, loss: 0.1139Epoch 7/10: [==================            ] 46/75 batches, loss: 0.1122Epoch 7/10: [==================            ] 47/75 batches, loss: 0.1145Epoch 7/10: [===================           ] 48/75 batches, loss: 0.1130Epoch 7/10: [===================           ] 49/75 batches, loss: 0.1123Epoch 7/10: [====================          ] 50/75 batches, loss: 0.1176Epoch 7/10: [====================          ] 51/75 batches, loss: 0.1167Epoch 7/10: [====================          ] 52/75 batches, loss: 0.1155Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.1151Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.1152Epoch 7/10: [======================        ] 55/75 batches, loss: 0.1135Epoch 7/10: [======================        ] 56/75 batches, loss: 0.1133Epoch 7/10: [======================        ] 57/75 batches, loss: 0.1121Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.1110Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.1100Epoch 7/10: [========================      ] 60/75 batches, loss: 0.1086Epoch 7/10: [========================      ] 61/75 batches, loss: 0.1071Epoch 7/10: [========================      ] 62/75 batches, loss: 0.1056Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.1066Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.1056Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.1042Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.1045Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.1064Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.1055Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.1042Epoch 7/10: [============================  ] 70/75 batches, loss: 0.1031Epoch 7/10: [============================  ] 71/75 batches, loss: 0.1021Epoch 7/10: [============================  ] 72/75 batches, loss: 0.1036Epoch 7/10: [============================= ] 73/75 batches, loss: 0.1025Epoch 7/10: [============================= ] 74/75 batches, loss: 0.1017Epoch 7/10: [==============================] 75/75 batches, loss: 0.1052
[2025-04-29 21:44:59,158][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.1052
[2025-04-29 21:44:59,591][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2935, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
[2025-04-29 21:44:59,592][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0376Epoch 8/10: [                              ] 2/75 batches, loss: 0.0340Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0368Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0494Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0536Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0485Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0598Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0570Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0808Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0755Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0731Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0692Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0732Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0703Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0856Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0894Epoch 8/10: [======                        ] 17/75 batches, loss: 0.1040Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.1006Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0971Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0947Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0968Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0948Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0914Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0889Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0881Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0987Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0965Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0935Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0945Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0934Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0910Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0888Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0866Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0853Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0834Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0821Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0816Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0801Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0786Epoch 8/10: [================              ] 40/75 batches, loss: 0.0776Epoch 8/10: [================              ] 41/75 batches, loss: 0.0773Epoch 8/10: [================              ] 42/75 batches, loss: 0.0787Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0779Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0800Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0808Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0796Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0782Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0787Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0799Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0787Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0773Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0829Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0820Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0815Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0867Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0858Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0853Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0841Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0830Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0825Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0838Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0832Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0820Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0812Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0820Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0809Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0800Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0808Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0820Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0852Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0843Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0839Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0829Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0822Epoch 8/10: [==============================] 75/75 batches, loss: 0.0887
[2025-04-29 21:45:14,878][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0887
[2025-04-29 21:45:15,425][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.2767, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
[2025-04-29 21:45:15,426][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:45:15,426][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:45:15,426][src.training.lm_trainer][INFO] - Training completed in 129.17 seconds
[2025-04-29 21:45:15,426][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:45:21,168][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.97571189279732, 'f1': 0.9758935993349959}
[2025-04-29 21:45:21,168][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
[2025-04-29 21:45:21,169][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9, 'f1': 0.9090909090909091}
[2025-04-29 21:45:23,425][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ru/ru/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁████
wandb:          best_val_f1 ▁██▇▇
wandb:        best_val_loss █▅▃▂▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▄▂▁▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁████▇██
wandb:               val_f1 ▁██▇▇▇██
wandb:             val_loss █▅▃▂▁▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89189
wandb:        best_val_loss 0.25237
wandb:                epoch 8
wandb:  final_test_accuracy 0.9
wandb:        final_test_f1 0.90909
wandb: final_train_accuracy 0.97571
wandb:       final_train_f1 0.97589
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89189
wandb:        learning_rate 2e-05
wandb:           train_loss 0.08875
wandb:           train_time 129.17096
wandb:         val_accuracy 0.90278
wandb:               val_f1 0.90411
wandb:             val_loss 0.27674
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_214255-9u3h5tlf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_214255-9u3h5tlf/logs
Experiment finetune_question_type_ru completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ru/results.json
Running experiment: finetune_complexity_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ru"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:45:46,722][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ru
experiment_name: finetune_complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:45:46,722][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:45:46,722][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:45:46,722][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:45:46,727][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-04-29 21:45:46,727][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:45:48,330][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:45:51,284][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:45:51,284][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:45:51,346][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,373][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,455][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-29 21:45:51,467][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:45:51,468][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-29 21:45:51,469][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:45:51,490][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,517][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,529][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-29 21:45:51,530][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:45:51,531][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-29 21:45:51,532][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:45:51,552][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,577][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:45:51,588][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-29 21:45:51,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:45:51,590][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-29 21:45:51,591][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-29 21:45:51,592][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:45:51,592][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:45:51,592][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:45:51,592][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:45:51,592][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:45:51,593][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:45:51,593][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:45:51,594][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:45:51,594][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:45:51,594][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:45:51,595][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:45:51,595][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-29 21:45:51,595][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-29 21:45:51,595][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-29 21:45:51,595][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-29 21:45:51,595][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:45:51,595][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:45:51,596][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:45:55,854][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:45:55,854][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,854][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,855][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,856][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,857][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,858][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,859][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,860][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,861][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,862][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,863][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,864][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,865][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,866][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,867][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,868][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,869][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,870][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-29 21:45:55,871][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:45:55,871][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-29 21:45:55,872][src.models.model_factory][INFO] - Model has 394,195,393 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:45:55,872][__main__][INFO] - Successfully created model for ru
[2025-04-29 21:45:55,872][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.0892Epoch 1/10: [                              ] 2/75 batches, loss: 0.1052Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1046Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1077Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1100Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1173Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1202Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1166Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1102Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1077Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1047Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1026Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1016Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.0997Epoch 1/10: [======                        ] 15/75 batches, loss: 0.0976Epoch 1/10: [======                        ] 16/75 batches, loss: 0.0959Epoch 1/10: [======                        ] 17/75 batches, loss: 0.0954Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.0946Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.0943Epoch 1/10: [========                      ] 20/75 batches, loss: 0.0947Epoch 1/10: [========                      ] 21/75 batches, loss: 0.0931Epoch 1/10: [========                      ] 22/75 batches, loss: 0.0926Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.0920Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.0912Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.0906Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.0901Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.0910Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.0902Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.0897Epoch 1/10: [============                  ] 30/75 batches, loss: 0.0890Epoch 1/10: [============                  ] 31/75 batches, loss: 0.0881Epoch 1/10: [============                  ] 32/75 batches, loss: 0.0887Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.0881Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.0870Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0871Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0863Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0858Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0863Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0867Epoch 1/10: [================              ] 40/75 batches, loss: 0.0869Epoch 1/10: [================              ] 41/75 batches, loss: 0.0861Epoch 1/10: [================              ] 42/75 batches, loss: 0.0856Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0852Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0846Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0843Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0834Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0830Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0825Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0817Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0815Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0809Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0806Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0794Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0787Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0789Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0784Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0778Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0775Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0767Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0763Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0756Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0750Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0746Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0746Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0737Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0732Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0726Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0725Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0716Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0711Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0707Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0707Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0702Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0697Epoch 1/10: [==============================] 75/75 batches, loss: 0.0691
[2025-04-29 21:46:14,508][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0691
[2025-04-29 21:46:14,920][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1260, Metrics: {'mse': 0.1326618194580078, 'rmse': 0.36422770276024835, 'r2': -1.8517677783966064}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0321Epoch 2/10: [                              ] 2/75 batches, loss: 0.0349Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0420Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0408Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0474Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0492Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0471Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0477Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0452Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0435Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0430Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0430Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0435Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0423Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0412Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0428Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0444Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0450Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0458Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0471Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0483Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0478Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0473Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0466Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0461Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0457Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0458Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0461Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0462Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0462Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0463Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0464Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0461Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0458Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0454Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0446Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0446Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0441Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0440Epoch 2/10: [================              ] 40/75 batches, loss: 0.0437Epoch 2/10: [================              ] 41/75 batches, loss: 0.0437Epoch 2/10: [================              ] 42/75 batches, loss: 0.0433Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0434Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0430Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0428Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0435Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0433Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0428Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0429Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0429Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0433Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0432Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0438Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0436Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0437Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0444Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0443Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0445Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0448Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0448Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0452Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0450Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0453Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0456Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0461Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0468Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0465Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0464Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0462Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0461Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0460Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0459Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0455Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0453Epoch 2/10: [==============================] 75/75 batches, loss: 0.0452
[2025-04-29 21:46:30,711][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0452
[2025-04-29 21:46:31,125][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0920, Metrics: {'mse': 0.09373506158590317, 'rmse': 0.30616182254798385, 'r2': -1.0149776935577393}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0435Epoch 3/10: [                              ] 2/75 batches, loss: 0.0400Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0375Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0425Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0445Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0424Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0398Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0404Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0414Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0420Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0410Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0404Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0404Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0383Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0378Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0365Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0360Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0369Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0367Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0369Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0365Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0370Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0373Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0380Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0374Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0371Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0368Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0359Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0361Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0355Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0356Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0357Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0353Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0351Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0350Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0345Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0342Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0341Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0341Epoch 3/10: [================              ] 40/75 batches, loss: 0.0335Epoch 3/10: [================              ] 41/75 batches, loss: 0.0333Epoch 3/10: [================              ] 42/75 batches, loss: 0.0332Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0331Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0328Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0327Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0328Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0328Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0328Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0326Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0327Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0325Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0331Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0331Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0331Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0329Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0332Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0331Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0327Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0327Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0324Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0324Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0325Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0322Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0322Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0320Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0320Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0318Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0316Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0315Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0313Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0311Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0315Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0313Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0311Epoch 3/10: [==============================] 75/75 batches, loss: 0.0314
[2025-04-29 21:46:46,983][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0314
[2025-04-29 21:46:47,400][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0582, Metrics: {'mse': 0.05857424437999725, 'rmse': 0.24202116514883001, 'r2': -0.25914251804351807}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0298Epoch 4/10: [                              ] 2/75 batches, loss: 0.0283Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0262Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0314Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0310Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0341Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0403Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0386Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0361Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0399Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0383Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0375Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0374Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0380Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0370Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0381Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0368Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0361Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0356Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0353Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0344Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0339Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0339Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0335Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0329Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0340Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0337Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0332Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0330Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0328Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0325Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0325Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0320Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0316Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0313Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0307Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0305Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0305Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0307Epoch 4/10: [================              ] 40/75 batches, loss: 0.0303Epoch 4/10: [================              ] 41/75 batches, loss: 0.0302Epoch 4/10: [================              ] 42/75 batches, loss: 0.0298Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0300Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0301Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0303Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0298Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0295Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0292Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0291Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0288Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0284Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0282Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0280Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0279Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0277Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0276Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0272Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0270Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0266Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0263Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0260Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0261Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0258Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0257Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0255Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0255Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0253Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0253Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0251Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0249Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0247Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0246Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0246Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0246Epoch 4/10: [==============================] 75/75 batches, loss: 0.0245
[2025-04-29 21:47:03,241][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0245
