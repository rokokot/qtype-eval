SLURM_JOB_ID: 58112515
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r24g37
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Tue Apr 29 14:12:35 CEST 2025
Walltime: 01-12:00:00
========================================================================
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Tue Apr 29 14:12:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-SXM2-16GB           Off |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0             32W /  300W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Starting standard experiments...
Running question_type experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:13:22,615][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type
experiment_name: layer_1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:13:22,615][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:13:22,616][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:13:22,616][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:13:22,620][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:13:22,620][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:13:26,490][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:13:29,618][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:13:29,619][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:29,737][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:29,828][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,247][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:13:30,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,261][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:13:30,262][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:30,276][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,298][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,340][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:13:30,342][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,342][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:13:30,343][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:30,357][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,380][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,415][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:13:30,417][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,417][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:13:30,418][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:13:30,433][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,450][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:13:30,450][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,451][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:13:30,451][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,452][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:13:30,452][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:13:30,453][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:13:30,453][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:13:41,631][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:13:41,632][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:13:41,634][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:13:41,635][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:13:41,635][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:03<03:24,  3.30s/it]Epoch 1/10:   5%|▍         | 3/63 [00:03<00:53,  1.11it/s]Epoch 1/10:   8%|▊         | 5/63 [00:03<00:27,  2.15it/s]Epoch 1/10:  11%|█         | 7/63 [00:03<00:16,  3.42it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:03<00:10,  4.93it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:03<00:07,  6.61it/s]Epoch 1/10:  21%|██        | 13/63 [00:03<00:05,  8.37it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:04<00:04, 10.12it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:04<00:03, 11.75it/s]Epoch 1/10:  30%|███       | 19/63 [00:04<00:03, 13.14it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:04<00:02, 14.32it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:04<00:02, 15.23it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:04<00:02, 15.99it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:02, 16.55it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:02, 16.97it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:01, 17.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:05<00:01, 17.50it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:05<00:01, 17.65it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:05<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:01, 17.85it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 17.90it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 17.94it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:00, 17.89it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:00, 17.88it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:00, 17.88it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 17.93it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 17.91it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 17.91it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 17.88it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:06<00:00, 17.92it/s]Epoch 1/10: 100%|██████████| 63/63 [00:06<00:00,  9.31it/s]
[2025-04-29 14:13:53,633][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6952
[2025-04-29 14:13:53,926][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.88it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.69it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.27it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.46it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.98it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.46it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.79it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.87it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.92it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.95it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.92it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.88it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.86it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.90it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.87it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.86it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.89it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.87it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.90it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.87it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.88it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.30it/s]
[2025-04-29 14:13:58,134][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6937
[2025-04-29 14:13:58,427][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.24it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.47it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.45it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.91it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.42it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.77it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.77it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.80it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.82it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.82it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.86it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.87it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.84it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.86it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.88it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.89it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.88it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.88it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.87it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.88it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.87it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.83it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.84it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.84it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.87it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.33it/s]
[2025-04-29 14:14:02,065][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6940
[2025-04-29 14:14:02,354][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.36363636363636365, 'f1': 0.36363636363636365}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.93it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.75it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.79it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.83it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.86it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.81it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.83it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.81it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.83it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:06,013][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6954
[2025-04-29 14:14:06,315][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 14:14:06,316][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▇▁▂█
wandb:           train_time ▁
wandb:         val_accuracy ██▁▅
wandb:               val_f1 ▁▁▅█
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69261
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69541
wandb:           train_time 19.45182
wandb:         val_accuracy 0.45455
wandb:               val_f1 0.625
wandb:             val_loss 0.69341
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141322-wjk0k0zg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141322-wjk0k0zg/logs
Standard experiment completed successfully: layer_1_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/results.json
Running complexity experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:14:23,379][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity
experiment_name: layer_1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:14:23,379][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:14:23,379][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:14:23,379][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:14:23,384][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:14:23,384][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:14:24,498][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:14:27,179][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:14:27,179][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,215][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,233][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,548][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:14:27,558][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,559][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:14:27,569][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,633][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,654][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,664][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:14:27,665][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,665][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:14:27,666][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,682][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,710][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:14:27,711][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,711][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:14:27,712][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,713][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,713][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,714][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,715][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,716][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,716][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:14:27,717][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:14:31,675][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:14:31,676][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:14:31,679][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:14:31,679][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:14:31,679][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:13,  1.18s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:20,  2.87it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:11,  5.04it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:07,  7.25it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05,  9.34it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 11.23it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 12.82it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.08it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:03, 15.09it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:02, 15.83it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.39it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.82it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.33it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.48it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.83it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.83it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.85it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.84it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.85it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.88it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 13.51it/s]
[2025-04-29 14:14:37,873][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1640
[2025-04-29 14:14:38,162][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1002, Metrics: {'mse': 0.1011941209435463, 'rmse': 0.3181102339497211, 'r2': -0.5597379207611084}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.05it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.31it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.39it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.51it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.78it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.78it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.28it/s]
[2025-04-29 14:14:42,374][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0670
[2025-04-29 14:14:42,665][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0667, Metrics: {'mse': 0.06676701456308365, 'rmse': 0.25839313954337806, 'r2': -0.02910172939300537}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.74it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.80it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.81it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.83it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.84it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:14:46,941][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0438
[2025-04-29 14:14:47,242][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0658, Metrics: {'mse': 0.06546419858932495, 'rmse': 0.25585972443767885, 'r2': -0.00902104377746582}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.64it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.41it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:14:51,474][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0416
[2025-04-29 14:14:51,782][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0667, Metrics: {'mse': 0.06633471697568893, 'rmse': 0.2575552697494053, 'r2': -0.022438645362854004}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:09,  6.28it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.42it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:55,440][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0434
[2025-04-29 14:14:55,746][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0672, Metrics: {'mse': 0.06681045144796371, 'rmse': 0.2584771778087259, 'r2': -0.02977120876312256}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:09,  6.45it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:04, 12.18it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:03, 14.53it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.74it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.46it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.89it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.19it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.36it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:59,406][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0407
[2025-04-29 14:14:59,710][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0675, Metrics: {'mse': 0.0670170858502388, 'rmse': 0.2588765842061402, 'r2': -0.03295612335205078}
[2025-04-29 14:14:59,711][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06577
wandb:     best_val_mse 0.06546
wandb:      best_val_r2 -0.00902
wandb:    best_val_rmse 0.25586
wandb:            epoch 6
wandb:   final_test_mse 0.05811
wandb:    final_test_r2 -0.00174
wandb:  final_test_rmse 0.24105
wandb:  final_train_mse 0.03091
wandb:   final_train_r2 -0.00695
wandb: final_train_rmse 0.17581
wandb:    final_val_mse 0.06546
wandb:     final_val_r2 -0.00902
wandb:   final_val_rmse 0.25586
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04069
wandb:       train_time 26.50273
wandb:         val_loss 0.06747
wandb:          val_mse 0.06702
wandb:           val_r2 -0.03296
wandb:         val_rmse 0.25888
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141423-mwyrufun
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141423-mwyrufun/logs
Standard experiment completed successfully: layer_1_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity/results.json
Running question_type experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:15:16,018][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type
experiment_name: layer_2_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:15:16,018][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:15:16,018][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:15:16,019][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:15:16,023][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:15:16,023][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:15:17,173][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:15:19,832][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:15:19,833][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:19,878][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:19,897][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:19,992][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:15:20,001][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,002][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:15:20,003][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:20,019][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,038][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,048][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:15:20,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,050][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:15:20,051][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:20,066][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,087][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,096][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:15:20,097][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,098][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:15:20,098][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,100][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:15:20,100][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:15:20,102][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:15:20,103][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:15:23,919][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:15:23,920][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:15:23,922][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:15:23,922][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:15:23,922][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.15it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:16,  3.73it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.28it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.66it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.74it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.47it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.85it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.91it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.71it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.31it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.75it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.06it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.45it/s]
[2025-04-29 14:15:29,975][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6953
[2025-04-29 14:15:30,244][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.02it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.77it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.59it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.38it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.17it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.49it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.32it/s]
[2025-04-29 14:15:34,439][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6934
[2025-04-29 14:15:34,717][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.97it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.49it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:15:38,396][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6942
[2025-04-29 14:15:38,687][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.13it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.88it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.33it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.62it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:15:42,333][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6944
[2025-04-29 14:15:42,631][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:15:42,632][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▄▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69216
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69442
wandb:           train_time 17.01866
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69292
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141516-33zrddzz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141516-33zrddzz/logs
Standard experiment completed successfully: layer_2_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type/results.json
Running complexity experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:15:58,484][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity
experiment_name: layer_2_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:15:58,484][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:15:58,485][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:15:58,485][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:15:58,489][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:15:58,490][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:15:59,571][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:16:02,379][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:16:02,380][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,404][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,420][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,476][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:16:02,487][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,488][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:16:02,489][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,499][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,517][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,525][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:16:02,527][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,527][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:16:02,528][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,539][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,556][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,567][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:16:02,569][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,569][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,571][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,572][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,572][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,573][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,573][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:16:02,574][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:16:02,574][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:16:02,574][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:16:06,168][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:16:06,169][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:16:06,171][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:16:06,171][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:16:06,171][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:45,  1.37it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:13,  4.32it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  7.07it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:05,  9.50it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.54it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:03, 13.17it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.41it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.37it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 16.05it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.56it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.92it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:01<00:02, 17.16it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.36it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.50it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.58it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.91it/s]
[2025-04-29 14:16:11,928][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1861
[2025-04-29 14:16:12,223][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1362, Metrics: {'mse': 0.13763229548931122, 'rmse': 0.37098826866804185, 'r2': -1.1213715076446533}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.10it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:16:16,456][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1043
[2025-04-29 14:16:16,747][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0880, Metrics: {'mse': 0.08877066522836685, 'rmse': 0.29794406392537315, 'r2': -0.3682512044906616}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.24it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.78it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:16:21,027][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0632
[2025-04-29 14:16:21,328][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0699, Metrics: {'mse': 0.07019999623298645, 'rmse': 0.2649528188809971, 'r2': -0.08201539516448975}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.76it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.05it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:16:25,553][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0478
[2025-04-29 14:16:25,861][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0656, Metrics: {'mse': 0.06553687155246735, 'rmse': 0.2560017022452533, 'r2': -0.010141134262084961}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  6.06it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.26it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.54it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.30it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:16:30,096][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0460
[2025-04-29 14:16:30,412][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0655, Metrics: {'mse': 0.06525885313749313, 'rmse': 0.2554581240389374, 'r2': -0.005856037139892578}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.12it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.77it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.22it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:16:34,667][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0417
[2025-04-29 14:16:34,982][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0663, Metrics: {'mse': 0.06594730168581009, 'rmse': 0.25680206713694903, 'r2': -0.01646721363067627}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.83it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.59it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.11it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.46it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.77it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:16:38,657][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0431
[2025-04-29 14:16:38,973][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0663, Metrics: {'mse': 0.06590130925178528, 'rmse': 0.25671250310763066, 'r2': -0.015758395195007324}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:16:42,668][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0412
[2025-04-29 14:16:42,977][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0667, Metrics: {'mse': 0.06632109731435776, 'rmse': 0.25752882812290695, 'r2': -0.022228717803955078}
[2025-04-29 14:16:42,978][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁
wandb:     best_val_mse █▃▁▁▁
wandb:      best_val_r2 ▁▆███
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁
wandb:           val_r2 ▁▆██████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06551
wandb:     best_val_mse 0.06526
wandb:      best_val_r2 -0.00586
wandb:    best_val_rmse 0.25546
wandb:            epoch 8
wandb:   final_test_mse 0.05877
wandb:    final_test_r2 -0.01309
wandb:  final_test_rmse 0.24242
wandb:  final_train_mse 0.03156
wandb:   final_train_r2 -0.02798
wandb: final_train_rmse 0.17764
wandb:    final_val_mse 0.06526
wandb:     final_val_r2 -0.00586
wandb:   final_val_rmse 0.25546
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04122
wandb:       train_time 35.27718
wandb:         val_loss 0.06672
wandb:          val_mse 0.06632
wandb:           val_r2 -0.02223
wandb:         val_rmse 0.25753
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141558-xz1z04rj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141558-xz1z04rj/logs
Standard experiment completed successfully: layer_2_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity/results.json
Running question_type experiment for language ar, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:16:58,771][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/question_type
experiment_name: layer_3_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:16:58,772][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:16:58,772][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:16:58,772][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:16:58,776][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:16:58,776][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:16:59,942][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:17:02,626][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:17:02,627][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,671][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,731][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:17:02,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,741][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:17:02,742][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,755][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,774][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,783][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:17:02,784][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,784][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:17:02,785][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,815][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,824][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:17:02,826][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,826][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,828][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:17:02,828][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,829][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:17:02,829][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,830][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:17:02,830][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:17:02,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:17:02,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:17:06,481][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:17:06,482][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:17:06,484][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:17:06,484][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:17:06,484][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:46,  1.33it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.23it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  6.96it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:05,  9.38it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.42it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:03, 13.06it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.33it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.31it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 16.00it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.51it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.88it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:01<00:02, 17.15it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.45it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.66it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.84it/s]
[2025-04-29 14:17:12,326][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6953
[2025-04-29 14:17:12,603][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.75it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.78it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:17:16,843][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6932
[2025-04-29 14:17:17,136][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:08,  6.90it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.58it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:03, 14.80it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.95it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 17.00it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.26it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.44it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.32it/s]
[2025-04-29 14:17:20,775][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6938
[2025-04-29 14:17:21,065][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:17:24,728][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 14:17:25,026][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:17:25,027][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69203
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69395
wandb:           train_time 16.94896
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69276
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141658-xdeqi2mh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141658-xdeqi2mh/logs
Standard experiment completed successfully: layer_3_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/question_type/results.json
Running complexity experiment for language ar, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:17:40,145][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/complexity
experiment_name: layer_3_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:17:40,145][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:17:40,145][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:17:40,145][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:17:40,150][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:17:40,150][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:17:41,036][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:17:43,758][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:17:43,758][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,775][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,796][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,847][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:17:43,857][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,857][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:17:43,858][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,873][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,892][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,902][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:17:43,903][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,903][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:17:43,904][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,916][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,944][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:17:43,946][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,946][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,948][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,948][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,949][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,949][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,950][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,950][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:17:43,951][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:17:43,951][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:17:47,480][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:17:47,481][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:17:47,483][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:17:47,483][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:17:47,483][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:51,  1.20it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.86it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.45it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.85it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.92it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.63it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.98it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.01it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.79it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.36it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.77it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.07it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.81it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.58it/s]
[2025-04-29 14:17:53,177][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1897
[2025-04-29 14:17:53,461][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1425, Metrics: {'mse': 0.14402025938034058, 'rmse': 0.3795000123588148, 'r2': -1.2198312282562256}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.63it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.40it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:03, 14.72it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.93it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.61it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 17.00it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.25it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.43it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.34it/s]
[2025-04-29 14:17:57,652][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1100
[2025-04-29 14:17:57,939][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0937, Metrics: {'mse': 0.09453416615724564, 'rmse': 0.3074640892157093, 'r2': -0.4570859670639038}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.79it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.10it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.44it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.77it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.27it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:18:02,204][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0673
[2025-04-29 14:18:02,504][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0728, Metrics: {'mse': 0.07317809760570526, 'rmse': 0.2705145053517561, 'r2': -0.12791788578033447}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.34it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.93it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:18:06,733][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0494
[2025-04-29 14:18:07,021][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0666, Metrics: {'mse': 0.06659357249736786, 'rmse': 0.2580573046774066, 'r2': -0.0264284610748291}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.49it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:18:11,250][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0458
[2025-04-29 14:18:11,554][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0656, Metrics: {'mse': 0.06545855849981308, 'rmse': 0.25584870236101076, 'r2': -0.00893402099609375}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.02it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:18:15,796][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0414
[2025-04-29 14:18:16,100][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0661, Metrics: {'mse': 0.06581141799688339, 'rmse': 0.25653736179528197, 'r2': -0.014372825622558594}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:09,  6.31it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:04, 12.04it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.43it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:18:19,777][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0419
[2025-04-29 14:18:20,088][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0661, Metrics: {'mse': 0.06579596549272537, 'rmse': 0.2565072425736267, 'r2': -0.014134645462036133}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.24it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.53it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.28it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:18:23,769][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0401
[2025-04-29 14:18:24,078][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0665, Metrics: {'mse': 0.06610532850027084, 'rmse': 0.2571095651668192, 'r2': -0.018902897834777832}
[2025-04-29 14:18:24,079][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁
wandb:     best_val_mse █▄▂▁▁
wandb:      best_val_r2 ▁▅▇██
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁
wandb:           val_r2 ▁▅▇█████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06563
wandb:     best_val_mse 0.06546
wandb:      best_val_r2 -0.00893
wandb:    best_val_rmse 0.25585
wandb:            epoch 8
wandb:   final_test_mse 0.05964
wandb:    final_test_r2 -0.02817
wandb:  final_test_rmse 0.24421
wandb:  final_train_mse 0.03229
wandb:   final_train_r2 -0.05173
wandb: final_train_rmse 0.17968
wandb:    final_val_mse 0.06546
wandb:     final_val_r2 -0.00893
wandb:   final_val_rmse 0.25585
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0401
wandb:       train_time 35.22494
wandb:         val_loss 0.06647
wandb:          val_mse 0.06611
wandb:           val_r2 -0.0189
wandb:         val_rmse 0.25711
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141740-5z8p1cf7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141740-5z8p1cf7/logs
Standard experiment completed successfully: layer_3_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/complexity/results.json
Running question_type experiment for language ar, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:18:41,062][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/question_type
experiment_name: layer_4_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:18:41,062][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:18:41,062][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:18:41,062][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:18:41,067][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:18:41,067][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:18:42,253][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:18:45,088][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:18:45,088][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,141][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,245][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:18:45,255][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,255][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:18:45,256][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,268][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,296][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:18:45,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,297][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:18:45,298][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,310][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,337][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:18:45,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,339][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,341][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:18:45,341][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,342][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:18:45,342][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,343][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:18:45,343][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:18:45,344][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:18:45,344][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:18:49,674][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:18:49,675][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:18:49,677][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:18:49,678][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:18:49,678][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.26it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.04it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.70it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.10it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.15it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.82it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.16it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.91it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.45it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.29it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.43it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:18:55,759][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6958
[2025-04-29 14:18:56,031][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.82it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.61it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.13it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.27it/s]
[2025-04-29 14:19:00,237][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6933
[2025-04-29 14:19:00,534][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.95it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.70it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.70it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:19:04,205][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6941
[2025-04-29 14:19:04,497][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.32it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.42it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.25it/s]
[2025-04-29 14:19:08,152][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6942
[2025-04-29 14:19:08,446][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:19:08,446][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69217
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69422
wandb:           train_time 16.97329
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69295
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141841-w2gsqofd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141841-w2gsqofd/logs
Standard experiment completed successfully: layer_4_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/question_type/results.json
Running complexity experiment for language ar, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:19:23,395][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/complexity
experiment_name: layer_4_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:19:23,395][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:19:23,395][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:19:23,395][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:19:23,400][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:19:23,400][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:19:24,290][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:19:26,967][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:19:26,968][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:26,982][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:26,999][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,049][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:19:27,058][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,059][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:19:27,060][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:27,071][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,089][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,098][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:19:27,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,100][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:19:27,101][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:27,117][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,146][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,163][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:19:27,164][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,164][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,167][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,167][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,168][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,168][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,169][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,169][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:19:27,170][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:19:27,170][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:19:27,170][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:19:30,976][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:19:30,976][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:19:30,978][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:19:30,979][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:19:30,979][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:50,  1.24it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.97it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.62it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.01it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.08it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.77it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.11it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.12it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.44it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.09it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.29it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.66it/s]
[2025-04-29 14:19:36,796][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1871
[2025-04-29 14:19:37,071][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1369, Metrics: {'mse': 0.138422429561615, 'rmse': 0.3720516490510625, 'r2': -1.1335501670837402}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.12it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.89it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.34it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:19:41,290][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1049
[2025-04-29 14:19:41,584][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0887, Metrics: {'mse': 0.08951589465141296, 'rmse': 0.29919206983376573, 'r2': -0.37973761558532715}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.25it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 12.00it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.40it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:19:45,854][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0642
[2025-04-29 14:19:46,143][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0703, Metrics: {'mse': 0.07052505761384964, 'rmse': 0.2655655429716921, 'r2': -0.08702576160430908}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.48it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:19:50,371][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0488
[2025-04-29 14:19:50,684][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0660, Metrics: {'mse': 0.06590817868709564, 'rmse': 0.25672588238643884, 'r2': -0.015864253044128418}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.55it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.28it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.87it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.30it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.14it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:19:54,919][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0478
[2025-04-29 14:19:55,226][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0658, Metrics: {'mse': 0.06557522714138031, 'rmse': 0.25607660404921867, 'r2': -0.010732293128967285}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.68it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:19:59,459][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0432
[2025-04-29 14:19:59,775][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0664, Metrics: {'mse': 0.06604313850402832, 'rmse': 0.2569885960583238, 'r2': -0.01794445514678955}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.72it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:20:03,470][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0449
[2025-04-29 14:20:03,778][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0661, Metrics: {'mse': 0.06575163453817368, 'rmse': 0.25642081533715955, 'r2': -0.013451337814331055}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.79it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:20:07,468][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0424
[2025-04-29 14:20:07,777][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0665, Metrics: {'mse': 0.06608042120933533, 'rmse': 0.2570611234888219, 'r2': -0.018519163131713867}
[2025-04-29 14:20:07,777][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁
wandb:     best_val_mse █▃▁▁▁
wandb:      best_val_r2 ▁▆███
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁
wandb:           val_r2 ▁▆██████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06581
wandb:     best_val_mse 0.06558
wandb:      best_val_r2 -0.01073
wandb:    best_val_rmse 0.25608
wandb:            epoch 8
wandb:   final_test_mse 0.05923
wandb:    final_test_r2 -0.02108
wandb:  final_test_rmse 0.24337
wandb:  final_train_mse 0.03161
wandb:   final_train_r2 -0.0298
wandb: final_train_rmse 0.1778
wandb:    final_val_mse 0.06558
wandb:     final_val_r2 -0.01073
wandb:   final_val_rmse 0.25608
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04237
wandb:       train_time 35.28297
wandb:         val_loss 0.06646
wandb:          val_mse 0.06608
wandb:           val_r2 -0.01852
wandb:         val_rmse 0.25706
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141923-1v4c48lx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141923-1v4c48lx/logs
Standard experiment completed successfully: layer_4_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/complexity/results.json
Running question_type experiment for language ar, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:20:25,209][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/question_type
experiment_name: layer_5_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:20:25,209][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:20:25,209][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:20:25,209][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:20:25,214][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:20:25,214][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:20:26,439][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:20:29,142][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:20:29,142][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,184][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,204][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,268][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:20:29,277][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,278][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:20:29,279][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,296][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,327][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:20:29,328][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,328][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:20:29,329][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,345][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,369][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,378][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:20:29,379][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,379][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:20:29,380][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,381][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:20:29,381][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,382][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:20:29,382][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,383][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:20:29,384][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:20:33,268][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:20:33,269][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:20:33,271][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:20:33,271][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:20:33,271][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:50,  1.23it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.95it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.58it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.98it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.04it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.72it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.05it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.08it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.83it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.39it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.80it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.10it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.47it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]
[2025-04-29 14:20:39,646][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6951
[2025-04-29 14:20:39,924][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.95it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.73it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.23it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.56it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.33it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:20:44,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6933
[2025-04-29 14:20:44,430][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.12it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.88it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.31it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.60it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:20:48,096][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-29 14:20:48,371][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.25it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 12.00it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.42it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.66it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:20:52,043][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6938
[2025-04-29 14:20:52,336][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5227272727272727, 'f1': 0.08695652173913043}
[2025-04-29 14:20:52,337][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▂▃
wandb:           train_time ▁
wandb:         val_accuracy ███▁
wandb:               val_f1 ▁▁▁█
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69241
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69384
wandb:           train_time 16.99443
wandb:         val_accuracy 0.52273
wandb:               val_f1 0.08696
wandb:             val_loss 0.69304
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142025-rtuh6f4z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142025-rtuh6f4z/logs
Standard experiment completed successfully: layer_5_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/question_type/results.json
Running complexity experiment for language ar, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:21:07,429][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/complexity
experiment_name: layer_5_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:21:07,429][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:21:07,429][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:21:07,429][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:21:07,433][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:21:07,434][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:21:08,390][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:21:11,117][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:21:11,118][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,132][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,148][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,201][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:21:11,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,211][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:21:11,212][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,227][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,249][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,259][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:21:11,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,261][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:21:11,262][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,276][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,293][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,301][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:21:11,303][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,303][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,305][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,305][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,306][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,306][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,307][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,307][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:21:11,308][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:21:11,308][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:21:14,703][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:21:14,703][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:21:14,706][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:21:14,706][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:21:14,706][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.29it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.09it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.77it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.19it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.25it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.92it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.21it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.20it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.93it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.46it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.82it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.78it/s]
[2025-04-29 14:21:20,431][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1767
[2025-04-29 14:21:20,706][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1306, Metrics: {'mse': 0.13202466070652008, 'rmse': 0.36335197908711064, 'r2': -1.0349392890930176}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.48it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.20it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:03, 14.56it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.79it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.49it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.39it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.53it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.77it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.80it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.81it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:21:24,913][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0973
[2025-04-29 14:21:25,191][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0847, Metrics: {'mse': 0.0853947103023529, 'rmse': 0.2922237332975419, 'r2': -0.3162165880203247}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:21:29,468][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0589
[2025-04-29 14:21:29,757][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0691, Metrics: {'mse': 0.06930001080036163, 'rmse': 0.2632489521353535, 'r2': -0.06814372539520264}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.63it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.16it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.30it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.24it/s]
[2025-04-29 14:21:33,963][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0453
[2025-04-29 14:21:34,265][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0661, Metrics: {'mse': 0.06601426750421524, 'rmse': 0.2569324181651962, 'r2': -0.017499446868896484}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.37it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.10it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.75it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.07it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.97it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:21:38,524][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0445
[2025-04-29 14:21:38,834][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0664, Metrics: {'mse': 0.06607770174741745, 'rmse': 0.2570558339104901, 'r2': -0.01847708225250244}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:21:42,534][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0408
[2025-04-29 14:21:42,842][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0671, Metrics: {'mse': 0.06674779206514359, 'rmse': 0.2583559406422534, 'r2': -0.02880549430847168}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.72it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:21:46,518][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0423
[2025-04-29 14:21:46,825][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0668, Metrics: {'mse': 0.0664687380194664, 'rmse': 0.2578153176587194, 'r2': -0.02450430393218994}
[2025-04-29 14:21:46,826][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁
wandb:     best_val_mse █▃▁▁
wandb:      best_val_r2 ▁▆██
wandb:    best_val_rmse █▃▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁
wandb:           val_r2 ▁▆█████
wandb:         val_rmse █▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06613
wandb:     best_val_mse 0.06601
wandb:      best_val_r2 -0.0175
wandb:    best_val_rmse 0.25693
wandb:            epoch 7
wandb:   final_test_mse 0.06133
wandb:    final_test_r2 -0.05725
wandb:  final_test_rmse 0.24764
wandb:  final_train_mse 0.03341
wandb:   final_train_r2 -0.08852
wandb: final_train_rmse 0.1828
wandb:    final_val_mse 0.06601
wandb:     final_val_r2 -0.0175
wandb:   final_val_rmse 0.25693
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04235
wandb:       train_time 30.66079
wandb:         val_loss 0.06684
wandb:          val_mse 0.06647
wandb:           val_r2 -0.0245
wandb:         val_rmse 0.25782
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142107-cks1kr0h
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142107-cks1kr0h/logs
Standard experiment completed successfully: layer_5_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/complexity/results.json
Running question_type experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:22:03,186][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type
experiment_name: layer_6_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:22:03,186][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:22:03,186][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:22:03,186][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:22:03,190][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:22:03,191][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:22:04,305][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:22:07,078][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:22:07,078][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,109][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,183][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:22:07,193][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,193][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:22:07,194][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,206][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,223][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,233][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:22:07,234][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,234][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:22:07,235][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,245][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,262][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,270][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:22:07,272][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,272][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,274][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:22:07,274][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,275][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:22:07,275][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,276][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:22:07,276][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:22:07,277][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:22:07,277][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:22:10,916][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:22:10,917][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:22:10,919][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:22:10,919][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:22:10,919][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:58,  1.06it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:17,  3.47it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  5.91it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.25it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.33it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.11it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.54it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.66it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.52it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.15it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.63it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.98it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.20it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.38it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.49it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.74it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.17it/s]
[2025-04-29 14:22:17,045][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6950
[2025-04-29 14:22:17,324][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.58it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:22:21,546][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6932
[2025-04-29 14:22:21,835][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.14it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.89it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.33it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.35it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:22:25,497][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6938
[2025-04-29 14:22:25,793][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.84it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.30it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.58it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.62it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.64it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:22:29,468][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6941
[2025-04-29 14:22:29,762][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:22:29,763][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69202
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69408
wandb:           train_time 17.16773
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69275
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142203-0ypf8v9g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142203-0ypf8v9g/logs
Standard experiment completed successfully: layer_6_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type/results.json
Running complexity experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:22:44,997][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity
experiment_name: layer_6_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:22:44,997][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:22:44,997][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:22:44,997][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:22:45,001][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:22:45,002][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:22:45,936][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:22:48,632][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:22:48,633][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,650][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,669][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,719][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:22:48,728][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,729][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:22:48,730][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,746][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,767][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,777][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:22:48,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,779][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:22:48,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,795][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,823][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:22:48,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,825][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:22:48,825][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,826][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,827][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,827][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,828][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,829][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,829][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:22:48,830][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:22:48,830][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:22:52,295][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:22:52,296][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:22:52,298][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:22:52,299][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:22:52,299][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:47,  1.31it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.16it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  6.86it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.28it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.33it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.99it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.26it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.25it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.95it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.46it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.86it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.12it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.33it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.62it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.78it/s]
[2025-04-29 14:22:58,064][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1949
[2025-04-29 14:22:58,343][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1520, Metrics: {'mse': 0.15365538001060486, 'rmse': 0.3919890049613699, 'r2': -1.3683404922485352}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.05it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.83it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.29it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.59it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.35it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:23:02,565][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1213
[2025-04-29 14:23:02,852][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1028, Metrics: {'mse': 0.10386957973241806, 'rmse': 0.3222880384569338, 'r2': -0.6009756326675415}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.18it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.92it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.36it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:23:07,118][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0760
[2025-04-29 14:23:07,423][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0782, Metrics: {'mse': 0.0787581279873848, 'rmse': 0.280638785607736, 'r2': -0.21392464637756348}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.50it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.07it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.05it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:23:11,640][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0538
[2025-04-29 14:23:11,946][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0683, Metrics: {'mse': 0.06853058934211731, 'rmse': 0.26178347797773127, 'r2': -0.056284308433532715}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.37it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.11it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.25it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.54it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.63it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.62it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.62it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.62it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.63it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.66it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.66it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:23:16,198][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0471
[2025-04-29 14:23:16,500][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0654, Metrics: {'mse': 0.06534486263990402, 'rmse': 0.2556264122501899, 'r2': -0.007181644439697266}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.57it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.29it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.89it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.30it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.12it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:23:20,740][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0409
[2025-04-29 14:23:21,029][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0653, Metrics: {'mse': 0.06507818400859833, 'rmse': 0.2551042610553542, 'r2': -0.0030711889266967773}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  6.11it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.85it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.27it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.57it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.33it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:23:25,243][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0403
[2025-04-29 14:23:25,556][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0656, Metrics: {'mse': 0.06532509624958038, 'rmse': 0.25558774667338885, 'r2': -0.0068770647048950195}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.70it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.67it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:23:29,247][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0392
[2025-04-29 14:23:29,541][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0661, Metrics: {'mse': 0.06579696387052536, 'rmse': 0.25650918866684946, 'r2': -0.014150023460388184}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.52it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:23:33,228][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0407
[2025-04-29 14:23:33,525][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0661, Metrics: {'mse': 0.06578850001096725, 'rmse': 0.2564926899756936, 'r2': -0.014019608497619629}
[2025-04-29 14:23:33,526][src.training.lm_trainer][INFO] - Early stopping at epoch 9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▂▁▁▁
wandb:      best_val_r2 ▁▅▇███
wandb:    best_val_rmse █▄▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇██████
wandb:         val_rmse █▄▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0653
wandb:     best_val_mse 0.06508
wandb:      best_val_r2 -0.00307
wandb:    best_val_rmse 0.2551
wandb:            epoch 9
wandb:   final_test_mse 0.05865
wandb:    final_test_r2 -0.01104
wandb:  final_test_rmse 0.24217
wandb:  final_train_mse 0.03161
wandb:   final_train_r2 -0.02984
wandb: final_train_rmse 0.1778
wandb:    final_val_mse 0.06508
wandb:     final_val_r2 -0.00307
wandb:   final_val_rmse 0.2551
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04074
wandb:       train_time 39.72733
wandb:         val_loss 0.06613
wandb:          val_mse 0.06579
wandb:           val_r2 -0.01402
wandb:         val_rmse 0.25649
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142245-px39e07s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142245-px39e07s/logs
Standard experiment completed successfully: layer_6_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity/results.json
Running question_type experiment for language ar, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:23:50,463][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/question_type
experiment_name: layer_7_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:23:50,463][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:23:50,463][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:23:50,463][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:23:50,468][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:23:50,468][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:23:51,669][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:23:54,364][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:23:54,364][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,413][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,495][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:23:54,504][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,504][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:23:54,505][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,516][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,533][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,541][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,555][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,573][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,584][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:23:54,586][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,586][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,588][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:23:54,588][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,589][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:23:54,589][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,590][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:23:54,590][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:23:54,591][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:23:54,591][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:23:58,312][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:23:58,313][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:23:58,315][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:23:58,316][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:23:58,316][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.19it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.85it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.45it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.83it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.91it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.63it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.98it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.02it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.79it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.36it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.79it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.06it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.53it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.53it/s]
[2025-04-29 14:24:04,245][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6935
[2025-04-29 14:24:04,509][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6916, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.08it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.58it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.81it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:24:08,726][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6935
[2025-04-29 14:24:09,020][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6918, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.50it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.22it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:03, 14.56it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.77it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.48it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.91it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:24:12,691][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6934
[2025-04-29 14:24:13,003][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6918, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:08,  6.92it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.60it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:03, 14.81it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.91it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.58it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.97it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.25it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:24:16,663][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6932
[2025-04-29 14:24:16,964][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:24:16,965][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▇▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69158
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69324
wandb:           train_time 17.05912
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69197
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142350-t4zsiews
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142350-t4zsiews/logs
Standard experiment completed successfully: layer_7_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/question_type/results.json
Running complexity experiment for language ar, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:24:32,037][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/complexity
experiment_name: layer_7_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:24:32,037][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:24:32,038][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:24:32,038][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:24:32,042][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:24:32,042][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:24:32,962][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:24:35,671][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:24:35,672][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,717][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,768][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:24:35,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,779][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:24:35,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,794][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,820][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:24:35,821][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,821][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:24:35,822][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,833][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,850][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,858][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:24:35,860][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,860][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,862][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,862][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,863][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,863][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,864][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,864][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:24:35,865][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:24:35,865][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:24:39,311][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:24:39,312][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:24:39,314][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:24:39,314][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:24:39,315][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.19it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.84it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.44it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.84it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.91it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.64it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.00it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.05it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.81it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.40it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.79it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.09it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.52it/s]
[2025-04-29 14:24:45,085][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2259
[2025-04-29 14:24:45,358][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2093, Metrics: {'mse': 0.21146632730960846, 'rmse': 0.459854680643362, 'r2': -2.2593994140625}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.23it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.02it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.44it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.33it/s]
[2025-04-29 14:24:49,545][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1953
[2025-04-29 14:24:49,836][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1810, Metrics: {'mse': 0.18294523656368256, 'rmse': 0.4277209798030517, 'r2': -1.8197944164276123}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.96it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.72it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:24:54,105][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1623
[2025-04-29 14:24:54,407][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1559, Metrics: {'mse': 0.15757113695144653, 'rmse': 0.3969523106765428, 'r2': -1.4286952018737793}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.45it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.25it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:24:58,619][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1378
[2025-04-29 14:24:58,915][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1343, Metrics: {'mse': 0.135787233710289, 'rmse': 0.3684931935738963, 'r2': -1.092932939529419}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.71it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.22it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:25:03,169][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1135
[2025-04-29 14:25:03,458][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1164, Metrics: {'mse': 0.11765534430742264, 'rmse': 0.3430092481368726, 'r2': -0.813460111618042}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  6.04it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.80it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.57it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.32it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:25:07,698][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0963
[2025-04-29 14:25:08,008][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1015, Metrics: {'mse': 0.10251451283693314, 'rmse': 0.32017887631280917, 'r2': -0.5800896883010864}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.70it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.01it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:25:12,261][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0802
[2025-04-29 14:25:12,571][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0897, Metrics: {'mse': 0.09052108973264694, 'rmse': 0.30086722940966326, 'r2': -0.39523112773895264}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.86it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.10it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.97it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:25:16,833][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0673
[2025-04-29 14:25:17,152][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0807, Metrics: {'mse': 0.08133428543806076, 'rmse': 0.2851916643909158, 'r2': -0.2536318302154541}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.62it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:25:21,404][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0568
[2025-04-29 14:25:21,708][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0746, Metrics: {'mse': 0.07503270357847214, 'rmse': 0.27392098053722014, 'r2': -0.15650343894958496}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.54it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.88it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.64it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:25:25,961][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0524
[2025-04-29 14:25:26,259][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0703, Metrics: {'mse': 0.0706331729888916, 'rmse': 0.2657690218759357, 'r2': -0.0886920690536499}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▃▃▂▂▁▁
wandb:     best_val_mse █▇▅▄▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇▇██
wandb:    best_val_rmse █▇▆▅▄▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▅▄▃▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▄▃▃▂▂▁▁
wandb:          val_mse █▇▅▄▃▃▂▂▁▁
wandb:           val_r2 ▁▂▄▅▆▆▇▇██
wandb:         val_rmse █▇▆▅▄▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07034
wandb:     best_val_mse 0.07063
wandb:      best_val_r2 -0.08869
wandb:    best_val_rmse 0.26577
wandb:            epoch 10
wandb:   final_test_mse 0.06941
wandb:    final_test_r2 -0.19663
wandb:  final_test_rmse 0.26346
wandb:  final_train_mse 0.04376
wandb:   final_train_r2 -0.42563
wandb: final_train_rmse 0.2092
wandb:    final_val_mse 0.07063
wandb:     final_val_r2 -0.08869
wandb:   final_val_rmse 0.26577
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05239
wandb:       train_time 46.10856
wandb:         val_loss 0.07034
wandb:          val_mse 0.07063
wandb:           val_r2 -0.08869
wandb:         val_rmse 0.26577
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142432-lz3hmfu1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142432-lz3hmfu1/logs
Standard experiment completed successfully: layer_7_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/complexity/results.json
Running question_type experiment for language ar, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:25:43,351][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/question_type
experiment_name: layer_8_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:25:43,351][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:25:43,351][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:25:43,351][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:25:43,355][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:25:43,356][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:25:44,472][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:25:47,195][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:25:47,196][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,230][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,246][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,307][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:25:47,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,317][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:25:47,318][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,331][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,351][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,362][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:25:47,363][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,363][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:25:47,364][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,375][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,392][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,402][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:25:47,403][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,403][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:25:47,404][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,405][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:25:47,405][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,406][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:25:47,406][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,407][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:25:47,407][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:25:47,408][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:25:51,090][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:25:51,091][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:25:51,093][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:25:51,093][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:25:51,093][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.15it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:16,  3.73it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.28it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.66it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.74it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.48it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.86it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.92it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.72it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.30it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.73it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.04it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.26it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.41it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.43it/s]
[2025-04-29 14:25:56,990][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6939
[2025-04-29 14:25:57,266][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.18it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.94it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.51it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.25it/s]
[2025-04-29 14:26:01,474][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6939
[2025-04-29 14:26:01,764][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6913, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.15it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.92it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.36it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:26:05,409][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6927
[2025-04-29 14:26:05,695][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6915, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.31it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.04it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.85it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.16it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:26:09,356][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6922
[2025-04-29 14:26:09,658][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6917, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:26:09,659][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6911
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69223
wandb:           train_time 17.03794
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69166
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142543-95pw8qx8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142543-95pw8qx8/logs
Standard experiment completed successfully: layer_8_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/question_type/results.json
Running complexity experiment for language ar, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:26:24,676][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/complexity
experiment_name: layer_8_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:26:24,676][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:26:24,676][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:26:24,676][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:26:24,680][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:26:24,681][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:26:25,625][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:26:28,325][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:26:28,326][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,349][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,369][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,421][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:26:28,431][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,432][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:26:28,432][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,444][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,470][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,482][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,498][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,506][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:26:28,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,507][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:26:28,508][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,509][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,509][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,510][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,510][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,511][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,511][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:26:28,512][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:26:31,941][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:26:31,942][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:26:31,944][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:26:31,945][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:26:31,945][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.25it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.02it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.67it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.07it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.12it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.80it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.12it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.89it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.82it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.10it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.45it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:26:37,635][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2280
[2025-04-29 14:26:37,915][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2037, Metrics: {'mse': 0.2057933658361435, 'rmse': 0.45364453687457046, 'r2': -2.1719601154327393}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.92it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.69it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.20it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.52it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.31it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:26:42,157][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1797
[2025-04-29 14:26:42,463][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1627, Metrics: {'mse': 0.16442202031612396, 'rmse': 0.4054898522973466, 'r2': -1.534290075302124}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.23it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.98it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.37it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.66it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.39it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:26:46,712][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1371
[2025-04-29 14:26:46,997][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1313, Metrics: {'mse': 0.13271470367908478, 'rmse': 0.36430029327339936, 'r2': -1.0455749034881592}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.65it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.16it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:26:51,200][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1069
[2025-04-29 14:26:51,503][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1074, Metrics: {'mse': 0.10848569124937057, 'rmse': 0.3293716612724455, 'r2': -0.6721252202987671}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:26:55,713][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0838
[2025-04-29 14:26:56,026][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0899, Metrics: {'mse': 0.09073126316070557, 'rmse': 0.3012163062662869, 'r2': -0.39847052097320557}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.55it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.24it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.85it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.25it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.29it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.41it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.48it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.55it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.03it/s]
[2025-04-29 14:27:00,269][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0648
[2025-04-29 14:27:00,572][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0781, Metrics: {'mse': 0.0786658525466919, 'rmse': 0.28047433491621276, 'r2': -0.2125023603439331}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.42it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.78it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.22it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.09it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.97it/s]
[2025-04-29 14:27:04,858][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0517
[2025-04-29 14:27:05,169][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0712, Metrics: {'mse': 0.07152225822210312, 'rmse': 0.2674364564192831, 'r2': -0.10239589214324951}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.34it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.14it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:27:09,416][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0460
[2025-04-29 14:27:09,736][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0672, Metrics: {'mse': 0.06731073558330536, 'rmse': 0.259443125912608, 'r2': -0.037482261657714844}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.63it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 14.11it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:27:13,996][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0404
[2025-04-29 14:27:14,306][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0656, Metrics: {'mse': 0.06557353585958481, 'rmse': 0.2560733017313301, 'r2': -0.010706305503845215}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 14.08it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.22it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:27:18,550][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0370
[2025-04-29 14:27:18,863][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0651, Metrics: {'mse': 0.06494658440351486, 'rmse': 0.25484619754572535, 'r2': -0.0010428428649902344}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁▁▁▁
wandb:     best_val_mse █▆▄▃▂▂▁▁▁▁
wandb:      best_val_r2 ▁▃▅▆▇▇████
wandb:    best_val_rmse █▆▅▄▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▅▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▁▁▁▁
wandb:          val_mse █▆▄▃▂▂▁▁▁▁
wandb:           val_r2 ▁▃▅▆▇▇████
wandb:         val_rmse █▆▅▄▃▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06509
wandb:     best_val_mse 0.06495
wandb:      best_val_r2 -0.00104
wandb:    best_val_rmse 0.25485
wandb:            epoch 10
wandb:   final_test_mse 0.05944
wandb:    final_test_r2 -0.02475
wandb:  final_test_rmse 0.24381
wandb:  final_train_mse 0.03265
wandb:   final_train_r2 -0.06368
wandb: final_train_rmse 0.1807
wandb:    final_val_mse 0.06495
wandb:     final_val_r2 -0.00104
wandb:   final_val_rmse 0.25485
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03699
wandb:       train_time 46.08004
wandb:         val_loss 0.06509
wandb:          val_mse 0.06495
wandb:           val_r2 -0.00104
wandb:         val_rmse 0.25485
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142624-ctupu10y
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142624-ctupu10y/logs
Standard experiment completed successfully: layer_8_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/complexity/results.json
Running question_type experiment for language ar, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:27:35,539][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/question_type
experiment_name: layer_9_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:27:35,539][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:27:35,539][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:27:35,539][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:27:35,543][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:27:35,543][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:27:36,908][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:27:39,618][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:27:39,619][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,674][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,744][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:27:39,754][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,754][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:27:39,755][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,787][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,797][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:27:39,798][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,799][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:27:39,800][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,818][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,839][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,849][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:27:39,850][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,850][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:27:39,851][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,852][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:27:39,852][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,853][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:27:39,853][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,854][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:27:39,854][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:27:39,855][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:27:43,624][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:27:43,625][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:27:43,627][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:27:43,627][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:27:43,628][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.27it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.05it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.71it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.10it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.15it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.83it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.42it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.32it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:27:49,565][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6946
[2025-04-29 14:27:49,840][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6881, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.87it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.64it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.49it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.77it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.08it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:27:54,064][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6938
[2025-04-29 14:27:54,355][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.56it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.30it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.90it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.32it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:27:58,630][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-29 14:27:58,918][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6873, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.41it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:28:03,148][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6892
[2025-04-29 14:28:03,454][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6870, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.5185185185185185}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.93it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.28it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.64it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:28:07,702][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6897
[2025-04-29 14:28:08,013][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6867, Metrics: {'accuracy': 0.75, 'f1': 0.6666666666666666}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.51it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:28:12,246][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6876
[2025-04-29 14:28:12,551][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6862, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.7346938775510204}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.56it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.31it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.91it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.10it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:28:16,810][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6862
[2025-04-29 14:28:17,127][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6859, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.77it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.21it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:28:21,389][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6858
[2025-04-29 14:28:21,710][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6855, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.90it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:28:25,992][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6840
[2025-04-29 14:28:26,313][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6849, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.67it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:28:30,563][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6849
[2025-04-29 14:28:30,886][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6844, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▃▃▃▇█▇▁▁▁▁
wandb:          best_val_f1 ▁▁▁▆▇█▇▇▇▇
wandb:        best_val_loss █▇▆▆▅▄▄▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▄▅▃▂▂▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▃▃▃▇█▇▁▁▁▁
wandb:               val_f1 ▁▁▁▆▇█▇▇▇▇
wandb:             val_loss █▇▆▆▅▄▄▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.47727
wandb:          best_val_f1 0.63492
wandb:        best_val_loss 0.68441
wandb:                epoch 10
wandb:  final_test_accuracy 0.28571
wandb:        final_test_f1 0.44444
wandb: final_train_accuracy 0.4995
wandb:       final_train_f1 0.66577
wandb:   final_val_accuracy 0.47727
wandb:         final_val_f1 0.63492
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68487
wandb:           train_time 46.19688
wandb:         val_accuracy 0.47727
wandb:               val_f1 0.63492
wandb:             val_loss 0.68441
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142735-zdooazfv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142735-zdooazfv/logs
Standard experiment completed successfully: layer_9_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/question_type/results.json
Running complexity experiment for language ar, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:28:47,843][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/complexity
experiment_name: layer_9_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:28:47,843][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:28:47,843][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:28:47,843][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:28:47,848][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:28:47,848][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:28:48,932][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:28:51,626][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:28:51,626][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,670][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,729][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:28:51,738][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,739][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:28:51,740][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,768][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,777][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:28:51,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,778][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:28:51,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,790][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,817][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:28:51,818][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,818][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:28:51,819][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,820][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,821][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,821][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,822][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,822][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,823][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:28:51,824][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:28:55,548][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:28:55,549][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:28:55,551][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:28:55,551][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:28:55,552][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.25it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  4.00it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.65it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.06it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.12it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.79it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.11it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.11it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.87it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]
[2025-04-29 14:29:01,421][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2492
[2025-04-29 14:29:01,682][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2003, Metrics: {'mse': 0.2022559642791748, 'rmse': 0.4497287674578699, 'r2': -2.1174371242523193}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.14it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.91it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.37it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:29:05,887][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1613
[2025-04-29 14:29:06,174][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1306, Metrics: {'mse': 0.13184159994125366, 'rmse': 0.36309998614879296, 'r2': -1.0321176052093506}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.94it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.63it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.38it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:29:10,434][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0969
[2025-04-29 14:29:10,742][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0913, Metrics: {'mse': 0.09185783565044403, 'rmse': 0.30308057616819334, 'r2': -0.41583478450775146}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.94it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:29:14,975][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0627
[2025-04-29 14:29:15,272][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0731, Metrics: {'mse': 0.07310894131660461, 'rmse': 0.2703866515133552, 'r2': -0.1268519163131714}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.68it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.18it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:29:19,537][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0471
[2025-04-29 14:29:19,839][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0677, Metrics: {'mse': 0.06739440560340881, 'rmse': 0.2596043250860987, 'r2': -0.038771986961364746}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.81it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.26it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.53it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.28it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.74it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:29:24,068][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0386
[2025-04-29 14:29:24,379][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0681, Metrics: {'mse': 0.06752856820821762, 'rmse': 0.2598625948616261, 'r2': -0.0408397912979126}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.97it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.63it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:29:28,050][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-04-29 14:29:28,340][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0702, Metrics: {'mse': 0.06950265914201736, 'rmse': 0.26363356983134256, 'r2': -0.07126712799072266}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.91it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.64it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.14it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:29:32,009][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0323
[2025-04-29 14:29:32,318][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0720, Metrics: {'mse': 0.07124053686857224, 'rmse': 0.2669092296429111, 'r2': -0.09805357456207275}
[2025-04-29 14:29:32,319][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁
wandb:     best_val_mse █▄▂▁▁
wandb:      best_val_r2 ▁▅▇██
wandb:    best_val_rmse █▅▃▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁
wandb:           val_r2 ▁▅▇█████
wandb:         val_rmse █▅▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06773
wandb:     best_val_mse 0.06739
wandb:      best_val_r2 -0.03877
wandb:    best_val_rmse 0.2596
wandb:            epoch 8
wandb:   final_test_mse 0.0661
wandb:    final_test_r2 -0.13961
wandb:  final_test_rmse 0.25711
wandb:  final_train_mse 0.0329
wandb:   final_train_r2 -0.07189
wandb: final_train_rmse 0.1814
wandb:    final_val_mse 0.06739
wandb:     final_val_r2 -0.03877
wandb:   final_val_rmse 0.2596
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03233
wandb:       train_time 35.19966
wandb:         val_loss 0.07203
wandb:          val_mse 0.07124
wandb:           val_r2 -0.09805
wandb:         val_rmse 0.26691
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142847-ck44bot8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142847-ck44bot8/logs
Standard experiment completed successfully: layer_9_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/complexity/results.json
Running question_type experiment for language ar, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:29:51,554][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/question_type
experiment_name: layer_10_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:29:51,554][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:29:51,555][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:29:51,555][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:29:51,559][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:29:51,559][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:29:52,757][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:29:55,616][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:29:55,616][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,648][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,664][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,725][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:29:55,736][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,737][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:29:55,738][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,770][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,779][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:29:55,781][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,781][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:29:55,782][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,794][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,812][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,820][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:29:55,822][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,822][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,824][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:29:55,824][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,825][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:29:55,825][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,826][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:29:55,826][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:29:55,827][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:29:55,827][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:29:59,634][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:29:59,635][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:29:59,637][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:29:59,638][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:29:59,638][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.17it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.80it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.38it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.77it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.84it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.57it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.92it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.97it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.76it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.35it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.75it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.03it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.23it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.41it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.53it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.62it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.70it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.70it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.71it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.71it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s]
[2025-04-29 14:30:05,607][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6888
[2025-04-29 14:30:05,897][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6694, Metrics: {'accuracy': 0.7272727272727273, 'f1': 0.76}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.96it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.74it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.23it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.55it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.32it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:30:10,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6702
[2025-04-29 14:30:10,441][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6465, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.40it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.13it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.81it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.23it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:30:14,854][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6517
[2025-04-29 14:30:15,156][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6215, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.21it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.92it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.64it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.09it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.57it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:30:19,415][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6341
[2025-04-29 14:30:19,740][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5996, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.36it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.07it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.74it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.19it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.59it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:30:24,014][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6165
[2025-04-29 14:30:24,341][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5767, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.80it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.24it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:30:28,607][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6051
[2025-04-29 14:30:28,935][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5518, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.56it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.99it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.93it/s]
[2025-04-29 14:30:33,224][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5867
[2025-04-29 14:30:33,551][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5314, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.71it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:30:37,805][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5740
[2025-04-29 14:30:38,131][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5040, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.38it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.10it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.70it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.12it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.86it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.09it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.26it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.37it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.42it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.51it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.56it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.60it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:30:42,414][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5558
[2025-04-29 14:30:42,743][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4829, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.26it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.97it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.65it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.12it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.94it/s]
[2025-04-29 14:30:47,037][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5394
[2025-04-29 14:30:47,368][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4612, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▂▇▇▇▇▇███
wandb:          best_val_f1 ▁▂▇▇▇▇▇███
wandb:        best_val_loss █▇▆▆▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▅▄▃▃▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▂▇▇▇▇▇███
wandb:               val_f1 ▁▂▇▇▇▇▇███
wandb:             val_loss █▇▆▆▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.90909
wandb:          best_val_f1 0.90909
wandb:        best_val_loss 0.46115
wandb:                epoch 10
wandb:  final_test_accuracy 0.50649
wandb:        final_test_f1 0.53659
wandb: final_train_accuracy 0.97688
wandb:       final_train_f1 0.97734
wandb:   final_val_accuracy 0.90909
wandb:         final_val_f1 0.90909
wandb:        learning_rate 1e-05
wandb:           train_loss 0.53936
wandb:           train_time 46.68831
wandb:         val_accuracy 0.90909
wandb:               val_f1 0.90909
wandb:             val_loss 0.46115
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142951-g20xkves
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142951-g20xkves/logs
Standard experiment completed successfully: layer_10_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/question_type/results.json
Running complexity experiment for language ar, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:31:05,829][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/complexity
experiment_name: layer_10_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:31:05,829][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:31:05,829][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:31:05,829][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:31:05,834][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:31:05,834][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:31:07,039][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:31:09,732][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:31:09,732][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,773][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,790][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,883][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:31:09,892][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,893][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:31:09,894][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,906][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,927][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,937][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:31:09,938][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,938][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:31:09,939][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,954][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,975][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,984][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:31:09,985][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,986][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:31:09,986][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,987][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,988][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,988][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,989][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,989][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,990][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:31:09,991][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:31:13,705][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:31:13,706][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:31:13,708][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:31:13,708][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:31:13,708][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.16it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.77it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.34it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.72it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.81it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.52it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.89it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.96it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.76it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.34it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.77it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.05it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.47it/s]
[2025-04-29 14:31:19,946][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2077
[2025-04-29 14:31:20,216][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0756, Metrics: {'mse': 0.07442639023065567, 'rmse': 0.27281200529055843, 'r2': -0.14715826511383057}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.91it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.66it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.19it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.52it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.31it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.81it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:31:24,458][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0855
[2025-04-29 14:31:24,756][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0959, Metrics: {'mse': 0.09494659304618835, 'rmse': 0.3081340504491322, 'r2': -0.46344268321990967}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.99it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.75it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.25it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:31:28,440][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0592
[2025-04-29 14:31:28,743][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0784, Metrics: {'mse': 0.07782407850027084, 'rmse': 0.2789696730834211, 'r2': -0.19952785968780518}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.81it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.56it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:31:32,423][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0499
[2025-04-29 14:31:32,701][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0695, Metrics: {'mse': 0.06909547001123428, 'rmse': 0.26286017197596573, 'r2': -0.06499099731445312}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.28it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.00it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:31:37,167][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0431
[2025-04-29 14:31:37,490][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0645, Metrics: {'mse': 0.0642058327794075, 'rmse': 0.25338869899703004, 'r2': 0.01037454605102539}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:31:41,736][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0401
[2025-04-29 14:31:42,065][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0598, Metrics: {'mse': 0.05953184515237808, 'rmse': 0.2439914858194402, 'r2': 0.08241623640060425}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.66it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.39it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:31:46,340][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-04-29 14:31:46,668][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0564, Metrics: {'mse': 0.056073009967803955, 'rmse': 0.23679740278939707, 'r2': 0.13572841882705688}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  5.15it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.98it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:31:50,941][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0317
[2025-04-29 14:31:51,268][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0548, Metrics: {'mse': 0.054489389061927795, 'rmse': 0.23342962335986364, 'r2': 0.16013729572296143}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.21it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 10.88it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.57it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.07it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:31:55,560][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0298
[2025-04-29 14:31:55,890][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0513, Metrics: {'mse': 0.050938233733177185, 'rmse': 0.22569500156888098, 'r2': 0.21487241983413696}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  5.12it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.81it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.53it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:32:00,163][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0304
[2025-04-29 14:32:00,478][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0480, Metrics: {'mse': 0.0476047620177269, 'rmse': 0.21818515535601155, 'r2': 0.2662522792816162}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▃▂▁
wandb:     best_val_mse █▇▅▄▃▃▂▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇█
wandb:    best_val_rmse █▇▆▄▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▅▄▃▃▂▂▁▁
wandb:          val_mse ▅█▅▄▃▃▂▂▁▁
wandb:           val_r2 ▄▁▄▅▆▆▇▇██
wandb:         val_rmse ▅█▆▄▄▃▂▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04797
wandb:     best_val_mse 0.0476
wandb:      best_val_r2 0.26625
wandb:    best_val_rmse 0.21819
wandb:            epoch 10
wandb:   final_test_mse 0.05468
wandb:    final_test_r2 0.05737
wandb:  final_test_rmse 0.23383
wandb:  final_train_mse 0.01994
wandb:   final_train_r2 0.35031
wandb: final_train_rmse 0.14122
wandb:    final_val_mse 0.0476
wandb:     final_val_r2 0.26625
wandb:   final_val_rmse 0.21819
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03044
wandb:       train_time 45.46754
wandb:         val_loss 0.04797
wandb:          val_mse 0.0476
wandb:           val_r2 0.26625
wandb:         val_rmse 0.21819
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143105-louodjgk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143105-louodjgk/logs
Standard experiment completed successfully: layer_10_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/complexity/results.json
Running question_type experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:32:18,900][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type
experiment_name: layer_11_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:32:18,901][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:32:18,901][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:32:18,901][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:32:18,905][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:32:18,905][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:32:20,104][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:32:22,821][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:32:22,822][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:22,856][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:22,873][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:22,983][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:32:22,992][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:22,993][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:32:22,993][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:23,006][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,025][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,036][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:32:23,037][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:23,037][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:32:23,038][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:23,049][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,071][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,081][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:32:23,082][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:23,082][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:32:23,083][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,084][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:32:23,084][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,085][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,086][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:32:23,086][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:32:23,087][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:32:26,783][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:32:26,783][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:32:26,785][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:32:26,786][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:32:26,786][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.27it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.06it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.73it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.14it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.20it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.86it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.18it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.16it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.69it/s]
[2025-04-29 14:32:33,032][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6834
[2025-04-29 14:32:33,316][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6684, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.6779661016949152}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.92it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.67it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.19it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:32:37,520][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6597
[2025-04-29 14:32:37,816][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6392, Metrics: {'accuracy': 0.6818181818181818, 'f1': 0.7407407407407407}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.80it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:32:42,258][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6371
[2025-04-29 14:32:42,581][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6072, Metrics: {'accuracy': 0.7954545454545454, 'f1': 0.8163265306122449}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.83it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.56it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:32:46,833][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6139
[2025-04-29 14:32:47,157][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5792, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 10.97it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.14it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:32:51,482][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5920
[2025-04-29 14:32:51,810][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5499, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.19it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.89it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.05it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.94it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.90it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.13it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 16.93it/s]
[2025-04-29 14:32:56,080][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5774
[2025-04-29 14:32:56,409][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5207, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.13it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.81it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.53it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.03it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.94it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.12it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:33:00,702][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5566
[2025-04-29 14:33:01,027][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.4979, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  5.11it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.79it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.51it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.91it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:33:05,302][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5419
[2025-04-29 14:33:05,631][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4691, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.18it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 10.85it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.03it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 15.92it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.50it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:33:09,938][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5195
[2025-04-29 14:33:10,262][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4482, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.54it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.88it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.29it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:33:14,537][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5073
[2025-04-29 14:33:14,865][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4282, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▃▆▇▇▇▇▇██
wandb:          best_val_f1 ▁▃▅▆▆▆▆▇▇█
wandb:        best_val_loss █▇▆▅▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▄▄▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▃▆▇▇▇▇▇██
wandb:               val_f1 ▁▃▅▆▆▆▆▇▇█
wandb:             val_loss █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.90909
wandb:          best_val_f1 0.90909
wandb:        best_val_loss 0.42815
wandb:                epoch 10
wandb:  final_test_accuracy 0.48052
wandb:        final_test_f1 0.52381
wandb: final_train_accuracy 0.97487
wandb:       final_train_f1 0.97542
wandb:   final_val_accuracy 0.90909
wandb:         final_val_f1 0.90909
wandb:        learning_rate 1e-05
wandb:           train_loss 0.5073
wandb:           train_time 46.71757
wandb:         val_accuracy 0.90909
wandb:               val_f1 0.90909
wandb:             val_loss 0.42815
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143218-smal0x1r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143218-smal0x1r/logs
Standard experiment completed successfully: layer_11_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type/results.json
Running complexity experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:33:33,335][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/complexity
experiment_name: layer_11_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:33:33,335][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:33:33,336][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:33:33,336][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:33:33,340][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:33:33,341][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:33:34,794][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:33:37,488][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:33:37,488][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,535][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,621][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:33:37,630][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,631][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:33:37,632][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,643][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,661][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,670][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:33:37,671][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,671][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:33:37,672][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,684][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,710][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:33:37,711][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,712][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:33:37,712][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,713][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,713][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,714][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,715][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,715][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,716][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:33:37,717][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:33:41,433][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:33:41,434][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:33:41,436][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:33:41,436][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:33:41,437][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.26it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.03it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.69it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.09it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.16it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.82it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.44it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.82it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.30it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]
[2025-04-29 14:33:47,476][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1645
[2025-04-29 14:33:47,750][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0831, Metrics: {'mse': 0.08177215605974197, 'rmse': 0.28595831175145436, 'r2': -0.2603808641433716}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.28it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.46it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:33:51,986][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0814
[2025-04-29 14:33:52,288][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0931, Metrics: {'mse': 0.09206391125917435, 'rmse': 0.30342035406210693, 'r2': -0.41901111602783203}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.98it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.73it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.22it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.55it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.78it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:33:55,970][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0633
[2025-04-29 14:33:56,273][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0752, Metrics: {'mse': 0.07431359589099884, 'rmse': 0.27260520151126766, 'r2': -0.14541971683502197}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.66it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.42it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:34:00,731][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0556
[2025-04-29 14:34:01,055][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0709, Metrics: {'mse': 0.0701315850019455, 'rmse': 0.2648236866330984, 'r2': -0.08096098899841309}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.72it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.47it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.69it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:34:05,299][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0490
[2025-04-29 14:34:05,626][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0661, Metrics: {'mse': 0.06547212600708008, 'rmse': 0.2558752156952292, 'r2': -0.009143233299255371}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.33it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.05it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.71it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:34:09,933][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0491
[2025-04-29 14:34:10,257][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0615, Metrics: {'mse': 0.06093612685799599, 'rmse': 0.24685243944104743, 'r2': 0.060771644115448}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.26it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.98it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.66it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.13it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:34:14,504][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0399
[2025-04-29 14:34:14,837][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0603, Metrics: {'mse': 0.059783611446619034, 'rmse': 0.24450687402733493, 'r2': 0.07853573560714722}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.22it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.91it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.61it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.07it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.91it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:34:19,126][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0352
[2025-04-29 14:34:19,438][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0577, Metrics: {'mse': 0.05719983950257301, 'rmse': 0.23916487932506522, 'r2': 0.11836022138595581}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.50it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.23it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.83it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.24it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.09it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.64it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.61it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.60it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.60it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.66it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.66it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.63it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.63it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.65it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.67it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:34:23,676][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0332
[2025-04-29 14:34:24,007][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0530, Metrics: {'mse': 0.052506301552057266, 'rmse': 0.2291425354491332, 'r2': 0.19070327281951904}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  5.08it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.77it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.49it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.00it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.91it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.50it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.95it/s]
[2025-04-29 14:34:28,317][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0330
[2025-04-29 14:34:28,629][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0497, Metrics: {'mse': 0.049211110919713974, 'rmse': 0.22183577466160406, 'r2': 0.2414931058883667}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▃▃▂▁
wandb:     best_val_mse █▆▅▄▄▃▃▂▁
wandb:      best_val_r2 ▁▃▄▅▅▆▆▇█
wandb:    best_val_rmse █▇▆▅▄▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▅▄▄▃▃▂▂▁
wandb:          val_mse ▆█▅▄▄▃▃▂▂▁
wandb:           val_r2 ▃▁▄▅▅▆▆▇▇█
wandb:         val_rmse ▇█▅▅▄▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0497
wandb:     best_val_mse 0.04921
wandb:      best_val_r2 0.24149
wandb:    best_val_rmse 0.22184
wandb:            epoch 10
wandb:   final_test_mse 0.0562
wandb:    final_test_r2 0.03112
wandb:  final_test_rmse 0.23707
wandb:  final_train_mse 0.01837
wandb:   final_train_r2 0.40154
wandb: final_train_rmse 0.13554
wandb:    final_val_mse 0.04921
wandb:     final_val_r2 0.24149
wandb:   final_val_rmse 0.22184
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03296
wandb:       train_time 46.01016
wandb:         val_loss 0.0497
wandb:          val_mse 0.04921
wandb:           val_r2 0.24149
wandb:         val_rmse 0.22184
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143333-xzdf9qf9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143333-xzdf9qf9/logs
Standard experiment completed successfully: layer_11_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/complexity/results.json
Running question_type experiment for language ar, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:34:47,042][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/question_type
experiment_name: layer_12_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:34:47,042][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:34:47,042][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:34:47,042][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:34:47,047][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:34:47,047][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:34:48,199][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:34:50,900][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:34:50,900][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:50,929][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:50,947][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,032][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:34:51,041][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,042][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:34:51,043][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:51,057][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,086][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:34:51,087][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,088][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:34:51,089][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:51,101][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,126][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:34:51,128][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,128][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,130][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:34:51,130][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,131][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:34:51,131][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,132][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:34:51,132][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:34:51,133][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:34:54,797][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:34:54,798][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:34:54,800][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:34:54,800][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 14:34:54,800][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:56,  1.10it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:16,  3.60it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.11it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.46it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.55it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.29it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.69it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.79it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.62it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.24it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.68it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.01it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.25it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.42it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.58it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s]
[2025-04-29 14:35:00,843][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6933
[2025-04-29 14:35:01,129][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6904, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.86it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.66it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:35:05,352][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6926
[2025-04-29 14:35:05,653][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6901, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 10.62it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.41it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 14.95it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 15.89it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.51it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 16.96it/s]
[2025-04-29 14:35:10,129][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6910
[2025-04-29 14:35:10,452][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6898, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.94it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.64it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.13it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:35:14,686][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6894
[2025-04-29 14:35:14,993][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6894, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 10.98it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.15it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:35:19,301][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6857
[2025-04-29 14:35:19,634][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6890, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.05it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.72it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.47it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.01it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.90it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.49it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.13it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.42it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:35:23,899][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6881
[2025-04-29 14:35:24,232][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6886, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.12it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.80it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.52it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.01it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.93it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.97it/s]
[2025-04-29 14:35:28,528][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6858
[2025-04-29 14:35:28,853][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6883, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.22it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.91it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.60it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.08it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.90it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:35:33,112][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6837
[2025-04-29 14:35:33,444][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6879, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.34it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:35:37,746][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6816
[2025-04-29 14:35:38,067][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6875, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.33it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.04it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.69it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.14it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.99it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:35:42,330][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6847
[2025-04-29 14:35:42,660][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6871, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.09523809523809523}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁█
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁█
wandb:        best_val_loss █▇▇▆▅▄▄▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▆▃▅▄▂▁▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁█
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁█
wandb:             val_loss █▇▇▆▅▄▄▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.56818
wandb:          best_val_f1 0.09524
wandb:        best_val_loss 0.68711
wandb:                epoch 10
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.50553
wandb:       final_train_f1 0.01992
wandb:   final_val_accuracy 0.56818
wandb:         final_val_f1 0.09524
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68466
wandb:           train_time 46.80303
wandb:         val_accuracy 0.56818
wandb:               val_f1 0.09524
wandb:             val_loss 0.68711
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143447-lo4u35yr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143447-lo4u35yr/logs
Standard experiment completed successfully: layer_12_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/question_type/results.json
Running complexity experiment for language ar, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:35:59,845][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/complexity
experiment_name: layer_12_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:35:59,845][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:35:59,845][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:35:59,845][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:35:59,850][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:35:59,850][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:36:01,293][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:36:03,983][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:36:03,983][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,029][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,091][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:36:04,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,101][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:36:04,102][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,113][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,138][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:36:04,140][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,140][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:36:04,141][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,169][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,178][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:36:04,179][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,179][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:36:04,180][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:36:04,180][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,181][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,181][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,182][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,182][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,183][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,183][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:36:04,184][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:36:07,780][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:36:07,781][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:36:07,783][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:36:07,783][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 14:36:07,783][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<01:01,  1.01it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:17,  3.34it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:10,  5.73it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.05it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.16it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 11.96it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.44it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.59it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.46it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:02, 16.13it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.60it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.94it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.21it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.37it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:04<00:00, 17.77it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.10it/s]
[2025-04-29 14:36:13,833][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2303
[2025-04-29 14:36:14,121][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2258, Metrics: {'mse': 0.22809863090515137, 'rmse': 0.4775967241356994, 'r2': -2.515758514404297}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:11,  5.53it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.26it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 13.89it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.82it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:36:18,373][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.2092
[2025-04-29 14:36:18,666][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2054, Metrics: {'mse': 0.20750455558300018, 'rmse': 0.4555266793317381, 'r2': -2.1983354091644287}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.58it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.08it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:36:23,181][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1846
[2025-04-29 14:36:23,508][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1866, Metrics: {'mse': 0.18848638236522675, 'rmse': 0.434150184112856, 'r2': -1.9052019119262695}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.76it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.46it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:36:27,730][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1716
[2025-04-29 14:36:28,050][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1710, Metrics: {'mse': 0.1726812869310379, 'rmse': 0.4155493796542571, 'r2': -1.661592960357666}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.04it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:36:32,335][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1544
[2025-04-29 14:36:32,662][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1569, Metrics: {'mse': 0.15852145850658417, 'rmse': 0.3981475335934962, 'r2': -1.443342924118042}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.10it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.78it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.51it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 16.86it/s]
[2025-04-29 14:36:36,973][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1430
[2025-04-29 14:36:37,301][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1440, Metrics: {'mse': 0.14541108906269073, 'rmse': 0.3813280596319798, 'r2': -1.2412683963775635}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.73it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.15it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.02it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.59it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:36:41,596][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.1285
[2025-04-29 14:36:41,903][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1324, Metrics: {'mse': 0.13371588289737701, 'rmse': 0.36567182404087, 'r2': -1.0610063076019287}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.02it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:36:46,187][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.1167
[2025-04-29 14:36:46,590][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1222, Metrics: {'mse': 0.12340044975280762, 'rmse': 0.35128400156114087, 'r2': -0.9020113945007324}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.38it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.11it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.75it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.20it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.66it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.64it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.65it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.70it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:36:50,937][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.1043
[2025-04-29 14:36:51,270][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1130, Metrics: {'mse': 0.11408672481775284, 'rmse': 0.33776726427786463, 'r2': -0.7584558725357056}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.13it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.76it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.19it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.12it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.28it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.38it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.51it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.55it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.57it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.59it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.60it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.62it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.62it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.61it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.62it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.63it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.62it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.62it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.92it/s]
[2025-04-29 14:36:55,567][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0972
[2025-04-29 14:36:55,881][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1056, Metrics: {'mse': 0.10653552412986755, 'rmse': 0.32639780043662603, 'r2': -0.6420667171478271}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▄▃▃▂▁▁
wandb:     best_val_mse █▇▆▅▄▃▃▂▁▁
wandb:      best_val_r2 ▁▂▃▄▅▆▆▇██
wandb:    best_val_rmse █▇▆▅▄▄▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▅▄▃▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▅▄▃▃▂▁▁
wandb:          val_mse █▇▆▅▄▃▃▂▁▁
wandb:           val_r2 ▁▂▃▄▅▆▆▇██
wandb:         val_rmse █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.10558
wandb:     best_val_mse 0.10654
wandb:      best_val_r2 -0.64207
wandb:    best_val_rmse 0.3264
wandb:            epoch 10
wandb:   final_test_mse 0.11551
wandb:    final_test_r2 -0.99135
wandb:  final_test_rmse 0.33987
wandb:  final_train_mse 0.08636
wandb:   final_train_r2 -1.81331
wandb: final_train_rmse 0.29387
wandb:    final_val_mse 0.10654
wandb:     final_val_r2 -0.64207
wandb:   final_val_rmse 0.3264
wandb:    learning_rate 1e-05
wandb:       train_loss 0.09722
wandb:       train_time 47.10005
wandb:         val_loss 0.10558
wandb:          val_mse 0.10654
wandb:           val_r2 -0.64207
wandb:         val_rmse 0.3264
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143559-x651jc5a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143559-x651jc5a/logs
Standard experiment completed successfully: layer_12_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/complexity/results.json
Running question_type experiment for language en, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:37:13,349][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/question_type
experiment_name: layer_1_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:37:13,349][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:37:13,350][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:37:13,350][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:37:13,354][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:37:13,355][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:37:14,828][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:37:17,622][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:37:17,623][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,719][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,803][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:37:17,814][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,815][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:37:17,816][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,827][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,844][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,853][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,866][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,894][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:37:17,896][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,896][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:37:17,897][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,898][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:37:17,898][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,899][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:37:17,899][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,900][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:37:17,900][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:37:17,901][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:37:21,604][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:37:21,604][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:37:21,607][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:37:21,607][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:37:21,607][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.02it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.68it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.10it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.15it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.83it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.15it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.16it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.90it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.44it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.10it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.31it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.06it/s]
[2025-04-29 14:37:28,133][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6955
[2025-04-29 14:37:28,531][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.91it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.18it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.21it/s]
[2025-04-29 14:37:33,447][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6976
[2025-04-29 14:37:33,853][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.04it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:37:38,216][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6934
[2025-04-29 14:37:38,626][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.13it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.88it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.31it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.60it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.31it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.61it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.66it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.68it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 14:37:42,998][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6941
[2025-04-29 14:37:43,401][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:37:43,402][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂█▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69288
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69406
wandb:           train_time 20.25218
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69296
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143713-e0h4ho3q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143713-e0h4ho3q/logs
Standard experiment completed successfully: layer_1_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/question_type/results.json
Running complexity experiment for language en, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:37:59,584][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/complexity
experiment_name: layer_1_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:37:59,584][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:37:59,584][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:37:59,584][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:37:59,589][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:37:59,589][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:38:00,715][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:38:03,415][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:38:03,416][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,447][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,495][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:38:03,506][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,507][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:38:03,507][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,518][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,536][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,545][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:38:03,546][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,546][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:38:03,547][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,558][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,575][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,583][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:38:03,585][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,585][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,587][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,587][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,588][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,588][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,589][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,589][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:38:03,590][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:38:03,590][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:38:06,992][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:38:06,992][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:38:06,995][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:38:06,995][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:38:06,995][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.05it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.72it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.14it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.18it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.87it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.16it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.16it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.91it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.11it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.05it/s]
[2025-04-29 14:38:13,390][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1279
[2025-04-29 14:38:13,783][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0473, Metrics: {'mse': 0.050655629485845566, 'rmse': 0.22506805523184664, 'r2': -0.2103743553161621}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.93it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.26it/s]
[2025-04-29 14:38:18,686][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0487
[2025-04-29 14:38:19,078][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0427, Metrics: {'mse': 0.04314103722572327, 'rmse': 0.20770420608577783, 'r2': -0.030819416046142578}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.59it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.38it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 14.93it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.89it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 14:38:24,516][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0385
[2025-04-29 14:38:24,966][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0469, Metrics: {'mse': 0.04666321724653244, 'rmse': 0.2160167059431572, 'r2': -0.11497890949249268}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.18it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:38:29,367][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0376
[2025-04-29 14:38:29,811][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0481, Metrics: {'mse': 0.04766707122325897, 'rmse': 0.21832789840801145, 'r2': -0.13896512985229492}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.84it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.55it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 14:38:34,229][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0364
[2025-04-29 14:38:34,691][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0491, Metrics: {'mse': 0.048533402383327484, 'rmse': 0.2203029786074793, 'r2': -0.15966546535491943}
[2025-04-29 14:38:34,692][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁
wandb:       train_loss █▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▁▆▇█
wandb:          val_mse █▁▄▅▆
wandb:           val_r2 ▁█▅▄▃
wandb:         val_rmse █▁▄▅▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04267
wandb:     best_val_mse 0.04314
wandb:      best_val_r2 -0.03082
wandb:    best_val_rmse 0.2077
wandb:            epoch 5
wandb:   final_test_mse 0.04052
wandb:    final_test_r2 -0.05136
wandb:  final_test_rmse 0.20129
wandb:  final_train_mse 0.02815
wandb:   final_train_r2 -0.04919
wandb: final_train_rmse 0.16777
wandb:    final_val_mse 0.04314
wandb:     final_val_r2 -0.03082
wandb:   final_val_rmse 0.2077
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03643
wandb:       train_time 26.28866
wandb:         val_loss 0.0491
wandb:          val_mse 0.04853
wandb:           val_r2 -0.15967
wandb:         val_rmse 0.2203
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143759-u8h2xmvt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143759-u8h2xmvt/logs
Standard experiment completed successfully: layer_1_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/complexity/results.json
Running question_type experiment for language en, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:38:52,577][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/question_type
experiment_name: layer_2_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:38:52,577][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:38:52,577][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:38:52,577][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:38:52,581][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:38:52,582][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:38:53,693][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:38:56,388][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:38:56,389][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,419][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,435][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,492][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:38:56,503][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,504][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:38:56,505][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,516][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,533][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,542][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:38:56,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,544][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:38:56,544][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,557][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,583][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:38:56,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,585][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:38:56,585][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,586][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,588][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:38:56,589][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:39:00,263][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:39:00,264][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:39:00,266][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:39:00,267][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:39:00,267][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:09,  1.07it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.52it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  5.99it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.34it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.43it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.19it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.62it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.74it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.56it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.19it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.65it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.99it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.20it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.38it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.65it/s]
[2025-04-29 14:39:07,003][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6958
[2025-04-29 14:39:07,400][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.53it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:39:12,339][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6976
[2025-04-29 14:39:12,745][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.08it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.85it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.31it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.61it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:39:17,107][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-29 14:39:17,514][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:11,  6.22it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.96it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.38it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 14:39:21,881][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6943
[2025-04-29 14:39:22,306][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:39:22,307][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69296
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69426
wandb:           train_time 20.42568
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143852-cgmjt4fs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143852-cgmjt4fs/logs
Standard experiment completed successfully: layer_2_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/question_type/results.json
Running complexity experiment for language en, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:39:48,582][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/complexity
experiment_name: layer_2_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:39:48,582][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:39:48,582][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:39:48,582][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:39:48,587][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:39:48,587][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:39:49,597][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:39:52,306][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:39:52,307][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:39:52,321][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,386][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:39:52,398][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:39:52,398][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:39:52,399][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:39:52,410][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,427][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,436][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:39:52,437][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:39:52,437][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:39:52,438][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:39:52,451][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,473][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:39:52,483][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:39:52,485][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:39:52,485][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:39:52,486][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:39:52,486][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:39:52,487][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:39:52,487][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:39:52,487][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:39:52,487][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:39:52,487][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:39:52,487][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:39:52,488][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:39:52,488][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:39:52,488][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:39:52,488][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:39:52,488][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:39:52,488][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:39:52,488][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:39:52,489][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:39:52,489][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:39:52,489][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:39:52,490][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:39:52,490][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:39:52,490][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:39:52,490][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:39:52,490][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:39:56,084][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:39:56,084][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:39:56,087][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:39:56,087][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:39:56,087][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.57it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.06it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.43it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.52it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.29it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.70it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.81it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.61it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.24it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.68it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.25it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.44it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.77it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.82it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.73it/s]
[2025-04-29 14:40:03,271][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1488
[2025-04-29 14:40:03,651][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0688, Metrics: {'mse': 0.07416564971208572, 'rmse': 0.27233371020144703, 'r2': -0.7721267938613892}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.30it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.07it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.47it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.75it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.46it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.91it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.37it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.53it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.65it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.32it/s]
[2025-04-29 14:40:08,542][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0740
[2025-04-29 14:40:08,939][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0429, Metrics: {'mse': 0.04550982266664505, 'rmse': 0.2133303135202427, 'r2': -0.08741950988769531}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.70it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.49it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.03it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.80it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.79it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.81it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.80it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:40:14,352][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0463
[2025-04-29 14:40:14,812][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0412, Metrics: {'mse': 0.0421878956258297, 'rmse': 0.20539692214302943, 'r2': -0.008044838905334473}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 10.98it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.68it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.01it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:40:19,760][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0394
[2025-04-29 14:40:20,165][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0442, Metrics: {'mse': 0.044374365359544754, 'rmse': 0.21065223796471935, 'r2': -0.06028878688812256}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.20it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:40:24,558][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0376
[2025-04-29 14:40:24,994][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0464, Metrics: {'mse': 0.04619155824184418, 'rmse': 0.21492221439824263, 'r2': -0.10370898246765137}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  6.01it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.76it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.22it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.54it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.26it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.72it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.04it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.79it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.79it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.80it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.79it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.79it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.80it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.77it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.23it/s]
[2025-04-29 14:40:29,350][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0376
[2025-04-29 14:40:29,780][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0474, Metrics: {'mse': 0.04710088297724724, 'rmse': 0.21702737840477002, 'r2': -0.12543654441833496}
[2025-04-29 14:40:29,781][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▂▂▃
wandb:          val_mse █▂▁▁▂▂
wandb:           val_r2 ▁▇██▇▇
wandb:         val_rmse █▂▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04119
wandb:     best_val_mse 0.04219
wandb:      best_val_r2 -0.00804
wandb:    best_val_rmse 0.2054
wandb:            epoch 6
wandb:   final_test_mse 0.04338
wandb:    final_test_r2 -0.1255
wandb:  final_test_rmse 0.20827
wandb:  final_train_mse 0.03043
wandb:   final_train_r2 -0.13415
wandb: final_train_rmse 0.17444
wandb:    final_val_mse 0.04219
wandb:     final_val_r2 -0.00804
wandb:   final_val_rmse 0.2054
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03759
wandb:       train_time 31.60276
wandb:         val_loss 0.04743
wandb:          val_mse 0.0471
wandb:           val_r2 -0.12544
wandb:         val_rmse 0.21703
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143948-d3fegt5c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143948-d3fegt5c/logs
Standard experiment completed successfully: layer_2_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/complexity/results.json
Running question_type experiment for language en, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:40:47,013][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_3/question_type
experiment_name: layer_3_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:40:47,013][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:40:47,013][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:40:47,013][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:40:47,017][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:40:47,018][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:40:48,216][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:40:50,928][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:40:50,928][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:40:50,967][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:50,985][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:51,039][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:40:51,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:40:51,050][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:40:51,051][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:40:51,067][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:51,091][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:51,100][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:40:51,102][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:40:51,102][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:40:51,103][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:40:51,115][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:51,136][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:40:51,144][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:40:51,146][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:40:51,146][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:40:51,147][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:40:51,147][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:40:51,147][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:40:51,147][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:40:51,148][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:40:51,148][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:40:51,148][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:40:51,149][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:40:51,149][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:40:51,149][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:40:51,150][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:40:51,150][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:40:51,150][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:40:51,150][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:40:51,150][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:40:51,150][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:40:51,150][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:40:51,151][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:40:54,716][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:40:54,717][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:40:54,719][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:40:54,719][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:40:54,719][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:13,  1.01it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:21,  3.33it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:12,  5.72it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.02it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.11it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 11.91it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.39it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.54it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.43it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:03, 16.09it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.58it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.94it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.19it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.38it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.59it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:03<00:02, 17.70it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.51it/s]
[2025-04-29 14:41:01,484][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6959
[2025-04-29 14:41:01,887][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 14:41:06,833][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6973
[2025-04-29 14:41:07,240][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.06it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.80it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.57it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:41:11,604][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6939
[2025-04-29 14:41:12,012][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.89it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.64it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.14it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.47it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.73it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:41:16,388][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6941
[2025-04-29 14:41:16,795][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:41:16,796][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69297
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69411
wandb:           train_time 20.48351
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144047-soyli22v
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144047-soyli22v/logs
Standard experiment completed successfully: layer_3_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_3/question_type/results.json
Running complexity experiment for language en, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:41:33,559][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_3/complexity
experiment_name: layer_3_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:41:33,559][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:41:33,560][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:41:33,560][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:41:33,564][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:41:33,564][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:41:34,531][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:41:37,211][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:41:37,211][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:41:37,227][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,246][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,305][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:41:37,316][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:41:37,316][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:41:37,317][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:41:37,329][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,347][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,357][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:41:37,358][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:41:37,358][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:41:37,359][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:41:37,370][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,388][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:41:37,397][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:41:37,398][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:41:37,399][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:41:37,399][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:41:37,400][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:41:37,400][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:41:37,400][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:41:37,400][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:41:37,400][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:41:37,401][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:41:37,401][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:41:37,401][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:41:37,402][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:41:37,402][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:41:37,402][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:41:37,403][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:41:37,403][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:41:37,403][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:41:37,403][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:41:37,403][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:41:37,403][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:41:37,404][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:41:41,013][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:41:41,014][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:41:41,016][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:41:41,016][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:41:41,016][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.27it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.05it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.71it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.11it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.17it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.84it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.16it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.17it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.91it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.46it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.86it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.14it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.34it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.49it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.05it/s]
[2025-04-29 14:41:47,711][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1518
[2025-04-29 14:41:48,083][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0734, Metrics: {'mse': 0.07897582650184631, 'rmse': 0.281026380437578, 'r2': -0.8870619535446167}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.21it/s]Epoch 2/10:   3%|▎         | 2/75 [00:00<00:09,  7.83it/s]Epoch 2/10:   5%|▌         | 4/75 [00:00<00:05, 12.03it/s]Epoch 2/10:   8%|▊         | 6/75 [00:00<00:04, 14.17it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:04, 15.40it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:00<00:04, 16.17it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:00<00:03, 16.67it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:00<00:03, 17.01it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:03, 17.25it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:03, 17.38it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  40%|████      | 30/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  51%|█████     | 38/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  80%|████████  | 60/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:04<00:00, 17.80it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:04<00:00, 17.81it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:04<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:41:53,042][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0782
[2025-04-29 14:41:53,440][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0451, Metrics: {'mse': 0.04810761287808418, 'rmse': 0.21933447717603402, 'r2': -0.14949166774749756}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.05it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.74it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.49it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.02it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:41:58,848][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0482
[2025-04-29 14:41:59,275][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0411, Metrics: {'mse': 0.0423535518348217, 'rmse': 0.20579978579877506, 'r2': -0.012003064155578613}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:42:04,223][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0393
[2025-04-29 14:42:04,659][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0435, Metrics: {'mse': 0.04391206055879593, 'rmse': 0.20955204737438365, 'r2': -0.049242258071899414}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  5.94it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.17it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.49it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.68it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 14:42:09,045][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0370
[2025-04-29 14:42:09,481][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0458, Metrics: {'mse': 0.04571666941046715, 'rmse': 0.21381456781629063, 'r2': -0.0923619270324707}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.47it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.53it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.58it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.63it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.65it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.66it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:42:13,878][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0368
[2025-04-29 14:42:14,320][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0469, Metrics: {'mse': 0.046684443950653076, 'rmse': 0.2160658324461623, 'r2': -0.11548614501953125}
[2025-04-29 14:42:14,321][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▁▂▂▂
wandb:          val_mse █▂▁▁▂▂
wandb:           val_r2 ▁▇██▇▇
wandb:         val_rmse █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04106
wandb:     best_val_mse 0.04235
wandb:      best_val_r2 -0.012
wandb:    best_val_rmse 0.2058
wandb:            epoch 6
wandb:   final_test_mse 0.04562
wandb:    final_test_r2 -0.1838
wandb:  final_test_rmse 0.21359
wandb:  final_train_mse 0.03229
wandb:   final_train_r2 -0.20356
wandb: final_train_rmse 0.17969
wandb:    final_val_mse 0.04235
wandb:     final_val_r2 -0.012
wandb:   final_val_rmse 0.2058
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03682
wandb:       train_time 31.59393
wandb:         val_loss 0.04693
wandb:          val_mse 0.04668
wandb:           val_r2 -0.11549
wandb:         val_rmse 0.21607
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144133-abav5zdc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144133-abav5zdc/logs
Standard experiment completed successfully: layer_3_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_3/complexity/results.json
Running question_type experiment for language en, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:42:32,295][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_4/question_type
experiment_name: layer_4_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:42:32,295][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:42:32,295][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:42:32,295][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:42:32,299][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:42:32,300][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:42:33,833][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:42:36,561][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:42:36,562][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:42:36,623][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,652][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,747][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:42:36,759][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:42:36,759][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:42:36,760][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:42:36,845][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,882][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,893][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:42:36,894][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:42:36,895][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:42:36,896][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:42:36,919][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,945][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:42:36,956][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:42:36,957][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:42:36,957][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:42:36,958][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:42:36,958][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:42:36,959][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:42:36,959][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:42:36,959][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:42:36,959][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:42:36,959][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:42:36,959][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:42:36,959][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:42:36,960][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:42:36,960][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:42:36,960][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:42:36,961][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:42:36,961][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:42:36,961][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:42:36,962][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:42:36,962][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:42:40,815][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:42:40,815][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:42:40,818][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:42:40,818][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:42:40,818][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.15it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.74it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.30it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.68it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.76it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.50it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.87it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.74it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.32it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.75it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.07it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.29it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.85it/s]
[2025-04-29 14:42:48,133][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6962
[2025-04-29 14:42:48,537][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.83it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.58it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.47it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.32it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.21it/s]
[2025-04-29 14:42:53,448][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6979
[2025-04-29 14:42:53,850][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  5.94it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.69it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:42:58,213][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6935
[2025-04-29 14:42:58,605][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.15it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.91it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.35it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.59it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.82it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 14:43:02,972][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6942
[2025-04-29 14:43:03,390][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:43:03,391][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69294
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69422
wandb:           train_time 20.31185
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.693
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144232-cq7afchp
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144232-cq7afchp/logs
Standard experiment completed successfully: layer_4_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_4/question_type/results.json
Running complexity experiment for language en, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:43:20,043][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_4/complexity
experiment_name: layer_4_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:43:20,043][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:43:20,043][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:43:20,043][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:43:20,048][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:43:20,048][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:43:21,101][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:43:23,820][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:43:23,820][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:43:23,838][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:23,857][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:23,921][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:43:23,932][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:43:23,933][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:43:23,934][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:43:23,947][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:23,966][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:23,975][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:43:23,977][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:43:23,977][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:43:23,978][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:43:23,994][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:24,013][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:43:24,022][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:43:24,024][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:43:24,024][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:43:24,025][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:43:24,025][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:43:24,025][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:43:24,025][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:43:24,025][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:43:24,026][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:43:24,026][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:43:24,026][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:43:24,026][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:43:24,026][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:43:24,026][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:43:24,026][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:43:24,027][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:43:24,027][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:43:24,027][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:43:24,028][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:43:24,028][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:43:24,028][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:43:24,028][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:43:24,028][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:43:24,028][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:43:24,028][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:43:24,029][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:43:24,029][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:43:27,534][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:43:27,535][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:43:27,537][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:43:27,538][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:43:27,538][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.14it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.72it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.27it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.65it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.74it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.47it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.85it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.74it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.32it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.74it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.05it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.50it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.82it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.88it/s]
[2025-04-29 14:43:34,112][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1497
[2025-04-29 14:43:34,509][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0694, Metrics: {'mse': 0.07480230927467346, 'rmse': 0.2735001083631841, 'r2': -0.7873392105102539}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.97it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.76it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.82it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.34it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.65it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.80it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.22it/s]
[2025-04-29 14:43:39,424][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0748
[2025-04-29 14:43:39,831][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0432, Metrics: {'mse': 0.045810289680957794, 'rmse': 0.21403338450101142, 'r2': -0.09459888935089111}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.85it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.55it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.07it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:43:45,257][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0473
[2025-04-29 14:43:45,697][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0414, Metrics: {'mse': 0.04242323711514473, 'rmse': 0.20596901979459126, 'r2': -0.013668179512023926}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.25it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.85it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.28it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:43:50,669][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0415
[2025-04-29 14:43:51,105][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0440, Metrics: {'mse': 0.04426656290888786, 'rmse': 0.21039620459715488, 'r2': -0.05771279335021973}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  6.08it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.82it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.25it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.55it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 14:43:55,478][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0393
[2025-04-29 14:43:55,886][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0459, Metrics: {'mse': 0.045835819095373154, 'rmse': 0.21409301505507636, 'r2': -0.09520900249481201}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  6.09it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.83it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.28it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.54it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.74it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.68it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 14:44:00,265][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0392
[2025-04-29 14:44:00,669][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0470, Metrics: {'mse': 0.046684570610523224, 'rmse': 0.2160661255507749, 'r2': -0.11548912525177002}
[2025-04-29 14:44:00,670][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▂▂▂
wandb:          val_mse █▂▁▁▂▂
wandb:           val_r2 ▁▇██▇▇
wandb:         val_rmse █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04139
wandb:     best_val_mse 0.04242
wandb:      best_val_r2 -0.01367
wandb:    best_val_rmse 0.20597
wandb:            epoch 6
wandb:   final_test_mse 0.04384
wandb:    final_test_r2 -0.13744
wandb:  final_test_rmse 0.20937
wandb:  final_train_mse 0.03072
wandb:   final_train_r2 -0.14506
wandb: final_train_rmse 0.17527
wandb:    final_val_mse 0.04242
wandb:     final_val_r2 -0.01367
wandb:   final_val_rmse 0.20597
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03916
wandb:       train_time 31.60152
wandb:         val_loss 0.04696
wandb:          val_mse 0.04668
wandb:           val_r2 -0.11549
wandb:         val_rmse 0.21607
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144320-6vxa904a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144320-6vxa904a/logs
Standard experiment completed successfully: layer_4_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_4/complexity/results.json
Running question_type experiment for language en, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:44:19,045][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_5/question_type
experiment_name: layer_5_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:44:19,046][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:44:19,046][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:44:19,046][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:44:19,050][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:44:19,051][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:44:20,508][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:44:23,203][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:44:23,204][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:44:23,233][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,251][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,314][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:44:23,325][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:44:23,326][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:44:23,327][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:44:23,339][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,357][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,367][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:44:23,368][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:44:23,368][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:44:23,369][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:44:23,381][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,400][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:44:23,409][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:44:23,411][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:44:23,411][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:44:23,412][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:44:23,412][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:44:23,412][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:44:23,412][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:44:23,412][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:44:23,413][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:44:23,413][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:44:23,413][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:44:23,413][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:44:23,414][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:44:23,414][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:44:23,414][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:44:23,415][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:44:23,415][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:44:23,415][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:44:23,415][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:44:23,415][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:44:23,415][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:44:23,415][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:44:27,113][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:44:27,114][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:44:27,116][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:44:27,116][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:44:27,117][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:02,  1.18it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.80it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.38it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.75it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.83it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.54it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.89it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.95it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.75it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.34it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.76it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.05it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.89it/s]
[2025-04-29 14:44:33,811][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6957
[2025-04-29 14:44:34,204][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.82it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.58it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.11it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.27it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 14:44:39,132][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6972
[2025-04-29 14:44:39,541][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.29it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.00it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.80it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.79it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 14:44:44,684][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6936
[2025-04-29 14:44:45,083][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.4861111111111111, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.03it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.17it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.04it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:44:49,483][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 14:44:49,904][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.05263157894736842}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  5.71it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.45it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.97it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.35it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.18it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 14:44:54,288][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6940
[2025-04-29 14:44:54,702][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5138888888888888, 'f1': 0.18604651162790697}
[2025-04-29 14:44:54,703][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▁
wandb:           train_loss ▅█▁▂▂
wandb:           train_time ▁
wandb:         val_accuracy ▅▅▁▅█
wandb:               val_f1 ▁▁▁▃█
wandb:             val_loss ▂▁▇▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69296
wandb:                epoch 5
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69396
wandb:           train_time 25.93151
wandb:         val_accuracy 0.51389
wandb:               val_f1 0.18605
wandb:             val_loss 0.69299
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144419-ok05q1tf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144419-ok05q1tf/logs
Standard experiment completed successfully: layer_5_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_5/question_type/results.json
Running complexity experiment for language en, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:45:12,086][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_5/complexity
experiment_name: layer_5_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:45:12,086][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:45:12,086][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:45:12,086][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:45:12,127][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:45:12,127][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:45:13,264][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:45:15,963][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:45:15,964][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:45:16,007][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,026][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,104][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:45:16,115][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:45:16,116][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:45:16,117][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:45:16,130][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,150][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,160][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:45:16,161][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:45:16,161][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:45:16,162][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:45:16,175][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,192][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:45:16,201][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:45:16,203][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:45:16,203][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:45:16,205][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:45:16,205][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:45:16,205][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:45:16,206][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:45:16,206][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:45:16,206][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:45:16,206][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:45:16,206][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:45:16,206][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:45:16,207][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:45:16,207][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:45:16,207][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:45:16,208][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:45:16,208][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:45:16,208][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:45:16,209][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:45:16,209][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:45:16,209][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:45:19,682][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:45:19,683][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:45:19,685][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:45:19,685][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:45:19,685][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:06,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:19,  3.61it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.12it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.48it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.57it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.33it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.74it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.82it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.64it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.24it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.68it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.23it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.40it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.52it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.59it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.76it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.75it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.71it/s]
[2025-04-29 14:45:26,864][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1403
[2025-04-29 14:45:27,248][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0653, Metrics: {'mse': 0.07040581852197647, 'rmse': 0.26534094769178856, 'r2': -0.6822887659072876}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.39it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.13it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.50it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.74it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.44it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.90it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.40it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.52it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.66it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.68it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.30it/s]
[2025-04-29 14:45:32,145][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0685
[2025-04-29 14:45:32,556][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0424, Metrics: {'mse': 0.04474625363945961, 'rmse': 0.21153310294008268, 'r2': -0.06917464733123779}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.07it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.68it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.12it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:45:37,713][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0436
[2025-04-29 14:45:38,131][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0423, Metrics: {'mse': 0.04306505620479584, 'rmse': 0.20752121868569448, 'r2': -0.02900385856628418}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.56it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.29it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.91it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.31it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.16it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:45:43,081][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0382
[2025-04-29 14:45:43,512][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0454, Metrics: {'mse': 0.045483969151973724, 'rmse': 0.21326970987923655, 'r2': -0.08680176734924316}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.79it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 14:45:47,900][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0365
[2025-04-29 14:45:48,321][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0473, Metrics: {'mse': 0.0470954068005085, 'rmse': 0.2170147617110608, 'r2': -0.12530577182769775}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  6.05it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.81it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:45:52,698][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0369
[2025-04-29 14:45:53,095][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0480, Metrics: {'mse': 0.047655727714300156, 'rmse': 0.21830191871419766, 'r2': -0.138694167137146}
[2025-04-29 14:45:53,096][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▂▃▃
wandb:          val_mse █▁▁▂▂▂
wandb:           val_r2 ▁██▇▇▇
wandb:         val_rmse █▁▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04227
wandb:     best_val_mse 0.04307
wandb:      best_val_r2 -0.029
wandb:    best_val_rmse 0.20752
wandb:            epoch 6
wandb:   final_test_mse 0.0428
wandb:    final_test_r2 -0.11065
wandb:  final_test_rmse 0.20689
wandb:  final_train_mse 0.02964
wandb:   final_train_r2 -0.10496
wandb: final_train_rmse 0.17218
wandb:    final_val_mse 0.04307
wandb:     final_val_r2 -0.029
wandb:   final_val_rmse 0.20752
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03695
wandb:       train_time 31.33204
wandb:         val_loss 0.048
wandb:          val_mse 0.04766
wandb:           val_r2 -0.13869
wandb:         val_rmse 0.2183
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144512-k1z1n403
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144512-k1z1n403/logs
Standard experiment completed successfully: layer_5_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_5/complexity/results.json
Running question_type experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:46:10,053][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type
experiment_name: layer_6_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:46:10,053][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:46:10,053][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:46:10,053][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:46:10,058][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:46:10,058][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:46:11,176][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:46:14,033][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:46:14,033][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:46:14,060][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,170][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:46:14,182][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:46:14,183][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:46:14,184][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:46:14,196][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,214][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,222][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:46:14,223][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:46:14,224][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:46:14,224][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:46:14,235][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,253][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:46:14,262][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:46:14,263][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:46:14,263][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:46:14,264][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:46:14,264][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:46:14,265][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:46:14,265][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:46:14,265][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:46:14,265][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:46:14,265][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:46:14,265][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:46:14,265][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:46:14,266][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:46:14,266][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:46:14,266][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:46:14,267][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:46:14,267][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:46:14,267][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:46:14,268][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:46:14,268][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:46:17,916][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:46:17,917][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:46:17,919][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:46:17,919][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:46:17,919][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:01,  1.20it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.89it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.50it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.89it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.97it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.67it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.99it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.04it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.82it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.38it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.79it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.08it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.29it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.91it/s]
[2025-04-29 14:46:24,635][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6955
[2025-04-29 14:46:25,011][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.46it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.22it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.57it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.78it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.47it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.90it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.37it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.52it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.65it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.27it/s]
[2025-04-29 14:46:29,912][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6968
[2025-04-29 14:46:30,315][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.17it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.92it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.34it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.62it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.38it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.86it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.23it/s]
[2025-04-29 14:46:34,671][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6935
[2025-04-29 14:46:35,065][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.88it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.62it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.15it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.24it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.74it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.67it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:46:39,442][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6938
[2025-04-29 14:46:39,835][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:46:39,835][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69294
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69384
wandb:           train_time 20.23248
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69305
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144610-8hgow3r8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144610-8hgow3r8/logs
Standard experiment completed successfully: layer_6_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/results.json
Running complexity experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:46:57,284][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity
experiment_name: layer_6_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:46:57,284][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:46:57,284][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:46:57,284][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:46:57,288][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:46:57,289][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:46:58,255][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:47:00,961][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:47:00,962][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:47:00,998][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,020][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,105][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:47:01,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:47:01,117][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:47:01,118][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:47:01,134][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,154][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,164][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:47:01,166][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:47:01,166][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:47:01,167][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:47:01,179][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,198][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:47:01,209][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:47:01,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:47:01,211][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:47:01,211][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:47:01,212][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:47:01,212][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:47:01,212][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:47:01,212][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:47:01,212][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:47:01,213][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:47:01,213][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:47:01,213][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:47:01,214][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:47:01,214][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:47:01,214][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:47:01,215][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:47:01,215][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:47:01,215][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:47:01,215][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:47:01,215][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:47:01,215][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:47:01,216][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:47:04,816][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:47:04,817][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:47:04,819][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:47:04,820][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:47:04,820][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.59it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.09it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.45it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.54it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.30it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.71it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.81it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.64it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.25it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.68it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.40it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.77it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.78it/s]
[2025-04-29 14:47:11,620][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1572
[2025-04-29 14:47:12,001][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0802, Metrics: {'mse': 0.08619427680969238, 'rmse': 0.2935886183245059, 'r2': -1.0595409870147705}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.84it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.63it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.16it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.49it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.31it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.23it/s]
[2025-04-29 14:47:16,931][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0867
[2025-04-29 14:47:17,340][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0488, Metrics: {'mse': 0.052406396716833115, 'rmse': 0.22892443451242403, 'r2': -0.25220751762390137}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.91it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.62it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.11it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.49it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.57it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.64it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:47:22,741][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0528
[2025-04-29 14:47:23,162][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0407, Metrics: {'mse': 0.042565129697322845, 'rmse': 0.20631318352767195, 'r2': -0.017058610916137695}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.95it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.73it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.22it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.53it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 14:47:28,099][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0401
[2025-04-29 14:47:28,531][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0417, Metrics: {'mse': 0.04243284463882446, 'rmse': 0.20599234121399868, 'r2': -0.01389765739440918}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.65it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.38it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.33it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 14:47:32,912][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0361
[2025-04-29 14:47:33,343][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0442, Metrics: {'mse': 0.04436869919300079, 'rmse': 0.21063878843413622, 'r2': -0.06015336513519287}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.99it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.67it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:47:37,744][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0355
[2025-04-29 14:47:38,157][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0461, Metrics: {'mse': 0.045911505818367004, 'rmse': 0.2142697034542378, 'r2': -0.09701740741729736}
[2025-04-29 14:47:38,158][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁
wandb:     best_val_mse █▃▁
wandb:      best_val_r2 ▁▆█
wandb:    best_val_rmse █▃▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▁▁▂▂
wandb:          val_mse █▃▁▁▁▂
wandb:           val_r2 ▁▆███▇
wandb:         val_rmse █▃▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04072
wandb:     best_val_mse 0.04257
wandb:      best_val_r2 -0.01706
wandb:    best_val_rmse 0.20631
wandb:            epoch 6
wandb:   final_test_mse 0.04982
wandb:    final_test_r2 -0.29269
wandb:  final_test_rmse 0.2232
wandb:  final_train_mse 0.03642
wandb:   final_train_r2 -0.35735
wandb: final_train_rmse 0.19083
wandb:    final_val_mse 0.04257
wandb:     final_val_r2 -0.01706
wandb:   final_val_rmse 0.20631
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03547
wandb:       train_time 31.6166
wandb:         val_loss 0.04606
wandb:          val_mse 0.04591
wandb:           val_r2 -0.09702
wandb:         val_rmse 0.21427
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144657-rli6xpde
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144657-rli6xpde/logs
Standard experiment completed successfully: layer_6_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity/results.json
Running question_type experiment for language en, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:47:57,028][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_7/question_type
experiment_name: layer_7_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:47:57,029][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:47:57,029][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:47:57,029][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:47:57,033][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:47:57,033][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:47:58,244][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:48:00,969][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:48:00,969][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:01,036][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,058][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,145][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:48:01,157][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:01,157][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:48:01,158][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:01,171][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,190][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,199][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:48:01,201][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:01,201][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:48:01,202][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:01,215][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,234][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:01,243][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:48:01,245][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:01,245][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:48:01,246][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:48:01,246][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:48:01,246][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:48:01,246][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:48:01,246][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:48:01,247][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:48:01,247][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:48:01,247][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:48:01,247][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:48:01,247][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:48:01,247][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:48:01,247][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:48:01,248][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:48:01,248][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:48:01,248][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:48:01,249][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:48:01,249][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:48:01,249][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:48:01,249][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:48:01,249][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:48:01,249][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:48:01,249][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:48:01,250][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:48:05,064][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:48:05,065][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:48:05,067][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:48:05,068][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:48:05,068][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.04it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.71it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.11it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.16it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.84it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.14it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.15it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.86it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.41it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.80it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.09it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.30it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.72it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.72it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.06it/s]
[2025-04-29 14:48:12,086][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6933
[2025-04-29 14:48:12,460][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  6.16it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.90it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.33it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.62it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.37it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.79it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.80it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.23it/s]
[2025-04-29 14:48:17,380][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6936
[2025-04-29 14:48:17,808][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.09it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.84it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.31it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.59it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.82it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.26it/s]
[2025-04-29 14:48:22,156][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6936
[2025-04-29 14:48:22,551][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.15it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.90it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.28it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 14:48:26,928][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6931
[2025-04-29 14:48:27,332][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:48:27,333][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▄██▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69287
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69313
wandb:           train_time 20.2298
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69291
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144757-09ed714i
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144757-09ed714i/logs
Standard experiment completed successfully: layer_7_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_7/question_type/results.json
Running complexity experiment for language en, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:48:43,729][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_7/complexity
experiment_name: layer_7_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:48:43,729][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:48:43,729][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:48:43,730][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:48:43,734][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:48:43,734][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:48:44,805][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:48:47,514][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:48:47,515][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:47,534][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,556][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,615][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:48:47,626][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:47,627][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:48:47,628][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:47,643][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,663][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,672][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:48:47,673][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:47,673][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:48:47,674][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:48:47,688][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,708][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:48:47,717][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:48:47,718][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:48:47,718][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:48:47,719][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:48:47,719][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:48:47,720][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:48:47,720][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:48:47,720][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:48:47,720][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:48:47,720][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:48:47,720][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:48:47,721][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:48:47,721][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:48:47,721][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:48:47,722][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:48:47,722][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:48:47,722][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:48:47,723][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:48:47,723][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:48:47,723][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:48:47,723][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:48:47,723][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:48:51,231][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:48:51,232][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:48:51,234][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:48:51,234][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:48:51,234][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:06,  1.12it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:19,  3.64it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.15it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.52it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.59it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.35it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.76it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.86it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.68it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.27it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.71it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.81it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.84it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.81it/s]
[2025-04-29 14:48:57,765][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1894
[2025-04-29 14:48:58,152][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1276, Metrics: {'mse': 0.1357690542936325, 'rmse': 0.3684685255128754, 'r2': -2.244089126586914}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.89it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.65it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.67it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.80it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.22it/s]
[2025-04-29 14:49:03,059][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1538
[2025-04-29 14:49:03,453][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1023, Metrics: {'mse': 0.10946360230445862, 'rmse': 0.33085284085898164, 'r2': -1.6155421733856201}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.78it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.52it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.05it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 14:49:08,863][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1244
[2025-04-29 14:49:09,321][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0821, Metrics: {'mse': 0.08818749338388443, 'rmse': 0.29696379136838286, 'r2': -1.1071672439575195}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 10.87it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 15.99it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:49:14,279][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0992
[2025-04-29 14:49:14,710][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0661, Metrics: {'mse': 0.07125631719827652, 'rmse': 0.2669387892350539, 'r2': -0.7026107311248779}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.14it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.04it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.53it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.63it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.66it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.66it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.67it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:49:19,689][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0780
[2025-04-29 14:49:20,108][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0545, Metrics: {'mse': 0.058665499091148376, 'rmse': 0.24220961808142213, 'r2': -0.40176355838775635}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.67it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.42it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 14.98it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.88it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.48it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.49it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 16.94it/s]
[2025-04-29 14:49:25,105][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0632
[2025-04-29 14:49:25,539][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0468, Metrics: {'mse': 0.05009932070970535, 'rmse': 0.22382877542823967, 'r2': -0.1970818042755127}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:12,  5.71it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.46it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:04, 14.00it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.38it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.73it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:49:30,515][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0515
[2025-04-29 14:49:30,933][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0425, Metrics: {'mse': 0.04502904415130615, 'rmse': 0.21220048103457767, 'r2': -0.07593178749084473}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.23it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.92it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.60it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.09it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.52it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.55it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:49:35,903][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0446
[2025-04-29 14:49:36,581][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0408, Metrics: {'mse': 0.04265338554978371, 'rmse': 0.2065269608302599, 'r2': -0.019167304039001465}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.76it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.49it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:04, 14.03it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:49:41,582][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0389
[2025-04-29 14:49:42,015][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0406, Metrics: {'mse': 0.041934117674827576, 'rmse': 0.20477821582098907, 'r2': -0.001981019973754883}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.64it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.38it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.93it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.32it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:49:46,983][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0348
[2025-04-29 14:49:47,408][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0411, Metrics: {'mse': 0.04210406914353371, 'rmse': 0.20519276094329864, 'r2': -0.006041884422302246}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁▁▁
wandb:     best_val_mse █▆▄▃▂▂▁▁▁
wandb:      best_val_r2 ▁▃▅▆▇▇███
wandb:    best_val_rmse █▆▅▄▃▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▅▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▁▁▁▁
wandb:          val_mse █▆▄▃▂▂▁▁▁▁
wandb:           val_r2 ▁▃▅▆▇▇████
wandb:         val_rmse █▆▅▄▃▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04056
wandb:     best_val_mse 0.04193
wandb:      best_val_r2 -0.00198
wandb:    best_val_rmse 0.20478
wandb:            epoch 10
wandb:   final_test_mse 0.04583
wandb:    final_test_r2 -0.1891
wandb:  final_test_rmse 0.21407
wandb:  final_train_mse 0.03277
wandb:   final_train_r2 -0.22162
wandb: final_train_rmse 0.18104
wandb:    final_val_mse 0.04193
wandb:     final_val_r2 -0.00198
wandb:   final_val_rmse 0.20478
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03479
wandb:       train_time 54.71102
wandb:         val_loss 0.04114
wandb:          val_mse 0.0421
wandb:           val_r2 -0.00604
wandb:         val_rmse 0.20519
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144843-yurh8uh1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_144843-yurh8uh1/logs
Standard experiment completed successfully: layer_7_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_7/complexity/results.json
Running question_type experiment for language en, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:50:07,328][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_8/question_type
experiment_name: layer_8_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:50:07,329][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:50:07,329][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:50:07,329][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:50:07,333][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:50:07,333][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:50:08,598][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:50:11,403][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:50:11,404][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:50:11,439][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,459][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,545][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:50:11,556][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:50:11,556][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:50:11,557][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:50:11,571][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,600][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:50:11,602][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:50:11,602][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:50:11,603][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:50:11,615][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,637][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:50:11,647][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:50:11,648][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:50:11,648][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:50:11,649][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:50:11,650][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:50:11,650][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:50:11,650][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:50:11,650][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:50:11,650][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:50:11,651][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:50:11,651][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:50:11,651][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:50:11,652][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:50:11,652][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:50:11,652][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:50:11,652][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:50:11,653][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:50:11,653][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:50:11,653][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:50:11,653][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:50:11,653][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:50:11,654][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:50:15,422][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:50:15,422][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:50:15,425][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:50:15,425][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:50:15,425][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.59it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.08it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.44it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.51it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.28it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.70it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.80it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.64it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.26it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.71it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.76it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.71it/s]
[2025-04-29 14:50:22,377][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6939
[2025-04-29 14:50:22,768][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.42it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.17it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.54it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.75it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.45it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.89it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.36it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.50it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.79it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.26it/s]
[2025-04-29 14:50:27,673][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6937
[2025-04-29 14:50:28,089][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.89it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.61it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.10it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:50:33,236][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6934
[2025-04-29 14:50:33,667][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.11it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:50:38,617][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6930
[2025-04-29 14:50:39,059][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.67it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.41it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.98it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.35it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.65it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:50:44,070][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6921
[2025-04-29 14:50:44,523][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  5.74it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.47it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.99it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:50:48,916][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6938
[2025-04-29 14:50:49,353][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.75it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.46it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 14.95it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.88it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.48it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 14:50:54,317][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6929
[2025-04-29 14:50:54,760][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.99it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.64it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.12it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:50:59,738][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6916
[2025-04-29 14:51:00,173][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.29it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.00it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.66it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.12it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:51:04,573][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6927
[2025-04-29 14:51:05,014][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.23it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.84it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.25it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.68it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.67it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.68it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.67it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.68it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:51:09,415][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6925
[2025-04-29 14:51:09,862][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:51:09,863][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁
wandb:        best_val_loss █▆▄▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▃▃▃▁▁▁
wandb:           train_loss █▇▆▅▃█▅▁▄▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▆▄▂▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69277
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 0.0
wandb:           train_loss 0.69255
wandb:           train_time 52.58734
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69277
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145007-wkdpmhtn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145007-wkdpmhtn/logs
Standard experiment completed successfully: layer_8_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_8/question_type/results.json
Running complexity experiment for language en, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:51:28,452][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_8/complexity
experiment_name: layer_8_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:51:28,452][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:51:28,452][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:51:28,452][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:51:28,456][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:51:28,457][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:51:29,682][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:51:32,388][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:51:32,389][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:51:32,451][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,468][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,534][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:51:32,545][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:51:32,546][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:51:32,547][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:51:32,560][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,578][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,588][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:51:32,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:51:32,590][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:51:32,591][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:51:32,602][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,619][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:51:32,628][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:51:32,630][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:51:32,630][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:51:32,631][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:51:32,631][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:51:32,631][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:51:32,631][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:51:32,631][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:51:32,632][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:51:32,632][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:51:32,632][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:51:32,632][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:51:32,632][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:51:32,632][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:51:32,632][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:51:32,633][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:51:32,633][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:51:32,633][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:51:32,634][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:51:32,634][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:51:32,634][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:51:32,634][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:51:32,634][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:51:32,634][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:51:32,634][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:51:32,635][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:51:32,635][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:51:36,340][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:51:36,341][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:51:36,343][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:51:36,343][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:51:36,343][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:57,  1.29it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.10it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.79it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.20it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.24it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.90it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.22it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.20it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.94it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.46it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.84it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.11it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.32it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.10it/s]
[2025-04-29 14:51:43,226][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1894
[2025-04-29 14:51:43,611][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1212, Metrics: {'mse': 0.12915784120559692, 'rmse': 0.35938536587568076, 'r2': -2.0861196517944336}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.67it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.41it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:05, 13.99it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.37it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.21it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.72it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 14:51:48,552][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1368
[2025-04-29 14:51:48,933][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0870, Metrics: {'mse': 0.09344331175088882, 'rmse': 0.30568498777481506, 'r2': -1.232750654220581}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.98it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:51:54,109][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0994
[2025-04-29 14:51:54,551][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0643, Metrics: {'mse': 0.06926251202821732, 'rmse': 0.26317771947529545, 'r2': -0.6549704074859619}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.65it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.39it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.98it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.18it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:51:59,477][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0714
[2025-04-29 14:51:59,912][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0501, Metrics: {'mse': 0.053818218410015106, 'rmse': 0.2319875393421274, 'r2': -0.28594183921813965}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.71it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.45it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 14.97it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.90it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.47it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 14:52:04,920][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0533
[2025-04-29 14:52:05,367][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0430, Metrics: {'mse': 0.04556874930858612, 'rmse': 0.21346838011421299, 'r2': -0.08882761001586914}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.93it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.11it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:52:10,328][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0419
[2025-04-29 14:52:10,793][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0407, Metrics: {'mse': 0.042464274913072586, 'rmse': 0.20606861700189233, 'r2': -0.014648675918579102}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.97it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.64it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.14it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.65it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.66it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.66it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 14:52:15,788][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0368
[2025-04-29 14:52:16,231][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0411, Metrics: {'mse': 0.04216228425502777, 'rmse': 0.20533456663462138, 'r2': -0.0074329376220703125}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.10it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.75it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.68it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.68it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 16.75it/s]
[2025-04-29 14:52:20,711][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0334
[2025-04-29 14:52:21,133][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0425, Metrics: {'mse': 0.043032631278038025, 'rmse': 0.2074430796098969, 'r2': -0.028229117393493652}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.43it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.11it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.75it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.28it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.38it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.97it/s]
[2025-04-29 14:52:25,555][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0314
[2025-04-29 14:52:25,963][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0438, Metrics: {'mse': 0.04409562796354294, 'rmse': 0.20998959013137516, 'r2': -0.05362856388092041}
[2025-04-29 14:52:25,964][src.training.lm_trainer][INFO] - Early stopping at epoch 9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁
wandb:     best_val_mse █▅▃▂▁▁
wandb:      best_val_r2 ▁▄▆▇██
wandb:    best_val_rmse █▆▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▄▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁
wandb:          val_mse █▅▃▂▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇█████
wandb:         val_rmse █▆▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04074
wandb:     best_val_mse 0.04246
wandb:      best_val_r2 -0.01465
wandb:    best_val_rmse 0.20607
wandb:            epoch 9
wandb:   final_test_mse 0.04882
wandb:    final_test_r2 -0.26672
wandb:  final_test_rmse 0.22095
wandb:  final_train_mse 0.03546
wandb:   final_train_r2 -0.32174
wandb: final_train_rmse 0.18831
wandb:    final_val_mse 0.04246
wandb:     final_val_r2 -0.01465
wandb:   final_val_rmse 0.20607
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03135
wandb:       train_time 47.70698
wandb:         val_loss 0.04385
wandb:          val_mse 0.0441
wandb:           val_r2 -0.05363
wandb:         val_rmse 0.20999
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145128-pw5xtcfl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145128-pw5xtcfl/logs
Standard experiment completed successfully: layer_8_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_8/complexity/results.json
Running question_type experiment for language en, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:52:44,375][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_9/question_type
experiment_name: layer_9_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:52:44,375][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:52:44,376][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:52:44,376][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:52:44,380][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:52:44,380][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:52:45,458][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:52:48,149][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:52:48,150][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:52:48,189][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,210][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,281][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:52:48,292][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:52:48,293][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:52:48,294][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:52:48,309][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,330][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,340][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:52:48,341][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:52:48,342][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:52:48,342][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:52:48,358][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,379][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:52:48,390][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:52:48,391][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:52:48,391][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:52:48,392][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:52:48,393][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:52:48,393][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:52:48,393][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:52:48,393][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:52:48,393][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:52:48,393][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:52:48,394][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:52:48,394][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:52:48,394][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:52:48,395][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:52:48,395][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:52:48,395][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:52:48,396][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:52:48,396][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:52:48,396][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:52:48,396][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:52:52,236][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:52:52,237][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:52:52,239][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:52:52,239][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:52:52,239][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:13,  1.01it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:21,  3.33it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:12,  5.71it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.01it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.10it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 11.90it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.37it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.52it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.41it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:03, 16.09it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.57it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.91it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.15it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.33it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.44it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:03<00:02, 17.70it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.69it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.69it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.69it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.42it/s]
[2025-04-29 14:52:59,370][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6935
[2025-04-29 14:52:59,794][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.20it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.95it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.36it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.63it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.37it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.22it/s]
[2025-04-29 14:53:04,712][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6941
[2025-04-29 14:53:05,117][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6919, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.88it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.61it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.10it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:53:10,273][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6931
[2025-04-29 14:53:10,718][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6909, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.26it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.87it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.04it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:53:15,646][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6912
[2025-04-29 14:53:16,083][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6900, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.29it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.98it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.65it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.13it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.95it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 14:53:21,063][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6901
[2025-04-29 14:53:21,490][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6892, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.77it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.50it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:53:26,433][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6928
[2025-04-29 14:53:26,873][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6884, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.69it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.44it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 14.99it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.88it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.48it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:53:31,837][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6899
[2025-04-29 14:53:32,288][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6875, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.06it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.73it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.46it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 14.97it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.90it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.49it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 14:53:37,266][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6868
[2025-04-29 14:53:37,703][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6867, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 10.75it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.49it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.00it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 15.91it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.49it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.87it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.67it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.94it/s]
[2025-04-29 14:53:42,738][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6884
[2025-04-29 14:53:43,181][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6859, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 10.88it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 14:53:48,157][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6865
[2025-04-29 14:53:48,578][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6850, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:        best_val_loss █▇▆▅▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ▇█▇▅▄▇▄▁▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.68504
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49916
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68651
wandb:           train_time 54.98979
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.68504
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145244-n22dnqlv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145244-n22dnqlv/logs
Standard experiment completed successfully: layer_9_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_9/question_type/results.json
Running complexity experiment for language en, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:54:08,840][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_9/complexity
experiment_name: layer_9_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:54:08,841][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:54:08,841][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:54:08,841][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:54:08,845][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:54:08,846][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:54:10,066][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:54:12,785][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:54:12,786][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:54:12,829][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:12,852][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:12,920][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:54:12,931][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:54:12,932][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:54:12,933][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:54:12,947][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:12,967][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:12,977][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:54:12,978][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:54:12,978][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:54:12,979][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:54:12,992][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:13,013][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:54:13,022][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:54:13,024][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:54:13,024][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:54:13,025][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:54:13,025][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:54:13,025][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:54:13,025][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:54:13,025][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:54:13,026][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:54:13,026][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:54:13,026][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:54:13,026][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:54:13,026][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:54:13,026][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:54:13,026][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:54:13,027][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:54:13,027][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:54:13,027][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:54:13,028][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:54:13,028][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:54:13,028][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:54:13,029][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:54:16,808][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:54:16,809][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:54:16,811][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:54:16,812][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:54:16,812][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:55,  1.34it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:16,  4.24it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:10,  6.96it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.36it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.40it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 13.05it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.31it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.27it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.99it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.52it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.90it/s]Epoch 1/10:  31%|███       | 23/75 [00:01<00:03, 17.15it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.34it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.79it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.19it/s]
[2025-04-29 14:54:23,596][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2064
[2025-04-29 14:54:23,978][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1156, Metrics: {'mse': 0.12352649122476578, 'rmse': 0.35146335687346664, 'r2': -1.9515633583068848}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.87it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.63it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.17it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.31it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:54:28,916][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1159
[2025-04-29 14:54:29,332][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0644, Metrics: {'mse': 0.06964369863271713, 'rmse': 0.26390092578980684, 'r2': -0.6640784740447998}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.20it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.84it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 14:54:34,491][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0663
[2025-04-29 14:54:34,919][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0444, Metrics: {'mse': 0.047432053834199905, 'rmse': 0.21778901219804433, 'r2': -0.13334965705871582}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.40it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:54:39,865][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0429
[2025-04-29 14:54:40,288][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0420, Metrics: {'mse': 0.04338591918349266, 'rmse': 0.20829286877733635, 'r2': -0.036670684814453125}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.72it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:54:45,270][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0344
[2025-04-29 14:54:45,708][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0456, Metrics: {'mse': 0.04597829282283783, 'rmse': 0.21442549480609302, 'r2': -0.09861326217651367}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.56it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.30it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.91it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.28it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:54:50,102][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0319
[2025-04-29 14:54:50,542][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0491, Metrics: {'mse': 0.04892808943986893, 'rmse': 0.2211969471757441, 'r2': -0.1690962314605713}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.38it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.09it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.74it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.21it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:54:54,942][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0302
[2025-04-29 14:54:55,382][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0518, Metrics: {'mse': 0.051295872777700424, 'rmse': 0.22648592180906174, 'r2': -0.22567248344421387}
[2025-04-29 14:54:55,383][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁
wandb:     best_val_mse █▃▁▁
wandb:      best_val_r2 ▁▆██
wandb:    best_val_rmse █▄▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▂▂
wandb:          val_mse █▃▁▁▁▁▂
wandb:           val_r2 ▁▆████▇
wandb:         val_rmse █▄▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04199
wandb:     best_val_mse 0.04339
wandb:      best_val_r2 -0.03667
wandb:    best_val_rmse 0.20829
wandb:            epoch 7
wandb:   final_test_mse 0.04551
wandb:    final_test_r2 -0.18089
wandb:  final_test_rmse 0.21333
wandb:  final_train_mse 0.03198
wandb:   final_train_r2 -0.19213
wandb: final_train_rmse 0.17884
wandb:    final_val_mse 0.04339
wandb:     final_val_r2 -0.03667
wandb:   final_val_rmse 0.20829
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03015
wandb:       train_time 36.72801
wandb:         val_loss 0.05182
wandb:          val_mse 0.0513
wandb:           val_r2 -0.22567
wandb:         val_rmse 0.22649
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145408-okcb62h9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145408-okcb62h9/logs
Standard experiment completed successfully: layer_9_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_9/complexity/results.json
Running question_type experiment for language en, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:55:14,125][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_10/question_type
experiment_name: layer_10_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:55:14,125][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:55:14,125][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:55:14,125][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:55:14,129][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:55:14,130][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:55:15,630][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:55:18,468][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:55:18,468][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:55:18,502][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,523][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,598][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:55:18,609][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:55:18,610][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:55:18,610][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:55:18,624][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,644][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,655][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:55:18,656][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:55:18,656][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:55:18,657][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:55:18,671][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,692][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:55:18,702][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:55:18,703][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:55:18,704][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:55:18,704][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:55:18,705][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:55:18,705][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:55:18,705][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:55:18,705][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:55:18,705][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:55:18,705][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:55:18,706][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:55:18,706][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:55:18,706][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:55:18,707][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:55:18,707][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:55:18,707][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:55:18,708][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:55:18,708][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:55:18,708][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:55:18,708][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:55:18,708][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:55:22,600][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:55:22,601][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:55:22,603][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:55:22,603][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:55:22,603][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.15it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.72it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.27it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.64it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.72it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.44it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.83it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.92it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.71it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.31it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.74it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.06it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.38it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.70it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.71it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.82it/s]
[2025-04-29 14:55:29,410][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6924
[2025-04-29 14:55:29,816][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6641, Metrics: {'accuracy': 0.5416666666666666, 'f1': 0.15384615384615385}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.73it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.48it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.24it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 14:55:34,760][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6659
[2025-04-29 14:55:35,173][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6146, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.6037735849056604}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.55it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.88it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.27it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 14:55:40,316][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6395
[2025-04-29 14:55:40,737][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5654, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.17it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:55:45,674][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6128
[2025-04-29 14:55:46,104][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5207, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8529411764705882}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.23it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.93it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.09it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:55:51,121][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5872
[2025-04-29 14:55:51,548][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.4812, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  5.87it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.61it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.09it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.71it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.04it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.68it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.66it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.66it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.66it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.67it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:55:56,502][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5633
[2025-04-29 14:55:56,940][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4427, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.91it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 14:56:01,923][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5489
[2025-04-29 14:56:02,344][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.4087, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.04it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:56:07,314][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5229
[2025-04-29 14:56:07,743][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.3793, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.25it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 10.93it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.57it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.06it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 15.99it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 14:56:12,726][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5127
[2025-04-29 14:56:13,156][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.3569, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.19it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:56:18,120][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.4934
[2025-04-29 14:56:18,552][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.3355, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▄▆▇▇█████
wandb:          best_val_f1 ▁▅▇▇██████
wandb:        best_val_loss █▇▆▅▄▃▃▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▄▃▃▂▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▄▆▇▇█████
wandb:               val_f1 ▁▅▇▇██████
wandb:             val_loss █▇▆▅▄▃▃▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.93056
wandb:          best_val_f1 0.93506
wandb:        best_val_loss 0.3355
wandb:                epoch 10
wandb:  final_test_accuracy 0.89091
wandb:        final_test_f1 0.89474
wandb: final_train_accuracy 0.97735
wandb:       final_train_f1 0.97714
wandb:   final_val_accuracy 0.93056
wandb:         final_val_f1 0.93506
wandb:        learning_rate 1e-05
wandb:           train_loss 0.49342
wandb:           train_time 54.79175
wandb:         val_accuracy 0.93056
wandb:               val_f1 0.93506
wandb:             val_loss 0.3355
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145514-ewz4cx6l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145514-ewz4cx6l/logs
Standard experiment completed successfully: layer_10_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_10/question_type/results.json
Running complexity experiment for language en, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:56:37,650][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_10/complexity
experiment_name: layer_10_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:56:37,650][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:56:37,651][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:56:37,651][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:56:37,655][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:56:37,656][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:56:39,115][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:56:41,916][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:56:41,917][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:56:41,966][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:41,986][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:42,067][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:56:42,079][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:56:42,080][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:56:42,081][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:56:42,095][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:42,115][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:42,126][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:56:42,128][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:56:42,128][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:56:42,129][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:56:42,143][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:42,167][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:56:42,177][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:56:42,179][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:56:42,179][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:56:42,180][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:56:42,181][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:56:42,181][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:56:42,181][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:56:42,181][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:56:42,182][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:56:42,182][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:56:42,182][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:56:42,182][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:56:42,182][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:56:42,182][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:56:42,182][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:56:42,183][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:56:42,183][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:56:42,183][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:56:42,183][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:56:42,183][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:56:42,183][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:56:42,183][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:56:42,184][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:56:42,184][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:56:42,184][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:56:42,185][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:56:42,185][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:56:46,339][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:56:46,340][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:56:46,342][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:56:46,343][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:56:46,343][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:06,  1.12it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:19,  3.64it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.15it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.52it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.60it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.34it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.76it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.83it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.65it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.26it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.70it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.74it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.76it/s]
[2025-04-29 14:56:53,663][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1726
[2025-04-29 14:56:54,054][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0659, Metrics: {'mse': 0.06436880677938461, 'rmse': 0.25371008411055446, 'r2': -0.5380393266677856}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.48it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.24it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.61it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.80it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.47it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.92it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.21it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.36it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.22it/s]
[2025-04-29 14:56:58,983][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0773
[2025-04-29 14:56:59,391][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0840, Metrics: {'mse': 0.0809103399515152, 'rmse': 0.2844474291525856, 'r2': -0.9332855939865112}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.47it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:05, 12.22it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.51it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.70it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.41it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.86it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.17it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.36it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.49it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.65it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.79it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.79it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.80it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.80it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.27it/s]
[2025-04-29 14:57:03,737][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0586
[2025-04-29 14:57:04,139][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0715, Metrics: {'mse': 0.06972543150186539, 'rmse': 0.2640557355973647, 'r2': -0.6660314798355103}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.95it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.70it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.21it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.52it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.26it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 14:57:08,510][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0453
[2025-04-29 14:57:08,913][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0698, Metrics: {'mse': 0.06825316697359085, 'rmse': 0.26125307074480647, 'r2': -0.6308529376983643}
[2025-04-29 14:57:08,913][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb:            epoch ▁▁▃▃▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁
wandb:       train_loss █▃▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▃▃
wandb:          val_mse ▁█▃▃
wandb:           val_r2 █▁▆▆
wandb:         val_rmse ▁█▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06588
wandb:     best_val_mse 0.06437
wandb:      best_val_r2 -0.53804
wandb:    best_val_rmse 0.25371
wandb:            epoch 4
wandb:   final_test_mse 0.05218
wandb:    final_test_r2 -0.35396
wandb:  final_test_rmse 0.22843
wandb:  final_train_mse 0.04204
wandb:   final_train_r2 -0.56682
wandb: final_train_rmse 0.20503
wandb:    final_val_mse 0.06437
wandb:     final_val_r2 -0.53804
wandb:   final_val_rmse 0.25371
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04531
wandb:       train_time 20.33444
wandb:         val_loss 0.06981
wandb:          val_mse 0.06825
wandb:           val_r2 -0.63085
wandb:         val_rmse 0.26125
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145637-vgx97wxc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145637-vgx97wxc/logs
Standard experiment completed successfully: layer_10_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_10/complexity/results.json
Running question_type experiment for language en, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:57:25,977][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_11/question_type
experiment_name: layer_11_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:57:25,977][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:57:25,977][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:57:25,977][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:57:25,981][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:57:25,982][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:57:26,894][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:57:29,634][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:57:29,634][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:57:29,653][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,673][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,730][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:57:29,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:57:29,742][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:57:29,743][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:57:29,754][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,774][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,784][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:57:29,785][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:57:29,785][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:57:29,786][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:57:29,802][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,825][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:57:29,835][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:57:29,837][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:57:29,837][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:57:29,838][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:57:29,838][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:57:29,838][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:57:29,838][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:57:29,838][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:57:29,839][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:57:29,839][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:57:29,839][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:57:29,840][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:57:29,840][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:57:29,840][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:57:29,841][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:57:29,841][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:57:29,841][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:57:29,841][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:57:29,841][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:57:29,841][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:57:29,841][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:57:29,842][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:57:33,327][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:57:33,327][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:57:33,330][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:57:33,330][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:57:33,330][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:00,  1.23it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.96it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.59it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.00it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.06it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.75it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.08it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.11it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.86it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.41it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.10it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.32it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.03it/s]
[2025-04-29 14:57:39,777][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6929
[2025-04-29 14:57:40,171][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6651, Metrics: {'accuracy': 0.5694444444444444, 'f1': 0.27906976744186046}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.60it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.34it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.66it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.82it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:03, 16.52it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.93it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.20it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.39it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.50it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.67it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.35it/s]
[2025-04-29 14:57:45,055][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6623
[2025-04-29 14:57:45,447][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6105, Metrics: {'accuracy': 0.7361111111111112, 'f1': 0.6779661016949152}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.93it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.66it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.12it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 14:57:50,817][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6300
[2025-04-29 14:57:51,256][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5551, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8235294117647058}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.68it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.43it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.01it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:57:56,212][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5993
[2025-04-29 14:57:56,628][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5038, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9333333333333333}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.18it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:58:01,611][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5670
[2025-04-29 14:58:02,014][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.4581, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.55it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.30it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.90it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.14it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 14:58:06,972][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5360
[2025-04-29 14:58:07,385][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4137, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9210526315789473}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.19it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.82it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.21it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:58:12,365][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5188
[2025-04-29 14:58:12,789][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3769, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9210526315789473}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  6.01it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.75it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.22it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.49it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.26it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:58:17,725][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.4947
[2025-04-29 14:58:18,147][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.3468, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9210526315789473}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.40it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.11it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.75it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.55it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 14:58:23,152][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.4799
[2025-04-29 14:58:23,571][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.3241, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9210526315789473}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.63it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.36it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.92it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.29it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 14:58:28,519][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.4609
[2025-04-29 14:58:28,935][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.3020, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9210526315789473}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▄▆███████
wandb:          best_val_f1 ▁▅▇███████
wandb:        best_val_loss █▇▆▅▄▃▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▄▃▃▂▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▄▆███████
wandb:               val_f1 ▁▅▇███████
wandb:             val_loss █▇▆▅▄▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.91667
wandb:          best_val_f1 0.92105
wandb:        best_val_loss 0.30205
wandb:                epoch 10
wandb:  final_test_accuracy 0.88182
wandb:        final_test_f1 0.88889
wandb: final_train_accuracy 0.98238
wandb:       final_train_f1 0.98222
wandb:   final_val_accuracy 0.91667
wandb:         final_val_f1 0.92105
wandb:        learning_rate 1e-05
wandb:           train_loss 0.46093
wandb:           train_time 54.73734
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.92105
wandb:             val_loss 0.30205
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145726-i6i2fqc4
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145726-i6i2fqc4/logs
Standard experiment completed successfully: layer_11_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_11/question_type/results.json
Running complexity experiment for language en, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:58:48,440][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_11/complexity
experiment_name: layer_11_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:58:48,440][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:58:48,440][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:58:48,440][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:58:48,445][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:58:48,445][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:58:49,756][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:58:52,475][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:58:52,476][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:58:52,513][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,533][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,605][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:58:52,617][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:58:52,618][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:58:52,619][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:58:52,634][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,660][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,671][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:58:52,673][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:58:52,673][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:58:52,674][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:58:52,690][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,712][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:58:52,721][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:58:52,723][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:58:52,723][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:58:52,724][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:58:52,724][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:58:52,724][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:58:52,724][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:58:52,725][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:58:52,725][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:58:52,725][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:58:52,725][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:58:52,725][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:58:52,725][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:58:52,725][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:58:52,726][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:58:52,726][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:58:52,726][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:58:52,727][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:58:52,727][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:58:52,727][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:58:52,728][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:58:52,728][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:58:56,651][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:58:56,652][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:58:56,654][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:58:56,654][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:58:56,654][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.27it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.07it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.74it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.15it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.21it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.89it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.19it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.18it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.92it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.47it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.84it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.13it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.34it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.48it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.08it/s]
[2025-04-29 14:59:03,559][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1492
[2025-04-29 14:59:03,948][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0710, Metrics: {'mse': 0.06900162994861603, 'rmse': 0.26268161326711853, 'r2': -0.648736834526062}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.80it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.59it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.12it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.48it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:59:08,877][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0820
[2025-04-29 14:59:09,275][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0762, Metrics: {'mse': 0.0742032527923584, 'rmse': 0.27240274006029824, 'r2': -0.7730252742767334}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.23it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.97it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.38it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.62it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 14:59:13,658][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0681
[2025-04-29 14:59:14,046][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0655, Metrics: {'mse': 0.06453105062246323, 'rmse': 0.2540296254818781, 'r2': -0.541916012763977}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.65it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.67it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.67it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.68it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:59:19,199][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0551
[2025-04-29 14:59:19,623][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0641, Metrics: {'mse': 0.06324425339698792, 'rmse': 0.2514841016783922, 'r2': -0.5111689567565918}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  6.06it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.80it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.23it/s]
[2025-04-29 14:59:24,523][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0440
[2025-04-29 14:59:24,944][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0580, Metrics: {'mse': 0.057506218552589417, 'rmse': 0.2398045423935698, 'r2': -0.37406349182128906}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.17it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 14:59:29,922][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0390
[2025-04-29 14:59:30,343][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0526, Metrics: {'mse': 0.052308280020952225, 'rmse': 0.2287100348059792, 'r2': -0.24986302852630615}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.19it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:59:35,284][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0359
[2025-04-29 14:59:35,723][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0543, Metrics: {'mse': 0.05369705706834793, 'rmse': 0.23172625459439836, 'r2': -0.2830467224121094}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.68it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.41it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.95it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.32it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:59:40,115][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0328
[2025-04-29 14:59:40,559][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0502, Metrics: {'mse': 0.049573492258787155, 'rmse': 0.2226510549240384, 'r2': -0.184517502784729}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.52it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.25it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.85it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 14:59:45,533][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0310
[2025-04-29 14:59:45,959][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0483, Metrics: {'mse': 0.047701410949230194, 'rmse': 0.21840652680089528, 'r2': -0.1397857666015625}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.55it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.86it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 14:59:50,906][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0287
[2025-04-29 14:59:51,327][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0491, Metrics: {'mse': 0.04831238463521004, 'rmse': 0.219800783973147, 'r2': -0.15438449382781982}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▆▄▂▂▁
wandb:     best_val_mse █▇▆▄▃▂▁
wandb:      best_val_r2 ▁▂▃▅▆▇█
wandb:    best_val_rmse █▇▆▄▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▅▅▃▂▂▁▁▁
wandb:          val_mse ▇█▅▅▄▂▃▁▁▁
wandb:           val_r2 ▂▁▄▄▅▇▆███
wandb:         val_rmse ▇█▆▅▄▂▃▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04835
wandb:     best_val_mse 0.0477
wandb:      best_val_r2 -0.13979
wandb:    best_val_rmse 0.21841
wandb:            epoch 10
wandb:   final_test_mse 0.02884
wandb:    final_test_r2 0.25155
wandb:  final_test_rmse 0.16984
wandb:  final_train_mse 0.02256
wandb:   final_train_r2 0.15898
wandb: final_train_rmse 0.15021
wandb:    final_val_mse 0.0477
wandb:     final_val_r2 -0.13979
wandb:   final_val_rmse 0.21841
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02868
wandb:       train_time 52.74303
wandb:         val_loss 0.04906
wandb:          val_mse 0.04831
wandb:           val_r2 -0.15438
wandb:         val_rmse 0.2198
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145848-m7z8uvvy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_145848-m7z8uvvy/logs
Standard experiment completed successfully: layer_11_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_11/complexity/results.json
Running question_type experiment for language en, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:00:12,436][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_12/question_type
experiment_name: layer_12_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:00:12,436][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:00:12,436][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:00:12,436][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:00:12,440][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 15:00:12,441][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:00:13,976][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:00:16,680][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:00:16,680][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:00:16,718][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,739][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,811][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 15:00:16,823][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:00:16,823][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 15:00:16,824][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:00:16,840][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,863][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,874][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 15:00:16,875][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:00:16,875][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 15:00:16,876][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:00:16,893][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,917][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:00:16,928][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 15:00:16,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:00:16,930][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 15:00:16,931][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 15:00:16,931][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:00:16,931][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:00:16,931][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:00:16,931][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:00:16,932][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 15:00:16,932][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:00:16,932][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:00:16,933][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:00:16,933][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:00:16,933][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:00:16,933][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:00:16,934][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:00:16,934][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 15:00:16,934][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:00:16,934][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 15:00:16,934][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:00:16,934][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:00:16,935][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:00:20,840][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:00:20,841][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:00:20,843][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:00:20,843][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 15:00:20,843][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:56,  1.31it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.13it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:10,  6.81it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.23it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.26it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.91it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.19it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.13it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.84it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.37it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.75it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.22it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.37it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.49it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.09it/s]
[2025-04-29 15:00:27,799][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6947
[2025-04-29 15:00:28,192][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.88it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.66it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.16it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 15:00:33,117][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6929
[2025-04-29 15:00:33,517][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6919, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.20it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.27it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:00:38,657][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6896
[2025-04-29 15:00:39,086][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.69it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.43it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.00it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.18it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:00:44,005][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6848
[2025-04-29 15:00:44,419][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6903, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.58it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.32it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.91it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.31it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.13it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:00:49,397][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6857
[2025-04-29 15:00:49,810][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6895, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.23it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.82it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.27it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:00:54,736][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6834
[2025-04-29 15:00:55,161][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6886, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.18it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.21it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.79it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.79it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:01:00,129][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6803
[2025-04-29 15:01:00,569][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.91it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.63it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.14it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:01:05,523][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6782
[2025-04-29 15:01:05,944][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6869, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:01:10,921][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6791
[2025-04-29 15:01:11,339][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.82it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.25it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:01:16,291][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6791
[2025-04-29 15:01:16,710][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6853, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:        best_val_loss █▇▇▆▅▄▃▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▄▄▃▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▇▇▆▅▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.68528
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.67913
wandb:           train_time 54.45808
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.68528
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150012-t7tsuker
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150012-t7tsuker/logs
Standard experiment completed successfully: layer_12_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_12/question_type/results.json
Running complexity experiment for language en, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:01:35,914][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_12/complexity
experiment_name: layer_12_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:01:35,914][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:01:35,914][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:01:35,914][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:01:35,919][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 15:01:35,919][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:01:37,112][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:01:39,791][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:01:39,791][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:01:39,823][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:39,841][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:39,911][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 15:01:39,922][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:01:39,923][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 15:01:39,924][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:01:39,939][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:39,959][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:39,970][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 15:01:39,971][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:01:39,971][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 15:01:39,972][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:01:39,988][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:40,008][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:01:40,017][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 15:01:40,019][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:01:40,019][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 15:01:40,020][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 15:01:40,020][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:01:40,020][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:01:40,020][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:01:40,021][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:01:40,021][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:01:40,021][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 15:01:40,021][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 15:01:40,021][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:01:40,022][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:01:40,022][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 15:01:40,022][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:01:40,023][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:01:40,023][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 15:01:40,023][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 15:01:40,024][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 15:01:40,024][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:01:40,024][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:01:40,024][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:01:43,826][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:01:43,827][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:01:43,830][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:01:43,830][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 15:01:43,830][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:10,  1.05it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.46it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  5.90it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.23it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.32it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.09it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.55it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.67it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.53it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.18it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.63it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.98it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.23it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.41it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.62it/s]
[2025-04-29 15:01:50,745][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1966
[2025-04-29 15:01:51,130][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1427, Metrics: {'mse': 0.15149036049842834, 'rmse': 0.3892176261404773, 'r2': -2.619736671447754}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  6.01it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.80it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.29it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.58it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.34it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.82it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.35it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 15:01:56,065][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1724
[2025-04-29 15:01:56,469][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1241, Metrics: {'mse': 0.13213005661964417, 'rmse': 0.363496982958104, 'r2': -2.1571383476257324}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.03it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:02:01,638][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1518
[2025-04-29 15:02:02,066][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1078, Metrics: {'mse': 0.1151609718799591, 'rmse': 0.33935375624848935, 'r2': -1.751676082611084}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.73it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 10.35it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.18it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 14.79it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 15.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.42it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.85it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:03, 17.10it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 16.96it/s]
[2025-04-29 15:02:07,043][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1356
[2025-04-29 15:02:07,469][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0953, Metrics: {'mse': 0.10208451747894287, 'rmse': 0.31950667830100654, 'r2': -1.4392249584197998}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  5.85it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.11it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.21it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 15:02:12,491][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-29 15:02:12,905][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0834, Metrics: {'mse': 0.08966527879238129, 'rmse': 0.29944161165806815, 'r2': -1.1424777507781982}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:02:17,855][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1082
[2025-04-29 15:02:18,272][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0742, Metrics: {'mse': 0.07992025464773178, 'rmse': 0.2827017061280879, 'r2': -0.9096282720565796}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.18it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:02:23,245][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0955
[2025-04-29 15:02:23,670][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0655, Metrics: {'mse': 0.07057694345712662, 'rmse': 0.2656632143468994, 'r2': -0.6863776445388794}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.90it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.60it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.07it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:02:28,633][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0841
[2025-04-29 15:02:29,066][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0591, Metrics: {'mse': 0.06370111554861069, 'rmse': 0.25239079925506536, 'r2': -0.5220853090286255}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.01it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:02:34,056][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0758
[2025-04-29 15:02:34,479][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0538, Metrics: {'mse': 0.05785331502556801, 'rmse': 0.24052716068163282, 'r2': -0.38235700130462646}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:01<01:14,  1.01s/it]Epoch 10/10:   4%|▍         | 3/75 [00:01<00:21,  3.28it/s]Epoch 10/10:   7%|▋         | 5/75 [00:01<00:12,  5.65it/s]Epoch 10/10:   9%|▉         | 7/75 [00:01<00:08,  7.93it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06, 10.02it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:05, 11.82it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:04, 13.30it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:04, 14.46it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 15.36it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:02<00:03, 16.02it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:03, 16.52it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:03, 16.87it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:02, 17.13it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:02, 17.31it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:02, 17.43it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:02<00:02, 17.53it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.59it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.63it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:02, 17.65it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.67it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.67it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.68it/s]Epoch 10/10:  60%|██████    | 45/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.69it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.69it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.69it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.69it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.68it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.68it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.70it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.70it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:05<00:00, 14.35it/s]
[2025-04-29 15:02:40,265][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0706
[2025-04-29 15:02:40,680][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0497, Metrics: {'mse': 0.05330289155244827, 'rmse': 0.23087418987935457, 'r2': -0.27362847328186035}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▄▃▂▂▁▁
wandb:     best_val_mse █▇▅▄▄▃▂▂▁▁
wandb:      best_val_r2 ▁▂▄▅▅▆▇▇██
wandb:    best_val_rmse █▇▆▅▄▃▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▅▄▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▄▄▃▂▂▁▁
wandb:          val_mse █▇▅▄▄▃▂▂▁▁
wandb:           val_r2 ▁▂▄▅▅▆▇▇██
wandb:         val_rmse █▇▆▅▄▃▃▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04969
wandb:     best_val_mse 0.0533
wandb:      best_val_r2 -0.27363
wandb:    best_val_rmse 0.23087
wandb:            epoch 10
wandb:   final_test_mse 0.07493
wandb:    final_test_r2 -0.94418
wandb:  final_test_rmse 0.27373
wandb:  final_train_mse 0.05757
wandb:   final_train_r2 -1.14592
wandb: final_train_rmse 0.23994
wandb:    final_val_mse 0.0533
wandb:     final_val_r2 -0.27363
wandb:   final_val_rmse 0.23087
wandb:    learning_rate 1e-05
wandb:       train_loss 0.07065
wandb:       train_time 55.65289
wandb:         val_loss 0.04969
wandb:          val_mse 0.0533
wandb:           val_r2 -0.27363
wandb:         val_rmse 0.23087
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150135-lkv3h7a1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150135-lkv3h7a1/logs
Standard experiment completed successfully: layer_12_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_12/complexity/results.json
Running question_type experiment for language fi, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:02:59,712][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_1/question_type
experiment_name: layer_1_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:02:59,713][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:02:59,713][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:02:59,713][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:02:59,717][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:02:59,718][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:03:00,940][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:03:03,677][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:03:03,677][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:03,723][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,743][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,834][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:03:03,846][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:03,846][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:03:03,847][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:03,860][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,878][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,888][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:03:03,889][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:03,889][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:03:03,890][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:03,902][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,922][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:03,931][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:03:03,933][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:03,933][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:03:03,934][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:03:03,934][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:03:03,934][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:03:03,934][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:03:03,934][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:03:03,935][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:03:03,935][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:03:03,935][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:03:03,935][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:03:03,935][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:03:03,935][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:03:03,935][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:03:03,936][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:03:03,936][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:03:03,936][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:03:03,937][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:03:03,937][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:03:03,937][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:03:03,937][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:03:03,937][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:03:03,937][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:03:03,937][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:03:03,938][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:03:07,715][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:03:07,716][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:03:07,718][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:03:07,718][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 15:03:07,718][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:52,  1.40it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:16,  4.39it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:09,  7.15it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.57it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.59it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 13.20it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.44it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.38it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 16.08it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.58it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.94it/s]Epoch 1/10:  31%|███       | 23/75 [00:01<00:03, 17.20it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.39it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.52it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.29it/s]
[2025-04-29 15:03:14,945][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-29 15:03:15,291][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.18it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.95it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.39it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.39it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.85it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 15:03:20,220][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6963
[2025-04-29 15:03:20,581][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.33it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:05, 12.06it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.41it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.64it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.38it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:03:24,973][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6923
[2025-04-29 15:03:25,330][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.16it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.90it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.32it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 15:03:29,706][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6946
[2025-04-29 15:03:30,059][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:03:30,060][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▄█▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃█▅
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69266
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69461
wandb:           train_time 20.02286
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69282
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150259-kkhdcqx7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150259-kkhdcqx7/logs
Standard experiment completed successfully: layer_1_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_1/question_type/results.json
Running complexity experiment for language fi, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:03:48,052][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_1/complexity
experiment_name: layer_1_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:03:48,052][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:03:48,052][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:03:48,052][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:03:48,057][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:03:48,057][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:03:49,214][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:03:51,931][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:03:51,932][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:51,964][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:51,986][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:52,053][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:03:52,064][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:52,065][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:03:52,066][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:52,089][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:52,114][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:52,126][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:03:52,127][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:52,127][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:03:52,128][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:03:52,145][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:52,170][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:03:52,183][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:03:52,184][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:03:52,184][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:03:52,186][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:03:52,186][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:03:52,187][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:03:52,187][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:03:52,187][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:03:52,187][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:03:52,187][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:03:52,187][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:03:52,187][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:03:52,188][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:03:52,188][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:03:52,188][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:03:52,188][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:03:52,188][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:03:52,188][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:03:52,188][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:03:52,189][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:03:52,189][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:03:52,189][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:03:52,190][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:03:52,190][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:03:52,190][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:03:52,190][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:03:52,190][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:03:55,778][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:03:55,779][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:03:55,781][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:03:55,782][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 15:03:55,782][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:06,  1.11it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:19,  3.62it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.13it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.49it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.57it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.32it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.73it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.82it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.65it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.27it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.71it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.44it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.73it/s]
[2025-04-29 15:04:02,716][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0947
[2025-04-29 15:04:03,064][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1343, Metrics: {'mse': 0.13447003066539764, 'rmse': 0.3667015553081247, 'r2': -1.051081657409668}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.85it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.64it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.15it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.50it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.27it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.32it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.80it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.69it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.19it/s]
[2025-04-29 15:04:07,993][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0363
[2025-04-29 15:04:08,357][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0900, Metrics: {'mse': 0.08999656140804291, 'rmse': 0.29999426895866343, 'r2': -0.3727245330810547}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.56it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.09it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 15:04:13,774][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-29 15:04:14,175][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0859, Metrics: {'mse': 0.0858670175075531, 'rmse': 0.29303074498685816, 'r2': -0.3097362518310547}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:04:19,146][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0309
[2025-04-29 15:04:19,516][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0847, Metrics: {'mse': 0.08465248346328735, 'rmse': 0.29095099838853855, 'r2': -0.29121077060699463}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:04:24,516][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0301
[2025-04-29 15:04:24,899][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0858, Metrics: {'mse': 0.08573048561811447, 'rmse': 0.29279768718026866, 'r2': -0.30765366554260254}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  6.04it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.79it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.24it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.55it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.27it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 15:04:29,284][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0324
[2025-04-29 15:04:29,660][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0829, Metrics: {'mse': 0.08283022791147232, 'rmse': 0.2878024112329018, 'r2': -0.26341569423675537}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.09it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.18it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:04:34,625][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0287
[2025-04-29 15:04:35,000][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0856, Metrics: {'mse': 0.08550343662500381, 'rmse': 0.29240970678998296, 'r2': -0.30419039726257324}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.57it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.29it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.86it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.27it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:04:39,404][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0302
[2025-04-29 15:04:39,790][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0845, Metrics: {'mse': 0.08440272510051727, 'rmse': 0.2905214709802311, 'r2': -0.2874011993408203}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.62it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.35it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.94it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.33it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.14it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:04:44,200][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0308
[2025-04-29 15:04:44,578][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0829, Metrics: {'mse': 0.08281547576189041, 'rmse': 0.28777678113755184, 'r2': -0.26319074630737305}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.54it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.88it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.14it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:04:49,576][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0301
[2025-04-29 15:04:49,972][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0851, Metrics: {'mse': 0.08501965552568436, 'rmse': 0.2915813017422145, 'r2': -0.2968113422393799}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁▁▁
wandb:     best_val_mse █▂▁▁▁▁
wandb:      best_val_r2 ▁▇████
wandb:    best_val_rmse █▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▁▁▁▁▁▁▁▁
wandb:          val_mse █▂▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▇████████
wandb:         val_rmse █▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08291
wandb:     best_val_mse 0.08282
wandb:      best_val_r2 -0.26319
wandb:    best_val_rmse 0.28778
wandb:            epoch 10
wandb:   final_test_mse 0.03944
wandb:    final_test_r2 0.00111
wandb:  final_test_rmse 0.1986
wandb:  final_train_mse 0.02013
wandb:   final_train_r2 0.00408
wandb: final_train_rmse 0.14186
wandb:    final_val_mse 0.08282
wandb:     final_val_r2 -0.26319
wandb:   final_val_rmse 0.28778
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03011
wandb:       train_time 52.35051
wandb:         val_loss 0.0851
wandb:          val_mse 0.08502
wandb:           val_r2 -0.29681
wandb:         val_rmse 0.29158
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150348-4gj5qe0l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150348-4gj5qe0l/logs
Standard experiment completed successfully: layer_1_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_1/complexity/results.json
Running question_type experiment for language fi, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:05:12,432][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_2/question_type
experiment_name: layer_2_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:05:12,432][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:05:12,432][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:05:12,432][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:05:12,437][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:05:12,437][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:05:13,672][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:05:16,515][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:05:16,515][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:05:16,567][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,590][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,665][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:05:16,678][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:05:16,679][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:05:16,680][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:05:16,698][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,723][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,735][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:05:16,736][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:05:16,736][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:05:16,737][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:05:16,753][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,779][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:05:16,789][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:05:16,791][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:05:16,791][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:05:16,792][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:05:16,792][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:05:16,792][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:05:16,792][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:05:16,793][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:05:16,793][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:05:16,793][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:05:16,793][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:05:16,793][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:05:16,793][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:05:16,793][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:05:16,794][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:05:16,794][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:05:16,794][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:05:16,795][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:05:16,795][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:05:16,795][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:05:16,796][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:05:16,796][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:05:20,738][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:05:20,739][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:05:20,741][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:05:20,741][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 15:05:20,741][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.15it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.74it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.29it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.66it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.75it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.48it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.86it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.73it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.30it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.06it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.28it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.82it/s]
[2025-04-29 15:05:27,548][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6945
[2025-04-29 15:05:27,900][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.82it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.59it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.49it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.27it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.19it/s]
[2025-04-29 15:05:32,823][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6966
[2025-04-29 15:05:33,181][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:10,  6.84it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:05, 12.52it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.77it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.88it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:03, 16.57it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.23it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.40it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.66it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.25it/s]
[2025-04-29 15:05:37,532][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6919
[2025-04-29 15:05:37,962][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:19,  3.79it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.63it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 11.73it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 13.71it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 14.98it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:04, 15.83it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.42it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:03, 16.81it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.08it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 16.73it/s]
[2025-04-29 15:05:42,448][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6950
[2025-04-29 15:05:42,804][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:05:42,805][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃█▆
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69272
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69504
wandb:           train_time 20.32156
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69285
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150512-dq2ue1wu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150512-dq2ue1wu/logs
Standard experiment completed successfully: layer_2_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_2/question_type/results.json
Running complexity experiment for language fi, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:05:59,522][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_2/complexity
experiment_name: layer_2_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:05:59,522][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:05:59,522][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:05:59,522][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:05:59,526][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:05:59,527][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:06:00,505][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:06:03,308][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:06:03,309][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:06:03,326][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,350][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,410][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:06:03,422][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:06:03,422][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:06:03,424][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:06:03,441][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,465][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,475][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:06:03,477][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:06:03,477][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:06:03,478][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:06:03,491][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,511][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:06:03,521][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:06:03,522][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:06:03,522][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:06:03,523][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:06:03,524][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:06:03,524][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:06:03,524][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:06:03,524][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:06:03,524][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:06:03,524][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:06:03,524][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:06:03,525][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:06:03,525][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:06:03,525][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:06:03,525][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:06:03,525][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:06:03,525][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:06:03,525][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:06:03,526][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:06:03,526][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:06:03,526][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:06:03,527][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:06:03,527][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:06:03,527][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:06:03,527][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:06:03,527][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:06:07,235][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:06:07,236][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:06:07,238][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:06:07,239][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 15:06:07,239][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:57,  1.29it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.11it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.78it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.20it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.27it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.93it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.25it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.21it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.93it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.47it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.85it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.12it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.33it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.10it/s]
[2025-04-29 15:06:17,041][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1117
[2025-04-29 15:06:18,518][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1832, Metrics: {'mse': 0.18346932530403137, 'rmse': 0.4283331942588986, 'r2': -1.798471450805664}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:42,  1.73it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:13,  5.23it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:08,  8.22it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:06, 10.65it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:01<00:05, 12.55it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:04, 14.00it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:04, 15.06it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:03, 15.84it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 16.41it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 16.81it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.08it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:03, 17.28it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.43it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:02, 17.54it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:02, 17.61it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:02<00:02, 17.66it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 15.69it/s]
[2025-04-29 15:06:23,861][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0527
[2025-04-29 15:06:24,838][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1202, Metrics: {'mse': 0.12031905353069305, 'rmse': 0.3468703699232511, 'r2': -0.8352357149124146}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  5.75it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.51it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.04it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.23it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:06:30,265][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0362
[2025-04-29 15:06:30,656][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0976, Metrics: {'mse': 0.09762103855609894, 'rmse': 0.3124436566104342, 'r2': -0.4890211820602417}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.13it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.90it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.33it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.60it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 15:06:35,596][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0318
[2025-04-29 15:06:35,988][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0890, Metrics: {'mse': 0.08891768753528595, 'rmse': 0.2981906898870016, 'r2': -0.3562682867050171}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  6.10it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.85it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.27it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:06:40,949][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0307
[2025-04-29 15:06:41,326][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0877, Metrics: {'mse': 0.08762136101722717, 'rmse': 0.29600905563382207, 'r2': -0.33649539947509766}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.55it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.25it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.67it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.66it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.64it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.64it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.61it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.61it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.62it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:06:46,299][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0339
[2025-04-29 15:06:46,665][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0856, Metrics: {'mse': 0.085512675344944, 'rmse': 0.29242550392355315, 'r2': -0.3043314218521118}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.25it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.95it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.87it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.10it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.38it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.50it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:02<00:02, 17.60it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.63it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.64it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.64it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.66it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.67it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.66it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.68it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.67it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.66it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:06:51,680][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0295
[2025-04-29 15:06:52,056][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0867, Metrics: {'mse': 0.08666723221540451, 'rmse': 0.29439298941279923, 'r2': -0.32194197177886963}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.79it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.50it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.03it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.38it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.40it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.55it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.64it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.64it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.65it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:06:56,473][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0312
[2025-04-29 15:06:56,850][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0870, Metrics: {'mse': 0.08689883351325989, 'rmse': 0.29478608093541303, 'r2': -0.32547450065612793}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.72it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.44it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.97it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.32it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.40it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.55it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:07:01,260][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0317
[2025-04-29 15:07:01,652][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0852, Metrics: {'mse': 0.08515553176403046, 'rmse': 0.2918142076116762, 'r2': -0.2988837957382202}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  6.10it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.84it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.27it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.53it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.28it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.64it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.60it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.59it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.58it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.58it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.59it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.60it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.61it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 15:07:06,595][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0310
[2025-04-29 15:07:06,962][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0854, Metrics: {'mse': 0.08531065285205841, 'rmse': 0.2920798740962109, 'r2': -0.30124998092651367}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁▁
wandb:     best_val_mse █▄▂▁▁▁▁
wandb:      best_val_r2 ▁▅▇████
wandb:    best_val_rmse █▄▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇███████
wandb:         val_rmse █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08524
wandb:     best_val_mse 0.08516
wandb:      best_val_r2 -0.29888
wandb:    best_val_rmse 0.29181
wandb:            epoch 10
wandb:   final_test_mse 0.03993
wandb:    final_test_r2 -0.0113
wandb:  final_test_rmse 0.19983
wandb:  final_train_mse 0.02021
wandb:   final_train_r2 -9e-05
wandb: final_train_rmse 0.14216
wandb:    final_val_mse 0.08516
wandb:     final_val_r2 -0.29888
wandb:   final_val_rmse 0.29181
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03099
wandb:       train_time 54.88887
wandb:         val_loss 0.08539
wandb:          val_mse 0.08531
wandb:           val_r2 -0.30125
wandb:         val_rmse 0.29208
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150559-46txljl0
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150559-46txljl0/logs
Standard experiment completed successfully: layer_2_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_2/complexity/results.json
Running question_type experiment for language fi, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:07:28,144][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_3/question_type
experiment_name: layer_3_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:07:28,144][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:07:28,144][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:07:28,144][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:07:28,149][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:07:28,149][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:07:29,301][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:07:32,119][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:07:32,119][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:07:32,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,172][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,239][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:07:32,250][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:07:32,251][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:07:32,252][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:07:32,269][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,292][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,302][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:07:32,303][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:07:32,303][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:07:32,304][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:07:32,320][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,341][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:07:32,355][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:07:32,356][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:07:32,356][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:07:32,358][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:07:32,358][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:07:32,358][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:07:32,358][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:07:32,359][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:07:32,359][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:07:32,359][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:07:32,360][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:07:32,360][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:07:32,360][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:07:32,361][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:07:32,361][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:07:32,361][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:07:32,361][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:07:32,361][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:07:32,361][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:07:32,362][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:07:32,362][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:07:36,279][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:07:36,280][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:07:36,282][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:07:36,282][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 15:07:36,283][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.15it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.75it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.31it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.68it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.76it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.49it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.86it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.73it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.33it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.76it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.07it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.29it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.74it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.81it/s]
[2025-04-29 15:07:43,248][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-29 15:07:43,586][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.48it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.21it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.54it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.76it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.46it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.91it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.18it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.38it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.22it/s]
[2025-04-29 15:07:48,513][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6964
[2025-04-29 15:07:48,868][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.08it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.29it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.65it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.59it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.58it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.58it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.60it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.59it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.61it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.58it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.61it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.67it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 15:07:53,997][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6919
[2025-04-29 15:07:54,370][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.63it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.37it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.96it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.31it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.13it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.65it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:07:58,774][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6948
[2025-04-29 15:07:59,155][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  5.72it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.46it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.03it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.38it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.19it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.67it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:08:03,556][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6945
[2025-04-29 15:08:03,926][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:08:03,926][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁
wandb:          best_val_f1 ▁▁
wandb:        best_val_loss █▁
wandb:                epoch ▁▁▃▃▅▅▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▁
wandb:           train_loss ▅█▁▅▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁
wandb:             val_loss ▂▁█▁▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69265
wandb:                epoch 5
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69446
wandb:           train_time 25.7435
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69276
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150728-koxknpxn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150728-koxknpxn/logs
Standard experiment completed successfully: layer_3_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_3/question_type/results.json
Running complexity experiment for language fi, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:08:21,012][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_3/complexity
experiment_name: layer_3_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:08:21,013][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:08:21,013][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:08:21,013][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:08:21,017][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:08:21,018][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:08:22,481][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:08:25,192][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:08:25,193][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:08:25,216][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,246][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,306][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:08:25,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:08:25,318][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:08:25,319][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:08:25,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,353][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,362][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:08:25,363][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:08:25,363][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:08:25,364][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:08:25,380][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,399][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:08:25,408][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:08:25,410][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:08:25,410][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:08:25,411][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:08:25,411][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:08:25,411][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:08:25,411][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:08:25,411][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:08:25,412][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:08:25,412][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:08:25,412][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:08:25,412][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:08:25,412][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:08:25,412][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:08:25,412][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:08:25,413][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:08:25,413][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:08:25,413][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:08:25,414][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:08:25,414][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:08:25,414][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:08:25,414][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:08:25,414][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:08:25,414][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:08:25,414][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:08:25,415][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:08:25,415][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:08:29,084][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:08:29,085][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:08:29,087][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:08:29,087][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 15:08:29,087][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:56,  1.32it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.18it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:10,  6.88it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.31it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.34it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.99it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.27it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.26it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.95it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.47it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.85it/s]Epoch 1/10:  31%|███       | 23/75 [00:01<00:03, 17.12it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.32it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.15it/s]
[2025-04-29 15:08:35,924][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1144
[2025-04-29 15:08:36,277][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1917, Metrics: {'mse': 0.19206345081329346, 'rmse': 0.43825044302692207, 'r2': -1.929558515548706}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.95it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.72it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.22it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:08:41,206][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0555
[2025-04-29 15:08:41,571][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1276, Metrics: {'mse': 0.1277133971452713, 'rmse': 0.3573701122719572, 'r2': -0.9480222463607788}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.04it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.82it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.29it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.59it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 15:08:46,732][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0367
[2025-04-29 15:08:47,110][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1013, Metrics: {'mse': 0.10125672817230225, 'rmse': 0.3182086236611168, 'r2': -0.5444766283035278}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.14it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.85it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.29it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.57it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:08:52,039][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0312
[2025-04-29 15:08:52,411][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0907, Metrics: {'mse': 0.09060609340667725, 'rmse': 0.30100846068952486, 'r2': -0.3820216655731201}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  6.05it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.78it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.22it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.52it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.28it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.73it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]
[2025-04-29 15:08:57,390][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0297
[2025-04-29 15:08:57,747][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0884, Metrics: {'mse': 0.08834399282932281, 'rmse': 0.2972271737733998, 'r2': -0.34751760959625244}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  6.04it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.79it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.24it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.28it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:09:02,671][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0329
[2025-04-29 15:09:03,048][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0859, Metrics: {'mse': 0.0858222171664238, 'rmse': 0.2929542919406094, 'r2': -0.30905282497406006}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:09:08,040][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0286
[2025-04-29 15:09:08,423][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0867, Metrics: {'mse': 0.086630679666996, 'rmse': 0.29433090165151876, 'r2': -0.3213843107223511}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.48it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:09:12,825][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0298
[2025-04-29 15:09:13,208][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0866, Metrics: {'mse': 0.08656018227338791, 'rmse': 0.2942111185414105, 'r2': -0.320309042930603}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.01it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.66it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.67it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.65it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:09:17,623][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0301
[2025-04-29 15:09:17,997][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0848, Metrics: {'mse': 0.08473549783229828, 'rmse': 0.29109362382624987, 'r2': -0.292477011680603}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.56it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.29it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.87it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.60it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.61it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.96it/s]
[2025-04-29 15:09:22,986][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0298
[2025-04-29 15:09:23,372][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0847, Metrics: {'mse': 0.08464924246072769, 'rmse': 0.29094542866442785, 'r2': -0.2911614179611206}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁▁▁
wandb:     best_val_mse █▄▂▁▁▁▁▁
wandb:      best_val_r2 ▁▅▇█████
wandb:    best_val_rmse █▄▂▁▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇███████
wandb:         val_rmse █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08473
wandb:     best_val_mse 0.08465
wandb:      best_val_r2 -0.29116
wandb:    best_val_rmse 0.29095
wandb:            epoch 10
wandb:   final_test_mse 0.03931
wandb:    final_test_r2 0.00449
wandb:  final_test_rmse 0.19827
wandb:  final_train_mse 0.01994
wandb:   final_train_r2 0.01304
wandb: final_train_rmse 0.14123
wandb:    final_val_mse 0.08465
wandb:     final_val_r2 -0.29116
wandb:   final_val_rmse 0.29095
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02976
wandb:       train_time 52.98574
wandb:         val_loss 0.08473
wandb:          val_mse 0.08465
wandb:           val_r2 -0.29116
wandb:         val_rmse 0.29095
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150821-34et72ah
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150821-34et72ah/logs
Standard experiment completed successfully: layer_3_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_3/complexity/results.json
Running question_type experiment for language fi, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:09:43,109][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_4/question_type
experiment_name: layer_4_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:09:43,109][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:09:43,109][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:09:43,109][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:09:43,113][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:09:43,114][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:09:44,376][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:09:47,043][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:09:47,043][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:09:47,080][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,101][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,172][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:09:47,184][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:09:47,184][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:09:47,185][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:09:47,201][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,223][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,233][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:09:47,234][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:09:47,235][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:09:47,235][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:09:47,251][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,271][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:09:47,280][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:09:47,282][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:09:47,282][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:09:47,283][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:09:47,283][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:09:47,283][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:09:47,283][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:09:47,283][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:09:47,284][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:09:47,284][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:09:47,284][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:09:47,285][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:09:47,285][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:09:47,285][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:09:47,286][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:09:47,286][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:09:47,286][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:09:47,286][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:09:47,286][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:09:47,286][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:09:47,286][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:09:47,287][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:09:51,026][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:09:51,027][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:09:51,029][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:09:51,029][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 15:09:51,029][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:05,  1.12it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:19,  3.66it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.19it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.55it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.62it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.37it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.77it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.84it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.67it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.28it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.72it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.23it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.40it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.50it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.68it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.71it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.74it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.71it/s]
[2025-04-29 15:09:57,685][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6947
[2025-04-29 15:09:58,039][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  6.12it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.86it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.33it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.37it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:10:02,962][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6970
[2025-04-29 15:10:03,312][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.18it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.94it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.36it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.63it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:10:07,682][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-29 15:10:08,035][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:11,  6.27it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:05, 12.00it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.40it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.63it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.37it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 15:10:12,413][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6952
[2025-04-29 15:10:12,773][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:10:12,773][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃█▅
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69264
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.6952
wandb:           train_time 20.18778
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69275
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150943-uyq519gn
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_150943-uyq519gn/logs
Standard experiment completed successfully: layer_4_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_4/question_type/results.json
Running complexity experiment for language fi, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:10:29,193][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_4/complexity
experiment_name: layer_4_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:10:29,193][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:10:29,193][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:10:29,193][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:10:29,198][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:10:29,198][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:10:30,146][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:10:32,859][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:10:32,860][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:10:32,879][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:32,901][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:32,956][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:10:32,967][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:10:32,968][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:10:32,969][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:10:32,990][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:33,011][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:33,021][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:10:33,023][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:10:33,023][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:10:33,024][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:10:33,040][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:33,061][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:10:33,071][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:10:33,073][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:10:33,073][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:10:33,074][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:10:33,074][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:10:33,074][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:10:33,075][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:10:33,075][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:10:33,075][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:10:33,075][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:10:33,075][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:10:33,075][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:10:33,075][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:10:33,076][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:10:33,076][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:10:33,076][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:10:33,077][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:10:33,077][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:10:33,077][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:10:33,078][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:10:33,078][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:10:36,670][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:10:36,670][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:10:36,673][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:10:36,673][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 15:10:36,673][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:03,  1.16it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.77it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.32it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.68it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.76it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.49it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.88it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.93it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.74it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.30it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.72it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.26it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.43it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.79it/s]
[2025-04-29 15:10:43,244][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1128
[2025-04-29 15:10:43,580][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1843, Metrics: {'mse': 0.1845654547214508, 'rmse': 0.4296108177425829, 'r2': -1.8151910305023193}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.71it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.41it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.68it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.85it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:03, 16.53it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.94it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.22it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.39it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.50it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.27it/s]
[2025-04-29 15:10:48,500][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0540
[2025-04-29 15:10:48,860][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1207, Metrics: {'mse': 0.1208346039056778, 'rmse': 0.3476127211505324, 'r2': -0.8430993556976318}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.38it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.20it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 14.81it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.79it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.40it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.83it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:03, 17.09it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 16.92it/s]
[2025-04-29 15:10:54,334][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0379
[2025-04-29 15:10:54,720][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0981, Metrics: {'mse': 0.09811263531446457, 'rmse': 0.3132293653450528, 'r2': -0.4965195655822754}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.10it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:10:59,704][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0335
[2025-04-29 15:11:00,091][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0897, Metrics: {'mse': 0.0896110013127327, 'rmse': 0.29935096678102224, 'r2': -0.36684346199035645}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.64it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.38it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.35it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:11:05,073][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0325
[2025-04-29 15:11:05,452][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0888, Metrics: {'mse': 0.08873680979013443, 'rmse': 0.2978872434162538, 'r2': -0.3535093069076538}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.50it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.20it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.21it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.66it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.68it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.68it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:11:10,408][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0359
[2025-04-29 15:11:10,796][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0858, Metrics: {'mse': 0.08573153614997864, 'rmse': 0.29279948112996823, 'r2': -0.30766963958740234}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.48it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.21it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:11:15,784][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0307
[2025-04-29 15:11:16,167][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0870, Metrics: {'mse': 0.08688610047101974, 'rmse': 0.29476448305557396, 'r2': -0.3252803087234497}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.04it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:11:20,559][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0324
[2025-04-29 15:11:20,938][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0868, Metrics: {'mse': 0.08671274781227112, 'rmse': 0.29447028341119774, 'r2': -0.32263612747192383}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.64it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.35it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.94it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.33it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.66it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.69it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 15:11:25,319][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0326
[2025-04-29 15:11:25,708][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0845, Metrics: {'mse': 0.08444280922412872, 'rmse': 0.2905904492995747, 'r2': -0.28801262378692627}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  5.82it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.56it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.08it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.23it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.66it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.66it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.65it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.64it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.61it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.60it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.61it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.62it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.62it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:11:30,679][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0322
[2025-04-29 15:11:31,065][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0846, Metrics: {'mse': 0.08446777611970901, 'rmse': 0.29063340503064855, 'r2': -0.28839337825775146}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁▁
wandb:     best_val_mse █▄▂▁▁▁▁
wandb:      best_val_r2 ▁▅▇████
wandb:    best_val_rmse █▄▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇███████
wandb:         val_rmse █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08453
wandb:     best_val_mse 0.08444
wandb:      best_val_r2 -0.28801
wandb:    best_val_rmse 0.29059
wandb:            epoch 10
wandb:   final_test_mse 0.03954
wandb:    final_test_r2 -0.00125
wandb:  final_test_rmse 0.19884
wandb:  final_train_mse 0.02002
wandb:   final_train_r2 0.00928
wandb: final_train_rmse 0.14149
wandb:    final_val_mse 0.08444
wandb:     final_val_r2 -0.28801
wandb:   final_val_rmse 0.29059
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03225
wandb:       train_time 52.8938
wandb:         val_loss 0.08455
wandb:          val_mse 0.08447
wandb:           val_r2 -0.28839
wandb:         val_rmse 0.29063
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151029-bv9h02iz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151029-bv9h02iz/logs
Standard experiment completed successfully: layer_4_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_4/complexity/results.json
Running question_type experiment for language fi, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:11:49,273][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_5/question_type
experiment_name: layer_5_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:11:49,273][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:11:49,273][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:11:49,273][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:11:49,277][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:11:49,278][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:11:50,484][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:11:53,165][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:11:53,166][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:11:53,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,216][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,282][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:11:53,294][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:11:53,294][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:11:53,295][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:11:53,308][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,329][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,338][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:11:53,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:11:53,339][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:11:53,340][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:11:53,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,375][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:11:53,384][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:11:53,386][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:11:53,386][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:11:53,387][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:11:53,387][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:11:53,387][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:11:53,388][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:11:53,388][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:11:53,388][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:11:53,388][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:11:53,388][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:11:53,388][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:11:53,388][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:11:53,389][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:11:53,389][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:11:53,389][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:11:53,390][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:11:53,390][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:11:53,390][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:11:53,391][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:11:53,391][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:11:57,096][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:11:57,096][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:11:57,099][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:11:57,099][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 15:11:57,099][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:01,  1.20it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.87it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.47it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.85it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.94it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.64it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.98it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.01it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.76it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.33it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.75it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.05it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.22it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.36it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.48it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.89it/s]
[2025-04-29 15:12:03,840][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6942
[2025-04-29 15:12:04,199][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.56it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.45it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:12:09,126][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6963
[2025-04-29 15:12:09,485][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.96it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.65it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.13it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.99it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:12:14,659][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6919
[2025-04-29 15:12:15,044][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.54it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.90it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.13it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.65it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:12:19,447][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6947
[2025-04-29 15:12:19,838][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.90it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.61it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.06it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.95it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.52it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.13it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.49it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.55it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.94it/s]
[2025-04-29 15:12:24,813][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6945
[2025-04-29 15:12:25,202][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.67it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:12:29,611][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6960
[2025-04-29 15:12:30,001][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:12,  5.86it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.59it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:04, 14.07it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.41it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.65it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.67it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:12:34,408][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6959
[2025-04-29 15:12:34,791][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:12:34,792][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁
wandb:          best_val_f1 ▁▁▁
wandb:        best_val_loss █▇▁
wandb:                epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████▁▁▁
wandb:           train_loss ▅█▁▅▅█▇
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁
wandb:             val_loss ▃▃█▁▆▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69272
wandb:                epoch 7
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69586
wandb:           train_time 35.99009
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69273
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151149-3rnpoc3x
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151149-3rnpoc3x/logs
Standard experiment completed successfully: layer_5_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_5/question_type/results.json
Running complexity experiment for language fi, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:12:54,303][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_5/complexity
experiment_name: layer_5_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:12:54,303][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:12:54,304][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:12:54,304][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:12:54,308][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:12:54,309][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:12:55,414][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:12:58,117][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:12:58,117][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:12:58,149][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,167][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,239][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:12:58,251][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:12:58,251][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:12:58,252][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:12:58,269][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,291][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,301][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:12:58,303][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:12:58,303][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:12:58,304][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:12:58,318][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:12:58,347][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:12:58,348][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:12:58,348][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:12:58,349][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:12:58,349][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:12:58,350][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:12:58,350][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:12:58,350][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:12:58,350][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:12:58,350][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:12:58,350][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:12:58,350][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:12:58,351][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:12:58,351][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:12:58,351][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:12:58,351][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:12:58,351][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:12:58,351][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:12:58,351][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:12:58,352][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:12:58,352][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:12:58,352][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:12:58,353][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:12:58,353][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:12:58,353][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:12:58,353][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:12:58,353][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:13:02,226][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:13:02,227][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:13:02,229][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:13:02,229][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 15:13:02,229][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:59,  1.23it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.96it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.60it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.01it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.06it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.75it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.09it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.09it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.85it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.10it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.30it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.46it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.57it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.84it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 15.00it/s]
[2025-04-29 15:13:09,096][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1049
[2025-04-29 15:13:09,443][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1760, Metrics: {'mse': 0.17626753449440002, 'rmse': 0.41984227335322016, 'r2': -1.688622236251831}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.64it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.40it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:05, 13.99it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.19it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.71it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 15:13:14,398][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0486
[2025-04-29 15:13:14,752][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1155, Metrics: {'mse': 0.11556725203990936, 'rmse': 0.3399518378239914, 'r2': -0.7627561092376709}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  5.73it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.46it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.02it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.23it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.74it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:13:19,918][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0345
[2025-04-29 15:13:20,302][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0954, Metrics: {'mse': 0.09542366117238998, 'rmse': 0.30890720479197303, 'r2': -0.4555044174194336}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.02it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.66it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:13:25,278][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0309
[2025-04-29 15:13:25,662][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0881, Metrics: {'mse': 0.08807379752397537, 'rmse': 0.296772299118323, 'r2': -0.3433964252471924}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.01it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:13:30,646][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0296
[2025-04-29 15:13:31,020][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0875, Metrics: {'mse': 0.08743328601121902, 'rmse': 0.29569120042912844, 'r2': -0.3336266279220581}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.87it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.57it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:13:35,981][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0329
[2025-04-29 15:13:36,377][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0852, Metrics: {'mse': 0.08511479198932648, 'rmse': 0.2917443949578577, 'r2': -0.298262357711792}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.25it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.94it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.12it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.54it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:13:41,372][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0284
[2025-04-29 15:13:41,738][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0863, Metrics: {'mse': 0.086260586977005, 'rmse': 0.2937015270253204, 'r2': -0.3157392740249634}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.95it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.62it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.67it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:13:46,151][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0299
[2025-04-29 15:13:46,521][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0863, Metrics: {'mse': 0.08622284233570099, 'rmse': 0.29363726319338457, 'r2': -0.31516361236572266}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.38it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.10it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.72it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.64it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.64it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.65it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.64it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.65it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.65it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.65it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.65it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.65it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.64it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:13:50,948][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0299
[2025-04-29 15:13:51,316][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0841, Metrics: {'mse': 0.08404967933893204, 'rmse': 0.28991322725762625, 'r2': -0.28201615810394287}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 10.99it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.11it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.29it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.39it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.46it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.50it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.55it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.57it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.59it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.64it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.61it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.61it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.61it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.61it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:13:56,302][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0296
[2025-04-29 15:13:56,693][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0846, Metrics: {'mse': 0.08449643105268478, 'rmse': 0.29068269823414805, 'r2': -0.2888305187225342}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▁▁▁▁
wandb:     best_val_mse █▃▂▁▁▁▁
wandb:      best_val_r2 ▁▆▇████
wandb:    best_val_rmse █▄▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▁▁▁▁▁▁▁
wandb:          val_mse █▃▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▆▇███████
wandb:         val_rmse █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08414
wandb:     best_val_mse 0.08405
wandb:      best_val_r2 -0.28202
wandb:    best_val_rmse 0.28991
wandb:            epoch 10
wandb:   final_test_mse 0.03958
wandb:    final_test_r2 -0.00239
wandb:  final_test_rmse 0.19895
wandb:  final_train_mse 0.0201
wandb:   final_train_r2 0.00535
wandb: final_train_rmse 0.14177
wandb:    final_val_mse 0.08405
wandb:     final_val_r2 -0.28202
wandb:   final_val_rmse 0.28991
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02959
wandb:       train_time 52.60167
wandb:         val_loss 0.08458
wandb:          val_mse 0.0845
wandb:           val_r2 -0.28883
wandb:         val_rmse 0.29068
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151254-3e5hx5t6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151254-3e5hx5t6/logs
Standard experiment completed successfully: layer_5_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_5/complexity/results.json
Running question_type experiment for language fi, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:14:14,871][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_6/question_type
experiment_name: layer_6_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:14:14,871][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:14:14,871][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:14:14,871][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:14:14,875][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:14:14,876][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:14:16,004][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:14:18,711][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:14:18,711][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:14:18,742][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,763][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,837][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:14:18,848][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:14:18,849][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:14:18,850][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:14:18,864][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,885][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,895][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:14:18,896][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:14:18,896][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:14:18,897][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:14:18,912][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,935][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:14:18,949][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:14:18,951][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:14:18,951][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:14:18,952][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:14:18,953][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:14:18,953][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:14:18,953][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:14:18,954][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:14:18,954][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:14:18,954][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:14:18,955][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:14:18,955][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:14:18,955][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:14:18,956][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:14:18,956][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:14:18,956][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:14:22,696][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:14:22,697][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:14:22,699][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:14:22,699][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 15:14:22,699][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:04,  1.16it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:19,  3.74it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.29it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.67it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.75it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.48it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.85it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.92it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.71it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.29it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.72it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.23it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.39it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.50it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.59it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.72it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.71it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.72it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.72it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.81it/s]
[2025-04-29 15:14:29,397][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6940
[2025-04-29 15:14:29,741][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  6.02it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.77it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.25it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.58it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:14:34,660][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6961
[2025-04-29 15:14:35,015][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.73it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:05, 12.42it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.68it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.84it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:03, 16.50it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.92it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.20it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.37it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.21it/s]
[2025-04-29 15:14:39,376][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-29 15:14:39,738][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:11,  6.70it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:05, 12.39it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.65it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.78it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.44it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.85it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.68it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.68it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.67it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.69it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.19it/s]
[2025-04-29 15:14:44,105][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6946
[2025-04-29 15:14:44,463][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:14:44,464][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃█▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69264
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69463
wandb:           train_time 20.13188
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69281
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151414-or9mxrll
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151414-or9mxrll/logs
Standard experiment completed successfully: layer_6_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_6/question_type/results.json
Running complexity experiment for language fi, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:15:01,268][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_6/complexity
experiment_name: layer_6_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:15:01,268][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:15:01,268][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:15:01,268][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:15:01,273][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:15:01,273][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:15:02,239][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:15:05,058][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:15:05,059][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:15:05,080][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,112][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,214][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:15:05,225][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:15:05,226][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:15:05,227][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:15:05,244][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,270][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,282][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:15:05,283][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:15:05,283][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:15:05,284][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:15:05,303][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:15:05,337][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:15:05,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:15:05,339][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:15:05,340][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:15:05,340][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:15:05,340][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:15:05,341][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:15:05,341][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:15:05,341][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:15:05,341][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:15:05,341][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:15:05,341][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:15:05,342][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:15:05,342][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:15:05,342][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:15:05,343][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:15:05,343][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:15:05,343][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:15:05,344][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:15:05,344][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:15:05,344][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:15:08,897][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:15:08,898][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:15:08,900][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:15:08,900][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 15:15:08,900][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:55,  1.33it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.22it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:10,  6.94it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.36it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.40it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 13.04it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.31it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.27it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.99it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.50it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:01<00:03, 17.15it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.33it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.17it/s]
[2025-04-29 15:15:15,380][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1181
[2025-04-29 15:15:15,708][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2046, Metrics: {'mse': 0.20496922731399536, 'rmse': 0.45273527288471277, 'r2': -2.126411199569702}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.86it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.63it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.13it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.48it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.04it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.54it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.57it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.63it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.64it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 15:15:20,652][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-29 15:15:21,007][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1389, Metrics: {'mse': 0.13910657167434692, 'rmse': 0.37296993400855644, 'r2': -1.1218030452728271}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.54it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.33it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 14.93it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.89it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.50it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 16.92it/s]
[2025-04-29 15:15:26,456][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0386
[2025-04-29 15:15:26,860][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1083, Metrics: {'mse': 0.10829415172338486, 'rmse': 0.32908076778107964, 'r2': -0.6518189907073975}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.75it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.45it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.02it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.37it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.19it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.65it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:15:31,813][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0307
[2025-04-29 15:15:32,199][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0941, Metrics: {'mse': 0.09405704587697983, 'rmse': 0.30668721179237296, 'r2': -0.4346592426300049}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.87it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.54it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.06it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.65it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.67it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:15:37,202][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0284
[2025-04-29 15:15:37,584][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0897, Metrics: {'mse': 0.08960840106010437, 'rmse': 0.2993466235989716, 'r2': -0.3668038845062256}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.16it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.83it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.55it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.05it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 16.98it/s]
[2025-04-29 15:15:42,559][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0315
[2025-04-29 15:15:42,942][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0867, Metrics: {'mse': 0.08659633994102478, 'rmse': 0.29427256063218804, 'r2': -0.32086050510406494}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.67it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.65it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.67it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:15:47,937][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0276
[2025-04-29 15:15:48,326][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0870, Metrics: {'mse': 0.08688534051179886, 'rmse': 0.2947631939571134, 'r2': -0.3252687454223633}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.75it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.17it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.67it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:15:52,741][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0287
[2025-04-29 15:15:53,130][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0873, Metrics: {'mse': 0.08723911643028259, 'rmse': 0.2953626862524828, 'r2': -0.33066487312316895}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 10.77it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.50it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.03it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:15:57,547][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0289
[2025-04-29 15:15:57,938][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0860, Metrics: {'mse': 0.08591607958078384, 'rmse': 0.2931144479222815, 'r2': -0.31048452854156494}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.19it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 10.87it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 15.94it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:16:02,899][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0288
[2025-04-29 15:16:03,292][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0858, Metrics: {'mse': 0.0856705978512764, 'rmse': 0.2926954011447334, 'r2': -0.3067401647567749}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁▁▁
wandb:     best_val_mse █▄▂▁▁▁▁▁
wandb:      best_val_r2 ▁▅▇█████
wandb:    best_val_rmse █▅▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇███████
wandb:         val_rmse █▅▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08575
wandb:     best_val_mse 0.08567
wandb:      best_val_r2 -0.30674
wandb:    best_val_rmse 0.2927
wandb:            epoch 10
wandb:   final_test_mse 0.03999
wandb:    final_test_r2 -0.01275
wandb:  final_test_rmse 0.19998
wandb:  final_train_mse 0.02021
wandb:   final_train_r2 -0.00033
wandb: final_train_rmse 0.14218
wandb:    final_val_mse 0.08567
wandb:     final_val_r2 -0.30674
wandb:   final_val_rmse 0.2927
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02876
wandb:       train_time 53.44409
wandb:         val_loss 0.08575
wandb:          val_mse 0.08567
wandb:           val_r2 -0.30674
wandb:         val_rmse 0.2927
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151501-bydtxn5p
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151501-bydtxn5p/logs
Standard experiment completed successfully: layer_6_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_6/complexity/results.json
Running question_type experiment for language fi, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:16:24,051][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_7/question_type
experiment_name: layer_7_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:16:24,051][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:16:24,051][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:16:24,051][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:16:24,055][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:16:24,056][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:16:25,212][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:16:27,907][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:16:27,908][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:16:27,945][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:27,968][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:28,041][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:16:28,052][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:16:28,053][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:16:28,054][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:16:28,067][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:28,087][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:28,098][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:16:28,099][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:16:28,099][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:16:28,100][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:16:28,118][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:28,140][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:16:28,150][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:16:28,152][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:16:28,152][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:16:28,152][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:16:28,153][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:16:28,153][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:16:28,153][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:16:28,153][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:16:28,154][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:16:28,154][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:16:28,154][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:16:28,155][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:16:28,155][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:16:28,155][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:16:28,156][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:16:28,156][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:16:28,156][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:16:28,156][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:16:28,156][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:16:28,156][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:16:28,156][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:16:28,157][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:16:32,041][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:16:32,042][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:16:32,044][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:16:32,045][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 15:16:32,045][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:59,  1.25it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.01it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.66it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.06it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.12it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.82it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.12it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.12it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.86it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.12it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.32it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.47it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.05it/s]
[2025-04-29 15:16:38,797][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6934
[2025-04-29 15:16:39,135][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.65it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.42it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:05, 13.97it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:16:44,073][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6935
[2025-04-29 15:16:44,434][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.13it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.89it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.35it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.60it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 15:16:48,813][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6933
[2025-04-29 15:16:49,168][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:11,  6.18it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.93it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.34it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.59it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:16:53,552][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6933
[2025-04-29 15:16:53,920][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:16:53,921][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69245
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69333
wandb:           train_time 20.10801
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69254
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151624-k9llx508
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151624-k9llx508/logs
Standard experiment completed successfully: layer_7_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_7/question_type/results.json
Running complexity experiment for language fi, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:17:10,924][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_7/complexity
experiment_name: layer_7_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:17:10,925][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:17:10,925][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:17:10,925][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:17:10,929][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:17:10,930][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:17:11,963][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:17:14,653][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:17:14,654][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:17:14,669][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,687][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,740][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:17:14,752][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:17:14,752][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:17:14,753][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:17:14,767][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,800][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:17:14,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:17:14,802][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:17:14,802][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:17:14,816][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,837][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:17:14,846][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:17:14,848][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:17:14,848][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:17:14,849][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:17:14,849][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:17:14,849][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:17:14,849][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:17:14,849][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:17:14,850][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:17:14,850][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:17:14,850][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:17:14,850][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:17:14,850][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:17:14,850][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:17:14,850][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:17:14,851][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:17:14,851][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:17:14,851][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:17:14,852][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:17:14,852][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:17:14,852][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:17:14,852][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:17:14,852][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:17:14,852][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:17:14,852][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:17:14,853][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:17:14,853][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:17:18,398][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:17:18,399][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:17:18,401][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:17:18,402][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 15:17:18,402][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:59,  1.24it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.96it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.58it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.98it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.03it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.73it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.07it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.09it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.84it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.40it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.79it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.09it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.29it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.95it/s]
[2025-04-29 15:17:24,905][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1446
[2025-04-29 15:17:25,285][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2840, Metrics: {'mse': 0.2844938337802887, 'rmse': 0.5333796338259352, 'r2': -3.3394060134887695}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.24it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.99it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.39it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.65it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.39it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.84it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 15:17:30,229][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1140
[2025-04-29 15:17:30,585][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2431, Metrics: {'mse': 0.2435804009437561, 'rmse': 0.4935386519248073, 'r2': -2.715350389480591}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.56it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.98it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:17:36,002][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0894
[2025-04-29 15:17:36,384][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2078, Metrics: {'mse': 0.20816712081432343, 'rmse': 0.4562533515650306, 'r2': -2.1751890182495117}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.80it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.68it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:17:41,346][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0684
[2025-04-29 15:17:41,735][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1781, Metrics: {'mse': 0.17840859293937683, 'rmse': 0.4223844137031773, 'r2': -1.7212798595428467}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.09it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.74it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.14it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.40it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.49it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.67it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:17:46,735][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0546
[2025-04-29 15:17:47,121][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1542, Metrics: {'mse': 0.15438109636306763, 'rmse': 0.3929135991068108, 'r2': -1.3547866344451904}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.94it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.61it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.10it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:17:52,090][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0432
[2025-04-29 15:17:52,470][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1348, Metrics: {'mse': 0.13493649661540985, 'rmse': 0.3673370340918675, 'r2': -1.058196783065796}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.08it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.73it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.50it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.55it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:17:57,468][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0357
[2025-04-29 15:17:57,857][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1206, Metrics: {'mse': 0.12065795809030533, 'rmse': 0.34735854400072746, 'r2': -0.8404051065444946}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.11it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.74it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 16.97it/s]
[2025-04-29 15:18:02,835][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0299
[2025-04-29 15:18:03,218][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1108, Metrics: {'mse': 0.11080760508775711, 'rmse': 0.3328777629817845, 'r2': -0.6901569366455078}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.13it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.69it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:18:08,235][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0272
[2025-04-29 15:18:08,700][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1033, Metrics: {'mse': 0.10332225263118744, 'rmse': 0.321437789675059, 'r2': -0.5759822130203247}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  5.76it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.49it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.02it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 17.01it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:18:13,671][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0252
[2025-04-29 15:18:14,055][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0986, Metrics: {'mse': 0.09864168614149094, 'rmse': 0.3140727402075687, 'r2': -0.5045890808105469}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▂▂▁▁▁
wandb:     best_val_mse █▆▅▄▃▂▂▁▁▁
wandb:      best_val_r2 ▁▃▄▅▆▇▇███
wandb:    best_val_rmse █▇▆▄▄▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▅▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▄▃▂▂▁▁▁
wandb:          val_mse █▆▅▄▃▂▂▁▁▁
wandb:           val_r2 ▁▃▄▅▆▇▇███
wandb:         val_rmse █▇▆▄▄▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.09865
wandb:     best_val_mse 0.09864
wandb:      best_val_r2 -0.50459
wandb:    best_val_rmse 0.31407
wandb:            epoch 10
wandb:   final_test_mse 0.0434
wandb:    final_test_r2 -0.09906
wandb:  final_test_rmse 0.20832
wandb:  final_train_mse 0.02202
wandb:   final_train_r2 -0.08984
wandb: final_train_rmse 0.1484
wandb:    final_val_mse 0.09864
wandb:     final_val_r2 -0.50459
wandb:   final_val_rmse 0.31407
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02515
wandb:       train_time 54.75695
wandb:         val_loss 0.09865
wandb:          val_mse 0.09864
wandb:           val_r2 -0.50459
wandb:         val_rmse 0.31407
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151710-8wi1b6py
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151710-8wi1b6py/logs
Standard experiment completed successfully: layer_7_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_7/complexity/results.json
Running question_type experiment for language fi, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:18:32,977][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_8/question_type
experiment_name: layer_8_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:18:32,977][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:18:32,977][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:18:32,977][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:18:32,981][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:18:32,981][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:18:34,191][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:18:37,003][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:18:37,003][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:18:37,037][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,061][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,128][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:18:37,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:18:37,140][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:18:37,141][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:18:37,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,177][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,186][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:18:37,188][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:18:37,188][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:18:37,189][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:18:37,202][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,224][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:18:37,233][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:18:37,235][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:18:37,235][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:18:37,236][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:18:37,236][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:18:37,236][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:18:37,236][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:18:37,237][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:18:37,237][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:18:37,237][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:18:37,237][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:18:37,237][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:18:37,237][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:18:37,237][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:18:37,238][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:18:37,238][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:18:37,238][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:18:37,239][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:18:37,239][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:18:37,239][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:18:37,240][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:18:40,971][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:18:40,972][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:18:40,974][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:18:40,974][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 15:18:40,974][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:00,  1.23it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.93it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.57it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.96it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.03it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.71it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.05it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.09it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.83it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.41it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.12it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.30it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.97it/s]
[2025-04-29 15:18:47,726][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6937
[2025-04-29 15:18:48,074][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.71it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.50it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:18:53,012][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6940
[2025-04-29 15:18:53,371][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:11,  6.57it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:05, 12.30it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.58it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.77it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.41it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 15:18:57,735][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6928
[2025-04-29 15:18:58,106][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.85it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.61it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.13it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.47it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.27it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.26it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.60it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.60it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.59it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.59it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.65it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.68it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.69it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:19:02,496][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6933
[2025-04-29 15:19:02,855][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6923, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
[2025-04-29 15:19:02,856][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▆█▁▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃██
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69222
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69335
wandb:           train_time 20.14083
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.69234
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151833-j1z475xf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151833-j1z475xf/logs
Standard experiment completed successfully: layer_8_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_8/question_type/results.json
Running complexity experiment for language fi, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:19:19,488][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_8/complexity
experiment_name: layer_8_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:19:19,488][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:19:19,488][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:19:19,488][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:19:19,493][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:19:19,493][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:19:20,451][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:19:23,177][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:19:23,177][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:19:23,196][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,215][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,278][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:19:23,289][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:19:23,290][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:19:23,291][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:19:23,305][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,326][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,335][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:19:23,336][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:19:23,337][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:19:23,337][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:19:23,350][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,370][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:19:23,379][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:19:23,380][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:19:23,381][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:19:23,381][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:19:23,382][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:19:23,382][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:19:23,382][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:19:23,382][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:19:23,382][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:19:23,383][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:19:23,383][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:19:23,383][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:19:23,384][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:19:23,384][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:19:23,384][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:19:23,385][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:19:23,385][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:19:23,385][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:19:23,385][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:19:23,385][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:19:23,385][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:19:23,386][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:19:26,904][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:19:26,905][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:19:26,908][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:19:26,908][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 15:19:26,908][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.59it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.09it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.44it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.53it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.30it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.71it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.79it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.62it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.21it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.67it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.22it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.40it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.52it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.82it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.84it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.85it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.85it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.71it/s]
[2025-04-29 15:19:33,573][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1448
[2025-04-29 15:19:33,914][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2736, Metrics: {'mse': 0.2741279602050781, 'rmse': 0.5235723065681359, 'r2': -3.1812949180603027}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.84it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.60it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.48it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.28it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.11it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.32it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.46it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.65it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.80it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.81it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.82it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.82it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.82it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.83it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.83it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.83it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.83it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.81it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.82it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 15:19:38,841][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1005
[2025-04-29 15:19:39,220][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2165, Metrics: {'mse': 0.21687491238117218, 'rmse': 0.465698306182417, 'r2': -2.308009624481201}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.59it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.37it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 14.94it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.90it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.52it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:19:44,670][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0696
[2025-04-29 15:19:45,072][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1737, Metrics: {'mse': 0.17397885024547577, 'rmse': 0.41710772019404746, 'r2': -1.6537127494812012}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.54it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.89it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.31it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:19:50,030][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0478
[2025-04-29 15:19:50,407][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1420, Metrics: {'mse': 0.14221130311489105, 'rmse': 0.3771091395271282, 'r2': -1.1691598892211914}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.22it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.85it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.99it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:19:55,397][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0358
[2025-04-29 15:19:55,762][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1198, Metrics: {'mse': 0.1199386864900589, 'rmse': 0.3463216517777352, 'r2': -0.8294339179992676}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.35it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.07it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.73it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.68it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:20:00,722][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0295
[2025-04-29 15:20:01,083][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1056, Metrics: {'mse': 0.10558025538921356, 'rmse': 0.32493115484547425, 'r2': -0.6104236841201782}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.23it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.94it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.62it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.11it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.99it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:20:06,091][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0251
[2025-04-29 15:20:06,473][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0969, Metrics: {'mse': 0.09692540019750595, 'rmse': 0.3113284442473992, 'r2': -0.47841060161590576}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.76it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.47it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.01it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 15.94it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.50it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.13it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.76it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:20:11,459][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0239
[2025-04-29 15:20:11,848][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0923, Metrics: {'mse': 0.09225314110517502, 'rmse': 0.30373202186331133, 'r2': -0.4071441888809204}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.86it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.60it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.21it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.56it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.64it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.60it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.60it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.61it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.62it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:20:16,869][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0234
[2025-04-29 15:20:17,263][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0888, Metrics: {'mse': 0.08872052282094955, 'rmse': 0.2978599046883443, 'r2': -0.35326099395751953}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  5.72it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.45it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.01it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.16it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.40it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.62it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.60it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.64it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.64it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.64it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.65it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:20:22,249][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0231
[2025-04-29 15:20:22,639][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0870, Metrics: {'mse': 0.08688829094171524, 'rmse': 0.29476819866077014, 'r2': -0.3253136873245239}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁▁▁▁
wandb:     best_val_mse █▆▄▃▂▂▁▁▁▁
wandb:      best_val_r2 ▁▃▅▆▇▇████
wandb:    best_val_rmse █▆▅▄▃▂▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▄▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▁▁▁▁
wandb:          val_mse █▆▄▃▂▂▁▁▁▁
wandb:           val_r2 ▁▃▅▆▇▇████
wandb:         val_rmse █▆▅▄▃▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.08696
wandb:     best_val_mse 0.08689
wandb:      best_val_r2 -0.32531
wandb:    best_val_rmse 0.29477
wandb:            epoch 10
wandb:   final_test_mse 0.04021
wandb:    final_test_r2 -0.01837
wandb:  final_test_rmse 0.20053
wandb:  final_train_mse 0.02022
wandb:   final_train_r2 -0.00059
wandb: final_train_rmse 0.1422
wandb:    final_val_mse 0.08689
wandb:     final_val_r2 -0.32531
wandb:   final_val_rmse 0.29477
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02312
wandb:       train_time 54.75861
wandb:         val_loss 0.08696
wandb:          val_mse 0.08689
wandb:           val_r2 -0.32531
wandb:         val_rmse 0.29477
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151919-lx0jf3xx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_151919-lx0jf3xx/logs
Standard experiment completed successfully: layer_8_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_8/complexity/results.json
Running question_type experiment for language fi, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:20:41,985][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_9/question_type
experiment_name: layer_9_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:20:41,986][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:20:41,986][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:20:41,986][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:20:41,990][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:20:41,990][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:20:43,211][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:20:46,063][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:20:46,064][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:20:46,115][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,136][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,208][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:20:46,221][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:20:46,222][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:20:46,222][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:20:46,237][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,259][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,269][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:20:46,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:20:46,271][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:20:46,272][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:20:46,286][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,308][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:20:46,317][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:20:46,319][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:20:46,319][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:20:46,320][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:20:46,320][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:20:46,320][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:20:46,321][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:20:46,321][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:20:46,321][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:20:46,321][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:20:46,321][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:20:46,321][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:20:46,321][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:20:46,322][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:20:46,322][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:20:46,322][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:20:46,323][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:20:46,323][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:20:46,323][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:20:46,324][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:20:46,324][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:20:50,174][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:20:50,174][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:20:50,177][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:20:50,177][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 15:20:50,177][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:01,  1.21it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:18,  3.90it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.52it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  8.91it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.97it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.67it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.00it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.04it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.81it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.34it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.74it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.44it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.56it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.94it/s]
[2025-04-29 15:20:57,694][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6946
[2025-04-29 15:20:58,049][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6915, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.84it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.58it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.12it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.76it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:21:02,969][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6940
[2025-04-29 15:21:03,328][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6912, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.64it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.38it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.18it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.01it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.24it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:21:08,478][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6920
[2025-04-29 15:21:08,855][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.50it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.24it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.86it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.27it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:21:13,787][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6924
[2025-04-29 15:21:14,171][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6909, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.96it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.64it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.06it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.95it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.52it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.66it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.67it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:21:19,156][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6908
[2025-04-29 15:21:19,542][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6908, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.76it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.19it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.04it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:21:24,500][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6916
[2025-04-29 15:21:24,894][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6906, Metrics: {'accuracy': 0.5873015873015873, 'f1': 0.23529411764705882}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.18it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.04it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.66it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.66it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.59it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.60it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.60it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.61it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.61it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.61it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:21:29,881][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6918
[2025-04-29 15:21:30,267][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6905, Metrics: {'accuracy': 0.7936507936507936, 'f1': 0.7868852459016393}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.50it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.02it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.38it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.17it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 15:21:35,221][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6913
[2025-04-29 15:21:35,635][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6903, Metrics: {'accuracy': 0.5555555555555556, 'f1': 0.6818181818181818}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.16it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.17it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.36it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.41it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.43it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.46it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.50it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.54it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.56it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:02<00:02, 17.58it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.60it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.60it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.60it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.60it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.61it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.62it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:21:40,668][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6905
[2025-04-29 15:21:41,052][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6899, Metrics: {'accuracy': 0.7301587301587301, 'f1': 0.7733333333333333}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.17it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.04it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.11it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.38it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.46it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.55it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.57it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.63it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.64it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.65it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:21:46,033][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6884
[2025-04-29 15:21:46,420][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6900, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.6666666666666666}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▃█▂▆
wandb:          best_val_f1 ▁▁▁▁▁▃█▇█
wandb:        best_val_loss █▇▆▅▅▄▃▃▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▅▄▅▅▄▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▃█▂▆▁
wandb:               val_f1 ▁▁▁▁▁▃█▇█▇
wandb:             val_loss █▇▆▅▅▄▃▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.73016
wandb:          best_val_f1 0.77333
wandb:        best_val_loss 0.68987
wandb:                epoch 10
wandb:  final_test_accuracy 0.65455
wandb:        final_test_f1 0.73973
wandb: final_train_accuracy 0.73891
wandb:       final_train_f1 0.78976
wandb:   final_val_accuracy 0.73016
wandb:         final_val_f1 0.77333
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68842
wandb:           train_time 53.75041
wandb:         val_accuracy 0.52381
wandb:               val_f1 0.66667
wandb:             val_loss 0.69
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152042-j74m5izj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152042-j74m5izj/logs
Standard experiment completed successfully: layer_9_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_9/question_type/results.json
Running complexity experiment for language fi, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:22:06,504][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_9/complexity
experiment_name: layer_9_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:22:06,504][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:22:06,504][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:22:06,504][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:22:06,509][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:22:06,509][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:22:07,764][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:22:10,654][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:22:10,654][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:22:10,694][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,718][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,800][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:22:10,812][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:22:10,812][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:22:10,814][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:22:10,830][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,858][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,870][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:22:10,871][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:22:10,872][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:22:10,873][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:22:10,892][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,920][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:22:10,931][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:22:10,933][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:22:10,933][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:22:10,934][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:22:10,934][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:22:10,935][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:22:10,935][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:22:10,935][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:22:10,935][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:22:10,935][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:22:10,935][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:22:10,935][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:22:10,936][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:22:10,936][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:22:10,936][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:22:10,936][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:22:10,936][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:22:10,936][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:22:10,936][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:22:10,937][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:22:10,937][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:22:10,937][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:22:10,938][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:22:10,938][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:22:10,938][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:22:10,938][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:22:15,107][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:22:15,108][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:22:15,110][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:22:15,110][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 15:22:15,110][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.10it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.60it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.10it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.45it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.54it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.28it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.71it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.81it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.64it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.24it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.68it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.23it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.41it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.77it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.83it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.82it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.83it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.84it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.71it/s]
[2025-04-29 15:22:23,473][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1611
[2025-04-29 15:22:23,821][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2638, Metrics: {'mse': 0.26430588960647583, 'rmse': 0.5141068853910399, 'r2': -3.031477928161621}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.90it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.69it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.78it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.80it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.81it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.82it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.81it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.82it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.82it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.83it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.83it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.84it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.83it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.84it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 15:22:28,756][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0828
[2025-04-29 15:22:29,120][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1731, Metrics: {'mse': 0.17333020269870758, 'rmse': 0.4163294401056783, 'r2': -1.6438188552856445}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  5.89it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.66it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.15it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.28it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.59it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 15:22:34,272][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0437
[2025-04-29 15:22:34,650][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1235, Metrics: {'mse': 0.12353545427322388, 'rmse': 0.3514761076847527, 'r2': -0.8842957019805908}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.46it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.20it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.28it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
[2025-04-29 15:22:39,573][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0295
[2025-04-29 15:22:39,955][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0973, Metrics: {'mse': 0.0972200259566307, 'rmse': 0.3118012603512544, 'r2': -0.4829045534133911}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.48it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.22it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.57it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.58it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.60it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.60it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.61it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 16.93it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.15it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.32it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.43it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.51it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.57it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.61it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.66it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]
[2025-04-29 15:22:44,967][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0237
[2025-04-29 15:22:45,344][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0870, Metrics: {'mse': 0.08695003390312195, 'rmse': 0.294872911443425, 'r2': -0.32625555992126465}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:12,  5.79it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.55it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:04, 14.09it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.21it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:22:50,260][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0252
[2025-04-29 15:22:50,642][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0833, Metrics: {'mse': 0.08314740657806396, 'rmse': 0.2883529201830007, 'r2': -0.2682536840438843}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.94it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.64it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.43it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:22:55,638][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0239
[2025-04-29 15:22:56,018][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0808, Metrics: {'mse': 0.08070410043001175, 'rmse': 0.2840846712337921, 'r2': -0.23098576068878174}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.02it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.69it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:23:00,988][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0231
[2025-04-29 15:23:01,367][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0801, Metrics: {'mse': 0.07995636016130447, 'rmse': 0.2827655568864505, 'r2': -0.21958041191101074}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.63it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.34it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.92it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.13it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:23:06,375][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0237
[2025-04-29 15:23:06,761][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0794, Metrics: {'mse': 0.07926018536090851, 'rmse': 0.2815318549665535, 'r2': -0.2089616060256958}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  5.76it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.48it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.00it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 17.02it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.68it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.68it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.66it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.66it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.65it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.65it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.66it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.67it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:23:11,729][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0238
[2025-04-29 15:23:12,110][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0792, Metrics: {'mse': 0.07904770225286484, 'rmse': 0.2811542321446804, 'r2': -0.2057206630706787}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁▁▁▁▁
wandb:     best_val_mse █▅▃▂▁▁▁▁▁▁
wandb:      best_val_r2 ▁▄▆▇██████
wandb:    best_val_rmse █▅▃▂▁▁▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁▁
wandb:          val_mse █▅▃▂▁▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇██████
wandb:         val_rmse █▅▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07918
wandb:     best_val_mse 0.07905
wandb:      best_val_r2 -0.20572
wandb:    best_val_rmse 0.28115
wandb:            epoch 10
wandb:   final_test_mse 0.03919
wandb:    final_test_r2 0.00745
wandb:  final_test_rmse 0.19797
wandb:  final_train_mse 0.0207
wandb:   final_train_r2 -0.02413
wandb: final_train_rmse 0.14386
wandb:    final_val_mse 0.07905
wandb:     final_val_r2 -0.20572
wandb:   final_val_rmse 0.28115
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02385
wandb:       train_time 54.32088
wandb:         val_loss 0.07918
wandb:          val_mse 0.07905
wandb:           val_r2 -0.20572
wandb:         val_rmse 0.28115
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152206-y85txqfd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152206-y85txqfd/logs
Standard experiment completed successfully: layer_9_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_9/complexity/results.json
Running question_type experiment for language fi, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:23:31,606][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_10/question_type
experiment_name: layer_10_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:23:31,606][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:23:31,606][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:23:31,606][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:23:31,610][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:23:31,611][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:23:32,922][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:23:35,742][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:23:35,743][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:23:35,795][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:35,818][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:35,896][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:23:35,907][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:23:35,908][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:23:35,909][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:23:35,928][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:35,960][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:35,970][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:23:35,971][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:23:35,971][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:23:35,972][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:23:35,986][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:36,009][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:23:36,018][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:23:36,019][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:23:36,020][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:23:36,020][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:23:36,021][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:23:36,021][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:23:36,021][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:23:36,021][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:23:36,021][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:23:36,021][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:23:36,022][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:23:36,022][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:23:36,022][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:23:36,023][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:23:36,023][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:23:36,023][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:23:36,024][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:23:36,024][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:23:36,024][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:23:36,024][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:23:36,024][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:23:40,032][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:23:40,033][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:23:40,035][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:23:40,035][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 15:23:40,035][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:11,  1.03it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:21,  3.40it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:12,  5.82it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.14it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.23it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.01it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.49it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.62it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.49it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.14it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.62it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.97it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.22it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.40it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.62it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.82it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.50it/s]
[2025-04-29 15:23:47,039][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6933
[2025-04-29 15:23:47,394][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6690, Metrics: {'accuracy': 0.6825396825396826, 'f1': 0.5}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.89it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.63it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.13it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.48it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.11it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.49it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.58it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.79it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.80it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.81it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:23:52,330][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6803
[2025-04-29 15:23:52,704][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6407, Metrics: {'accuracy': 0.7936507936507936, 'f1': 0.723404255319149}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.55it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.28it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.91it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.33it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.16it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.71it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.04it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.63it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.13it/s]
[2025-04-29 15:23:57,845][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6625
[2025-04-29 15:23:58,221][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6140, Metrics: {'accuracy': 0.873015873015873, 'f1': 0.8620689655172413}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.08it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.74it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:24:03,173][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6500
[2025-04-29 15:24:03,551][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5877, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9180327868852459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.48it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.21it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.84it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.24it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.66it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.67it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:24:08,527][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6311
[2025-04-29 15:24:08,912][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5599, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9180327868852459}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.64it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:24:13,862][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6239
[2025-04-29 15:24:14,249][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5353, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.9}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.04it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:24:19,236][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6086
[2025-04-29 15:24:19,624][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5130, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.9}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.83it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.57it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.08it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.19it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.67it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.67it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.66it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.65it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.66it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.65it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.64it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.65it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.66it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.64it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]
[2025-04-29 15:24:24,577][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5978
[2025-04-29 15:24:24,966][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4913, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.9}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.21it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 10.90it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.09it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.53it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.87it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.11it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.28it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.47it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.61it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.57it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.60it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.61it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.62it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.92it/s]
[2025-04-29 15:24:29,979][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5848
[2025-04-29 15:24:30,368][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4730, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8813559322033898}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.02it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.66it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.11it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.86it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.09it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.26it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.37it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.44it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.49it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.53it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.59it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.65it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.64it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.65it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.66it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.91it/s]
[2025-04-29 15:24:35,375][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5668
[2025-04-29 15:24:35,763][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4504, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9180327868852459}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▄▇█████▇█
wandb:          best_val_f1 ▁▅▇█████▇█
wandb:        best_val_loss █▇▆▅▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▆▅▄▃▃▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▄▇█████▇█
wandb:               val_f1 ▁▅▇█████▇█
wandb:             val_loss █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.92063
wandb:          best_val_f1 0.91803
wandb:        best_val_loss 0.45041
wandb:                epoch 10
wandb:  final_test_accuracy 0.76364
wandb:        final_test_f1 0.80303
wandb: final_train_accuracy 0.92385
wandb:       final_train_f1 0.92702
wandb:   final_val_accuracy 0.92063
wandb:         final_val_f1 0.91803
wandb:        learning_rate 1e-05
wandb:           train_loss 0.56677
wandb:           train_time 54.48387
wandb:         val_accuracy 0.92063
wandb:               val_f1 0.91803
wandb:             val_loss 0.45041
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152331-wo0v0aiw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152331-wo0v0aiw/logs
Standard experiment completed successfully: layer_10_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_10/question_type/results.json
Running complexity experiment for language fi, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:24:55,058][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_10/complexity
experiment_name: layer_10_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:24:55,058][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:24:55,058][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:24:55,058][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:24:55,063][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:24:55,063][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:24:56,290][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:24:59,149][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:24:59,149][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:24:59,255][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,274][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,340][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:24:59,353][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:24:59,354][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:24:59,355][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:24:59,370][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,392][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,401][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:24:59,403][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:24:59,403][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:24:59,404][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:24:59,418][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,439][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:24:59,449][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:24:59,450][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:24:59,451][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:24:59,451][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:24:59,452][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:24:59,452][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:24:59,452][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:24:59,452][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:24:59,452][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:24:59,453][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:24:59,453][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:24:59,453][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:24:59,454][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:24:59,454][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:24:59,454][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:24:59,455][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:24:59,455][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:24:59,455][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:24:59,455][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:24:59,455][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:24:59,455][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:24:59,456][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:25:03,336][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:25:03,337][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:25:03,339][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:25:03,340][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 15:25:03,340][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:54,  1.35it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:16,  4.27it/s]Epoch 1/10:   7%|▋         | 5/75 [00:00<00:09,  7.01it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.42it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.45it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 13.06it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.34it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.30it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 16.02it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.54it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.91it/s]Epoch 1/10:  31%|███       | 23/75 [00:01<00:03, 17.15it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.32it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.61it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.74it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.18it/s]
[2025-04-29 15:25:10,179][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1283
[2025-04-29 15:25:10,530][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0920, Metrics: {'mse': 0.09188202023506165, 'rmse': 0.3031204714879245, 'r2': -0.401483416557312}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.29it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:05, 12.04it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.45it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.39it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.85it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.32it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:25:15,464][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0550
[2025-04-29 15:25:15,822][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0776, Metrics: {'mse': 0.07740585505962372, 'rmse': 0.2782190774544832, 'r2': -0.1806774139404297}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.18it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.73it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.68it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.66it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.67it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:25:21,022][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0415
[2025-04-29 15:25:21,408][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0765, Metrics: {'mse': 0.07639522105455399, 'rmse': 0.2763968542776021, 'r2': -0.16526222229003906}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.19it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:25:26,354][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0341
[2025-04-29 15:25:26,736][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0765, Metrics: {'mse': 0.07637642323970795, 'rmse': 0.2763628470683206, 'r2': -0.16497540473937988}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.36it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.08it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.00it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.68it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.97it/s]
[2025-04-29 15:25:31,743][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0295
[2025-04-29 15:25:32,136][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0748, Metrics: {'mse': 0.07473448663949966, 'rmse': 0.27337609010207836, 'r2': -0.13993072509765625}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.82it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.54it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.95it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.52it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.42it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.51it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.56it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:25:37,092][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0295
[2025-04-29 15:25:37,477][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0733, Metrics: {'mse': 0.07319244742393494, 'rmse': 0.2705410272471348, 'r2': -0.11640989780426025}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.13it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.81it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.49it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.01it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.93it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.46it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.53it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.56it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.61it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.68it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 16.98it/s]
[2025-04-29 15:25:42,488][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0262
[2025-04-29 15:25:42,873][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0713, Metrics: {'mse': 0.07118797302246094, 'rmse': 0.2668107438287689, 'r2': -0.08583557605743408}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 10.95it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.65it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.13it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.02it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.55it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.67it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.66it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.64it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.64it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.65it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:25:47,843][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0262
[2025-04-29 15:25:48,226][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0682, Metrics: {'mse': 0.06809933483600616, 'rmse': 0.26095849255390435, 'r2': -0.038724303245544434}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.36it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.07it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.18it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.03it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.57it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.49it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.52it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.56it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.61it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.63it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.62it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.64it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.63it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.91it/s]
[2025-04-29 15:25:53,284][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0263
[2025-04-29 15:25:53,669][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0667, Metrics: {'mse': 0.06667406111955643, 'rmse': 0.2582132086465687, 'r2': -0.01698446273803711}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.65it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.37it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.94it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.29it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.14it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.39it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.50it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.64it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.66it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.66it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.67it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.97it/s]
[2025-04-29 15:25:58,645][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0243
[2025-04-29 15:25:59,034][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0682, Metrics: {'mse': 0.06816721707582474, 'rmse': 0.2610885234471725, 'r2': -0.03975975513458252}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▄▃▃▂▁▁
wandb:     best_val_mse █▄▄▄▃▃▂▁▁
wandb:      best_val_r2 ▁▅▅▅▆▆▇██
wandb:    best_val_rmse █▄▄▄▃▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▄▃▃▂▁▁▁
wandb:          val_mse █▄▄▄▃▃▂▁▁▁
wandb:           val_r2 ▁▅▅▅▆▆▇███
wandb:         val_rmse █▄▄▄▃▃▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06672
wandb:     best_val_mse 0.06667
wandb:      best_val_r2 -0.01698
wandb:    best_val_rmse 0.25821
wandb:            epoch 10
wandb:   final_test_mse 0.02927
wandb:    final_test_r2 0.25886
wandb:  final_test_rmse 0.17107
wandb:  final_train_mse 0.01498
wandb:   final_train_r2 0.25851
wandb: final_train_rmse 0.12241
wandb:    final_val_mse 0.06667
wandb:     final_val_r2 -0.01698
wandb:   final_val_rmse 0.25821
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02428
wandb:       train_time 53.79997
wandb:         val_loss 0.06818
wandb:          val_mse 0.06817
wandb:           val_r2 -0.03976
wandb:         val_rmse 0.26109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152455-1chehpof
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152455-1chehpof/logs
Standard experiment completed successfully: layer_10_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_10/complexity/results.json
Running question_type experiment for language fi, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:26:18,621][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_11/question_type
experiment_name: layer_11_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:26:18,622][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:26:18,622][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:26:18,622][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:26:18,626][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:26:18,626][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:26:19,890][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:26:22,598][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:26:22,599][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:26:22,658][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,694][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,796][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:26:22,808][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:26:22,808][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:26:22,810][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:26:22,835][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,863][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,876][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:26:22,877][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:26:22,877][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:26:22,878][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:26:22,900][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,929][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:26:22,942][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:26:22,944][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:26:22,944][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:26:22,945][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:26:22,945][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:26:22,946][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:26:22,946][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:26:22,946][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:26:22,947][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:26:22,947][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:26:22,947][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:26:22,948][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:26:22,948][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:26:22,948][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:26:22,949][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:26:22,949][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:26:26,877][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:26:26,878][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:26:26,880][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:26:26,880][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 15:26:26,880][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.04it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.69it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.10it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.15it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.82it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.14it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.14it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.90it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.11it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.30it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.46it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.08it/s]
[2025-04-29 15:26:33,685][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6920
[2025-04-29 15:26:34,028][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6657, Metrics: {'accuracy': 0.8412698412698413, 'f1': 0.8148148148148148}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.76it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.51it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.44it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.76it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 15:26:38,961][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6760
[2025-04-29 15:26:39,318][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6311, Metrics: {'accuracy': 0.8571428571428571, 'f1': 0.8363636363636363}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.37it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.06it/s]
[2025-04-29 15:26:44,476][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6567
[2025-04-29 15:26:44,863][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5974, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.73it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.16it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.70it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:26:49,819][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6404
[2025-04-29 15:26:50,201][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5639, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.77it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 15:26:55,178][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6152
[2025-04-29 15:26:55,565][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5285, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.19it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.81it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.60it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.65it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:27:00,527][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6067
[2025-04-29 15:27:00,915][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4978, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.42it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.14it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.20it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.94it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]
[2025-04-29 15:27:05,908][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5881
[2025-04-29 15:27:06,300][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.4696, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.81it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.53it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.07it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.41it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.20it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.70it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.14it/s]
[2025-04-29 15:27:11,247][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5718
[2025-04-29 15:27:11,634][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4434, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.77it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.52it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:04, 14.04it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.39it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.18it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.68it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.58it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.64it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.68it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.66it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.64it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.64it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.64it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.66it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.67it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:27:16,647][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5586
[2025-04-29 15:27:17,031][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4226, Metrics: {'accuracy': 0.9206349206349206, 'f1': 0.9152542372881356}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.13it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.78it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.59it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.32it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.41it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.48it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.53it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.60it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.60it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.65it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.65it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.66it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.94it/s]
[2025-04-29 15:27:22,035][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5391
[2025-04-29 15:27:22,434][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.3972, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▂▇▇▇▇▇▇▇█
wandb:          best_val_f1 ▁▂▇▇▇▇▇▇▇█
wandb:        best_val_loss █▇▆▅▄▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▆▄▄▃▂▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▂▇▇▇▇▇▇▇█
wandb:               val_f1 ▁▂▇▇▇▇▇▇▇█
wandb:             val_loss █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.93651
wandb:          best_val_f1 0.93333
wandb:        best_val_loss 0.39724
wandb:                epoch 10
wandb:  final_test_accuracy 0.80909
wandb:        final_test_f1 0.83465
wandb: final_train_accuracy 0.95146
wandb:       final_train_f1 0.95261
wandb:   final_val_accuracy 0.93651
wandb:         final_val_f1 0.93333
wandb:        learning_rate 1e-05
wandb:           train_loss 0.53909
wandb:           train_time 54.30844
wandb:         val_accuracy 0.93651
wandb:               val_f1 0.93333
wandb:             val_loss 0.39724
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152618-ydes4nf1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152618-ydes4nf1/logs
Standard experiment completed successfully: layer_11_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_11/question_type/results.json
Running complexity experiment for language fi, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:27:42,509][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_11/complexity
experiment_name: layer_11_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:27:42,509][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:27:42,509][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:27:42,509][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:27:42,514][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:27:42,514][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:27:43,737][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:27:46,475][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:27:46,476][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:27:46,509][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,532][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,605][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:27:46,617][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:27:46,618][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:27:46,619][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:27:46,633][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,656][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,665][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:27:46,667][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:27:46,667][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:27:46,668][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:27:46,682][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,702][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:27:46,711][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:27:46,712][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:27:46,713][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:27:46,713][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:27:46,714][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:27:46,714][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:27:46,714][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:27:46,714][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:27:46,714][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:27:46,715][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:27:46,715][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:27:46,715][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:27:46,716][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:27:46,716][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:27:46,716][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:27:46,717][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:27:46,717][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:27:46,717][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:27:46,717][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:27:46,717][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:27:46,717][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:27:46,718][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:27:50,536][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:27:50,537][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:27:50,539][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:27:50,539][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 15:27:50,539][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:07,  1.09it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.57it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.07it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.42it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.51it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.27it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.69it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.80it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.63it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.26it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.69it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.53it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.66it/s]
[2025-04-29 15:27:57,438][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1039
[2025-04-29 15:27:57,783][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0802, Metrics: {'mse': 0.07994470745325089, 'rmse': 0.2827449512427249, 'r2': -0.21940279006958008}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:11,  6.21it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.95it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.39it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.40it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.85it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.37it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.19it/s]
[2025-04-29 15:28:02,718][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0562
[2025-04-29 15:28:03,067][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0745, Metrics: {'mse': 0.07423467934131622, 'rmse': 0.2724604179350025, 'r2': -0.1323072910308838}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.02it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.17it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.06it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:28:08,242][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0441
[2025-04-29 15:28:08,637][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0729, Metrics: {'mse': 0.07270931452512741, 'rmse': 0.2696466475317789, 'r2': -0.10904073715209961}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.04it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.18it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.07it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.00it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.61it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.77it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.78it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:28:13,577][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0382
[2025-04-29 15:28:13,957][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0725, Metrics: {'mse': 0.07230160385370255, 'rmse': 0.2688895755764856, 'r2': -0.102821946144104}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:20,  3.57it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.74it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 11.83it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 13.72it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 14.99it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:04, 15.85it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.41it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:03, 16.81it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.09it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.42it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.58it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.62it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.70it/s]
[2025-04-29 15:28:19,035][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0342
[2025-04-29 15:28:19,396][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0717, Metrics: {'mse': 0.07159888744354248, 'rmse': 0.2675796842877696, 'r2': -0.09210324287414551}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 10.99it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.09it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.91it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.16it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.34it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.67it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.67it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.65it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.64it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.64it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 16.97it/s]
[2025-04-29 15:28:24,372][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0319
[2025-04-29 15:28:24,760][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0690, Metrics: {'mse': 0.06887437403202057, 'rmse': 0.26243927684708435, 'r2': -0.05054605007171631}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.98it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.64it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.13it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.38it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.57it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:02<00:02, 17.60it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.60it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.61it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.61it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.61it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.64it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.63it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.62it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 16.96it/s]
[2025-04-29 15:28:29,767][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0266
[2025-04-29 15:28:30,150][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0662, Metrics: {'mse': 0.06615643203258514, 'rmse': 0.25720892681356367, 'r2': -0.009089112281799316}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.03it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.67it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.54it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.12it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.28it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.38it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.51it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.60it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:02<00:02, 17.59it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.61it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 16.92it/s]
[2025-04-29 15:28:35,140][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0246
[2025-04-29 15:28:35,520][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0652, Metrics: {'mse': 0.06513809412717819, 'rmse': 0.2552216568537596, 'r2': 0.006443619728088379}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 10.96it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.63it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.10it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 15.97it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 16.86it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.11it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.37it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.47it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.51it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.54it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.57it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:02<00:02, 17.60it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.60it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.61it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.61it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.60it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.61it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.60it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.60it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.61it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.62it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 16.93it/s]
[2025-04-29 15:28:40,573][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0252
[2025-04-29 15:28:40,968][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0622, Metrics: {'mse': 0.06220975145697594, 'rmse': 0.2494188273907484, 'r2': 0.051109910011291504}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 10.97it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.60it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 15.91it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.47it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.83it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.08it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.24it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.36it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.44it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.49it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.52it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.56it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.58it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.59it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:02<00:02, 17.60it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.61it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.62it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.62it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.63it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 16.88it/s]
[2025-04-29 15:28:45,969][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0229
[2025-04-29 15:28:46,363][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0642, Metrics: {'mse': 0.06421306729316711, 'rmse': 0.25340297412060325, 'r2': 0.020553231239318848}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▅▅▄▃▂▁
wandb:     best_val_mse █▆▅▅▅▄▃▂▁
wandb:      best_val_r2 ▁▃▄▄▄▅▆▇█
wandb:    best_val_rmse █▆▅▅▅▄▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▅▅▄▃▂▁▂
wandb:          val_mse █▆▅▅▅▄▃▂▁▂
wandb:           val_r2 ▁▃▄▄▄▅▆▇█▇
wandb:         val_rmse █▆▅▅▅▄▃▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06224
wandb:     best_val_mse 0.06221
wandb:      best_val_r2 0.05111
wandb:    best_val_rmse 0.24942
wandb:            epoch 10
wandb:   final_test_mse 0.02979
wandb:    final_test_r2 0.24552
wandb:  final_test_rmse 0.1726
wandb:  final_train_mse 0.01504
wandb:   final_train_r2 0.25573
wandb: final_train_rmse 0.12264
wandb:    final_val_mse 0.06221
wandb:     final_val_r2 0.05111
wandb:   final_val_rmse 0.24942
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02287
wandb:       train_time 54.04367
wandb:         val_loss 0.06421
wandb:          val_mse 0.06421
wandb:           val_r2 0.02055
wandb:         val_rmse 0.2534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152742-emqot6tb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152742-emqot6tb/logs
Standard experiment completed successfully: layer_11_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_11/complexity/results.json
Running question_type experiment for language fi, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:29:06,363][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_12/question_type
experiment_name: layer_12_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:29:06,363][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:29:06,363][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:29:06,363][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:29:06,367][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-29 15:29:06,368][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:29:07,927][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:29:10,645][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:29:10,646][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:29:10,732][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:10,767][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:10,881][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:29:10,894][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:29:10,894][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:29:10,895][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:29:10,921][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:10,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:10,993][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:29:10,994][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:29:10,994][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:29:10,996][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:29:11,023][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:11,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:29:11,099][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:29:11,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:29:11,101][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:29:11,102][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:29:11,102][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:29:11,102][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:29:11,102][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:29:11,103][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:29:11,103][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-29 15:29:11,103][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-29 15:29:11,103][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:29:11,103][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:29:11,103][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:29:11,103][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:29:11,104][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-29 15:29:11,104][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:29:11,104][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:29:11,105][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:29:11,105][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:29:11,105][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:29:11,106][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:29:11,106][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:29:16,079][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:29:16,080][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:29:16,082][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:29:16,082][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 15:29:16,082][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:14,  1.01s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:21,  3.29it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:12,  5.67it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  7.96it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.06it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 11.87it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.34it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.51it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.42it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:03, 16.07it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.55it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.88it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.17it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.35it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.48it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.64it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.74it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.73it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.45it/s]
[2025-04-29 15:29:23,571][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6938
[2025-04-29 15:29:23,916][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.99it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.77it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.34it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.47it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.57it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.78it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.80it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.16it/s]
[2025-04-29 15:29:28,848][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6927
[2025-04-29 15:29:29,209][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6916, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.33it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.03it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.70it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.15it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.61it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.96it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.68it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.68it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:29:34,376][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6916
[2025-04-29 15:29:34,753][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6912, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.92it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.29it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.02it/s]
[2025-04-29 15:29:39,707][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6903
[2025-04-29 15:29:40,088][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6908, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.11it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.76it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.05it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.58it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.93it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.52it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.57it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.61it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:29:45,079][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6897
[2025-04-29 15:29:45,464][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6904, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.05it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.71it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.14it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.01it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.56it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.92it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.67it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:29:50,404][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6872
[2025-04-29 15:29:50,791][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6899, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 11.25it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.87it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.30it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 17.05it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.75it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.76it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.77it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.78it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:29:55,762][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6870
[2025-04-29 15:29:56,132][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6895, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.17it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:05, 13.79it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.23it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.09it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.62it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 16.97it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.19it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.46it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.62it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.63it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.68it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.70it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.69it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:30:01,083][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6863
[2025-04-29 15:30:01,478][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6891, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.84it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.58it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.23it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.72it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.73it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 15:30:06,454][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6854
[2025-04-29 15:30:06,841][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6887, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:12,  5.89it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.62it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:04, 14.14it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.23it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.73it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.30it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.40it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.45it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.50it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.54it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.53it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.55it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.57it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.57it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.61it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.62it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.64it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.66it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.68it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.70it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.68it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.68it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.65it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.65it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.65it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.65it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:30:11,795][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6846
[2025-04-29 15:30:12,167][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6884, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:        best_val_loss █▇▆▆▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▅▃▃▂▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▇▆▆▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.52381
wandb:          best_val_f1 0
wandb:        best_val_loss 0.68837
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.49958
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.52381
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68457
wandb:           train_time 54.36938
wandb:         val_accuracy 0.52381
wandb:               val_f1 0
wandb:             val_loss 0.68837
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152906-zrpb7f8z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_152906-zrpb7f8z/logs
Standard experiment completed successfully: layer_12_question_type_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_12/question_type/results.json
Running complexity experiment for language fi, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:30:32,037][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_12/complexity
experiment_name: layer_12_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:30:32,037][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:30:32,037][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:30:32,038][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:30:32,042][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-04-29 15:30:32,042][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:30:33,337][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:30:36,185][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:30:36,186][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:30:36,220][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,240][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,315][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-29 15:30:36,327][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:30:36,328][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-29 15:30:36,329][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:30:36,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,363][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,373][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-29 15:30:36,374][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:30:36,374][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-29 15:30:36,375][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:30:36,388][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:30:36,424][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-29 15:30:36,426][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:30:36,426][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-29 15:30:36,427][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-29 15:30:36,427][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:30:36,428][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:30:36,428][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:30:36,428][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:30:36,428][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:30:36,428][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-29 15:30:36,428][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-29 15:30:36,428][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-29 15:30:36,429][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:30:36,429][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:30:36,429][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:30:36,429][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:30:36,429][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:30:36,429][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:30:36,430][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:30:36,430][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:30:36,430][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-29 15:30:36,431][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-29 15:30:36,431][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-29 15:30:36,431][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-29 15:30:36,431][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:30:36,431][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:30:36,432][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:30:40,296][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:30:40,297][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:30:40,299][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:30:40,300][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 15:30:40,300][__main__][INFO] - Successfully created model for fi
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:08,  1.09it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.56it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  6.05it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.41it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.50it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.26it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.69it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.80it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.62it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.24it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.68it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.25it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.39it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.50it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.58it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.63it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.68it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.76it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.66it/s]
[2025-04-29 15:30:47,697][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1513
[2025-04-29 15:30:48,040][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.3136, Metrics: {'mse': 0.3141840100288391, 'rmse': 0.5605211949862726, 'r2': -3.7922725677490234}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.68it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.39it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:05, 13.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.36it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.21it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.71it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.07it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.32it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.48it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.59it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.66it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.77it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 15:30:53,012][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1323
[2025-04-29 15:30:53,377][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2861, Metrics: {'mse': 0.2866162061691284, 'rmse': 0.5353654883994003, 'r2': -3.371778964996338}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:01<01:26,  1.17s/it]Epoch 3/10:   4%|▍         | 3/75 [00:01<00:24,  2.90it/s]Epoch 3/10:   7%|▋         | 5/75 [00:01<00:13,  5.09it/s]Epoch 3/10:   9%|▉         | 7/75 [00:01<00:09,  7.30it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:07,  9.38it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:05, 11.25it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:04, 12.79it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:04, 14.05it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:02<00:03, 15.05it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:02<00:03, 15.80it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:03, 16.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:03, 16.75it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:02, 17.05it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:02, 17.25it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:02, 17.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:02<00:02, 17.49it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.56it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:02, 17.60it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:02, 17.64it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.67it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  60%|██████    | 45/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.78it/s]Epoch 3/10:  71%|███████   | 53/75 [00:04<00:01, 17.78it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:04<00:01, 17.78it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.78it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:05<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:05<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:05<00:00, 13.95it/s]
[2025-04-29 15:30:59,575][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1182
[2025-04-29 15:30:59,960][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2626, Metrics: {'mse': 0.26310235261917114, 'rmse': 0.5129350374259601, 'r2': -3.013120651245117}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  5.80it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.55it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.09it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.26it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.75it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.06it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.76it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.74it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.74it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.09it/s]
[2025-04-29 15:31:04,903][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1055
[2025-04-29 15:31:05,293][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2403, Metrics: {'mse': 0.24079133570194244, 'rmse': 0.4907049375153489, 'r2': -2.6728086471557617}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:12,  5.84it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 11.54it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:04, 14.08it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.45it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 16.24it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.74it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.27it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.65it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.69it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 17.10it/s]
[2025-04-29 15:31:10,318][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0920
[2025-04-29 15:31:10,704][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2200, Metrics: {'mse': 0.22036181390285492, 'rmse': 0.4694271124496911, 'r2': -2.3611955642700195}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:06, 11.09it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:05, 13.75it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:04, 15.22it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:04, 16.08it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 6/10:  20%|██        | 15/75 [00:00<00:03, 17.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:03, 17.38it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 6/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.69it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 75/75 [00:04<00:00, 17.03it/s]
[2025-04-29 15:31:15,670][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0820
[2025-04-29 15:31:16,055][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2029, Metrics: {'mse': 0.20322437584400177, 'rmse': 0.4508041435523877, 'r2': -2.099797010421753}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.23it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:06, 10.91it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:05, 13.58it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:04, 15.08it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:04, 15.95it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:00<00:03, 16.88it/s]Epoch 7/10:  20%|██        | 15/75 [00:00<00:03, 17.15it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:03, 17.33it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 7/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  60%|██████    | 45/75 [00:02<00:01, 17.69it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  71%|███████   | 53/75 [00:03<00:01, 17.69it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 15:31:21,051][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0715
[2025-04-29 15:31:21,434][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1865, Metrics: {'mse': 0.18679721653461456, 'rmse': 0.4322004356020648, 'r2': -1.8492321968078613}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:06, 11.53it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:04, 14.07it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:04, 15.43it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:04, 16.19it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:00<00:03, 16.69it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:00<00:03, 17.03it/s]Epoch 8/10:  20%|██        | 15/75 [00:00<00:03, 17.25it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:03, 17.40it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:03, 17.50it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:01<00:03, 17.57it/s]Epoch 8/10:  31%|███       | 23/75 [00:01<00:02, 17.61it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:01<00:02, 17.65it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]
[2025-04-29 15:31:26,381][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0624
[2025-04-29 15:31:26,773][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1727, Metrics: {'mse': 0.1730138510465622, 'rmse': 0.41594933711518545, 'r2': -1.638993501663208}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:12,  5.71it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:06, 11.43it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:05, 13.97it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:04, 15.33it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:04, 16.15it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:00<00:03, 16.67it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:00<00:03, 17.01it/s]Epoch 9/10:  20%|██        | 15/75 [00:00<00:03, 17.23it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:03, 17.39it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:03, 17.49it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:01<00:03, 17.54it/s]Epoch 9/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:01<00:02, 17.70it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 9/10:  71%|███████   | 53/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.73it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.74it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.75it/s]Epoch 9/10: 100%|██████████| 75/75 [00:04<00:00, 17.04it/s]
[2025-04-29 15:31:31,765][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0546
[2025-04-29 15:31:32,163][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1601, Metrics: {'mse': 0.1603766679763794, 'rmse': 0.40047055818921246, 'r2': -1.446237564086914}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:13,  5.54it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:06, 11.25it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:05, 13.85it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:04, 16.11it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 10/10:  20%|██        | 15/75 [00:00<00:03, 17.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:03, 17.47it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 10/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:01<00:02, 17.67it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:02<00:02, 17.66it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:02<00:02, 17.66it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.65it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.64it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  60%|██████    | 45/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  71%|███████   | 53/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.63it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.64it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.63it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.62it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 15:31:37,133][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0493
[2025-04-29 15:31:37,522][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1487, Metrics: {'mse': 0.1489061415195465, 'rmse': 0.3858835854497396, 'r2': -1.2712769508361816}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▄▃▃▂▁▁
wandb:     best_val_mse █▇▆▅▄▃▃▂▁▁
wandb:      best_val_r2 ▁▂▃▄▅▆▆▇██
wandb:    best_val_rmse █▇▆▅▄▄▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▅▄▃▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▅▄▃▃▂▁▁
wandb:          val_mse █▇▆▅▄▃▃▂▁▁
wandb:           val_r2 ▁▂▃▄▅▆▆▇██
wandb:         val_rmse █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.14869
wandb:     best_val_mse 0.14891
wandb:      best_val_r2 -1.27128
wandb:    best_val_rmse 0.38588
wandb:            epoch 10
wandb:   final_test_mse 0.06919
wandb:    final_test_r2 -0.75234
wandb:  final_test_rmse 0.26305
wandb:  final_train_mse 0.04216
wandb:   final_train_r2 -1.08619
wandb: final_train_rmse 0.20532
wandb:    final_val_mse 0.14891
wandb:     final_val_r2 -1.27128
wandb:   final_val_rmse 0.38588
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04928
wandb:       train_time 55.53131
wandb:         val_loss 0.14869
wandb:          val_mse 0.14891
wandb:           val_r2 -1.27128
wandb:         val_rmse 0.38588
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153032-2kevqnbe
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153032-2kevqnbe/logs
Standard experiment completed successfully: layer_12_complexity_fi
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/fi/layer_12/complexity/results.json
Running question_type experiment for language id, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:31:56,920][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_1/question_type
experiment_name: layer_1_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:31:56,920][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:31:56,920][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:31:56,920][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:31:56,925][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:31:56,925][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:31:58,231][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:32:00,953][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:32:00,953][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:00,992][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,013][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,085][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:32:01,094][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:01,095][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:32:01,096][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:01,108][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,128][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,139][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:32:01,141][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:01,141][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:32:01,142][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:01,158][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,196][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:01,207][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:32:01,208][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:01,208][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:32:01,209][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:32:01,209][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:32:01,210][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:32:01,210][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:32:01,210][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:32:01,210][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:32:01,210][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:32:01,210][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:32:01,210][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:32:01,211][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:32:01,211][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:32:01,211][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:32:01,212][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:32:01,212][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:32:01,212][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:32:01,213][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:32:01,213][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:32:05,163][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:32:05,164][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:32:05,166][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:32:05,166][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 15:32:05,166][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:55,  1.06it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:16,  3.46it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:09,  5.92it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.25it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.35it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:04, 12.13it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.56it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.69it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.54it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.16it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.64it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 16.94it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.17it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.35it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.48it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.56it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.63it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.67it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.71it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.69it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.70it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.72it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.78it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 13.95it/s]
[2025-04-29 15:32:11,229][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6945
[2025-04-29 15:32:11,616][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:10,  5.87it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.64it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.16it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.49it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.29it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.77it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.07it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.28it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.03it/s]
[2025-04-29 15:32:15,702][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6951
[2025-04-29 15:32:16,752][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:47,  1.24it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:14,  3.99it/s]Epoch 3/10:   8%|▊         | 5/60 [00:01<00:08,  6.64it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:01<00:05,  9.04it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:04, 11.08it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:03, 12.76it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:03, 14.09it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:02, 15.09it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 15.82it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 16.36it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 16.75it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:02, 17.03it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:02, 17.26it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:01, 17.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:01, 17.50it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.58it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.64it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.67it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.67it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.69it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.69it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.70it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.71it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 60/60 [00:04<00:00, 14.34it/s]
[2025-04-29 15:32:20,938][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6930
[2025-04-29 15:32:22,268][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:53,  1.09it/s]Epoch 4/10:   5%|▌         | 3/60 [00:01<00:15,  3.59it/s]Epoch 4/10:   8%|▊         | 5/60 [00:01<00:09,  6.08it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:01<00:06,  8.43it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:04, 10.52it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:03, 12.27it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:03, 13.68it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:03, 14.76it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 15.61it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 16.21it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:02, 16.64it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:02, 16.97it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:02, 17.18it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:01, 17.33it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:01, 17.45it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.55it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.60it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.64it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.65it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.67it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.68it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.69it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.70it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.69it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.70it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.70it/s]Epoch 4/10: 100%|██████████| 60/60 [00:04<00:00, 13.97it/s]
[2025-04-29 15:32:26,564][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 15:32:27,741][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:32:27,742][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▆█▁▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▇▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69348
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69396
wandb:           train_time 20.81632
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69386
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153156-epnlnbcj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153156-epnlnbcj/logs
Standard experiment completed successfully: layer_1_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_1/question_type/results.json
Running complexity experiment for language id, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:32:45,290][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_1/complexity
experiment_name: layer_1_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:32:45,291][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:32:45,291][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:32:45,291][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:32:45,295][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:32:45,296][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:32:46,324][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:32:49,041][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:32:49,041][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:49,064][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,083][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,135][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:32:49,144][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:49,145][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:32:49,145][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:49,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,176][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,186][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:32:49,187][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:49,188][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:32:49,188][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:32:49,201][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,221][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:32:49,229][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:32:49,231][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:32:49,231][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:32:49,232][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:32:49,232][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:32:49,232][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:32:49,232][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:32:49,233][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:32:49,233][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:32:49,233][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:32:49,233][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:32:49,233][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:32:49,233][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:32:49,234][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:32:49,234][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:32:49,234][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:32:49,235][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:32:49,235][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:32:49,235][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:32:49,236][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:32:49,236][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:32:52,749][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:32:52,750][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:32:52,752][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:32:52,753][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 15:32:52,753][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:50,  1.17it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:15,  3.79it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.36it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.73it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.81it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.53it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.90it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.95it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.74it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.33it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.77it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.07it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.29it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.42it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.51it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.58it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.75it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.73it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.73it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.29it/s]
[2025-04-29 15:32:58,598][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1456
[2025-04-29 15:32:58,981][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1432, Metrics: {'mse': 0.1397121697664261, 'rmse': 0.37378091145271997, 'r2': -2.3416643142700195}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  5.90it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.67it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.18it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.55it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.30it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.79it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.11it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.33it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.48it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.63it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.67it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.81it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.75it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.05it/s]
[2025-04-29 15:33:03,061][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0673
[2025-04-29 15:33:03,465][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0792, Metrics: {'mse': 0.07720273733139038, 'rmse': 0.27785380568095586, 'r2': -0.8465508222579956}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.86it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 10.49it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:04, 13.31it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 14.90it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 15.85it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.49it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 16.91it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.76it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.79it/s]
[2025-04-29 15:33:08,059][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0499
[2025-04-29 15:33:08,503][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0611, Metrics: {'mse': 0.05968192592263222, 'rmse': 0.2442988455204654, 'r2': -0.42748451232910156}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:11,  5.35it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.03it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:04, 13.71it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.17it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.07it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.58it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.32it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.44it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:33:12,612][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0484
[2025-04-29 15:33:13,035][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0572, Metrics: {'mse': 0.056017305701971054, 'rmse': 0.2366797534686291, 'r2': -0.3398333787918091}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.26it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 10.96it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:04, 13.65it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.14it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.03it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.60it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.83it/s]
[2025-04-29 15:33:17,193][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0496
[2025-04-29 15:33:17,621][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0570, Metrics: {'mse': 0.05581990256905556, 'rmse': 0.23626235961120756, 'r2': -0.3351118564605713}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:11,  5.28it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 10.98it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:04, 13.66it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.15it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.03it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.59it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.82it/s]
[2025-04-29 15:33:21,735][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0466
[2025-04-29 15:33:22,168][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0567, Metrics: {'mse': 0.05547322332859039, 'rmse': 0.23552754261145423, 'r2': -0.32681989669799805}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:11,  5.20it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 10.90it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:04, 13.60it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.09it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.00it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.58it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.55it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:33:26,318][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0465
[2025-04-29 15:33:26,753][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0553, Metrics: {'mse': 0.05422293394804001, 'rmse': 0.23285818419810803, 'r2': -0.29691529273986816}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.46it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.15it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.79it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.22it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.45it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.66it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.64it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.66it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.77it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.78it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 17.00it/s]
[2025-04-29 15:33:30,828][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0469
[2025-04-29 15:33:31,235][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0548, Metrics: {'mse': 0.053714051842689514, 'rmse': 0.23176292163046597, 'r2': -0.28474366664886475}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.40it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.12it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 13.77it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.23it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.38it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.65it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.73it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.88it/s]
[2025-04-29 15:33:35,381][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0488
[2025-04-29 15:33:35,820][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0575, Metrics: {'mse': 0.05623681843280792, 'rmse': 0.23714303370077713, 'r2': -0.34508371353149414}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.42it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.12it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 13.77it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.20it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.06it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.96it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.82it/s]
[2025-04-29 15:33:39,390][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0489
[2025-04-29 15:33:39,830][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0550, Metrics: {'mse': 0.053945500403642654, 'rmse': 0.23226170670957072, 'r2': -0.2902795076370239}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁▁▁▁
wandb:     best_val_mse █▃▁▁▁▁▁▁
wandb:      best_val_r2 ▁▆██████
wandb:    best_val_rmse █▃▂▁▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▆████████
wandb:         val_rmse █▃▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05481
wandb:     best_val_mse 0.05371
wandb:      best_val_r2 -0.28474
wandb:    best_val_rmse 0.23176
wandb:            epoch 10
wandb:   final_test_mse 0.04081
wandb:    final_test_r2 -0.001
wandb:  final_test_rmse 0.20203
wandb:  final_train_mse 0.03625
wandb:   final_train_r2 0.00106
wandb: final_train_rmse 0.19039
wandb:    final_val_mse 0.05371
wandb:     final_val_r2 -0.28474
wandb:   final_val_rmse 0.23176
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04891
wandb:       train_time 45.43383
wandb:         val_loss 0.05505
wandb:          val_mse 0.05395
wandb:           val_r2 -0.29028
wandb:         val_rmse 0.23226
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153245-caea82eo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153245-caea82eo/logs
Standard experiment completed successfully: layer_1_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_1/complexity/results.json
Running question_type experiment for language id, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:33:57,271][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_2/question_type
experiment_name: layer_2_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:33:57,271][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:33:57,271][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:33:57,271][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:33:57,275][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:33:57,276][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:33:58,429][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:34:01,267][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:34:01,267][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:01,296][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,384][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:34:01,394][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:01,394][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:34:01,395][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:01,410][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,431][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,441][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:34:01,442][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:01,442][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:34:01,443][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:01,458][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:01,489][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:34:01,491][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:01,491][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:34:01,492][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:34:01,492][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:34:01,492][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:34:01,492][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:34:01,492][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:34:01,493][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:34:01,493][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:34:01,493][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:34:01,494][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:34:01,494][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:34:01,494][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:34:01,494][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:34:01,495][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:34:01,495][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:34:01,495][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:34:01,495][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:34:01,495][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:34:01,495][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:34:01,495][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:34:05,238][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:34:05,239][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:34:05,241][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:34:05,241][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 15:34:05,242][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:52,  1.12it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:15,  3.65it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.17it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.53it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.62it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.35it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.76it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.85it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.67it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.27it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.71it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.02it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.24it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.42it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.53it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.62it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.64it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.70it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.72it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.74it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.76it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.14it/s]
[2025-04-29 15:34:11,182][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6948
[2025-04-29 15:34:11,581][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.26it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 12.04it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.43it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.69it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.44it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.87it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.38it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.51it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.58it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.65it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.69it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.70it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.09it/s]
[2025-04-29 15:34:15,656][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6951
[2025-04-29 15:34:16,069][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  6.15it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.91it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.33it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.63it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.36it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.84it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.54it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.08it/s]
[2025-04-29 15:34:19,585][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6931
[2025-04-29 15:34:19,976][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:08,  6.63it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 12.34it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.64it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.80it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.47it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.88it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.15it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.54it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.61it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.10it/s]
[2025-04-29 15:34:23,487][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6942
[2025-04-29 15:34:23,880][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:34:23,881][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▇█▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69335
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69419
wandb:           train_time 16.94337
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69362
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153357-lqny7xf8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153357-lqny7xf8/logs
Standard experiment completed successfully: layer_2_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_2/question_type/results.json
Running complexity experiment for language id, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:34:40,117][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_2/complexity
experiment_name: layer_2_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:34:40,117][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:34:40,117][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:34:40,117][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:34:40,121][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:34:40,122][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:34:41,166][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:34:43,860][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:34:43,861][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:43,881][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:43,903][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:43,960][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:34:43,969][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:43,970][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:34:43,971][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:43,986][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:44,008][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:44,018][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:34:44,020][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:44,020][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:34:44,021][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:34:44,037][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:44,058][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:34:44,069][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:34:44,071][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:34:44,071][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:34:44,072][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:34:44,072][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:34:44,072][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:34:44,072][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:34:44,072][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:34:44,073][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:34:44,073][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:34:44,073][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:34:44,073][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:34:44,073][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:34:44,073][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:34:44,073][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:34:44,074][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:34:44,074][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:34:44,074][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:34:44,075][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:34:44,075][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:34:44,075][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:34:44,075][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:34:44,075][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:34:44,075][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:34:44,075][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:34:44,076][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:34:44,076][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:34:47,670][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:34:47,670][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:34:47,672][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:34:47,673][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 15:34:47,673][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:48,  1.21it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.88it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.46it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  8.85it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.92it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.65it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.99it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.03it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.80it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.37it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.79it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.10it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.28it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.43it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.63it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.78it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.83it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.83it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.83it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.84it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.85it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.35it/s]
[2025-04-29 15:34:53,494][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1631
[2025-04-29 15:34:53,873][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1963, Metrics: {'mse': 0.1919334977865219, 'rmse': 0.4381021545102488, 'r2': -3.590704917907715}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.55it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 12.30it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.66it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.82it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.50it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.94it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.21it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.42it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.51it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.59it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.66it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.71it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.81it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.80it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.82it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.82it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.11it/s]
[2025-04-29 15:34:57,941][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0950
[2025-04-29 15:34:58,472][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1266, Metrics: {'mse': 0.12347403913736343, 'rmse': 0.35138872938294907, 'r2': -1.953277349472046}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.06it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 10.75it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:04, 13.50it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.04it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 15.98it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.55it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 16.96it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.71it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.80it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.81it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.81it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.81it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.81it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.82it/s]
[2025-04-29 15:35:03,057][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0646
[2025-04-29 15:35:03,496][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0890, Metrics: {'mse': 0.08666502684354782, 'rmse': 0.29438924376333425, 'r2': -1.0728719234466553}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.41it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.12it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.78it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.23it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.09it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.65it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.00it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.73it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.76it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:35:07,606][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0538
[2025-04-29 15:35:08,029][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0711, Metrics: {'mse': 0.06939788162708282, 'rmse': 0.2634347767988935, 'r2': -0.6598726511001587}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.25it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 10.98it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:04, 13.66it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.15it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.04it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.23it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.73it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:35:12,165][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0514
[2025-04-29 15:35:12,589][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0633, Metrics: {'mse': 0.061837512999773026, 'rmse': 0.24867149615461162, 'r2': -0.47904229164123535}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:11,  5.36it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 11.05it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:04, 13.73it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.19it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.07it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.57it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.90it/s]
[2025-04-29 15:35:16,691][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0479
[2025-04-29 15:35:17,121][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0601, Metrics: {'mse': 0.05873990058898926, 'rmse': 0.24236315848121237, 'r2': -0.4049530029296875}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:11,  5.28it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 10.97it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:04, 13.64it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.12it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 15.99it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.56it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.33it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:35:21,256][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0472
[2025-04-29 15:35:21,675][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0579, Metrics: {'mse': 0.05668371915817261, 'rmse': 0.23808342898692594, 'r2': -0.3557727336883545}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:11,  5.18it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 10.87it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:04, 13.57it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.08it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 15.97it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.54it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.90it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.15it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.33it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.43it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.68it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:35:25,785][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0474
[2025-04-29 15:35:26,208][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0567, Metrics: {'mse': 0.05552133917808533, 'rmse': 0.2356296653184512, 'r2': -0.3279707431793213}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.65it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.38it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 13.94it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.33it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.12it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.99it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:35:30,350][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0506
[2025-04-29 15:35:30,783][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0577, Metrics: {'mse': 0.05650569498538971, 'rmse': 0.2377092656700401, 'r2': -0.35151469707489014}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:11,  5.25it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 10.95it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:04, 13.61it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.09it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 15.99it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.56it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.93it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.35it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.83it/s]
[2025-04-29 15:35:34,351][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0500
[2025-04-29 15:35:34,796][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0569, Metrics: {'mse': 0.0557408407330513, 'rmse': 0.23609498243938032, 'r2': -0.33322083950042725}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁▁▁
wandb:     best_val_mse █▄▃▂▁▁▁▁
wandb:      best_val_r2 ▁▅▆▇████
wandb:    best_val_rmse █▅▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁▁
wandb:          val_mse █▄▃▂▁▁▁▁▁▁
wandb:           val_r2 ▁▅▆▇██████
wandb:         val_rmse █▅▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05669
wandb:     best_val_mse 0.05552
wandb:      best_val_r2 -0.32797
wandb:    best_val_rmse 0.23563
wandb:            epoch 10
wandb:   final_test_mse 0.041
wandb:    final_test_r2 -0.0056
wandb:  final_test_rmse 0.20249
wandb:  final_train_mse 0.03631
wandb:   final_train_r2 -0.00071
wandb: final_train_rmse 0.19056
wandb:    final_val_mse 0.05552
wandb:     final_val_r2 -0.32797
wandb:   final_val_rmse 0.23563
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05002
wandb:       train_time 45.48668
wandb:         val_loss 0.05692
wandb:          val_mse 0.05574
wandb:           val_r2 -0.33322
wandb:         val_rmse 0.23609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153440-t38vrogc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153440-t38vrogc/logs
Standard experiment completed successfully: layer_2_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_2/complexity/results.json
Running question_type experiment for language id, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:35:52,272][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_3/question_type
experiment_name: layer_3_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:35:52,272][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:35:52,273][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:35:52,273][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:35:52,277][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:35:52,277][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:35:53,497][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:35:56,199][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:35:56,199][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:35:56,246][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,265][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,365][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:35:56,374][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:35:56,375][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:35:56,376][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:35:56,392][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,424][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:35:56,426][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:35:56,426][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:35:56,427][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:35:56,439][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,460][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:35:56,469][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:35:56,473][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:35:56,473][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:35:56,474][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:35:56,474][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:35:56,475][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:35:56,475][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:35:56,475][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:35:56,475][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:35:56,475][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:35:56,475][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:35:56,475][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:35:56,476][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:35:56,476][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:35:56,476][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:35:56,477][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:35:56,477][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:35:56,477][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:35:56,478][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:35:56,478][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:36:00,252][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:36:00,253][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:36:00,255][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:36:00,255][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 15:36:00,255][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:50,  1.16it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:15,  3.77it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.33it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.71it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.78it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.51it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.88it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.95it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.73it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.32it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.73it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.05it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.25it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.42it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.53it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.57it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.60it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.76it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.76it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.24it/s]
[2025-04-29 15:36:06,342][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6947
[2025-04-29 15:36:06,721][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  5.95it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.69it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.18it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.52it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.28it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.75it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.08it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.30it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.41it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.67it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.75it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.03it/s]
[2025-04-29 15:36:10,805][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6947
[2025-04-29 15:36:11,233][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  6.08it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.83it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.29it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.58it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.34it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.77it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.11it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.69it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:36:14,761][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6931
[2025-04-29 15:36:15,173][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.20it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.93it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.36it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.63it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.37it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.78it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.08it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.29it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.69it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.99it/s]
[2025-04-29 15:36:18,707][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 15:36:19,112][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:36:19,113][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▆▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69334
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.694
wandb:           train_time 16.98586
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69353
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153552-v12ijmv3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153552-v12ijmv3/logs
Standard experiment completed successfully: layer_3_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_3/question_type/results.json
Running complexity experiment for language id, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:36:35,203][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_3/complexity
experiment_name: layer_3_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:36:35,204][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:36:35,204][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:36:35,204][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:36:35,208][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:36:35,209][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:36:36,239][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:36:38,991][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:36:38,991][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:36:39,008][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,028][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,081][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:36:39,090][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:36:39,091][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:36:39,092][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:36:39,104][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,133][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:36:39,135][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:36:39,135][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:36:39,136][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:36:39,147][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,166][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:36:39,175][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:36:39,177][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:36:39,177][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:36:39,178][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:36:39,178][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:36:39,178][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:36:39,178][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:36:39,179][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:36:39,179][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:36:39,179][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:36:39,179][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:36:39,179][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:36:39,179][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:36:39,180][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:36:39,180][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:36:39,180][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:36:39,181][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:36:39,181][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:36:39,181][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:36:39,182][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:36:39,182][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:36:42,788][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:36:42,789][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:36:42,791][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:36:42,791][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 15:36:42,792][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:52,  1.11it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:15,  3.64it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.15it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.52it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.60it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.35it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.75it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.82it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.65it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.25it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.69it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.00it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.21it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.37it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.49it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.58it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.72it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.72it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.74it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.73it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.19it/s]
[2025-04-29 15:36:48,685][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1659
[2025-04-29 15:36:49,063][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2050, Metrics: {'mse': 0.20050568878650665, 'rmse': 0.4477786158209285, 'r2': -3.795736312866211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.49it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 12.23it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.59it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.80it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.50it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.94it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.20it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.38it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.50it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.58it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.65it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.70it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.19it/s]
[2025-04-29 15:36:53,123][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0982
[2025-04-29 15:36:53,510][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1355, Metrics: {'mse': 0.13212449848651886, 'rmse': 0.3634893375142094, 'r2': -2.1601808071136475}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.42it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 11.16it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 13.80it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.25it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.11it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.03it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.27it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.81it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.91it/s]
[2025-04-29 15:36:58,082][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0676
[2025-04-29 15:36:58,516][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0960, Metrics: {'mse': 0.0934985876083374, 'rmse': 0.3057753875123657, 'r2': -1.2363183498382568}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.40it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.12it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.78it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.23it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.07it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.32it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.45it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.50it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.84it/s]
[2025-04-29 15:37:02,634][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0546
[2025-04-29 15:37:03,054][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0758, Metrics: {'mse': 0.07383538782596588, 'rmse': 0.2717266785318767, 'r2': -0.7660099267959595}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.27it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 10.95it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:04, 13.64it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.13it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.02it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.60it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.23it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.70it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.88it/s]
[2025-04-29 15:37:07,194][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0511
[2025-04-29 15:37:07,611][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0662, Metrics: {'mse': 0.06455504149198532, 'rmse': 0.25407684170735695, 'r2': -0.5440406799316406}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:09,  5.92it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:04, 11.66it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 14.17it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.49it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.25it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.72it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 17.04it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:37:11,720][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0470
[2025-04-29 15:37:12,128][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0616, Metrics: {'mse': 0.060208871960639954, 'rmse': 0.24537496196767908, 'r2': -0.44008803367614746}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.48it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.20it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.82it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.24it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.11it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.88it/s]
[2025-04-29 15:37:16,266][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0463
[2025-04-29 15:37:16,688][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0589, Metrics: {'mse': 0.05762399733066559, 'rmse': 0.24004998923279625, 'r2': -0.3782625198364258}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:11,  5.33it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.04it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:04, 13.70it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.18it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.03it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.57it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.45it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.51it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.65it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.69it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.84it/s]
[2025-04-29 15:37:20,828][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0465
[2025-04-29 15:37:21,253][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0572, Metrics: {'mse': 0.05600009858608246, 'rmse': 0.23664339962501058, 'r2': -0.3394218683242798}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.77it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:04, 11.51it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 14.01it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.37it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.18it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.69it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.22it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.38it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.56it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.66it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.66it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.68it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.68it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.68it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.69it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.91it/s]
[2025-04-29 15:37:25,443][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0493
[2025-04-29 15:37:25,846][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0578, Metrics: {'mse': 0.05657726526260376, 'rmse': 0.23785975965388462, 'r2': -0.3532266616821289}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.48it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.21it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 13.83it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.23it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.08it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.58it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.69it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:37:29,406][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0489
[2025-04-29 15:37:29,824][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0569, Metrics: {'mse': 0.05569617077708244, 'rmse': 0.2360003618155753, 'r2': -0.3321523666381836}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁▁▁▁
wandb:     best_val_mse █▅▃▂▁▁▁▁▁
wandb:      best_val_r2 ▁▄▆▇█████
wandb:    best_val_rmse █▅▃▂▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁▁
wandb:          val_mse █▅▃▂▁▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇██████
wandb:         val_rmse █▅▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05688
wandb:     best_val_mse 0.0557
wandb:      best_val_r2 -0.33215
wandb:    best_val_rmse 0.236
wandb:            epoch 10
wandb:   final_test_mse 0.04065
wandb:    final_test_r2 0.00304
wandb:  final_test_rmse 0.20162
wandb:  final_train_mse 0.03621
wandb:   final_train_r2 0.00218
wandb: final_train_rmse 0.19028
wandb:    final_val_mse 0.0557
wandb:     final_val_r2 -0.33215
wandb:   final_val_rmse 0.236
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04888
wandb:       train_time 45.9322
wandb:         val_loss 0.05688
wandb:          val_mse 0.0557
wandb:           val_r2 -0.33215
wandb:         val_rmse 0.236
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153635-huhhlbtc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153635-huhhlbtc/logs
Standard experiment completed successfully: layer_3_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_3/complexity/results.json
Running question_type experiment for language id, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:37:48,673][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_4/question_type
experiment_name: layer_4_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:37:48,674][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:37:48,674][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:37:48,674][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:37:48,678][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:37:48,678][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:37:49,895][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:37:52,592][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:37:52,592][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:37:52,630][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,649][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,721][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:37:52,730][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:37:52,730][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:37:52,731][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:37:52,745][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,765][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,775][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:37:52,776][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:37:52,776][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:37:52,777][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:37:52,790][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:37:52,824][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:37:52,825][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:37:52,825][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:37:52,826][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:37:52,827][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:37:52,827][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:37:52,827][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:37:52,828][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:37:52,828][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:37:52,828][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:37:52,829][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:37:52,829][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:37:52,829][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:37:52,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:37:52,830][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:37:52,830][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:37:56,681][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:37:56,681][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:37:56,684][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:37:56,684][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 15:37:56,684][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:48,  1.22it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.93it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.54it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  8.94it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 11.01it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.71it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 14.05it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.04it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.80it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.37it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.77it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.07it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.29it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.44it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.56it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.63it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.63it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.71it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.74it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.74it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.35it/s]
[2025-04-29 15:38:02,642][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6952
[2025-04-29 15:38:03,023][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:10,  5.87it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.65it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.13it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.47it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.28it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.78it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.09it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.57it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.60it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:38:07,108][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6951
[2025-04-29 15:38:07,511][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  6.23it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.98it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.39it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.65it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.39it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.83it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.30it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.67it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.07it/s]
[2025-04-29 15:38:11,028][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6932
[2025-04-29 15:38:11,417][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.22it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.96it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.38it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.63it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.37it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.80it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.29it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.43it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.74it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.04it/s]
[2025-04-29 15:38:14,941][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6945
[2025-04-29 15:38:15,333][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:38:15,333][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▇▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69338
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69453
wandb:           train_time 16.87457
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153748-9i5bnvy9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153748-9i5bnvy9/logs
Standard experiment completed successfully: layer_4_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_4/question_type/results.json
Running complexity experiment for language id, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:38:31,820][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_4/complexity
experiment_name: layer_4_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:38:31,820][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:38:31,820][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:38:31,820][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:38:31,825][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:38:31,825][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:38:32,841][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:38:35,550][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:38:35,550][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:38:35,570][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,664][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:38:35,673][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:38:35,674][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:38:35,675][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:38:35,687][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,707][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,716][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:38:35,718][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:38:35,718][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:38:35,719][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:38:35,731][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,749][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:38:35,758][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:38:35,760][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:38:35,760][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:38:35,761][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:38:35,761][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:38:35,761][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:38:35,761][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:38:35,762][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:38:35,762][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:38:35,762][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:38:35,762][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:38:35,762][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:38:35,762][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:38:35,763][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:38:35,763][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:38:35,763][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:38:35,764][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:38:35,764][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:38:35,764][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:38:35,765][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:38:35,765][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:38:39,259][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:38:39,260][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:38:39,262][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:38:39,262][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 15:38:39,263][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:49,  1.19it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.84it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.43it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.81it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.88it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.60it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.95it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.99it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.77it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.34it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.76it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.08it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.28it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.43it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.55it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.61it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.75it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.33it/s]
[2025-04-29 15:38:45,013][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1642
[2025-04-29 15:38:45,386][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1974, Metrics: {'mse': 0.19295644760131836, 'rmse': 0.43926808170104775, 'r2': -3.6151719093322754}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:10,  5.68it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.45it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.00it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.40it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.20it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.70it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.07it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.27it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.44it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.56it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.64it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.70it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.80it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.82it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.82it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.04it/s]
[2025-04-29 15:38:49,469][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0947
[2025-04-29 15:38:49,858][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1270, Metrics: {'mse': 0.1237667128443718, 'rmse': 0.35180493578739314, 'r2': -1.9602775573730469}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.45it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 11.19it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 13.82it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.25it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.12it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.03it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.61it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.79it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:38:54,417][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0654
[2025-04-29 15:38:54,860][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0893, Metrics: {'mse': 0.08692911267280579, 'rmse': 0.29483743431390425, 'r2': -1.079188346862793}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.63it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.36it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.95it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.35it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.18it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.69it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.23it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:38:58,975][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0552
[2025-04-29 15:38:59,397][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0716, Metrics: {'mse': 0.06982091069221497, 'rmse': 0.26423646737764067, 'r2': -0.6699907779693604}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:09,  5.90it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:04, 11.63it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:03, 14.13it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.47it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.26it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.75it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 17.06it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.29it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.43it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.70it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.70it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.96it/s]
[2025-04-29 15:39:03,576][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0537
[2025-04-29 15:39:03,987][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0642, Metrics: {'mse': 0.0626622810959816, 'rmse': 0.25032435178380386, 'r2': -0.49876928329467773}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:09,  6.02it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:04, 11.77it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 14.24it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.53it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.31it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.76it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 17.08it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.67it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.75it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.99it/s]
[2025-04-29 15:39:08,071][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0497
[2025-04-29 15:39:08,503][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0609, Metrics: {'mse': 0.05948011204600334, 'rmse': 0.24388544861472022, 'r2': -0.42265748977661133}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.39it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.11it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.76it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.22it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.09it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.65it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.68it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.68it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.68it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.68it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:39:12,643][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0493
[2025-04-29 15:39:13,070][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0586, Metrics: {'mse': 0.057350873947143555, 'rmse': 0.2394804249769562, 'r2': -0.3717299699783325}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.49it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.23it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.85it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.28it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.13it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.67it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 17.03it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.77it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.79it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.75it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]
[2025-04-29 15:39:17,173][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0489
[2025-04-29 15:39:17,607][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0568, Metrics: {'mse': 0.055632591247558594, 'rmse': 0.23586562116501547, 'r2': -0.33063173294067383}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.64it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.37it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 13.95it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.34it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.15it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 17.00it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.45it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.67it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.68it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.93it/s]
[2025-04-29 15:39:21,747][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0520
[2025-04-29 15:39:22,183][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0577, Metrics: {'mse': 0.056463293731212616, 'rmse': 0.23762006171872907, 'r2': -0.35050058364868164}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.64it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.35it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 13.91it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.31it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.13it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.65it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.69it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.70it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.90it/s]
[2025-04-29 15:39:25,736][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0515
[2025-04-29 15:39:26,148][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0563, Metrics: {'mse': 0.05516624450683594, 'rmse': 0.23487495504381886, 'r2': -0.31947755813598633}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁▁▁▁▁
wandb:     best_val_mse █▄▃▂▁▁▁▁▁
wandb:      best_val_r2 ▁▅▆▇█████
wandb:    best_val_rmse █▅▃▂▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▁▁▁▁▁
wandb:          val_mse █▄▃▂▁▁▁▁▁▁
wandb:           val_r2 ▁▅▆▇██████
wandb:         val_rmse █▅▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05631
wandb:     best_val_mse 0.05517
wandb:      best_val_r2 -0.31948
wandb:    best_val_rmse 0.23487
wandb:            epoch 10
wandb:   final_test_mse 0.04064
wandb:    final_test_r2 0.00335
wandb:  final_test_rmse 0.20159
wandb:  final_train_mse 0.03613
wandb:   final_train_r2 0.00419
wandb: final_train_rmse 0.19009
wandb:    final_val_mse 0.05517
wandb:     final_val_r2 -0.31948
wandb:   final_val_rmse 0.23487
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05147
wandb:       train_time 45.89048
wandb:         val_loss 0.05631
wandb:          val_mse 0.05517
wandb:           val_r2 -0.31948
wandb:         val_rmse 0.23487
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153831-87kyjnkm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153831-87kyjnkm/logs
Standard experiment completed successfully: layer_4_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_4/complexity/results.json
Running question_type experiment for language id, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:39:45,663][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_5/question_type
experiment_name: layer_5_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:39:45,663][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:39:45,663][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:39:45,663][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:39:45,667][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:39:45,668][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:39:46,968][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:39:49,681][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:39:49,681][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:39:49,735][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,755][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,828][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:39:49,837][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:39:49,838][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:39:49,839][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:39:49,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,877][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,887][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:39:49,889][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:39:49,889][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:39:49,890][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:39:49,907][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,926][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:39:49,936][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:39:49,937][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:39:49,938][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:39:49,938][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:39:49,939][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:39:49,939][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:39:49,939][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:39:49,939][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:39:49,939][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:39:49,940][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:39:49,940][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:39:49,940][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:39:49,941][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:39:49,941][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:39:49,941][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:39:49,941][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:39:49,942][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:39:49,942][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:39:49,942][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:39:49,942][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:39:49,942][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:39:49,942][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:39:53,825][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:39:53,826][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:39:53,828][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:39:53,828][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 15:39:53,828][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:52,  1.13it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:15,  3.67it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.20it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.57it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.64it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.38it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.77it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.84it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.64it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.26it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.70it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.03it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.27it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.43it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.62it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.76it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.77it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.13it/s]
[2025-04-29 15:39:59,767][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6947
[2025-04-29 15:40:00,145][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.14it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.89it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.34it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.62it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.38it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.83it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.48it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.58it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.65it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.70it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.71it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.70it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.73it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.04it/s]
[2025-04-29 15:40:04,225][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6947
[2025-04-29 15:40:04,628][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  5.99it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.76it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.22it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.51it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.30it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.80it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.12it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.44it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.56it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.46it/s]
[2025-04-29 15:40:08,276][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6929
[2025-04-29 15:40:08,673][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.60it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.32it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.92it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.29it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.12it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.02it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.96it/s]
[2025-04-29 15:40:12,212][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 15:40:12,603][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:40:12,603][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6933
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69398
wandb:           train_time 17.0853
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69356
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153945-t21qo6oq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_153945-t21qo6oq/logs
Standard experiment completed successfully: layer_5_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_5/question_type/results.json
Running complexity experiment for language id, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:40:28,936][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_5/complexity
experiment_name: layer_5_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:40:28,936][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:40:28,936][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:40:28,936][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:40:28,941][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:40:28,941][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:40:29,963][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:40:32,675][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:40:32,676][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:40:32,692][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,712][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,768][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:40:32,777][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:40:32,777][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:40:32,778][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:40:32,790][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,817][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:40:32,819][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:40:32,819][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:40:32,819][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:40:32,831][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,849][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:40:32,857][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:40:32,859][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:40:32,859][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:40:32,860][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:40:32,860][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:40:32,860][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:40:32,860][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:40:32,860][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:40:32,861][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:40:32,861][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:40:32,861][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:40:32,861][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:40:32,861][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:40:32,861][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:40:32,861][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:40:32,862][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:40:32,862][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:40:32,862][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:40:32,863][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:40:32,863][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:40:32,863][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:40:32,863][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:40:32,863][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:40:32,863][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:40:32,863][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:40:32,864][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:40:32,864][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:40:36,443][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:40:36,444][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:40:36,446][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:40:36,446][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 15:40:36,446][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:52,  1.13it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:15,  3.68it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.22it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.58it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.67it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.40it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.81it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.89it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.72it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.31it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.73it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.02it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.25it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.42it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.62it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.78it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.83it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.84it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.24it/s]
[2025-04-29 15:40:42,253][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1545
[2025-04-29 15:40:42,631][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1880, Metrics: {'mse': 0.18367083370685577, 'rmse': 0.428568353599348, 'r2': -3.3930768966674805}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:08,  6.67it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 12.42it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.71it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.87it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.55it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.97it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.22it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.41it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.54it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.62it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.68it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.73it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.80it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.80it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.14it/s]
[2025-04-29 15:40:46,692][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0878
[2025-04-29 15:40:47,084][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1194, Metrics: {'mse': 0.11631842702627182, 'rmse': 0.34105487392246986, 'r2': -1.782127857208252}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.56it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 11.32it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 13.90it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.32it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.18it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.69it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.05it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.29it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.54it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.67it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.73it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.71it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:40:51,646][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0602
[2025-04-29 15:40:52,057][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0844, Metrics: {'mse': 0.08217263221740723, 'rmse': 0.286657691711573, 'r2': -0.9654217958450317}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.10it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.84it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.30it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.58it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.36it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.81it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.35it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.54it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.61it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.67it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.77it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.03it/s]
[2025-04-29 15:40:56,139][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0514
[2025-04-29 15:40:56,553][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0689, Metrics: {'mse': 0.06718344241380692, 'rmse': 0.2591976898311536, 'r2': -0.6069072484970093}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:10,  5.60it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 11.35it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:03, 13.92it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.32it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.17it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.66it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 17.02it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.41it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.73it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.73it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.74it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.78it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.96it/s]
[2025-04-29 15:41:00,698][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0500
[2025-04-29 15:41:01,123][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0625, Metrics: {'mse': 0.06098233163356781, 'rmse': 0.24694600955182047, 'r2': -0.4585878849029541}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:09,  5.99it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:04, 11.75it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 14.24it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.51it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.28it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.72it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 17.05it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.27it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.91it/s]
[2025-04-29 15:41:05,244][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0467
[2025-04-29 15:41:05,655][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0597, Metrics: {'mse': 0.05832111835479736, 'rmse': 0.24149765703790455, 'r2': -0.3949364423751831}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.55it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.30it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.88it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.30it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.13it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.76it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.78it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:41:09,794][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0465
[2025-04-29 15:41:10,206][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0579, Metrics: {'mse': 0.05657397210597992, 'rmse': 0.23785283707784508, 'r2': -0.35314786434173584}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.48it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.20it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.81it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.25it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.63it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.99it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.22it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.69it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:41:14,328][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0464
[2025-04-29 15:41:14,749][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0568, Metrics: {'mse': 0.05556022375822067, 'rmse': 0.23571216294077968, 'r2': -0.3289008140563965}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.40it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.12it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:04, 13.75it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.18it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.04it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.59it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.45it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:41:18,895][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0491
[2025-04-29 15:41:19,322][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0579, Metrics: {'mse': 0.05657549574971199, 'rmse': 0.23785603996895263, 'r2': -0.3531843423843384}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:09,  6.07it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:04, 11.81it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 14.26it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.54it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.30it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.75it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 17.07it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.65it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.62it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.61it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.62it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.59it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.59it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.60it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.61it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.60it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.59it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.60it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.62it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.63it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.65it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:41:22,876][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0487
[2025-04-29 15:41:23,294][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0570, Metrics: {'mse': 0.05575473979115486, 'rmse': 0.23612441591490463, 'r2': -0.3335533142089844}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▁▁▁▁
wandb:     best_val_mse █▄▂▂▁▁▁▁
wandb:      best_val_r2 ▁▅▇▇████
wandb:    best_val_rmse █▅▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▂▁▁▁▁▁▁
wandb:          val_mse █▄▂▂▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇▇██████
wandb:         val_rmse █▅▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05679
wandb:     best_val_mse 0.05556
wandb:      best_val_r2 -0.3289
wandb:    best_val_rmse 0.23571
wandb:            epoch 10
wandb:   final_test_mse 0.04145
wandb:    final_test_r2 -0.01652
wandb:  final_test_rmse 0.20359
wandb:  final_train_mse 0.03642
wandb:   final_train_r2 -0.00372
wandb: final_train_rmse 0.19084
wandb:    final_val_mse 0.05556
wandb:     final_val_r2 -0.3289
wandb:   final_val_rmse 0.23571
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04874
wandb:       train_time 45.25832
wandb:         val_loss 0.05698
wandb:          val_mse 0.05575
wandb:           val_r2 -0.33355
wandb:         val_rmse 0.23612
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154028-kxx75wob
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154028-kxx75wob/logs
Standard experiment completed successfully: layer_5_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_5/complexity/results.json
Running question_type experiment for language id, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:41:41,135][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_6/question_type
experiment_name: layer_6_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:41:41,136][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:41:41,136][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:41:41,136][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:41:41,140][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:41:41,140][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:41:42,387][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:41:45,179][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:41:45,180][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:41:45,217][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,239][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,329][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:41:45,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:41:45,339][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:41:45,340][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:41:45,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,373][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,382][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:41:45,384][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:41:45,384][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:41:45,385][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:41:45,398][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,420][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:41:45,430][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:41:45,431][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:41:45,431][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:41:45,432][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:41:45,433][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:41:45,433][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:41:45,433][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:41:45,433][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:41:45,433][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:41:45,433][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:41:45,434][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:41:45,434][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:41:45,434][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:41:45,435][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:41:45,435][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:41:45,435][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:41:45,436][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:41:45,436][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:41:45,436][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:41:45,436][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:41:45,436][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:41:49,239][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:41:49,240][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:41:49,242][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:41:49,242][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 15:41:49,242][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:47,  1.25it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.98it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.62it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  9.01it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 11.07it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.78it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 14.10it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.11it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.86it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.40it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.79it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.10it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.31it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.44it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.56it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.63it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.78it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.45it/s]
[2025-04-29 15:41:55,774][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6944
[2025-04-29 15:41:56,152][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.00it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.76it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.24it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.56it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.31it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.79it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.46it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.59it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.64it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.68it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:42:00,241][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6946
[2025-04-29 15:42:00,631][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  5.93it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.68it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.18it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.52it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.28it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.77it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.44it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.00it/s]
[2025-04-29 15:42:04,164][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6929
[2025-04-29 15:42:04,545][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.29it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 12.02it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.42it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.68it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.40it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.86it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.35it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.49it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.63it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.69it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.73it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.06it/s]
[2025-04-29 15:42:08,065][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 15:42:08,463][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:42:08,464][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▇█▁▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69338
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69397
wandb:           train_time 16.84418
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69364
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154141-u0v6p8kk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154141-u0v6p8kk/logs
Standard experiment completed successfully: layer_6_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_6/question_type/results.json
Running complexity experiment for language id, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:42:24,538][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_6/complexity
experiment_name: layer_6_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:42:24,538][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:42:24,538][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:42:24,539][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:42:24,543][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:42:24,543][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:42:25,471][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:42:28,257][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:42:28,258][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:42:28,273][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,293][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,349][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:42:28,360][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:42:28,360][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:42:28,361][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:42:28,375][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,395][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,405][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:42:28,406][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:42:28,406][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:42:28,407][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:42:28,422][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,442][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:42:28,451][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:42:28,453][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:42:28,453][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:42:28,454][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:42:28,454][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:42:28,454][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:42:28,455][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:42:28,455][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:42:28,455][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:42:28,455][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:42:28,455][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:42:28,455][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:42:28,456][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:42:28,456][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:42:28,456][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:42:28,457][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:42:28,457][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:42:28,457][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:42:28,457][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:42:28,457][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:42:28,457][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:42:28,457][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:42:28,458][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:42:28,458][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:42:28,458][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:42:28,458][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:42:28,458][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:42:31,985][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:42:31,985][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:42:31,988][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:42:31,988][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 15:42:31,988][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:50,  1.17it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:15,  3.79it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.37it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.75it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.82it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.54it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.91it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.96it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.74it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.32it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.76it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.06it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.28it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.44it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.55it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.64it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.67it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.76it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.29it/s]
[2025-04-29 15:42:37,682][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1704
[2025-04-29 15:42:38,078][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2181, Metrics: {'mse': 0.2134866714477539, 'rmse': 0.4620461789126211, 'r2': -4.106217861175537}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  5.91it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.68it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.21it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.53it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.34it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.77it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.08it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.60it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.66it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.71it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.81it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.81it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.82it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.82it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.08it/s]
[2025-04-29 15:42:42,165][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1068
[2025-04-29 15:42:42,555][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1502, Metrics: {'mse': 0.14656704664230347, 'rmse': 0.38284075885713037, 'r2': -2.5056207180023193}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.12it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 10.79it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:04, 13.56it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.07it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 15.99it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.58it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.86it/s]
[2025-04-29 15:42:47,144][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0731
[2025-04-29 15:42:47,571][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1072, Metrics: {'mse': 0.10449867695569992, 'rmse': 0.3232625511185914, 'r2': -1.4994206428527832}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.42it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.13it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.78it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.23it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.77it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:42:51,686][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0572
[2025-04-29 15:42:52,118][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0833, Metrics: {'mse': 0.08114001154899597, 'rmse': 0.2848508584312078, 'r2': -0.9407232999801636}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:09,  5.93it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:04, 11.66it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:03, 14.16it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.48it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.24it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.74it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 17.03it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.67it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.68it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.69it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.68it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.69it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]
[2025-04-29 15:42:56,245][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0509
[2025-04-29 15:42:56,641][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0706, Metrics: {'mse': 0.06887180358171463, 'rmse': 0.26243437957271265, 'r2': -0.6472898721694946}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:10,  5.45it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 11.16it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 13.79it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.21it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.09it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 16.96it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.67it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.90it/s]
[2025-04-29 15:43:00,758][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0459
[2025-04-29 15:43:01,172][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0640, Metrics: {'mse': 0.0625399723649025, 'rmse': 0.25007993195157124, 'r2': -0.49584388732910156}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.47it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.19it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.81it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.20it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.05it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.33it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.51it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.62it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.64it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.66it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.67it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.96it/s]
[2025-04-29 15:43:05,295][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0456
[2025-04-29 15:43:05,706][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0603, Metrics: {'mse': 0.05896568298339844, 'rmse': 0.24282850529416525, 'r2': -0.41035330295562744}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.58it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.31it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.90it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.27it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.11it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.53it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.63it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.65it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.67it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.68it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.69it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.68it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:43:09,811][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0451
[2025-04-29 15:43:10,236][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0581, Metrics: {'mse': 0.05682263895869255, 'rmse': 0.23837499650486113, 'r2': -0.3590954542160034}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:11,  5.29it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.00it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:04, 13.67it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.15it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.00it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.56it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.93it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.31it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.43it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.68it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.68it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.67it/s]
[2025-04-29 15:43:14,419][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0477
[2025-04-29 15:43:14,850][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0582, Metrics: {'mse': 0.056933626532554626, 'rmse': 0.23860768330578674, 'r2': -0.36175012588500977}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.55it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.28it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 13.86it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.26it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.60it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.44it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.60it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.63it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.65it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:43:18,406][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0473
[2025-04-29 15:43:18,841][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0576, Metrics: {'mse': 0.05635400488972664, 'rmse': 0.23738998481344287, 'r2': -0.34788668155670166}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▂▁▁▁▁
wandb:     best_val_mse █▅▃▂▂▁▁▁▁
wandb:      best_val_r2 ▁▄▆▇▇████
wandb:    best_val_rmse █▆▄▂▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▂▁▁▁▁▁
wandb:          val_mse █▅▃▂▂▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇▇█████
wandb:         val_rmse █▆▄▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05757
wandb:     best_val_mse 0.05635
wandb:      best_val_r2 -0.34789
wandb:    best_val_rmse 0.23739
wandb:            epoch 10
wandb:   final_test_mse 0.0409
wandb:    final_test_r2 -0.00303
wandb:  final_test_rmse 0.20223
wandb:  final_train_mse 0.03631
wandb:   final_train_r2 -0.00072
wandb: final_train_rmse 0.19056
wandb:    final_val_mse 0.05635
wandb:     final_val_r2 -0.34789
wandb:   final_val_rmse 0.23739
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04728
wandb:       train_time 45.93144
wandb:         val_loss 0.05757
wandb:          val_mse 0.05635
wandb:           val_r2 -0.34789
wandb:         val_rmse 0.23739
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154224-3jc26zqq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154224-3jc26zqq/logs
Standard experiment completed successfully: layer_6_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_6/complexity/results.json
Running question_type experiment for language id, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:43:37,226][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_7/question_type
experiment_name: layer_7_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:43:37,226][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:43:37,227][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:43:37,227][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:43:37,231][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:43:37,231][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:43:38,424][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:43:41,124][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:43:41,124][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:43:41,160][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,182][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,255][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:43:41,264][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:43:41,264][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:43:41,265][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:43:41,282][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,304][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,315][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:43:41,316][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:43:41,316][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:43:41,317][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:43:41,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,352][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:43:41,362][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:43:41,363][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:43:41,364][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:43:41,365][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:43:41,365][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:43:41,365][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:43:41,365][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:43:41,365][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:43:41,366][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:43:41,366][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:43:41,366][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:43:41,367][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:43:41,367][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:43:41,367][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:43:41,367][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:43:41,368][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:43:41,368][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:43:41,368][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:43:41,368][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:43:41,368][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:43:41,368][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:43:41,368][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:43:45,226][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:43:45,227][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:43:45,229][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:43:45,230][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 15:43:45,230][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:51,  1.14it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:15,  3.72it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.27it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.64it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.72it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.44it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.82it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.89it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.71it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.31it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.75it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.06it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.24it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.40it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.52it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.61it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.76it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.82it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.23it/s]
[2025-04-29 15:43:51,170][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6926
[2025-04-29 15:43:51,558][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.09it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.86it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.30it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.58it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.34it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.81it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.13it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.34it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.46it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.60it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.06it/s]
[2025-04-29 15:43:55,630][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6925
[2025-04-29 15:43:56,029][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  6.45it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 12.17it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.54it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.75it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.46it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.89it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.16it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.33it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.61it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.65it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.09it/s]
[2025-04-29 15:43:59,543][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6924
[2025-04-29 15:43:59,929][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.17it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.92it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.34it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.62it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.36it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.82it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.43it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.52it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.62it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.68it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.99it/s]
[2025-04-29 15:44:03,464][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6927
[2025-04-29 15:44:03,858][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:44:03,859][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▆▃▁█
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▇█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69337
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69265
wandb:           train_time 16.90832
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69344
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154337-yh3znbs0
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154337-yh3znbs0/logs
Standard experiment completed successfully: layer_7_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_7/question_type/results.json
Running complexity experiment for language id, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:44:19,894][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_7/complexity
experiment_name: layer_7_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:44:19,894][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:44:19,894][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:44:19,894][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:44:19,899][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:44:19,899][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:44:20,871][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:44:23,609][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:44:23,610][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:44:23,626][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,698][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:44:23,707][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:44:23,708][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:44:23,709][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:44:23,721][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,739][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,748][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:44:23,749][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:44:23,749][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:44:23,750][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:44:23,762][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,782][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:44:23,791][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:44:23,792][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:44:23,792][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:44:23,793][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:44:23,794][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:44:23,794][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:44:23,794][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:44:23,794][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:44:23,794][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:44:23,795][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:44:23,795][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:44:23,795][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:44:23,796][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:44:23,796][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:44:23,796][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:44:23,797][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:44:23,797][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:44:23,797][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:44:23,797][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:44:23,797][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:44:23,797][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:44:23,798][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:44:27,257][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:44:27,258][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:44:27,260][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:44:27,260][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 15:44:27,260][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:45,  1.29it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:13,  4.10it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.78it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  9.19it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 11.25it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.91it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 14.23it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.20it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.92it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.45it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.84it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.14it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.35it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.49it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.81it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.83it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.84it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.85it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.85it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.58it/s]
[2025-04-29 15:44:32,951][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1959
[2025-04-29 15:44:33,326][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2937, Metrics: {'mse': 0.2881101369857788, 'rmse': 0.5367589188693364, 'r2': -5.891077518463135}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:10,  5.72it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.46it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.04it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.42it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.26it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.76it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.30it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.47it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.58it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.66it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.71it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.82it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.82it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.83it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.82it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.82it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.83it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.05it/s]
[2025-04-29 15:44:37,416][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1672
[2025-04-29 15:44:37,807][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2582, Metrics: {'mse': 0.25307580828666687, 'rmse': 0.5030664054443179, 'r2': -5.05311918258667}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.18it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 10.89it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:04, 13.59it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.09it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.01it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.60it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.23it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.57it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.73it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.79it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:44:42,400][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1439
[2025-04-29 15:44:42,841][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2260, Metrics: {'mse': 0.2212185114622116, 'rmse': 0.47033871992662013, 'r2': -4.291149616241455}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.08it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.84it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.31it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.58it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.36it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.84it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.11it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.44it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:44:46,919][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1212
[2025-04-29 15:44:47,334][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1966, Metrics: {'mse': 0.19230926036834717, 'rmse': 0.43853079751409385, 'r2': -3.5996923446655273}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:10,  5.51it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 11.23it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:03, 13.83it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.27it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.63it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 17.00it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.49it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.77it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.75it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.78it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]
[2025-04-29 15:44:51,500][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1023
[2025-04-29 15:44:51,927][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1710, Metrics: {'mse': 0.16707204282283783, 'rmse': 0.4087444713055307, 'r2': -2.996063232421875}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:10,  5.50it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 11.23it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 13.86it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.30it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.15it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.67it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.41it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.60it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.67it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.73it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.75it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.78it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:44:56,045][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0871
[2025-04-29 15:44:56,457][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1488, Metrics: {'mse': 0.14528948068618774, 'rmse': 0.381168572532138, 'r2': -2.4750638008117676}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.64it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.38it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.96it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.36it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.20it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.73it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 17.06it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.29it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.44it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.63it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.68it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.74it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:45:00,560][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0757
[2025-04-29 15:45:00,971][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1292, Metrics: {'mse': 0.1260141283273697, 'rmse': 0.3549846874547826, 'r2': -2.0140316486358643}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.64it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.39it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.97it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.35it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.17it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.67it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 17.01it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.68it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.65it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.63it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.61it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.61it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.61it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.63it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.64it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:45:05,081][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0657
[2025-04-29 15:45:05,489][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1134, Metrics: {'mse': 0.110558420419693, 'rmse': 0.33250326377299366, 'r2': -1.6443588733673096}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.83it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:04, 11.57it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 14.08it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.42it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.20it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.70it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 17.03it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.25it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.40it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.50it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.70it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.92it/s]
[2025-04-29 15:45:09,635][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0576
[2025-04-29 15:45:10,065][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1004, Metrics: {'mse': 0.09778271615505219, 'rmse': 0.3127022803803199, 'r2': -1.3387870788574219}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.45it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.18it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:03, 13.82it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.24it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.09it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.63it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.96it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.88it/s]
[2025-04-29 15:45:14,183][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0494
[2025-04-29 15:45:14,619][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0904, Metrics: {'mse': 0.08807425945997238, 'rmse': 0.2967730773840046, 'r2': -1.1065781116485596}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▄▃▂▂▁▁
wandb:     best_val_mse █▇▆▅▄▃▂▂▁▁
wandb:      best_val_r2 ▁▂▃▄▅▆▇▇██
wandb:    best_val_rmse █▇▆▅▄▃▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▄▄▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▅▄▃▂▂▁▁
wandb:          val_mse █▇▆▅▄▃▂▂▁▁
wandb:           val_r2 ▁▂▃▄▅▆▇▇██
wandb:         val_rmse █▇▆▅▄▃▃▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.09039
wandb:     best_val_mse 0.08807
wandb:      best_val_r2 -1.10658
wandb:    best_val_rmse 0.29677
wandb:            epoch 10
wandb:   final_test_mse 0.05127
wandb:    final_test_r2 -0.25749
wandb:  final_test_rmse 0.22644
wandb:  final_train_mse 0.046
wandb:   final_train_r2 -0.26775
wandb: final_train_rmse 0.21448
wandb:    final_val_mse 0.08807
wandb:     final_val_r2 -1.10658
wandb:   final_val_rmse 0.29677
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04942
wandb:       train_time 46.36574
wandb:         val_loss 0.09039
wandb:          val_mse 0.08807
wandb:           val_r2 -1.10658
wandb:         val_rmse 0.29677
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154419-t7pqqqme
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154419-t7pqqqme/logs
Standard experiment completed successfully: layer_7_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_7/complexity/results.json
Running question_type experiment for language id, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:45:33,290][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_8/question_type
experiment_name: layer_8_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:45:33,290][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:45:33,290][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:45:33,290][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:45:33,294][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:45:33,295][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:45:34,569][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:45:37,282][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:45:37,282][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:45:37,335][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,354][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,426][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:45:37,435][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:45:37,436][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:45:37,437][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:45:37,450][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,472][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,482][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:45:37,484][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:45:37,484][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:45:37,484][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:45:37,499][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,520][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:45:37,531][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:45:37,532][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:45:37,532][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:45:37,533][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:45:37,533][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:45:37,534][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:45:37,534][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:45:37,534][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:45:37,534][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:45:37,534][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:45:37,534][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:45:37,534][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:45:37,535][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:45:37,535][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:45:37,535][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:45:37,536][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:45:37,536][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:45:37,536][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:45:37,537][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:45:37,537][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:45:41,518][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:45:41,519][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:45:41,521][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:45:41,521][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 15:45:41,521][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:48,  1.23it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.94it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.56it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  8.95it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 11.01it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.71it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 14.04it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.06it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.83it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.39it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.79it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.08it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.29it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.44it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.61it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.77it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.84it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.84it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.42it/s]
[2025-04-29 15:45:47,452][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6928
[2025-04-29 15:45:47,837][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.08it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.86it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.34it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.64it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.39it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.86it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.18it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.37it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.52it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.60it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.66it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.70it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.72it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.79it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.77it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.08it/s]
[2025-04-29 15:45:51,918][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6924
[2025-04-29 15:45:52,311][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:09,  6.09it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:04, 11.86it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 14.32it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.61it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.37it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.83it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.48it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.59it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.65it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.70it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.76it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.78it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.79it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 17.00it/s]
[2025-04-29 15:45:55,844][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6921
[2025-04-29 15:45:56,253][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:09,  6.30it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 12.01it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.41it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.65it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.41it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.86it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.14it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.47it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.55it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.63it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.76it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.76it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 17.08it/s]
[2025-04-29 15:45:59,768][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6924
[2025-04-29 15:46:00,162][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 15:46:00,163][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▄▁▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▆▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69354
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69244
wandb:           train_time 16.8735
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69362
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154533-71t7ru6f
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154533-71t7ru6f/logs
Standard experiment completed successfully: layer_8_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_8/question_type/results.json
Running complexity experiment for language id, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:46:16,245][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_8/complexity
experiment_name: layer_8_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:46:16,245][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:46:16,245][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:46:16,245][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:46:16,250][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:46:16,250][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:46:17,198][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:46:19,910][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:46:19,910][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:46:19,927][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:19,947][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:20,001][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:46:20,011][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:46:20,012][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:46:20,013][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:46:20,026][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:20,047][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:20,058][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:46:20,059][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:46:20,059][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:46:20,060][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:46:20,079][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:20,104][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:46:20,115][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:46:20,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:46:20,117][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:46:20,117][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:46:20,118][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:46:20,118][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:46:20,118][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:46:20,118][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:46:20,118][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:46:20,119][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:46:20,119][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:46:20,119][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:46:20,120][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:46:20,120][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:46:20,120][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:46:20,121][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:46:20,121][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:46:20,121][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:46:20,121][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:46:20,121][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:46:20,121][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:46:20,122][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:46:23,698][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:46:23,699][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:46:23,701][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:46:23,702][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 15:46:23,702][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:46,  1.27it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  4.05it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.72it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  9.12it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 11.18it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.85it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 14.17it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:02, 15.18it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.92it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.46it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.86it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.14it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.34it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.49it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.78it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.81it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.52it/s]
[2025-04-29 15:46:29,377][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1979
[2025-04-29 15:46:29,755][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2872, Metrics: {'mse': 0.28166458010673523, 'rmse': 0.5307208118274007, 'r2': -5.736911296844482}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:10,  5.89it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.64it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.18it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.53it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.31it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.80it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.13it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.32it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.46it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.56it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.64it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.69it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.74it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.77it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.76it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.02it/s]
[2025-04-29 15:46:33,847][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1546
[2025-04-29 15:46:34,259][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2348, Metrics: {'mse': 0.2299644649028778, 'rmse': 0.4795461030004079, 'r2': -4.50033712387085}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.22it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 10.94it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:04, 13.65it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.14it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.03it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.62it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.39it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.60it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.63it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.68it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.78it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.76it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:46:38,833][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1225
[2025-04-29 15:46:39,264][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1922, Metrics: {'mse': 0.1878972053527832, 'rmse': 0.4334711124778481, 'r2': -3.494163990020752}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.69it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:04, 11.43it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 14.00it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.38it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.18it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.69it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.04it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.28it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.41it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.53it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.68it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.73it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.73it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.75it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.73it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.97it/s]
[2025-04-29 15:46:43,359][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0972
[2025-04-29 15:46:43,786][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1574, Metrics: {'mse': 0.1537027359008789, 'rmse': 0.39204940492351076, 'r2': -2.67629337310791}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:11,  5.26it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 10.98it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:04, 13.64it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.12it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.02it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.59it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.57it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.72it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.74it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.83it/s]
[2025-04-29 15:46:47,964][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0763
[2025-04-29 15:46:48,384][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1297, Metrics: {'mse': 0.12649592757225037, 'rmse': 0.35566265979471384, 'r2': -2.0255556106567383}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:10,  5.58it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 11.31it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 13.91it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.33it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.14it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.67it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.64it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.65it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.68it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.89it/s]
[2025-04-29 15:46:52,511][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0630
[2025-04-29 15:46:52,929][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1082, Metrics: {'mse': 0.10547042638063431, 'rmse': 0.32476210736573674, 'r2': -1.5226631164550781}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:10,  5.59it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:05, 11.32it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 13.92it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.28it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.11it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.63it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 16.98it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.61it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.68it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.69it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.70it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.90it/s]
[2025-04-29 15:46:57,072][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0528
[2025-04-29 15:46:57,490][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0917, Metrics: {'mse': 0.0893845483660698, 'rmse': 0.29897248764070217, 'r2': -1.1379179954528809}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:10,  5.58it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.31it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:03, 13.92it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.33it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.13it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.65it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.99it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.22it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.35it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.67it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.69it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.71it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.99it/s]
[2025-04-29 15:47:01,609][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0468
[2025-04-29 15:47:02,022][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0802, Metrics: {'mse': 0.07814237475395203, 'rmse': 0.27953957636433524, 'r2': -0.8690253496170044}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.59it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.33it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 13.91it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.31it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.11it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.55it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.64it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.68it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.71it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.85it/s]
[2025-04-29 15:47:06,169][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0442
[2025-04-29 15:47:06,589][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0724, Metrics: {'mse': 0.07060129195451736, 'rmse': 0.2657090362680904, 'r2': -0.6886560916900635}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:10,  5.40it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 11.11it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:04, 13.74it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.17it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 16.04it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.60it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.95it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.17it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.34it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.68it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.69it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.73it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.74it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.90it/s]
[2025-04-29 15:47:10,737][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0423
[2025-04-29 15:47:11,152][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0670, Metrics: {'mse': 0.06544777750968933, 'rmse': 0.25582763242013035, 'r2': -0.5653934478759766}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▂▂▁▁▁
wandb:     best_val_mse █▆▅▄▃▂▂▁▁▁
wandb:      best_val_r2 ▁▃▄▅▆▇▇███
wandb:    best_val_rmse █▇▆▄▄▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▅▃▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▄▃▂▂▁▁▁
wandb:          val_mse █▆▅▄▃▂▂▁▁▁
wandb:           val_r2 ▁▃▄▅▆▇▇███
wandb:         val_rmse █▇▆▄▄▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06704
wandb:     best_val_mse 0.06545
wandb:      best_val_r2 -0.56539
wandb:    best_val_rmse 0.25583
wandb:            epoch 10
wandb:   final_test_mse 0.04252
wandb:    final_test_r2 -0.04271
wandb:  final_test_rmse 0.20619
wandb:  final_train_mse 0.03764
wandb:   final_train_r2 -0.03729
wandb: final_train_rmse 0.19401
wandb:    final_val_mse 0.06545
wandb:     final_val_r2 -0.56539
wandb:   final_val_rmse 0.25583
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04228
wandb:       train_time 46.49487
wandb:         val_loss 0.06704
wandb:          val_mse 0.06545
wandb:           val_r2 -0.56539
wandb:         val_rmse 0.25583
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154616-yxv81qkw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154616-yxv81qkw/logs
Standard experiment completed successfully: layer_8_complexity_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_8/complexity/results.json
Running question_type experiment for language id, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:47:29,725][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_9/question_type
experiment_name: layer_9_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 15:47:29,725][__main__][INFO] - Normalized task: question_type
[2025-04-29 15:47:29,725][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 15:47:29,725][__main__][INFO] - Determined Task Type: classification
[2025-04-29 15:47:29,729][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-04-29 15:47:29,730][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:47:31,165][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:47:33,875][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:47:33,875][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:47:33,909][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:33,930][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:33,999][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:47:34,008][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:47:34,009][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:47:34,010][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:47:34,027][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:34,050][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:34,062][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:47:34,063][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:47:34,063][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:47:34,064][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:47:34,078][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:34,098][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:47:34,108][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:47:34,110][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:47:34,110][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:47:34,111][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:47:34,111][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:47:34,111][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:47:34,111][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:47:34,112][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-29 15:47:34,112][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:47:34,112][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:47:34,113][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 15:47:34,113][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 15:47:34,113][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 15:47:34,114][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 15:47:34,114][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:47:34,114][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:47:34,115][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:47:38,071][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:47:38,072][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:47:38,074][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 15:47:38,075][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 15:47:38,075][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:48,  1.21it/s]Epoch 1/10:   5%|▌         | 3/60 [00:00<00:14,  3.89it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.50it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:05,  8.88it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.95it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.65it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.98it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.97it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.76it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.32it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:01<00:02, 16.76it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.07it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.27it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.42it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.51it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.59it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.60it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.65it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.68it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.73it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.78it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.73it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.75it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.35it/s]
[2025-04-29 15:47:43,965][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6937
[2025-04-29 15:47:44,353][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6946, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  6.20it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.95it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.38it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.66it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.41it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.85it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.13it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.35it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.47it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.57it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.64it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.69it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.73it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.76it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.77it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.74it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.73it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.75it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.80it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.80it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.81it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.78it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.07it/s]
[2025-04-29 15:47:48,430][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6929
[2025-04-29 15:47:48,830][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6945, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.50it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 11.22it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 13.88it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.29it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.17it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.65it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.00it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.24it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.42it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.54it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.64it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.70it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.72it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.75it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.77it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.77it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.79it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.79it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.79it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.80it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.81it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.80it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.79it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.79it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.80it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.99it/s]
[2025-04-29 15:47:53,116][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-29 15:47:53,532][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6941, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.62it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.34it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.94it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.35it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.18it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.72it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.04it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:00<00:02, 17.28it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:02, 17.41it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:01<00:01, 17.66it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:01<00:01, 17.70it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.72it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.73it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.74it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.75it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 60/60 [00:03<00:00, 16.93it/s]
[2025-04-29 15:47:57,613][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6920
[2025-04-29 15:47:58,022][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:10,  5.43it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:05, 11.09it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:04, 13.75it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:03, 15.20it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:03, 16.06it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:00<00:02, 16.99it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:00<00:02, 17.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:02, 17.36it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:01<00:02, 17.56it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.64it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.67it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.67it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.69it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.68it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.67it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.67it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.69it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.68it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.70it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.72it/s]Epoch 5/10: 100%|██████████| 60/60 [00:03<00:00, 16.81it/s]
[2025-04-29 15:48:02,166][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6918
[2025-04-29 15:48:02,578][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:10,  5.39it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:05, 11.09it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:03, 13.75it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:03, 15.20it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:03, 16.07it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:00<00:02, 16.61it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:00<00:02, 16.97it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:00<00:02, 17.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:02, 17.37it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:02, 17.47it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:01<00:01, 17.61it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:01<00:01, 17.65it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:01<00:01, 17.64it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.66it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.68it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.70it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.71it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.69it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 60/60 [00:03<00:00, 16.80it/s]
[2025-04-29 15:48:06,704][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6916
[2025-04-29 15:48:07,120][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:09,  6.00it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:04, 11.74it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:03, 14.21it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:03, 15.53it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:03, 16.31it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:00<00:02, 16.79it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:00<00:02, 17.10it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:00<00:02, 17.31it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:02, 17.45it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:02, 17.51it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:01<00:02, 17.58it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:01<00:02, 17.62it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:01<00:01, 17.65it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:01<00:01, 17.67it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.70it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.71it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.73it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.70it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.69it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.70it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.71it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.71it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.73it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]
[2025-04-29 15:48:11,239][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6910
[2025-04-29 15:48:11,646][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:11,  5.33it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:05, 11.04it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:04, 13.70it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:03, 15.13it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:03, 16.00it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:00<00:02, 16.57it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:00<00:02, 16.90it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:00<00:02, 17.13it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:02, 17.30it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:02, 17.43it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:01<00:02, 17.51it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:01<00:02, 17.59it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:01<00:01, 17.60it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:01<00:01, 17.63it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.68it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.71it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.70it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.71it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.70it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.71it/s]Epoch 8/10: 100%|██████████| 60/60 [00:03<00:00, 16.80it/s]
[2025-04-29 15:48:15,777][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6897
[2025-04-29 15:48:16,192][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:10,  5.45it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:05, 11.18it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:03, 13.80it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:03, 15.20it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:03, 16.05it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:00<00:02, 16.58it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:00<00:02, 16.94it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:00<00:02, 17.19it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:02, 17.35it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:02, 17.46it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:01<00:02, 17.54it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:01<00:02, 17.60it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:01<00:01, 17.63it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:01<00:01, 17.66it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.67it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.69it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.72it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.70it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.72it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 60/60 [00:03<00:00, 16.85it/s]
[2025-04-29 15:48:20,338][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6922
[2025-04-29 15:48:20,755][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:11,  5.26it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:05, 10.96it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:04, 13.62it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:03, 15.12it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:03, 15.98it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:00<00:02, 16.55it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:00<00:02, 16.92it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:00<00:02, 17.18it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:02, 17.32it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:02, 17.44it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:01<00:02, 17.52it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:01<00:02, 17.58it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:01<00:01, 17.63it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:01<00:01, 17.66it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:01<00:01, 17.67it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.69it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.70it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.70it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.72it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.72it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.73it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.74it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 60/60 [00:03<00:00, 16.87it/s]
[2025-04-29 15:48:24,880][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6881
[2025-04-29 15:48:25,293][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:        best_val_loss █▇▆▅▅▄▃▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▆▆▅▅▃▆▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▇▆▅▅▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69274
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.52096
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68808
wandb:           train_time 46.0936
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69274
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154729-1isy29ub
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_154729-1isy29ub/logs
Standard experiment completed successfully: layer_9_question_type_id
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_9/question_type/results.json
Running complexity experiment for language id, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 15:48:42,719][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/id/layer_9/complexity
experiment_name: layer_9_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 15:48:42,720][__main__][INFO] - Normalized task: complexity
[2025-04-29 15:48:42,720][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 15:48:42,720][__main__][INFO] - Determined Task Type: regression
[2025-04-29 15:48:42,724][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-04-29 15:48:42,725][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 15:48:43,849][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 15:48:46,533][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 15:48:46,534][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:48:46,562][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,651][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-29 15:48:46,660][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:48:46,660][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-29 15:48:46,661][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:48:46,675][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,695][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,704][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-29 15:48:46,705][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:48:46,705][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-29 15:48:46,706][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 15:48:46,720][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,740][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 15:48:46,749][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-29 15:48:46,750][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 15:48:46,751][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-29 15:48:46,751][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-29 15:48:46,752][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:48:46,752][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:48:46,752][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:48:46,752][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:48:46,752][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:48:46,753][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:48:46,753][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:48:46,753][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:48:46,754][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 15:48:46,754][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 15:48:46,754][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 15:48:46,755][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-29 15:48:46,755][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-29 15:48:46,755][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-29 15:48:46,755][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-29 15:48:46,755][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 15:48:46,755][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 15:48:46,755][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 15:48:50,510][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 15:48:50,511][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 15:48:50,513][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 15:48:50,513][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 15:48:50,513][__main__][INFO] - Successfully created model for id
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:00<00:53,  1.10it/s]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:15,  3.61it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:08,  6.12it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:06,  8.48it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:04, 10.56it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:01<00:03, 12.31it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:01<00:03, 13.72it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:01<00:03, 14.83it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:01<00:02, 15.64it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:01<00:02, 16.25it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:02, 16.70it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:02<00:02, 17.02it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:02<00:02, 17.24it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:02<00:01, 17.41it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:02<00:01, 17.52it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:02<00:01, 17.61it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:02<00:01, 17.66it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:03<00:01, 17.79it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:03<00:00, 17.82it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.80it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:04<00:00, 17.81it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 60/60 [00:04<00:00, 14.10it/s]
[2025-04-29 15:48:56,356][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2224
[2025-04-29 15:48:56,741][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2899, Metrics: {'mse': 0.2842061519622803, 'rmse': 0.5331098873236927, 'r2': -5.797700881958008}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:09,  5.95it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:04, 11.70it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:03, 14.21it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:03, 15.55it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:03, 16.34it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:00<00:02, 16.82it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:00<00:02, 17.13it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:00<00:02, 17.35it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:02, 17.47it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:02, 17.59it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:01<00:02, 17.67it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:01<00:02, 17.72it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:01<00:01, 17.75it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:01<00:01, 17.78it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:01<00:01, 17.79it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.80it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.81it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.82it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.83it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.83it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.83it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.83it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.83it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.83it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.83it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:02<00:00, 17.78it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.79it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.80it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 60/60 [00:03<00:00, 17.08it/s]
[2025-04-29 15:49:00,830][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1412
[2025-04-29 15:49:01,228][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1996, Metrics: {'mse': 0.19499528408050537, 'rmse': 0.44158270355676904, 'r2': -3.6639370918273926}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:10,  5.37it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:05, 11.09it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:03, 13.76it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:03, 15.21it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:03, 16.10it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:00<00:02, 16.64it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:00<00:02, 17.02it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:00<00:02, 17.23it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:02, 17.38it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:02, 17.48it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:01<00:02, 17.59it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:01<00:02, 17.66it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:01<00:01, 17.74it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:02<00:01, 17.79it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:02<00:01, 17.80it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:02<00:01, 17.75it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:02<00:00, 17.75it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:02<00:00, 17.77it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:03<00:00, 17.77it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:03<00:00, 17.78it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:03<00:00, 17.80it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:03<00:00, 17.81it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 60/60 [00:03<00:00, 16.94it/s]
[2025-04-29 15:49:05,569][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0923
[2025-04-29 15:49:05,982][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1388, Metrics: {'mse': 0.13518303632736206, 'rmse': 0.36767245793962056, 'r2': -2.2333357334136963}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:10,  5.51it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:05, 11.25it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:03, 13.87it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:03, 15.31it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:03, 16.16it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:00<00:02, 16.70it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:00<00:02, 17.04it/s]slurmstepd: error: *** JOB 58112515 ON r24g37 CANCELLED AT 2025-04-29T15:49:07 ***
